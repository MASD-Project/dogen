#+options: date:nil toc:nil author:nil num:nil
#+title: Product Backlog
#+tags: { reviewing(r) }
#+tags: { story(s) epic(e) }

This document contains the [[http://www.mountaingoatsoftware.com/agile/scrum/product-backlog][product backlog]] for Dogen.

* Product Vision

Here we define what we consider to be the [[http://www.scaledagileframework.com/vision/][the vision]] for Dogen; what
guides us when we think about the product and what can and cannot go
into the product backlog.

** Vision Statement

The vision for Dogen is:

- to create a suite of code generation tools targeting component and
  product generation, according to a well-specified product line;
- for these tools to have the potential to be useful to developers in
  multiple programming languages, and to be designed to be integrated
  with IDEs and text editors;
- to make the suite extensible, such that users can come up with
  modifications suited to their particular domain.

** Vision Quotes

- code that can be reduced to an heuristic must be codegened; code
  that cannot, must be helped.

* Release Checklist

Steps to create a new release.

** Close previous sprint

To be done on the last Sunday of the sprint.

1. Make a copy of current sprint backlog and name it current
   sprint + 1 but don't check it in.
2. Move all untouched stories into product backlog.
3. Close current sprint: close all open tasks, delete tasks we did not
   work on, update clocks.
4. Push commit and wait for builds. This ensures that if there are any
   failures you can fix them before the release tag.
4. Tag commit and sign it with key.
5. Push tag. You can generate new builds overnight.

** Open new sprint

To be done on the Monday morning of the new sprint.

1. Open new sprint, updating CMake version, README, appveyor version
   and decoration formatter tests. Build all and run tests (some will
   fail). This should all be in one commit. *DO NOT PUSH*.
2. Regenerate the LAM models for C# and C++ with the new version, and
   commit the changed files. Push the commit. (Ensure there are no
   pending builds still running or tests will fail due to version
   change).
3. Create a demo. Publish it on youtube.
4. Write up release notes, publish them in github.
5. Download the binaries from bin tray and manually upload them to github.
6. Update bintray with the correct release notes.
7. When tag build is finished, announce on gitter, twitter and linked
   in.

* V2 Release

Release goals:

- finish the dogen API and implement all functionality in terms of
  metamodel elements - e.g. no special cases such as profiles,
  licences etc.
- we also expected to define the LPS (Logical-Physical Space).

** Core work

The most significant stories needed by v2. These are either features
or refactoring that must go in.

*** Clang-cl windows build is broken                                  :story:
    :PROPERTIES:
    :CUSTOM_ID: 8CF1E48A-E643-E534-F05B-639E459C8C1B
    :END:

We are failing to link:

: lld-link: error: undefined symbol: __declspec(dllimport) public: void __cdecl boost::archive::archive_exception::`vbase dtor'(void)

Notes:

- at present we are picking up the LLVM clang-cl, not the visual studio one. It
  means we are using LLD to link. Its not entirely obvious how to change it to
  use VS without losing ninja.
- GitHub does not support allow failures in the build matrix. See this
  discussion: [[https://github.com/actions/toolkit/issues/399][#399: Please support something like "allow-failure" for a given
  job]]. Added our use case to that discussion.
- for now, commented out clang-cl build.

Links:

- [[https://github.com/mlpack/mlpack/issues/1630][undefined reference to boost::archive... when building using cmake #1630]]
- [[https://stackoverflow.com/questions/71457208/how-to-build-with-clang-on-github-actions-for-windows-without-hardcoding-paths][How to build with clang on Github Actions for Windows without hardcoding
  paths?]]
- [[https://gitlab.kitware.com/cmake/cmake/-/issues/19174][How to use clang-cl and lld-link with Visual Studio generator]]

*** Towards a dynamic PMM                                              :epic:

Now that we are starting to see all text templates as mustache templates, the
next logical step is to make the PMM itself dynamic. That is to say, just like
we interpret the mustache templates on the fly, allowing users to make changes
to them, add new templates etc, so must the PMM also be a dynamic concept rather
than baked in to the code generator.

This is not a huge conceptual leap; in fact it is similar to what was already
achieved with profiles and configuration. It is also in keeping with loose
meta-modeling. However, as with configuration  and profiles, we must be careful
in how we implement this because the PMM is expected to exist when we begin the
transform chains. If it becomes a regular model (or at least, its elements
become regular model elements) then as with configuration, we need a "pre" step
in which we gather all of the PMM's elements, set them up and then start the
regular transforms. In a way, we are saying that the transforms have "stages":

- PMM: parts, facets, archetypes, etc.
- Variability: profiles, configuration
- LM: user model.

Of course we already have PMM elements in the LMM. However, these are designed
exclusively for code-generation. What we are now saying is that we will use them
as data, and skip the code-generation step altogether. This means Dogen must
distribute the PMM with it. It also means users can override it as required.
This is no different from what we already do with profiles.

We can even leave the =text= model as is, with the existing structure; but the
key difference is that we need to extract all of the PMM elements, process them
first, and then finally process the templates which are bound against those
elements.

Merged stories:

*Add instances of physical meta-model elements*

We made a modeling error with regards to the physical meta-model
elements. We assumed that the user configuration of the meta-model
elements could be stored with the PMM. This is incorrect because the
PMM is created from static data; it is as it was code generated by the
state of the =text.cpp= and =text.csharp= models. However, users can
apply their own configuration to these elements: change backend
directory, facet directory etc. These properties are relative to the
models the users load. Worse, they are possibly different for each
reference - though that particular problem will have to be addressed
separately.

This now causes a big conceptual problem: we assumed that artefacts
were instances of archetypes but yet there is a need to have an
archetype instance where the model specific configuration is
stored. The quick hack, is to create some types that sit in between
the meta-type and the instance type:

- =backend_instance=
- =archetype_instance=
- etc.

This is not very nice but it does solve the problem at hand. We can
then associate these with physical models. Alternatively we could use
a more neutral name like =_properties=, =_configuration=... Actually
we already had some suitable types for enablement, they can be
repurposed for this.

Notes:

- add transform to populate meta-model properties
- update enablement to use the properties, deprecate existing ones.
- merge local enablement transform with the reading of local
  properties; merge global enablement transform with the reading of
  meta-model properties. Add comments on local facet (for
  profiles). Add the missing properties to the global field groups.
- actually we can just rename both transforms instead of creating new
  ones.
- backends and parts also need a file path, just like artefacts.
- the meta-model properties also need a file path, which represents
  the component path. Paths can then be computed "recursively": the
  backend path is the component path plus the backend directory and so
  forth.
- that which we called "meta-model" in the PM is really the "component
  meta-model". In the future as we model more physical aspects we will
  have other kinds of meta-models (product, family, etc.). The "model"
  is really the component model because its an instance of a
  component. The product model will be made up of artefacts and will
  have parts and so forth but it will be different from the component
  model. Or perhaps we will just have other kinds of components inside
  the product model. In which case we need to consider having a notion
  of "component types" and possibly "component groups"
  (e.g. "projects").
- technical spaces and their associated versions should be declared by
  the text models and should be part of the PM. The TS should be
  declared on the "global" text model so that backends can reuse them
  (e.g. we can declare XML with associated extensions and then use it
  where required).

*Allow arbitrary physical containment*

We need to allow archetypes to be contained by any physical element,
except for archetypes. We also need to allow facets to belong to any
physical element other than facets.

*Implement M2T chains via code generation*

We need to update the =backend= and =part= transforms to be a set of
calls to their "children", based on the PMM. Once this is done we can
remove all of the existing infrastructure in the TS models:

- repositories
- initialisers
- workflows
- traits
- registrars

Notes:

- in the new world we no longer need a M2T interface at the text
  transform level. The backend chain knows of all of the facet chains;
  and the facet chains know of all of the archetypes. We can dispatch
  the element using the visitor into a concrete type and then find the
  archetypes that process that type. However, we do not want to
  generate an apply method per logical element...

*Implement backend and facet transform*

The backend transform should:

- return the ID of the backend;
- use the facet and archetype transforms to process all elements.

Check backlog for a story on this.

*** Consider modeling relations at a higher level of abstration        :epic:
    :PROPERTIES:
    :CUSTOM_ID: E19AC760-A5C5-CC84-61DB-E6D7B9562ECF
    :END:

Note: this story captures the high-level analysis for implementing relations
across dogen. We then need to create specific stories for its implementation.

At present we model relations in logical model as two object templates:

- =Generalisable= for inheritance (implements and extends).
- =Associatable= for composition.

In reality, we should have created the UML relationships as a top-level
construct:

- association: composition, aggregation
- dependency
- generalisation
- realisation

Relationships should have an associated comment or description.

Notes:

- relationships should already exist in the codec model. These exist for "local"
  relationships only (that is, elements in the same model). They can be used for
  generalisation. This does mean generalisation could be "remote" though as we
  some times inherit from other diagrams. We need a way to distinguish between
  local and remote relations, which could be by "resolving" the GUID into an
  element.
- relationships can be user-annotated, and used for UML diagram generation.
- generalisation and realisation remove the need for the parent meta-data.
- relationships can be derived from attributes. This is what the "resolver"
  does. It is in fact not a resolver but a transform that converts properties in
  the element into relationships.
- relationships should use the GUID as well as the qualified name.
- relationships should really be modeled as org-mode headings. However, one
  downside of this approach is that we will create a lot of noise when
  generating documentation. However, given we will only use them for local
  relationships (generalisation, UML purposes), maybe the noise is not that bad.
- transparent and opaque associations as well associative container keys need to
  be mapped to the appropriate UML stereotypes. Leaves and root parents as well.
  If none is appropriate we should create them.
- object templates are incorrectly modeled as stereotypes. These are
  realisations.
- profiles are also incorrectly modeled as stereotypes. These are also
  relations. However, the problem will be that once we remove them from
  stereotypes we cannot see them in UML. We need to have a section in the
  documentation which shows these properties for an element.
- The name of the relation is its description, e.g. "throws". We can have
  duplicate relation names.

Links:

- [[https://www.guru99.com/uml-relationships-with-example.html#:~:text=Relationships%20in%20UML%20are%20used,Dependency%20%2C%20Generalization%20%2C%20and%20Realization][UML Relationships Types: Association, Dependency, Generalization]].
- [[file:v1/sprint_backlog_31.org::#1ECCD69A-EE17-BAE4-7FE3-DA5F2E6E01FB][codec implementation]]

*** Missing PlantUML features                                         :story:

This story keeps track of features that we need to implement when
exporting into PlantUML.

- generalisation via meta-data does not work. This would be very
  complicated: we need to resolve =masd.generalization.parent= into
  the value and then look for all classes with the stereotype of the
  concept (e.g. =meta_element= for logical model).
- we need to unpack all properties and resolve those into types for
  the current model in order to model associations. Also, we need a
  simple resolver to find the types in the right namespaces. The
  logic would be very similar to the existing resolver. One possible
  solution is to move name trees into the codec model and perform
  these transforms earlier in the pipeline.
- add tests to check that PlantUML output has not changed. We need to
  have support for diffing on conversion, which we may not yet
  have. Also the existing tests do not take into account the action.
- add plantuml representations to reference models.
- improve layout: [[https://crashedmind.github.io/PlantUMLHitchhikersGuide/layout/layout.html][layout]]. See also [[https://stackoverflow.com/questions/61639970/align-packages-vertically-in-plantuml][SO: Align packages vertically in
  PlantUML]]
- add comments on relations:

: someclass o-- otherclass : a description of the relation

  This means the org-mode model must contain the comment. We could
  either use the comment on the association itself, or have a field in
  properties:

: :masd.codec.relation_comment: a description of the relation

  This is then copied as is into the UML diagram. By separating the
  two means we can still make "proper" comments at the attribute
  level.
- there should be meta-data to determine if a comment will show up in
  the diagram as a note or not. It should apply to both classes and
  methods.
- we should allow users to set the location of the comment for classes
  etc (bottom, top, left, right).
- we should allow enabling comments on methods and attributes, with
  direction (left or right).

Notes:

- we need to move features such as name trees and generalisation into
  the codec model.
- actually perhaps we need to allow users to "tweak" the
  relationships. We probably don't want every single relationship to
  become a link in UML because then we can't see the wood for the
  trees. A better approach may be to force users to declare the
  relationships as meta-data, like we suggested above for comments,
  and then transport those annotations to the UML diagram. This also
  means we could add a full reference to the types, which solves the
  issue of having to merge models. Finally, it means we can declare
  any kind of relation, as per the plant uml syntax - in fact we
  should probably expose the plant UML syntax directly, e.g. the user
  is expected to declare a native plantuml relation:

: :masd.codec.plantuml_relation: AS_IS_RELATION

  We could perhaps default to the current class, but that's about
  it. This would also allow for things like hidden links, etc. In
  effect the only relation you get for free is inheritance. Actually
  we should also have a "raw" way of inserting relations in a
  namespace. These are placed at the end of the namespace.
- logical model namespaces are not documented correctly.

Links:

- [[https://forum.plantuml.net/8770/how-to-force-vertical-arrangement-of-packages][How to force vertical arrangement of packages?]]

*** Weaving as model merging                                          :story:

We probably should use the MDE term weaving for model merging:

#+begin_quote
A model compiler takes a set of /executable UML models/ and /weaves/ them
together according to a /consistent set of rules/. This task involves executing
the mapping functions between the various source and target models to produce a
/single all-encompassing metamodel/ [...] that the includes all the
\marginpar{Model \\ compilers} structure, behavior and logic --- everything ---
in the system. [...] /Weaving the models together/ at once addresses the problem
of /architectural mismatch/, a term coined by David Garlan to refer to
components that do not fit together without the addition of tubes and tubes of
glue code, the /very problem MDA is intended to avoid!/ A model compiler imposes
a single architectural structure on the system as a whole.
#+end_quote

@article{mellor2004agile,
  title={Agile mda},
  author={Mellor, Stephen J},
  journal={MDA Journal, www. bptrends. com June},
  year={2004}
}

*** Rename profiles to configurations                                 :story:

This is more in keeping with its role.

*** Rename =instantiation_domain_name=                                :story:

This can be shortened to just =instantiation_domain=.

: masd.variability.instantiation_domain_name

*** Rename feature bundles to feature sets                            :story:

Bundles is not a name found in the literature.

*** Add "composite" physical elements                                 :story:

At present we allow users to reference facets without naming parts, e.g.:

: masd.cpp.types

In practice this is shorthand for:

: masd.cpp.include.types
: masd.cpp.src.types

One way to address this is to create the notion of "composite"
physical elements, at present only needed for facets. The idea is that
=masd.cpp.types= gets expanded into all parts that contains that
facet. An alternative is to allow a notation with wildcards, like
RabbitMQ:

#+begin_quote
Topic Exchange

Topic exchanges route messages to queues based on wildcard matches
between the routing key and the routing pattern, which is specified by
the queue binding. Messages are routed to one or many queues based on
a matching between a message routing key and this pattern.

The routing key must be a list of words, delimited by a period
(.). Examples are agreements.us and agreements.eu.stockholm which in
this case identifies agreements that are set up for a company with
offices in lots of different locations. The routing patterns may
contain an asterisk (“*”) to match a word in a specific position of
the routing key (e.g., a routing pattern of "agreements.*.*.b.*" only
match routing keys where the first word is "agreements" and the fourth
word is "b"). A pound symbol (“#”) indicates a match of zero or more
words (e.g., a routing pattern of "agreements.eu.berlin.#" matches any
routing keys beginning with "agreements.eu.berlin").
#+end_quote

This notation would allow for more generalised matching, though its
not clear we need this level of flexibility.

Links:

- [[https://www.cloudamqp.com/blog/part4-rabbitmq-for-beginners-exchanges-routing-keys-bindings.html][Part 4: RabbitMQ Exchanges, routing keys and bindings]]

*** Flat directory mode                                               :story:

In the past it was possible to generate code without creating
directories for facets. This was called flat directory mode. It is a
useful feature because it allows users to generate code for its
classes without having to have a dogen-like directory
structure. However, it seems we've removed this feature at some
point. We still have a test for it in the C++ reference model, but it
does not do anything. We need to reactivate this feature, perhaps
renaming the configuration name. We should also look for stories on
this as we must have something on this in the backlog.

Actually maybe there is some confusion on this as we have
=disable_facet_folders=.

*** org-roam integration                                              :story:

Assorted notes on org-roam:

- org-roam v2 now forces all entries to have an ID. We can use these
  as element identifiers. These are just GUIDs associated with
  org-mode headlines. We care about both the "model" level ID as well
  as the element level IDs.
- we need to introduce a model element called "reference" (we must
  already have some analysis on this). The reference will contain the
  org-roam ID for the model we are referencing. This will ensure we
  see the backlinks when we open models and facilitate navigation.
- org-roam must have a path on which to search files. We need to make
  sure we can see across products. For example, if we have product A
  referencing product B, both directories with all org-files must be
  in the path.
- by writing the ID into the file as a comment, we can now handle
  moves. This is done as follows: first, get a list of all files in a
  component and their IDs. Then, as a step on the physical model, for
  all files where overwrite is disabled, load up the previous content
  and set the current content to be the previous content. With this
  the file will move to its new location. Alternatively, if this
  increases generation time too much we can only load files based on
  the file name. But first try the easy approach.
- we can make a really simple "format" for references, just a
  meta-model element annotation as an org-tag (e.g. =reference=) and
  the contents of the reference must be an org-link
  (e.g. =[[file:x.org][some link]]=. It should be trivial to parse
  this. The org-link is actually an org-roam link. This means we need
  to have the directories where the models are stored in some path
  lookup in Dogen. We need to find all org-mode documents, open them
  and get their IDs.
- we need to allow adding elements which do not contribute to
  code. This is so we can add random notes in models. They can be seen
  as UML notes. We could simply add a "note" metatype to the logical
  model.
- with this approach, if the agile stores have links to the org-models
  we should also see them in org-roam. However, over time we'll
  probably end up with too many links. But it would be nice to open a
  model and see a list of stories done against it. We may need a way
  to enable / disable this.
- when we add build2 support, we need to be able to package the models
  with the header files.

Links:

- [[https://www.orgroam.com/manual.html][org-roam manual]]

*** Rename external and model modules                                 :story:

These are really product modules and component modules.

*** Rename object to data type                                        :story:

See DTO on wikipedia.

*** Consider modeling DTOs explicitly                                 :story:

There is a generalisation for POCO, POJO, etc called DTO:

#+begin_quote
In the field of programming a data transfer object (DTO) is an
object that carries data between processes. The motivation for its use
is that communication between processes is usually done resorting to
remote interfaces (e.g., web services), where each call is an
expensive operation. Because the majority of the cost of each call
is related to the round-trip time between the client and the server,
one way of reducing the number of calls is to use an object (the DTO)
that aggregates the data that would have been transferred by the
several calls, but that is served by one call only.
#+end_quote

We should really have this concept somewhere in Dogen.

Links:

- [[https://en.wikipedia.org/wiki/Data_transfer_object][wikipedia: Data transfer object]]

*** Export models to HTML                                             :story:

We should have a target that runs emacs and outputs an HTML representation of
the models.

Links:

- [[https://github.com/DogLooksGood/org-html-themify][org-html-themify GH]]: "This package will generate inlined CSS style in
  your exported HTML, according to your installed Emacs color theme."
- [[https://github.com/fniessen/org-html-themes][org-html-themes]]: "Though you can easily override CSS stylesheets and
  add your own HTML themes, we can say (or write) that Org mode
  provides a basic HTML support."
- [[https://www.reddit.com/r/emacs/comments/3pvbag/is_there_a_collection_of_css_styles_for_org/][reddit: Is there a collection of CSS styles for Org exports? What
  are your favorites?]]
- [[http://thomasf.github.io/solarized-css/][solarized-css]]
- [[https://github.com/dakrone/dakrone.github.com/blob/master/org.css][org.css GH]]

*** Ignorable headlines in org-mode                                   :story:

It should be possible to add documents that do not get expressed as
source code in models, but still come out in the documentation.

Notes:

- apparently org-mode supports headlines marked with =COMMENT=.

Links:

- [[https://www.reddit.com/r/emacs/comments/lxwo61/quick_tip_for_those_with_a_literate_emacs_config/][reddit: Quick tip for those with a literate emacs config]]

*** Implement code merging support                                    :story:

Once we have support for operations we now have in place all of the
requirements for code merging support. We then just need to read the
C++ source block in the operation, and associate the =custom_id= of
the operation with the generated code. Note that the org mode file
does not have the source code for the operation at all. Instead, we
have a link (see the transclusion story below) to the GUID in the
file. Users click on the link and edit the file directly. There is a
transform in the physical layer that reads the block and replaces the
GUID, e.g. given a file like so:

#+begin_src c++
void some_class::some_func() {
// {{{ 507ab9d0-d1a5-4fa9-97a9-d78a48a08c3d
<some content>
// }}}
}
#+end_src

We get all the text between:

: // {{{ 507ab9d0-d1a5-4fa9-97a9-d78a48a08c3d

and

: // }}}

And use it to replace the GUID prior to writing the file. Note also
that the function signature is written by Dogen. A operation is a
list of attributes. However, we also need to find a way to associate
some additional properties that are not needed for regular attributes:

- direction: =in=, =out=, =inout=. determine if its a reference, etc.
- mutability: =query= if its const.
- comments: used to construct the method's comment. Each attribute has
  one.
- default value.

For the operation we also need:

- return value: this can be directly added to the operation itself.
- class scope: static or not.
- visibility: protected, private, public.
- inheritance type: leaf, abstract, final.
- stereotype: No use for it yet.

Notes:

- in UML these are named parameters.

Merged stories:

*Add support for operations*

#+begin_quote
*Story*: As a dogen user, I want to specify operations via the
frontend so that I dogen can generate the header file and I can
manually add the implementation.
#+end_quote

This story is a requirement in order to implement merging support.

We now have in place almost all building blocks needed to implement
operations. Note that operations should be the only use case for
code-merging. Anything more complicated than this should just be
handcrafted. The basic idea is that we define operations just like we
define attributes (see simplified parsing below). We then code
generate the function signature for the operation on the header and
implementation files. In the implementation c++ file we add markers
like so:

#+begin_src c++
void some_class::some_func() {
// {{{ 507ab9d0-d1a5-4fa9-97a9-d78a48a08c3d
<some content>
// }}}
}
#+end_src

Users can freely edit =<some content>=. During the generation
pipeline, the implementation file will have the UUID written out as a
string, e.g.:

: 507ab9d0-d1a5-4fa9-97a9-d78a48a08c3d

Then, when we are generating files, as part of the artefact pipeline,
we first read the file we are about to write and extract any blocks
such as the above (block extraction transform). For each block we
perform a search and replace on the generated file. Then we do the
writing. The org-mode file has a link to the file. This means we have
full LSP support for code writing.

Notes:

- we need to handle the initial case. We should create a map of
  "required UUIDs".
- we also need some meta-data that allows injecting include files for
  the operation implementation.
- org-mode supports "text search"; see the [[https://orgmode.org/manual/External-Links.html#FOOT27][external links]]
  section. This means we can do a link like so:

#+begin_src
file:f1.cpp::507ab9d0-d1a5-4fa9-97a9-d78a48a08c3d
#+end_src

Links:

- [[https://emacs.stackexchange.com/questions/63391/how-do-i-link-to-an-id-on-a-non-org-mode-file][SO: How do I link to an ID on a non org-mode file?]]
- [[https://github.com/nobiot/org-transclusion/issues/59][Transclusion of non-org mode files #59]]

*Existing support*

When we did the expansion and indexing work for properties, we omitted
operations altogether. This is fine for now, as we only have a
half-baked support for them anyway, but will need to be revisited as
we start to use it in anger. In particular:

- we need sets of operations: local, inherited, all
- we need an operations indexer

Actually we removed the half-baked support.

*Simplify argument parsing for operations*

One of the main stumbling blocks for adding support for operations has
been the complexity of parsing all the permutations allowed by
C++. However, for the majority of use cases we have found so far
(particularly for the definition of interfaces), we don't actually
require access to the full expressiveness of c++:

- we don't use raw pointers that much;
- we always pass types by const reference or const (simple types);
- in cases where we need full power, we can always hand-craft those
  classes.

If we were to only allow types in arguments to be those already
available for attributes, we could start parsing operations really
easily. Once we have those, a few use cases open up:

- generation of skeleton for classes (header and c++);
- full generation of interfaces;
- we can start work on services/remoting (see other stories)
- we can start to consider generating SWIG interfaces,

In addition, Dia has a direction for each parameter (e.g. In, In &
Out, etc) so this can also be leveraged.

Notes:

- we should disable parsing of operations if hand-crafted is on (or do
  a shallow parse and just literally copy the string; we have another
  story for this in the backlog).
- we should make all operations simplified: e.g. not support the
  complexities of C++ etc. We only support simple UML like
  syntax. Additional information must be conveyed via mapping.

*Basic support for operations*

We could add support for operations without having full parsing
support for all variations in c++. This could be achieved as follows:

- do not support LAM to start off with. Mapping of types requires
  parsing all expressions on operations and this will not work until
  we support pointers, references and const.
- treat parameters in operations as a string - e.g. no parsing into
  name trees. Because we cannot parse, we also cannot resolve
  dependencies (e.g. the includes etc will not be right). One
  alternative is to have a hard-coded support for just =const= and
  =&=, which are the only use cases we have at present. If the parser
  could just ignore those we could still build name trees and have
  dependency support.
- for value objects, associate every operation with a protected
  region. For handcrafted objects, they are used just for creating the
  initial "class template" or the interface.
- when code-generating the operation, merely write the variable name
  and type without any transformation at all.
- users must associate a GUID with the operation for the protected
  region. This has to be done manually to preserve traceability
  links.
- since the protected region is bounded by function scope, we can
  simply look for protected region begin and end markers, and copy the
  region into the generated artefact.
- the purpose of these operations is just to: a) add trivial behaviour
  to value objects b) generate interfaces. It is kept really simple.

*** "Inferred" models                                                 :story:

[[https://www.reddit.com/r/cpp/comments/ksf2az/code_generation_using_attributes/][This reddit post]] inspired an idea. The gist of it is, one could annotate an
object with some attributes, e.g.:

: [[Register]]
: class Service

Via something like LSP one should be able to retrieve both the class, the
attributes and the properties. With this, it would be "trivial" to create a
Dogen model on the fly and generate it. The attributes could contain stereotypes
to configure the object, such as binding it into a profile, etc. This would make
integration of generated and non-generated code much easier.

*** Add custom IDs to attributes in org model                         :story:

At present we do not seem to be reading the custom id properties in
attributes. This means that if we add them to a model, rountripping
will fail. We should read them in.

*** Artefacts do not belong in codec models                           :story:

At present we implemented a trimmed-down version of artefact in codec model. In
reality, we need the physical model artefacts to be available because we are
getting more and more use cases:

- store a diff against the file;
- use the write artefact transforms;

And so forth. In truth we should compose codec with physical model in order to
implement reading and writing:

- the physical model reads a file from the filesystem and generates an artefact;
- in orchestration, we extract the content and supply it to the codec for
  processing;
- we then take the string output and place it in an artefact as required (i.e.
  for model to model transforms). This means we can run the physical model's
  transforms for writting, diffing, etc.

When we do this splitting, we can then implement tests for org-to-org
transforms.

Notes

- the problem is that we need to read the target in order to get the references;
  this means we need to generate the codec model first. Wwe would have to split
  the codec transform into two across orchestration to implement this: load
  codec model, in orchestration load artefacts for references, then load codec
  model, etc.

*** Add model name as title in PlantUML diagram                       :story:

At present its not possible to know the name of the model by looking at the SVG.
We should have a comment with the title. It could be added to the existing model
level comment with documentation.

*** Create a "manual tests" stereotype with profiles                  :story:

At present we have a =tests= facet that contains only the generated
tests, and writes to the =generated_tests= directory. We also have
created folders for manual tests under the =tests= directory. However,
the problem is that we still have no way to tell dogen about the
manual tests. This means we must use a regex to ignore the contents of
the folder. A better approach is:

- create two distinct test facets:
  - manual tests
  - generated tests
- create a profile that enables manual tests. When enabled, we simply
  create a skeleton boost test file. We must set it to override so
  that we update the contents of the file manually with real tests.

Tasks:

- rename tests facet to =generated_tests=
- create a new facet =manual_tests=. Copy most of the contents of the
  existing facet (main, cmake). Make the directory name =tests=.
- create a template for tests with associated meta-model entity
  (e.g. =masd::test=). It probably will also need its own namespace
  (=test=).
- create a stereotype that enables manual tests in the dogen model.
- update all models, adding =masd::test= for each manual test, with
  the new stereotype.

Notes:

- the present situation of ignoring regexes is a bit dangerous, e.g.:

: #DOGEN masd.extraction.ignore_files_matching_regex=.*/test/.*
: #DOGEN masd.extraction.ignore_files_matching_regex=.*/tests/.*

  This means any type in a model in the path "tests" will be
  ignored. This results in a lot of surprising behaviour because some
  times we create packages called tests (for example the "tests"
  facet).

*Previous understanding*

*Rationale*: the right solution for this is to split generated tests
from manual tests such that we do not have to mix and match the two
types of code.

At present we are ignoring all of the contents of =tests=. This means
whenever we delete a type we are left behind with its tests. A better
solution is to create model elements for each handcrafted test marked
as "masd::handcrafted_test". This disables all facets except for
tests. We can then remove the regex.

Whilst variability overrides will address the underlying issue in a
more maintainable way (e.g. the deletion of generated tests), we
should still create a profile and model all tests as proper entities
instead of bypassing the modeling system. We want to move to a world
were *all* files in the system can be attributable to modeling
entities.

*** Modeling of dogen models themselves                               :story:

*Note*: we probably already have this story in the backlog, or at least some of
its ideas. Do a grep and merge as required.

At present we have placed the dogen models in a top-level =models= directory.
However, if we think about it, it makes more sense to consider the model itself
a file that belongs to the component. For this we need:

- a models facet;
- an assets type of model. The model type can be Dia, JSON or org; actually we
  should not name this "model", it is a component model. We can also have
  "product models".
- an associated CMakeLists in the facet, responsible for generating the model.

We are no longer worried about Dia and JSON directories because we expect that
JSON models are automatically generated from Dia models in the nightly build for
testing purposes.

As we did with generated tests, when the model facet is enabled (which should be
by default, lest we confuse everyone), we generate the model.

Notes:

- this could enable a bootstrapping scenario: supply a model name and a
  directory to dogen and it will first generate the model itself and then
  generate it. Actually this may not be a good idea as we did not choose a
  profile. We probably would also need to supply the profile. We should look at
  build2 to see how the bootstrapping works. We should also keep in mind that
  there are two bootstrapping cases: the product case and the component case.
  Since the product itself will have a model, we need to make sure these are
  consistent. See [[*Introduce dogen projects][Introduce dogen projects]].

Merged stories:

*Consider creating a =modeling= folder/part*

At present we were thinking about having several folders related to
modeling, for each component:

- templates
- models
- etc

It may make more sense to gather them all under =modeling=. We could also have a
product level folder with the same name.

So =modeling= is the part, and =templates=, =models= etc are facets.

*Consider creating a "model" meta-model element*

This is not yet particularly clear, and we may already have something on the
backlog for this, but the gist of it is that we have a number of elements in the
meta-model that are functions of the model itself:

- the root module.
- projects and solutions for the model itself.
- common ODB options.
- top-level CMakefile (and perhaps others).
- msbuild

In all those cases we are duplicating information available at the model level,
with perhaps a tiny bit of extra processing. It would make more sense for there
to be an element that models the model itself and then to have formatters for
this element. We should wait until we finish with the fabric refactor (and
possibly the generation refactor too) until we look into this. We should also
take into account the product models.

In addition, cases such as common ODB options need to be rethought; if all we
are supplying is a different representation of data that already exists in the
model itself, then we should either rely on the model meta-type or move those
properties to the new meta-type. Seems a tad excessive having common ODB options
just for a "post-processed" set of properties.

*** Allow defaulting to target directory                              :story:

At present we default the location of the log file and the byproduct directory
to the location of the binary. It may make more sense to default it to the
location of the target model, allowing us to keep all generated data next to the
model now that we have the =PRODUCT.modeling= directories. We probably want to
make this optional in case the old behaviour was more sensible.

Names:

- =default-to-target-directory=?

*** Deprecate managed directories                                     :story:

There should only be one "managed directory" at the input stage, which is the
component directory (for component models). If parts have relative directories
off of the component directory then we should add to the list of managed
directories inside the PM pipeline.

*** Move hello world model from models directory                      :story:

It is confusing to have it mixed up with product models. Use a regular
dogen model to test the package. We could have it on the reference
model as a stand alone example, or we could create a "hello dogen"
product for a trivial example of dogen usage.

Actually, we need to address the entire samples use case. The easy
thing to do is just to add all dogen org models as examples. Or we
could just add the hello world model.

Merged stories:

*Create or update samples folder*

We should add samples to the package. These could be organsided by
injector (dia, json), then by language type (lam, cpp, csharp) or vice
versa.

We could also try to generate all of these models when testing the
package.

On the other hand, once we create a proper package for dogen headers,
with SOs etc, we should really include the dogen models there. In
effect, it will be symmetric with PDM packages.

*** Consider renaming =text= to =logical_physical=                    :story:

This is really the right name for the model; the text processing part
are the transforms that are done on the model.

Notes:

- rename =logical_physical_region= to just =region=.
- actually another way of looking at this is trying to figure out what
  is the dominant responsibility of the component. The LPS will
  probably be 2 or 3 types whereas the M2T transforms will be 99% of
  the types. We probably should name the model after lps and the
  component after the M2Ts. So rename instead the model to LPS.

*** Validate no two artefacts have the same ID                        :story:

At present it is possible to generate two artefacts with the same path
(which is the physical ID) and then have them overwrite each
other. This causes diffs that are very difficult to get to the bottom
of. It would be better to fail with a validation that detects
duplicates.

Merged stories:

*Add a validator for text model*

The validator should check the paths. This can also be done in
physical model.

:                 /*
:                  * FIXME: we are still generating artefacts for global
:                  * module.
:                  */
:                 if (aptr->file_path().empty()) {
:                     BOOST_LOG_SEV(lg, error) << empty_path
:                                              << aptr->name().id();
:                     // BOOST_THROW_EXCEPTION(transform_exception(empty_path +
:                     //         aptr->name().id().value()));
:                     continue;
:                 }

*** Fix name of configuration tracing file                            :story:

This name looks incorrect:

: 00000-configuration--initial_input.json

*** Rename =name= to =codec= name                                     :story:

- add codec ID to name.

Notes:

- variability is also using the name class.

*** Add descriptions to PMM elements                                  :story:

We need to read a description attribute for:

- backend
- facet
- part
- archetype

And populate these on the LM PMM, and then code generate them. The
description should be the comment of the associated element.

*** Create a physical ID in logical-physical space                    :story:

Artefacts are points in logical-physical space. They should have an ID
which is composed by both logical and physical location. We could
create a very simple builder that concatenates both, for example:

: <dogen><variability><entities><default_value_override>|<masd><cpp><types><class_header>

The use of =|= would make it really easy to split out IDs as required,
and to visually figure out which part is which. Note though that the
ID is an opaque identifier and the splitting happens for
troubleshooting purposes only, not in the code. With the physical
model, all references are done using these IDs. So for example, if an
artefact =a0= depends on artefact =a1=, the dependency is recorded as
the ID of =a1=. The physical model should also be indexed by ID
instead of being a list of artefacts.

We already created =logical_meta_physical_id= type so maybe we don't
need this ID as well.

*** Factor out duplication in stitch and wale templates               :story:

At present we are duplicating a lot of stuff in stitch templates. If
we look at the directives, we can group them as follows:

1. Hard-coded. These have the same value for all templates:

: <#@ masd.stitch.stream_variable_name=ast.stream() #>
: <#@ masd.stitch.inclusion_dependency=<boost/throw_exception.hpp> #>
: <#@ masd.stitch.inclusion_dependency="dogen.utility/types/log/logger.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.physical/types/helpers/meta_name_factory.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.logical/types/helpers/meta_name_factory.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/traits.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/traits.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/assistant.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/inclusion_constants.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/formatting_error.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.text/types/formatters/sequence_formatter.hpp" #>
: <#@ masd.stitch.wale.kvp.meta_name_factory=logical::helpers::meta_name_factory #>

2. Facet-dependent. These have the same value for a given facet:

: <#@ masd.stitch.containing_namespaces=dogen::text::cpp::transforms::types #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/types/traits.hpp" #>

3. Meta-element dependent. If we know who the meta-element is, we can
   generate these:

: <#@ masd.stitch.inclusion_dependency="dogen.logical/types/entities/structural/object.hpp" #>
: <#@ masd.stitch.wale.kvp.yarn_element=logical::entities::structural::object #>
: <#@ masd.stitch.wale.kvp.meta_element=object #>

4. M2T transform dependent. If we know the name of the transform, we
   can generate these:

: <#@ masd.stitch.wale.kvp.class.simple_name=class_implementation_transform #>
: <#@ masd.stitch.wale.kvp.archetype.simple_name=class_implementation #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/types/class_implementation_transform.hpp" #>

5. Not needed in the new world:

: <#@ masd.stitch.wale.text_template=cpp_artefact_transform_implementation.wale #>

6. Are dependent on the content of the template and so must be added manually:

: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/io/traits.hpp" #>
: <#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/io/inserter_implementation_helper.hpp" #>

We can address the first point and possibly the second point by
creating profiles. For point 3 and 4 we could inject these values as
part of transforms. Finally, we could so some simple filtering of
meta-data: any key starting with =masd.stitch.= is added to the KVP
container for the template. Some of these are injected manually.

Sadly we cannot share profiles between C++ and C# because at present
we cannot inherit across models. We could consider fixing this via
meta-data. Actually having said that we need to be able to use:

: masd.variability.profile = dogen.profiles.base.disable_all_facets

In the same model; this may work across models as well (modulus
possible problems with merging).

At any rate the profiles should be kept within the =text*= models
given they are used only for M2T transforms.

*** Stitch templates should be bound to Dogen M2T transforms          :story:

At present we have tried to create some kind of generic implementation
of a templating engine. However in practice we only need it for the
implementation of the apply method of a M2T transform. We could take
advantage of this in order to simplify templates; we could assume that
the only thing we could code-generate in a stitch template is the
inside of the apply method. We need to check but T4 does something
similar. This would mean that many things would be hard-coded such as
the name of the stream variable etc.

Everything else should be supplied as meta-data parameters to the
modeling element for archetypes: includes, etc. This means the
templates would be much simpler. This can only be done once we use the
PMM to compute paths. Also, we probably require a way to inject the
dependencies. This will probably require merging code generation as
well.

Also this can only be done when we remove the current implementation
of helpers and move to PDMs.

*** Name all transform exceptions consistently                        :story:

It seems on engine we call them "transform exception" but on assets we
call them "transformation error". Check all other models and them
these consistently.

Merged stories:

*Rename =transformation_error= to =transform_exception=*

In keeping with the framework guidelines for naming exceptions. We need to
also look at all other exceptions.

*** Processing of boost tests                                         :story:

We could create a meta-model element to represent the handcrafted
tests. These could then be populated by locating the physical file and
looking for patterns:

: BOOST_AUTO_TEST_SUITE(stitch_parser_tests)
: BOOST_AUTO_TEST_CASE(empty_string_results_in_empty_template)

We could then read up these strings into the model element and then
create:

- the CDash runner. We wouldn't need any CMake magic for this any
  more.
- org file with all the tests. If there is an environment variable to
  set the log level, we could then control it from org-babel.

*** Model projections                                                 :story:

When we have a model element for models themselves, we could add
support for model projections.

Notes:

- model to PlantUML diagram.
- is the project file not a model projection?
- look for a similar story on this.

*** Add references as links to org documents                          :story:

Try adding a =references= tag. Content is a list of links to org
models. However, because of the way our referencing works in dogen, we
need to do some kind of hack. Perhaps the "text" of the link could be
the simple path to the file and the link the relative path. To start
off with, it can be ignored and managed manually. This will be spun
into its own story for the future.

*** Empty model generation produces no diffs                          :story:

At present if we do not generate any files at all, the diffing system
breaks down. This is because we always diff the generated files
against the actual files but if there are no generated files then we
have nothing to diff. Logs:

: 2020-12-24 11:09:56.742871 [INFO] [physical.helpers.file_status_collector] Started collecting status for model: dogen
: 2020-12-24 11:09:56.742874 [DEBUG] [physical.helpers.file_status_collector] Initial directories: [ "/work/DomainDrivenConsulting/masd/dogen/integration/projects/dogen/cpp" ]
: 2020-12-24 11:09:56.742888 [DEBUG] [physical.helpers.file_status_collector] Filtered directories: [  ]
: 2020-12-24 11:09:56.742892 [DEBUG] [physical.helpers.file_status_collector] No directories to analyse.
: 2020-12-24 11:09:56.742895 [DEBUG] [physical.transforms.gather_external_artefacts_transform] No files found.
: 2020-12-24 11:09:56.742898 [DEBUG] [physical.transforms.gather_external_artefacts_transform] Finished transform: gather external artefacts transform.

We need to look at the actual files and report all of those that
exist but have not match in the generated files collection.

*** Generated CMakefiles do not take tests into account               :story:

At present most models have:

: masd::build::cmakelists, dogen::handcrafted::cmake

One of the reasons is because the template is not taking generated
tests into account. It should have:

: if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/generated_tests)
:    add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/generated_tests)
: endif()

*** Consider creating a "combined" template                           :story:

We have a large number of templates that look very similar: the M2T
transforms. We could support them with a "clever" combination of wale
and stitch templates: the top-level wale template could stay as it is,
plus a "preamble" and "postamble":

: void primitive_implementation_transform::
: apply(const text::transforms::context& ctx, const text::entities::model& lps,
:     const logical::entities::element& e, physical::entities::artefact& a) const {
:     tracing::scoped_transform_tracer stp(lg, "primitive implementation",
:         transform_id, e.name().qualified().dot(), *ctx.tracer(), e);

and a closing bracket:

: }

However, maybe this is just too much work for just a few lines of code
and it will increase the complexity a lot for a small win.

*** JSON models do not create containment structure                   :story:

At present the JSON models represent containment via qualified names,
e.g.:

:       "name": "posix_time::ptime",

However, this is then not propagated correctly into the codec
model. This means that if you have a JSON model with namespaces, it
will result in an org model (say) without namespaces. We need to
figure out what we did for Dia. Perhaps we can use the qualified names
to create packages.

We have a similar problem with org-mode models; the containment
properties are not populated, so rountripping of org to org will not
work.

*** Running convert without parameters should work                    :story:

We tried converting like so:

: $ dogen.cli convert cpp.boost.json cpp.boost.org
: Error: File not found:
: Failed to execute command.

It seems we ended up with empty source and target. The correct
incantation is as follows:

: $ dogen.cli convert --help
: Dogen is a Model Driven Engineering tool that processes models encoded in supported codecs.
: Dogen is created by the MASD project.
: Displaying options specific to the convert command.
: For global options, type --help.
:
: Convert:
:   -s [ --source ] arg      Input model to read, in any of the supported
:                            formats.
:   -d [ --destination ] arg Output model to convert to, in any of the supported
:                           formats.

We should at least give a proper error message. Ideally we should be
able to infer source and destination from the positional arguments.

*** Reorganise dogen output                                           :story:

At present we have many different artefacts we're interested in:

- the target model;
- the reference models;
- log file;
- tracing data;
- reports;
- etc.

In the past we decided to place all references in either the shared
library directory, or the same directory as the target models. In
terms of the output, we created the "by-products" directory where all
files other than log files go, and the directory for the log
file. However, it perhaps makes more sense to have an holistic view:
why not place _all_ files, except for references, in the same
directory as the target model? If we did this, we would be able to
create org files that cross reference artefacts:

- link to all tracing files in the org file with all transforms.
- link to log file.

This only makes sense when we move all models into the component
directory, under =modeling=.

*** Consider adding support for C++ container generation              :story:

At present we are creating a lot of maps etc. These have common access
patterns:

- "strict" find: find or throw.
- "strict" insert: if duplicate, throw.

We should catalogue all of these, there are probably more. A better
approach would be to have a type much like primitive which wraps the
container with the right accessors. It would also make domain
terminology much saner. We should probably expose the typical C++
infrastructure such as iterators, ranges, etc. This may also be useful
for containers such as list and vector though we don't have use cases
for these yet.

Links:

- [[https://www.internalpointers.com/post/writing-custom-iterators-modern-cpp][Writing a custom iterator in modern C++]]
- [[https://www.reedbeta.com/blog/ranges-compatible-containers/][Making Your Own Container Compatible With C++20 Ranges]]

*** Consider adding a codec meta-model                                :story:

As we did with the PMM, it would be nice if you could create an
element with a stereotype, say:

: masd::codec

And then have it create a meta-model entry for the codec, with an
associated name and description. These can then be used for
documentation. It should also include the codec extension. The codec
interface should return the meta-type of the codec.

*** Allow uses of "dictionaries" for invalid class names              :story:

It would be nice if we had a meta-type that represented a dictionary
of invalid names and then could associate it with meta-types and
possibly technical spaces. For example, we could say that certain
words are invalid for all technical spaces, other only for technical
space x etc. In addition, certain words could be valid for certain
meta-types (say archetypes) but invalid for others (say objects). If
these were meta-types we could place them in a dogen model and even
allow users to create their own lists. Maybe the better term is
"whitelist"?

Links:

- [[https://english.stackexchange.com/questions/51088/alternative-term-to-blacklist-and-whitelist][Alternative term to “Blacklist” and “Whitelist”]]

Merged stories:

*Reserved keywords are relative to model elements*

At present we do a blanket check for reserved keywords across all
elements. Some elements however are unaffected by this such as
profiles, archetypes (to an extent) etc. We need a way to check
different kinds of elements against different lists.

*** Technical space composition                                       :story:

There are some formatters which are really not specific to a technical
space:

- CMake can be used with several languages such as C, C++, etc.
- Visual studio solutions are common to many technical spaces (F#, C#,
  C++, etc).

It seems we need to create a set of generation models which can be
used in conjunction with the "dominant" technical space. These are
triggered by the presence of meta-elements. Or perhaps we can just say
that we iterate through all "non-dominant" technical spaces ("main"
and "secondary"?  "subsidiary"?) and generate anything for which there
is an enabled and matching meta-element.

- emacs terms: major and minor technical spaces.

*** Add technical space version to element                            :story:

We should add the version of the TS to the element itself rather than
querying it on the model. We probably should also have the major TS
name in the element as well.

Merged stories:

*Create the concept of a technical space version*

We need a simple way to compare versions of technical spaces and have
them mapped into "identifiers" that users can relate to. For example,
C++ versions such as C++ 98 etc are the identifier; we should also
have a simple natural number mapping for each of these. We also need
to take into account the TRs - e.g. a type may be defined on a TR but
not be available on a version.

This should be done when we add technical spaces to the meta-model.

We have now added a "temporary" =technical_space_version= that could
be used for this (in =identification=). However, this needs to be done
as data.

*Drop the "c++-" prefix in meta-data for standard*

At present we do:

: quilt.cpp.standard=c++-98

The "c++-" seems a bit redundant.

*Add a C++ version to types*

Not all system model types are available for all versions. This
applies to the C++ standard (e.g. 98, 11, 14 etc) but also to
boost. We need to be able to mark a type against a version; the user
then declares which version it is using in the model. If the user
attempts to use types that are not available for that version we
should throw.

We have now added a "temporary" =technical_space_version= that could
be used for this (in =identification=).

*Add facet validation against language standard*

With the move of enablement to yarn, we can no longer validate facets
against the language standard. For example, we should not allow
hashing on C++ 98. The code was as follows:

#+begin_src c++
void enablement_expander::validate_enabled_facets(
    const global_enablement_configurations_type& gcs,
    const formattables::cpp_standards cs) const {
    BOOST_LOG_SEV(lg, debug) << "Validating enabled facets.";

    if (cs == formattables::cpp_standards::cpp_98) {
        using formatters::hash::traits;
        const auto arch(traits::class_header_archetype());

        const auto i(gcs.find(arch));
        if (i == gcs.end()) {
            BOOST_LOG_SEV(lg, error) << archetype_not_found << arch;
            BOOST_THROW_EXCEPTION(expansion_error(archetype_not_found + arch));
        }

        const auto& gc(i->second);
        if (gc.facet_enabled()) {
            const auto fctn(gc.facet_name());
            BOOST_LOG_SEV(lg, error) << incompatible_facet << fctn;
            BOOST_THROW_EXCEPTION(expansion_error(incompatible_facet + fctn));
        }
    }

    BOOST_LOG_SEV(lg, debug) << "Validated enabled facets.";
}
#+end_src

It was called from the main transform method in enablement transform,
prior to uptading facet enablement.

What we really need is the concept of a technical space in the
metamodel, as well as a "version" for that technical space, and then
also the concept of a facet. Then we are effectively building
(weaving?) an instance of a theoretical TS based on the configuration
(positive variability). We can then validate the configuration. This
should all now be part of archetypes. The versions can be attributes
of technical space with a string version (e.g. "c++ 98) and a numeric
version (1 say) so that we can make comparisons (e.g. c++ 17 > c++
98). Each formatter can then declare its compatibility against the
versions of the technical space.

Notes:

- this will require the injection of TS and the TS version as well.

Merged stories

*Facets incompatible with standards*

Some facets may not be supported for all settings of a language. For
example the hash facet is not compatible with C++ 98. We need to have
some kind of facet/formatter level validation for this.

*** Model functionality provided by assistant in the logical model    :story:

We need to look at all the methods in assistant and see if we can
replace them by data which is processed in transforms.

*** Refactor the templating model                                     :story:

We should use "standard" conventions in this model as well:

- add entities namespace. Name entities after their "type",
  e.g. =stitch_template=, etc.
- add transforms for the template expansion.

*** Remove annotations from stitch templates                          :story:

In the new world, stitch templates don't have all of the required
information to build the boilerplate:

- they cannot expand wale templates because the KVPs will be in the
  element itself, not the template. Strictly speaking this is not an
  problem we have right now though.
- more importantly, the include dependencies cannot be computed by the
  template. This is because the dependencies are really a function of
  the model type we are expressing on the template. Instead, we did a
  quick hack and supplied the includes as KVPs. So they are kind of
  parameters but kind of not really parameters because they are
  hard-coded to the template. It solved the immediate problem of
  having them formatted and placed in the right part of the file, but
  now we can see this is not the right approach.

In reality, we should not have any annotations at all in
templates. The boilerplate and includes should be supplied as KVPs and
applied as variables. They should be composed externally with access
to data from the model element. Thus we then need a way to associate
includes with model elements. This is captured as a separate story.

We seem to be using features to read values out of the templates. We
need to see if this adds any value.

*** Move all stitch include dependencies to legacy transform          :story:

We should move all of the inclusion dependencies in stitch templates
into the legacy dependencies transform, e.g.:

#+begin_src
<#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/hash/traits.hpp" #>
<#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/assistant.hpp" #>
<#@ masd.stitch.inclusion_dependency="dogen.logical/types/entities/helper_properties.hpp" #>
<#@ masd.stitch.inclusion_dependency="dogen.text.cpp/types/transforms/hash/date_helper_transform.hpp" #>
#+end_src

Once we do this we should also remove support for
=inclusion_dependency= so that it is not possible to add these any
more. We need to check to see if all the headers we have at present
are really required (for example traits should not be).

This is a prerequisite for moving to relationship based dependencies.

*** Improve modeling of attribute properties                          :story:

When we decommissioned formattables we did a lift and shift of a few
properties into the model itself:

- =streaming_properties=
- =aspect_properties=
- =assistant_properties=

This was the first phase of this work. Once the PDM work has been
carried out and we determine which of these properties are really
required, we should move them to the attributes instead of the model
because that is how they are ultimately used.

*** Prune non-generatable types from logical model                    :story:

Add a pruning transform that filters out all non-generatable types
from logical model.

Actually we can't just do this directly else the inclusion will not
work. However we do have a "reducer" transform in the formattables
namespace which needs to be moved to the new world.

*** Feature initializer with no features does not compile             :story:

We removed all the features from =masd::variability::initializer= and
the compilation failed with the following error:

#+begin_quote
[5/19] Building CXX object projects/dogen.text.cpp/src/CMakeFiles/dogen.text.cpp.lib.dir/types/feature_initializer.cpp.o
FAILED: projects/dogen.text.cpp/src/CMakeFiles/dogen.text.cpp.lib.dir/types/feature_initializer.cpp.o
/usr/bin/clang++-11  -DENABLE_CPP_REF_IMPL_TESTS -DENABLE_CSHARP_REF_IMPL_TESTS -DLZMA_API_STATIC -D_GLIBCXX_USE_CXX11_ABI=1 -Istage/include -I../../../../projects/dogen/include -I../../../../projects/dogen.identification/include -I../../../../projects/dogen.physical/include -I../../../../projects/dogen.cli/include -I../../../../projects/dogen.utility/include -I../../../../projects/dogen.variability/include -I../../../../projects/dogen.dia/include -I../../../../projects/dogen.codec/include -I../../../../projects/dogen.codec.dia/include -I../../../../projects/dogen.codec.json/include -I../../../../projects/dogen.codec.org_mode/include -I../../../../projects/dogen.tracing/include -I../../../../projects/dogen.logical/include -I../../../../projects/dogen.orchestration/include -I../../../../projects/dogen.templating/include -I../../../../projects/dogen.text/include -I../../../../projects/dogen.text.cpp/include -I../../../../projects/dogen.text.csharp/include -I../../../../projects/dogen.relational/include -isystem /work/DomainDrivenConsulting/masd/vcpkg/masd/installed/x64-linux/include -Wall -Wextra -Wconversion -Wno-mismatched-tags -pedantic -Werror -Wno-system-headers -Woverloaded-virtual -Wwrite-strings  -frtti -fvisibility-inlines-hidden -fvisibility=hidden  -O3 -DNDEBUG -fPIC   -std=gnu++17 -MD -MT projects/dogen.text.cpp/src/CMakeFiles/dogen.text.cpp.lib.dir/types/feature_initializer.cpp.o -MF projects/dogen.text.cpp/src/CMakeFiles/dogen.text.cpp.lib.dir/types/feature_initializer.cpp.o.d -o projects/dogen.text.cpp/src/CMakeFiles/dogen.text.cpp.lib.dir/types/feature_initializer.cpp.o -c ../../../../projects/dogen.text.cpp/src/types/feature_initializer.cpp
../../../../projects/dogen.text.cpp/src/types/feature_initializer.cpp:26:52: error: unused parameter 'rg' [-Werror,-Wunused-parameter]
register_entities(variability::helpers::registrar& rg) {
#+end_quote

We could perhaps issue a dogen warning for the absence of features but
the code should compile.

*** Create a patch for tests                                          :story:

At present we only output the head of the first 5 diffs when a model
fails. However, in some cases we may want to look at the entire
diff. It would be nice if we could output the patch into the
byproducts directory for each test.

*** Wale should throw on non-required keys                            :story:

At present we throw if we do not supply required keys. We should also
throw if we supply non-required keys.

*** Remove wale instantiation from stitch                             :story:

Though we've split wale out of stitch in the logical model, its still
possible to instantiate a wale template within stitch. We should
remove this as well.

*** Decouple physical model from logical model                        :story:

At present we have a dependency of the logical model on the physical
model. This is for two reasons:

- variability (=variability_profiles_chain=): we need to instantiate
  the template domains.
- helpers (=helper_properties_transform=): we need access to the
  helpers in order to create the helper properties.

We should not really couple these two models. For the first case we
could supply the domains as an argument when constructing the context
and rely only on =std= types. Helpers will be decommissioned with
PDMs. At that point we should look into making these two models
independent again.

Actually we could do a quick change now and address this:

- do the variability change now.
- instead of adding helper transform, add this work into the logical
  to physical projector.

Actually that would not work as we are projecting "the other way
around". This work will have to wait.

*** Consider making technical spaces a core concept                   :story:

At present we are trying to instantiate a stitch template. It requires
knowing the technical space that the original archetype belongs to so
that we can locate the appropriate decoration. However, because we are
in the text model and the archetype is a physical concept, we have no
way of knowing what the original TS was for a given archetype. We
could of course locate the associated M2T transform etc - but this
perhaps hints at a bigger problem with the conceptual model: technical
spaces are much more of a pervasive concept than just logical model:

- injectors belong to technical spaces.
- logical model entities belong to technical spaces.
- text transforms belong to technical spaces.
- physical model entities such as archetypes and artefacts belong to
  technical spaces.

It would be nice if we could have this modeled correctly, in some kind
of shared model. At present, the only model which does this is
=variability= but it does not make a lot of sense to put TS'
there. Perhaps we should wait until we have enough entities to see
what the name of this "core" model should be.

Merged stories:

*Add technical spaces to PM and LM*

Technical spaces and their associated versions should be declared by
the text models and should be part of the PM. The TS should be
declared on the "global" text model so that backends can reuse them
(e.g. we can declare XML with associated extensions and then use it
where required).

Notes:

- TS should also have a physical name. However, they are not
  associated with a backend. Actually they don't need a physical name
  but they need some kind of identification.

*** Move io code in types in C++ to io facet                          :story:

Originally we implemented io support for inheritance by making use of
virtual functions. This is still the easiest way to do type
dispatching; however, we then placed the io implementation in
types. This is a bit annoying because it clutters types with io
machinery. Another way of doing this is:

- create a class to do the streaming for each type, call it =dumper=;
- when there is no inheritance, =operator<<= simply calls the
  appropriate dumper.
- when there is inheritance, to_stream calls the appropriate dumper
  directly; =operator<<= calls =to_stream=. in an ideal world we could
  even make it private and =operator<<= a friend.

With this, we no longer need all the complications of supporting io
helpers in types (enabled in helpers, etc). We just need to determine
if io is enabled (and in inheritance), in which case we output
=to_stream= and for implementation, also include/use the dumper. Note
that we still need to declare the dumpers in the io headers - at least
for types involved in inheritance, but probably in all cases for
consistency.

In fact we should go one step further and rename the io facet
"dumper", "data_dumper" or some such name. We called it "io" because
it uses iostreams in C++ but that is just an implementation
detail. The facet itself should be mainly composed of the dumpers
themselves and then simply have =operator<<= as entry points to call
the dumpers.

This is a dependency in order to move to PDMs because we will not have
helpers. Therefore there will be no way to handle the complex
relationships between types and IO.

*** Analysis of MDE papers to read                                    :story:

Links:

- [[https://ulir.ul.ie/bitstream/handle/10344/2126/2007_Botterweck.pdf;jsessionid=AC6FF39BA414E6065602C7851860C43D?sequence=2][Model-Driven Derivation of Product Architectures]]
- [[https://madoc.bib.uni-mannheim.de/993/1/abwl_02_05.pdf][A Taxonomy of Metamodel Hierarchies]]

*** Order of headers is hard-coded                                    :story:

In inclusion expander, we have hacked the sorting:

:        // FIXME: hacks for headers that must be last
:        const bool lhs_is_gregorian(
:            lhs.find_first_of(boost_serialization_gregorian) != npos);
:        const bool rhs_is_gregorian(
:            rhs.find_first_of(boost_serialization_gregorian) != npos);
:        if (lhs_is_gregorian && !rhs_is_gregorian)
:            return true;

This could be handled via meta-data, supplying some kind of flag (sort
last?). We should try to generate the code in the "natural order" and
see if the code compiles with latest boost. We should also test this
with latest boost to see if its still needed.

*** Create a de-normalised representation of archetype properties     :story:

At present we have a two-step process: we first read the global
configuration for a model, create the corresponding properties
(e.g. backend, facet, archetype properties) and then we post-process
these to create the =denormalised_archetype_properties=. However, we
never really need to think about the individual properties because
they are always used in the context of an artefact, which means we
care about the de-normalised archetype properties only. Therefore we
should:

- have a =archetype_properties= that is composed of all other
  properties;
- change the =meta_model_properties_transform= to create internal
  indices of properties as a first step for the final property
  generation but do not expose these containers.

Notes:

- we can't remove the top-level containers just yet because they are
  used within the formatables namespace. However, these appear to be
  legacy use cases, so we should be able to do so when we get rid of
  this namespace.

*** Move default constructor work from resolver                       :story:

At present we are populating the default constructor for the bundle in
the resolver:

#+begin_src c++
void resolver::resolve_feature_template_bundles(const indices& idx,
    entities::model& m) {
    for (auto& pair : m.variability_elements().feature_template_bundles()) {
        auto& fb(*pair.second);
        for (auto& ft : fb.feature_templates()) {
            resolve_name_tree(m, idx, fb.name(), ft.parsed_type());
            if (ft.parsed_type().is_current_simple_type())
                fb.requires_manual_default_constructor(true);
        }
    }
}
#+end_src

This is very confusing because one would assume the resolver just
resolves. We need to move this logic to =technical_space_properties=.

*** Create a technical space specific property for default functions  :story:

In assistant we have:

#+begin_src c++
bool assistant::supports_defaulted_functions() const {
    return !is_cpp_standard_98();
}

bool assistant::supports_move_operator() const {
    return !is_cpp_standard_98();
}
#+end_src

This should really be in =technical_space_properties=. Check to see if
we missed any.

*** Default constructor incorrectly generated in C++ 98               :story:

We have this logic in =technical_space_properties_transform=:

#+begin_src c++
    /*
     * In C++ 98 we must always create a default constructor because
     * we cannot make use of the defaulted functions.
     */
    if (is_cpp_standard_98 || src_tsp.requires_manual_default_constructor())
        dest_tsp.requires_manual_default_constructor(true);
#+end_src

This is actually incorrect: we can use default constructors in C++ 98,
as long as there are no other constructors. The problem is we are
relying on the default constructor in test data generator so if we fix
this with an =&&= instead of an =||= we break that code. We need to
figure out what the correct implement is.

*** Detect absence of configuration in bundles                        :story:

It would be nice if when we call =make_static_configuration= it would
populate some flag stating whether none of the config was
populated. The specific use case is that we may want to detect absence
of all elements and do something in that case (for example, missing
streaming properties).

*** Refactor streaming properties processing                          :story:

At present we copied across the logic from =text.cpp= where the
streaming properties are stored as a class and the final processing
happens in assistant. However, when we get rid of helpers, we could do
all the processing in the streaming processing transform and store it
in attributes.

*** Add method to check if string is valid enum                       :story:

We have a method to convert a string to an enum, but sometimes we just
want to know if its valid without converting. We should have a method
that just returns true or false, or throws, if the string is not a
valid enum.

*** Split =utility= into multiple models                              :story:

What we have in the =utility= library at present is in fact a
combination of different things:

- log could be an extension to boost log, so part of =boost= PDM.
- testing could be an extension of =Boost.Test= so part of =boost=
  PDM.
- =filesystem= also belongs to boost.
- =formatters= are helpers in the LPS model.
- =hash= is an extension of either boost or std models.
- ...

We need to finish the analysis on this and make sure we have the right
model and PDMs to house these types.

*** Create a factory transform for parts and archetype kinds          :story:

- integrate their generation into PMM chains.

Notes:

- it does not make a lot of sense to have an archetype kind
  transform. That is, as with TSs, archetype kinds only provide
  attributes (e.g. data) about physical space, but they won't be
  expressed as actual physical elements. Parts however are connected
  to the transforms; they will in the future be used as part of the
  transform chain.
- do we instantiate template domains over parts? We need to do so in
  order to support directory overrides. The problem is that in order
  for the part to become part of the topology of physical space, we
  now need to make sure we can still convert archetypes into facets. A
  lot of the code is going to break once we add path.

*** Rename =archetype_name_set=                                       :story:

We haven't yet found the right name for this but the idea is that we
have a container of all the meta-names which refer to archetypes in a
region of physical space.

- archetype name meta region? it is only meta-names.

*** Add dependencies to artefacts                                     :story:

 We need to propagate the dependencies between logical model elements
 into the physical model. We still need to distinguish between "types"
 of dependencies:

 - transparent_associations
 - opaque_associations
 - associative_container_keys
 - parents

 Basically, anything which we refer to when we are building the
 dependencies for inclusion needs to be represented. We could create a
 data structure for this purpose such as "dependencies". We should also
 include "namespace" dependencies. These can be obtained by =sort |
 uniq= of all of the namespaces for which there are dependencies. These
 are then used for C#.

 Note however that all dependencies are recorded as logical-physical
 IDs.

 We also need a way to populate the dependencies as a transform. This
 must be done in =m2t= because we need the formatters. We can rely on
 the same approach as =inclusion_dependencies= but instead of creating
 /inclusion dependencies/, we are just creating /dependencies/.

 This will also address the uses of traits, e.g.:

 : const auto ch_arch(traits::archetype_class_header_factory_archetype_qn());

 This is because the traits are used to express dependencies.

 Notes:

 - we did the work to record the relations at the archetype level and
   started updating the archetypes with these in =text.cpp=. However,
   we only did a couple of types.
 - in order to instantiate meta-relations onto the LPS, we need to be
   able to resolve a relation type such as "transparent" into a
   concrete archetype. This means the archetype must have a label of
   that relation type.
 - artefacts must have relations stored as LPS points with both the
   logical name and physical meta-name. At this point we no longer care
   about relation type since it has been resolved.
 - a part is really a "meta-part". We still need to instantiate it with
   the actual project path. The physical model needs to contain this
   instantiation.
 - artefacts need to know their parts.
 - archetypes do not have part populated and their type is incorrect
   (=physical_id=).
- parts should have a root folder. These are specified through
  meta-data. The path is relative to the project path. Different
  models can have different part paths. This means we need to remember
  them when computing a reference to an artefact. Actually this is
  only needed because of split projects. We need to deprecate it as it
  makes things very complicated.
- parts need a directory name. This must be supplied by meta data with
  the part name:

: masd.physical.part.folder_name.implementation=src

  Where implementation is a KVP.

- physical model must be split by backend. Backend must have an
  associated folder name or blank for no folder:

: masd.physical.backend.folder_name.cpp=src

- actually we will have exactly the same problem with facets too. We
  need to create instances of all the meta-model elements.
- due to the fact that you can configure physical meta-elements, we
  have no choice but keep track of the referenced models. This is
  because we could have overwritten them differently in any of the
  referenced models.
- actually we found a much more profound problem, which already exists
  in dogen: if you configure backend/facet/archetypes differently in
  say M0 and M1, and if M1 references M0, the paths will not be
  constructed correctly. That is because we assume that we can
  reconstruct M0 paths using M1's configuration, which is true at
  present merely because we use the same variability settings for all
  models within a product; and on the rare cases we don't, we never
  make use of these models from other models - e.g. test models. To
  fix this properly would require a fairly complex set of changes to
  Dogen: we would need to keep track of the references and their types
  all the way through to code generation. This will not be
  easy. However, what we can do is to start introducing the notion of
  reference models and elements; initially this can be used just to
  check that all references have the same configuration. Eventually,
  as use cases arrive we can extend it to implement this per-model
  configuration properly. This also means that it is not possible to
  refer to a model that has more than one backend for now from a model
  that only has a backend.

*** Improve support for references                                    :story:

At present we have limited support for references in the presence of
variability. This is because once we start changing configuration
points such as the backend directory, facet directories etc, in a
model which is referenced from another model then the path resolution
will start to fail. This is because we expect all models to have the
same configuration for all configuration elements that affect file
paths. Since they do at present we never noticed this problem.

The correct solution is to introduce reference models and reference
elements. These just need to have a small number of properties:

- configuration of root module;
- model and element logical name, as well as meta-element name.

With this we could also stop creating elements for referenced models
which would probably result on a major reduction of processing
time. Then we have two ways of introducing these models:

1. "the best way": do not fully parse reference models at all, just
   extract the reference properties. This will require a lot of
   changes on the pipeline.
2. "the quick hack": for all references, load the codec model into the
   logical model and then convert it into a reference model. We do a
   lot of unnecessary processing but it should be easier.

We could even start by taking approach 2 and then eventually move to
approach 1. Either way we need to do this once we move to the new
world of dependency generation.

*** Replace =facet_default= with labels                               :story:

We need to stop using the enumeration to determine the canonical
header and use instead the new labelling mechanism.

The right label is probably =transparent=.

*** Add dependency generation to PM                                   :story:

We should store the dependencies in the following format:

- relative path
- dot notation
- colon notation
- header guard: not very nice but its the easiest way to solve this
  problem for now.

Archetypes should record their own information for this. This involves
reading meta-data for certain cases (e.g. PDMs). One archetype can
have more than one of these entries. We could map this like an RPM:

- provides
- requires

or

- exports
- imports

Once we are generating the provides/exports we can then use the maps
to populate the imports.

Merged stories:

*Add dependencies between artefacts in the PM*

During logical model conversion, we need to create a map in the
physical model capturing for each artefact:

- id of the dependent element
- archetype
- relation type

Note however that the full purpose of this transform is to resolve
this triplet into a relative path to create a dependency. So we may
not need to store this in the model and just have it in the transform
as an intermediate state.

For C# dependencies are written as the fully qualified element
name. We then need further processing to determine what the using
statements should be. As we do not have any usings at present this
will have to be handled in another story. For now we should just make
sure we record the dependencies.

*** Add archetype ownership model                                     :story:

Archetypes can be owned by either a part or directly by a backend. In
the future, they can also be owned by a product, a component, etc. We
don't need to worry about this yet. Parts are owned by a backend. We
need to ensure the current code supports this correctly. Archetypes
that live at the project level must be owned by the backend, not the
part.

*** Implement dependencies in terms of new physical types             :story:

- add dependency types to physical model.
- add dependency types to logical model, as required.
- compute dependencies in generation. We need a way to express
  dependencies as a file dependency as well as a model
  dependency. This caters for both C++ and C#/Java.
- remove dependency code from C++ and C# model.

Notes:

- in light of the new physical model, we need a transform that calls
  the formatter to obtain dependencies. The right way to do this is to
  have another registrar (=dependencies_transform=?) and to have the
  formatters implement both interfaces. This means we can simply not
  implement the interface (and not register) when we have no
  dependencies - though of course given the existing wale
  infrastructure, we will then need yet another template for
  formatters which do not need d

Merged stories:

*Formatter dependencies and model processing*

At present we are manually adding the includes required by a formatter
as part of the "inclusion_dependencies" building. There are several
disadvantages to this approach:

- we are quite far down the pipeline. We've already passed all the
  model building checks, etc. Thus, there is no way of knowing what
  the formatter dependencies are. At present this is not a huge
  problem because we have so few formatters and their dependencies are
  mainly on the standard library and a few core boost models. However,
  as we add more formatters this will become a bigger problem. For
  example, we've added formatters now that require access to
  variability headers; in an ideal world, we should now need to have a
  reference to this model (for example, so that when we integrate
  package management we get the right dependencies, etc).
- we are hard-coding the header files. At present this is not a big
  problem. To be honest, we can't see when this would be a big
  problem, short of models changing their file names and/or
  locations. Nonetheless, it seems "unclean" to depend on the header
  file directly.
- the dependency is on c++ code rather than expressed via a model.

In an ideal world, we would have some kind of way of declaring a
formatter meta-model element, with a set of dependencies declared via
meta-data. These are on the model itself. They must be declared
against a specific archetype. We then would process these as part of
resolution. We would then map the header files as part of the existing
machinery for header files.

However one problem with this approach is that we are generating the
formatter code using stitch at present. For this to work we would need
to inject a fragment of code into the stitch template somehow with the
dependencies. Whilst this is not exactly ideal, the advantage is that
we could piggy-back on this mechanism to inject the postfix fields as
well, so that we don't need to define these manually in each
model. However, this needs some thinking because the complexity of
defining a formatter will increase yet again. When there are problems,
it will be hard to troubleshoot.

*Move dependencies into archetypes*

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform. We need to figure out an interface that
supplies archetypes with the data needed to create the dependencies
container.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*Tidy-up of inclusion terminology*

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

*** Top-level "inclusion required" should be "tribool"                :story:

One of the most common use cases for inclusion required is to have it
set to true for all types where we provide an override, but false for
all other cases. This makes sense in terms of use cases:

- either we need to supply some includes; in which case where we do
  not supply includes we do not want the system to automatically
  compute include paths;
- or we don't supply any includes, in which case:
  - we either don't require any includes at all (hardware built-ins);
  - or we want all includes to be computed by the system.

The problem is that we do not have a way to express this logic in the
meta-data. The only way would be to convert the top-level
=requires_includes= to an enumeration:

- yes, compute them
- yes, where supplied
- no

We need to figure out how to implement this. For now we are manually
adding flags.

*** Implement meta-name validator correctly                           :story:

The logic in the meta- name validator is completely wrong. We are
checking for facet defaults without taking into account the logical
model element component. Thus, there are fundamental problems with the
meta-model validator that are not easy to fix. We need a facet default
for every logical meta-model element. That is, we need to loop through
all logical meta-model elements and ensure they have a facet default;
but this should only be done for meta-model elements which support a
facet default. This cannot be done until:

- we know which elements require a facet default;
- we have created a logical meta-model.

Merged stories:

*Set expectation for facet default*

At present we are only warning when a facet does not have a facet
default. This is because some facets do not have facet defaults (such
as build, visual studio, etc). However, we know this upfront so on the
facet factory we should set up the expectation. Then we can throw.

*** Update archetype generator to handle decoration                   :story:

Once relations have been moved into the generator type, we need to
create a special handling for archetypes.

Notes:

- instead of obtaining all of its relations from the archetype, we
  need to also query the logical model element. these will supply
  additional constant relations which need to be transformed into
  physical counterparts and resolved.
- relations in archetype can be ignored entirely for the purposes of
  artefact projection.
- the archetype transform can then be implemented as a "regular"
  transform, handling decoration, boilerplate, namespaces, includes,
  etc. We need to remove the includes from the stitch template.
- once all of this is done, remove support for includes and
  configuration from stitch.

*** Create a logical meta-model                                       :story:

At present we did a quick hack and created the notion of meta-names in
the logical model. In fact, what we really need is the idea of a
"meta-element". We don't need this to be done completely cleanly; the
meta-element is merely just an object really. We just need to have a
way to add:

- virtual meta-element property to the base type.
- static meta-element in each leaf.
- generated code which constructs a static meta-element for each
  descendant.
- meta-data to supply meta-element properties. We just need maybe two:
  stereotype and description.
- transform that generates the logical meta-model. It should be
  indexed by stereotype.

Notes:

- the LMM can be part of the boostrapping phase as is the PMM.
- the stereotype, which is defined in =ident= replaces the meta-name.
- the meta-name factory, transforms etc are deprecated.

Merged stories:

*Replace meta-model IDs with stereotypes*

We probably already have a story for this, check backlog.

*Generating a meta-model for dogen*

We are making use of coding meta-types a fair bit in dogen:

- we have meta-names, which we use for things such as indexing,
  formatter discovery etc.
- we need to know which types are generatable.
- we are associating meta-types with technical spaces (intrinsic).

It would be nice if somehow we were able to generate some basic
reflection code that enabled us to ask for a meta-model's element
meta-class. For example, given an element, we should be able to do:

: element.meta_class().intrinsic_technical_space()

The =meta_class()= method should be static and code-generated by
dogen. This still requires a lot of thinking though. Look for
reflection stories in backlog.

We could also have a way to access a collection of all
meta-types. This would be useful in order to generate a list of all
the valid stereotypes which as model elements. However, for this there
is some mapping required as we want
=masd::coding::meta_model::enumeration= to map to =masd::enumeration=.

Actually the right approach for this is to annotate all elements in
coding as "meta-elements". They would then have all of these
properties as features (stereotypes, etc). The code generator would
then generate static methods for each class with this information. We
could then return a =meta_class= from these classes, populated with
relevant data.

For this to work we need to define a =meta_class= meta-element, which
has properties such as:

- stereotype
- the comments become the documentation of the stereotype
- is generatable

Dogen then generates the appropriate code for these types, as required
by Dogen (pun intended).

Interestingly, we have been binding formatters against model elements
via the meta-name, but perhaps what we are really looking for is the
stereotype instead. The meta-name is more or less meaningless but the
stereotype is used by the UML profile. Of course we can't just call it
"stereotype because that could mean anything, but it is in effect some
kind of meta-type. The formatter binds against this, as does the UML
modeling element; the meta-name and the assets class is simply an
implementation detail.

Merged stories:

*Meta-element configuration*

It is becoming obvious that there is a need to explicitly model
meta-elements. A good example are the wale templates for handcrafted
types. Say we want to associate a wale template with =entry_point=. We
need one for C# and one for C++. We have the following alternatives:

- the user needs to manually supply these as meta-data parameters
  every time it creates an entry point.
- next level is to have profiles: we could define a profile with these
  parameters and the user is responsible for applying it to entry
  points as required. This is flexible because users could choose
  different wale templates. However, its painful that there isn't a
  default wale template.
- we could hard-code the default wale template into the field's value,
  so that you'd get that value if you don't supply any. At least now
  there is a default, but its hard-coded.
- finally, if we could create a =<<metaclass= in a diagram, we could
  then add the default values there _as data_.

A related point is to do with cases where the meta-element requires
specific configuration:

- entry point, interface: should be types only.

For these cases, it would be nice if we could associate a stereotype
with the metaclass and then all instances would inherit these.

This is actually simpler to implement than it may appear. We just need
to have a meta-element of type =metaclass=, with an annotation and a
name. The name of the metaclass is the element it is configuring
(e.g. =entry_point=). We could have a transform that sets the static
stereotype of the element based on the name for good measure. Then,
during the profile merging, we could simply merge first its annotation
into all elements of this type. Then we would continue with the
profile merging. We could also do a hack and have a =root_module=
=metaclass= which could be used for defaults. However, it is not clear
when we should use this as opposed to plain stereotypes. The good
thing about stereotypes is that we make the relationships explicit,
whereas with =metaclass= we are making them slightly less transparent.

If dogen had a way to set static properties, we could have a transform
that updated all elements with their metaclass, so that you could
simply do:

: element.metaclass().name();
: element.metaclass().annotation();

This should be very simple to do, we just need a way to know if an
attribute is static or not.

*Consider creating a meta-element for logical elements*

We probably either have a story for this or these ideas are scattered
over a few stories. At present we have a number of properties
associated with assets (i.e. logical) model elements which we set
manually (each of these probably has a separate story):

- meta-name
- stereotype
- whether a type is intrinsically generatable or not
- RGB colour
- whether the element supports dynamic stereotypes or not.

There are probably more. These are all functions of the meta-type we
are modeling. Just like we need "formatters" as a meta-type, we also
need a way to express these within the model itself. The ideal
solution would be to have a meta-type for these elements such that
when we code-generate, we can add the additional elements that are
specific to the logical model meta-types. However, the snag with that
approach is that these elements have all attributes of an =object=
plus these additional fields. We would have to create templates that
intersperse object functionality with this meta-data, resulting on a
lot of code duplication.

A better approach is to copy what we did for ORM. We can have
additional stereotypes associated with a meta-type, and those would
result in the population of meta-data information. If those are
present then we emit code for it (seems a bit much to create a facet
for it as we did with ORM).

Notes:

- we probably don't need the meta-name. We just use it to bind
  formatters to meta-types. We can use the "intrinsic stereotype" for
  this.
- we could use the same ORM-like approach for formatters and just
  extend object.
- we probably need a namespace for masd types. At present we have
  =templating= but these are actually masd/dogen specific templates,
  so they should also be in the same namespace.

Merged stories:

*Investigate use of =is_element_type= in stereotypes*

We seem to only be considering a subset of the elements when doing the
stereotype transform, yet its all working. Need to figure out why, and
if this is no longer required, remove this method.

*Add logical entity for meta-model elements*

We now have a clear pattern for all logical model elements: they need
a stereotype and an associated description, which must be registered
somewhere (to avoid duplication and for documentation purposes). This
is known at compile time - i.e. static.

We could easily create a logical model representation of these
elements and then allow users to annotate types with it. However, they
must still remain =objects= because we still need to be able to model
regular OO relationships.

*** Add file extensions to decoration                                 :story:

Create something really simple:

- extension groups
- extensions

Model this after modelines and modeline groups. We just need to define
an extension group that has all the extensions we have currently in
use. Extensions belong to a TS. Extensions can have a label. If there
is more than one extension for a given TS they must have a
label. Example:

=extension_type:odb_headers=

We then need to label archetypes with these. This is only needed for
cases where there is more than one extension for a given TS (c++
headers and implementation).

*** Move decoration to =text= model                                   :story:

Last sprint we thought that decorations belonged to the logical
model. We were partially right; the part of decorations that refers
only to the modeling of entities is correctly placed in the logical
model. However, the transformation of those elements into text needs
to be placed in the text model. And the output of those
transformations should rightly belong to the archetype set (preamble,
postamble) if not to the artefact themselves. However, for this to
work we need a way to associate technical spaces with artefacts. Then
we can simply ask for all technical spaces in a plane. Or
alternatively we could try to generate the decoration using only the
meta-data. Basically this needs to be done when creating either the
text model or the artefact repository.

*** Consider creating a label for generated files                     :story:

We could label all files which are not generated as "manual". Not
clear how exactly that would be useful.

*** Replace initialisers with facet-based initialisation              :story:

Now that we have facets, archetypes, etc as proper meta-model
elements, it is becoming clear that the initialiser is just a facet in
disguise. We have enough information to generate all initialisers as
part of the code generation of facets and backends. Once we do this,
we have reached the point where it is possible to create a new
meta-model element and add a formatter for it and code will be
automatically generated without any manual intervention. Similarly,
deleting formatters will delete all traces of it from the code
generator.

*** Injector types with regards to containment                        :story:

It seems we have two models for injectors:

- those where element containment is represented through nesting,
  e.g. XML, JSON, org-mode. These can of course be flat too, but its
  natural to represent elements as containers.
- those where element containment is represented through "links",
  e.g. Dia. When we represent containment through links, we need to
  create a graph of the elements and then transform them into a
  qualified path.

At present we left it to the dia injector to resolve the link
containment. It makes more sense to model the containment type in the
injection model and then to have a transform that does the graphing
for link models. We also need a transform that does the name nesting
for nested models. Both do nothing for the converse case. This will
simplify injector code.

Notes:

- linked models must supply the original model ID as well as container
  ID. Nested models may or may not supply this information.
- we should transform nested models into flat models as part of the
  injection chain. The final model should be a flat model.
- perhaps we should have a notion of a nested model and a nested
  element. This way the type system encodes this information.

*** Mine the build2 layout terminology                                :story:

It seems build2 is modeling a lot of concepts that are similar to ours
in project layout. We should use their terminology where possible.

Links:

- [[https://build2.org/bdep/doc/bdep-new.xhtml#src-layout][bdep-new source layout]]
- [[https://build2.org/build2-toolchain/doc/build2-toolchain-intro.xhtml#proj-struct][Canonical Project Structure]]

*** Make physical model name a qualified name                         :story:

At present we are setting up the extraction model name from the simple
name of the model. It should really be the qualified name. Hopefully
this will only affect tracing and diffing.

*** Add a PMM enablement satisfiability transform                     :story:

For now this transform can simply check that there are no enabled
archetypes that depend on disabled archetypes. In the future we could
have a flag that enables archetypes as required.

Merged stories:

*Investigate the use of solvers for enablement*

A long standing problem we've had in Dogen is how to solve enablement
requirements. This appears to be a well-researched problem within
MDE. One such solution is Alloy.

Since Alloy is written in Java it will not be easy to integrate it
within the Dogen workflows. However, perhaps we can use it as a
starting point to understand how SAT solving can be used to address
our problem. If we could create an output that targets alloy and then
get alloy to produce a solution for our problem, we can then try to
understand how the alloy language maps to SAT solving and remove alloy
from the process. This should be doable given we have very simple
needs.

Links:

- [[https://www.doc.ic.ac.uk/project/examples/2007/271j/suprema_on_alloy/Final%20Report/LaTeX/report.pdf][A Guide To Alloy]]: very simple guide on how to use Alloy.
- [[https://github.com/gsdlab/claferIG][claferIG GitHuub repo]]: Support for reasoning on Clafer models by
  instantiation and counter example generation.
- https://alloytools.org/: home page for Alloy.
- [[https://www.amazon.co.uk/Software-Abstractions-Logic-Language-Analysis/dp/0262017156][Software Abstractions: Logic, Language, and Analysis (The MIT
  Press)]]: Alloy book.
- [[https://github.com/AlloyTools/org.alloytools.alloy][Alloy GitHub repo]]
- [[https://github.com/necavit/li-sat-solver][li-sat-solver GitHub repo]]: Very simple C++ SAT solver
  implementation, based on the [[http://en.wikipedia.org/wiki/DPLL_algorithm][DPLL]] algorithm.
- [[https://sahandsaba.com/understanding-sat-by-implementing-a-simple-sat-solver-in-python.html][Understanding SAT by Implementing a Simple SAT Solver in Python]]
- [[https://codingnest.com/modern-sat-solvers-fast-neat-underused-part-1-of-n/][Modern SAT solvers: fast, neat and underused (part 1 of N)]]
- [[https://github.com/master-keying/minisat/][Mini-SAT GitHub repo]]: Production-ready MiniSAT. Forked off MiniSAT
  2.2, this repository aims at providing a production-ready version of
  the famous library.
- [[https://github.com/dobrichev/fsss2][fsss2 GitHub repo]]: Fast Simple Sudoku Solver 2.
- [[https://www.reddit.com/r/cpp/comments/94dkme/modern_sat_solvers_fast_neat_and_underused_part_1/][reddit: Modern SAT solvers: fast, neat and underused (part 1 of N)]]
- [[https://en.wikipedia.org/wiki/Conjunctive_normal_form][Wikipedia: Conjunctive normal form]]

*** Add a PM enablement satisfiability transform                      :story:

To start with, this should just check to see if any of the
dependencies are disabled. If so it throws. In the future we can add
solving.

*** KVPs with invalid field name still works                          :story:

As a test we created an invalid KVP:

: +#DOGEN masd.labelz.a_labelz=a,b,c

This should have failed because the name of the KVP is =label=, so
=labelz= shouldn't have matched. However there was no error. We are
probably adding the =z.= to the key. We need to check how variability
is handling this.

*** Add a PM transform to prune disabled artefacts                    :story:

We must first start by expanding the physical space into all possible
points. Once enablement is performed though we can prune all artefacts
that are disabled. Note that we cannot prune based on global
information because archetypes may be enabled locally. However, once
all of the local information has been processed and the enabled flag
has been set, we can then remove all of those with the flag set to
false.

In a world with solving, we just need to make sure solving is slotted
in after enablement and before pruning. It should just work.

This transform is done within the =m2t= model, not the =physical=
model, because we need to remove the artefacts from the =m2t=
collection.

*** Add primitives to feature selector                                :story:

It would be nice to be able to associate a primitive to the selector,
so that instead of:

:             ftg.enabled = s.get_by_name(fct.value(), enabled_feature);

We could simply do:

:             ftg.enabled = s.get_by_name(fct, enabled_feature);

This would also mean that you couldn't use a string by mistake.

*** Add the notion of a major and a minor technical space             :story:

When we move visual studio and other elements out of the current
technical spaces, we will need some way of distinguishing between a
"primary" technical space (e.g. C++, C# etc) and a "secondary"
technical space (e.g. visual studio, etc). We could use emacs'
convention and call these major and minor technical spaces.

This should be a property of the backend.

*** Add documentation to archetypes headers                           :story:

At present we are ignoring the documentation we supply with the
archetype. We need to populate the wale KVPs with it and make use of
it in the wale template.

*** Associate git details with components                             :story:

If the model of the component had access to the git repository
location then we could make use of it. The primary component could
have links to all of the secondary components; you can then point
dogen to the primary component and it would create the "standard" git
layout, e.g.:

: primary.remote
: primary.local
: secondary_1.remote
: secondary_1.local
: ...

If we had backup remotes, we could also setup the remotes
(e.g. bitbucket, etc). In effect, dogen would act product level
operations (build all components, run all tests, etc). It would know
for each component what the appropriate command is (e.g. build2,
cmake, dotnet, etc).

Notes:

- should we have a top-level =masd= binary for this kind of stuff?
  seems like there is a pretty obvious boundary between plain code
  generation and product level operations such as these. The =masd=
  component could be built using dogen's libraries, but at least this
  way we wouldn't have to link against git etc for all of these
  product level use cases.

*** Consider modeling primitive value as attribute                    :story:

At present we have used meta-data:

: #DOGEN masd.primitive.underlying_element=float

It may be more obvious if we model these as an attribute called
value. It would make the type of the primitive more visible.

*** Add the ability to cast an enum to the underlying type            :story:

As per stack overflow, is now not easy to convert an enum to its
underlying type. However, it also proposes some simple solutions:

C++ 11:

#+begin_src C++
template <typename E>
constexpr typename std::underlying_type<E>::type to_underlying(E e) noexcept {
    return static_cast<typename std::underlying_type<E>::type>(e);
}
std::cout << foo(to_underlying(b::B2)) << std::endl;
#+end_src

C++ 14:

#+begin_src C++
#include <type_traits>

template <typename E>
constexpr auto to_underlying(E e) noexcept
{
    return static_cast<std::underlying_type_t<E>>(e);
}
#+end_src

We could easily add these methods to our enums.

Links:

- [[https://stackoverflow.com/questions/8357240/how-to-automatically-convert-strongly-typed-enum-into-int][How to automatically convert strongly typed enum into int?]]

*** Add =enum= size constant for all enums                            :story:

A common use case is to declare an array of the size of an
enumeration. It should take into account that invalid is not a valid
value. We could declare a const named after the enum that contains the
size. In fact we could declare with and without invalid.

Actually for our use case we need to remove invalid, so this is not
needed. Just total count will do. In terms of the name, SO says:

#+begin_quote
Length() tends to refer to contiguous elements - a string has a length
for example.

Count() tends to refer to the number of elements in a looser
collection.

Size() tends to refer to the size of the collection, often this can be
different from the length in cases like vectors (or strings), there
may be 10 characters in a string, but storage is reserved for 20. It
also may refer to number of elements - check source/documentation.

Capacity() - used to specifically refer to allocated space in
collection and not number of valid elements in it. If type has both
"capacity" and "size" defined then "size" usually refers to number of
actual elements.
#+end_quote

C# calls it =Length=.

Links:

- [[https://stackoverflow.com/questions/300522/count-vs-length-vs-size-in-a-collection][count vs length vs size in a collection]]

*** Improve referencing status                                        :story:

We did a very quick hack to move inclusion status into the physical
model. However, there are a number of things that need looking at:

- we should make referability a meta-data parameter so that we can use
  profiles. We should also do the same for
  =wale_template_reference=. There is no advantage of using an
  attribute and we can save a lot of time by using profiles.
- note also that some archetypes are intrinsically non-referable:
  =cpp=, =CMakeLists= etc. Perhaps we could make this a property of
  the kind as well.

*** Merging of collections does not overwrite keys                    :story:

In variability, given a profile with a collection C and an element
with a collection K, the merge of the two collections will result in
duplicate keys if an entry exists on both C and K. We should take K.

*** Referability and logical model                                    :story:

We have modeled referability as a physical property but in reality its
a combination:

- at the logical model level, we know if a model element can be
  referred or not. We also know that referability works in sets:
  classes of elements can refer to each other but not across other
  classes. This requires building a proper taxonomy for referability.
- at the physical level we inherit the logical referability
  properties, but then in addition, we need to state that for each
  facet and each logical model element, there exists one and only one
  default archetype.

The domain model should reflect these findings.

Notes:

- we already have some kind of concept for this because we use this in
  the resolver. Investigate how its being used.

*** Remove empty tracing directories                                  :story:

At present when you add regexes for tracing filtering, we create a lot
of empty directories. It doesn't seem easy to stop the directory
generation but perhaps we could add the tracing directory to the file
transforms and run the "remove empty directories" transform over it.

*** Split enablement features                                         :story:

At present we are instantiating the =enabled= feature across the
entire =masd= template instantiation domain. This is a very
"efficient" way to do it because we only define one feature. However,
it also means its now possible to disable a facet or backend at the
element level. And worse, the binding point is global:

: #DOGEN masd.variability.default_binding_point=any
: #DOGEN masd.variability.generate_static_configuration=false
: #DOGEN masd.variability.instantiation_domain_name=masd

The right thing to do is to create four separate features, one for
the backend, one for the features and one for the archetype
(global). Then another one for the archetype, locally. Each with the
correct binding point.

*** Dimension vs view vs perspective                                  :story:

We need to find the definition for how these terms are used within
UML and see which one is more appropriate for MASD.

*** Private and public includes                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to hide some internal types from
users so that I don't increase coupling for no reason.
#+end_quote

NOTE: We should use the terms =internal= and =external= to avoid
confusion with C++ scopes. This follows Microsoft terminology for C#
assemblies.

At present we are making all headers in a model public. However, for
models such as cpp this doesn't make any sense since only one type
should be available to the outside world. What we really need is a
separation between public and private headers, a functionality similar
to =internal= in C#. In conjunction with using shared objects, this
should improve build times.

In order to do this:

- add a new config parameter: default visibility to private or default
  visibility to public. This is just so we don't have to mark all
  types manually - instead we just need to mark the exceptions.
- add two new stereotypes: =public= and =private=.
- add enum to sml: =visibility_type= (check with .Net for
  names). Valid values are =public=, =private=. Objects, enumerations,
  etc will have this enum.
- locator will now respect this value when producing an absolute file
  path. If public files go under =include/public=, if private files go
  under =include/private=.
- CMakelists for the component will add to the include path the
  private directory. Same for the spec CMakelists. Need to check that
  this not add to the global include path.
- CMakelists for the include files will only package the public
  headers.
- mark all the types accordingly in all our models. fix all the
  ensuing breakage. we will probably need to move forward on the IoC
  front in order for this to work as we don't want to expose
  implementations - e.g. =workflow_interface= will be public but
  =workflow= will be private; this means we need some kind of factory
  to generate =workflow_interface=.

More thoughts on this:

- we don't really need to have different directories for this; we
  could just put all the include files in the same directory. At
  packaging time, we should only package the public files (this would
  have to be done using CPack).
- also the GCC/MSVC visibility pragmas should take into account these
  options and only export public types.
- the slight problem with this is that we need some tests to ensure
  the packages we create are actually exporting all public types; we
  could easily have a public type that depends on a private type
  etc. We should also validate yarn to ensure this does not
  happen. This can be done by ensuring that a type marked as external
  only depends on types also marked as external and so forth.
- this could also just be a packaging artefact - we would only package
  public headers. Layout of source code would remain the same.
- when module support is available, we could use this to determine
  what is exported on the module interfaces.

*** Associate includes with model elements                            :story:

The right solution for the formatter includes is to supply them as
meta-data in the model element. This has the advantage that we can
then make use of profiles. At present we have one way to supply
includes: the primary and secondary includes:

: "masd.generation.cpp.io.class_header.primary_inclusion_directive": "<boost/property_tree/json_parser.hpp>",
: "masd.generation.cpp.io.class_header.secondary_inclusion_directive": "<boost/algorithm/string.hpp>",

This does a part of the job: we can associate up to two include
directives with one facet and element. However:

- by using this machinery we are effectively replacing the original
  include.
- the includes will occur for anyone who references the type. Though
  however, since the includes are applicable only to the class
  implementation this is less of a problem. Technically its still
  incorrect though because these are not the includes needed to use
  the type but the includes needed to define the type.

For formatters, we kind of need to make the includes only happen when
we are building the formatter. If we could have a similar machinery,
but without adding to types referencing the type, this would give us a
way to declare all of the formatters dependencies. Then, we could
switch to building all of the stitch boilerplate outside of stitch and
supplying it as a KVP.

*** Simplify the get for optional features                            :story:

At present we do something like this for optional features:

:            if (s.has_configuration_point(fg.archetype_overwrite)) {
:                a.enablement_properties().archetype_overwrite(
:                    s.get_boolean_content(fg.archetype_overwrite));
:            }

We could simply generate an extra get method which returns an optional
(=get_optional_...=). The main use case appears to be enablement.

*** Enable logging in unit tests from the environment                 :story:

At present in order to enable logging we need to change the macros
used in unit tests. It would be much nicer if one could setup a
environment variable with the log level. This would mean that in the
future when we have org-babel integration we could set the variable
from the org-file and have a link to the log from the org-file.

*** Name clashes between generated code and non-generated code        :story:

We named a type =archetype_generator=. This caused it to be redefined
because we also use that name in =test_data= facet. These clashes
should not really happen. The main problem is that we did not want to
use different namespaces in facets. We should then at least tell users
when they are about to create a name clash. For this we need a way to
get all the "registered" names and then compare the names the user
wants to define against those.

*** Conversion does not validate missing options                      :story:

When we fail to add the mandatory options in conversion, we don't get
a sensible error:

: $ ./dogen.cli convert dogen.logical.dia dogen.logical.org
: Error: Extension cannot be empty.
: Failed to execute command.

*** Cross-compilation for all targets                                 :story:

We need to move to cross-compilation to avoid having to have builds on
all supported OS's. We need to also be able to run all tests so for
this we need something like Wine. We should use clang on all OSs.

Links:

- [[https://github.com/ruslo/polly][GH repo]]: Collection of CMake toolchain files and scripts for
  cross-platform build and CI testing (GCC, Visual Studio, iOS,
  Android, Clang analyzer, sanitizers etc.)
- [[https://github.com/darlinghq/darling][darling GH]]: Wine but for OSX. Can be used to run DMGs (see readme).
- [[https://github.com/mstorsjo/msvc-wine][msvc wine GH]]: "This is a reproducible Dockerfile for cross compiling
  with MSVC on Linux, usable as base image for CI style setups."

*** Merge visitor with object                                         :story:

At present we are violating the principle that all meta-model elements
must be present in the user model; we are injecting visitors. However,
it can be argued that visitor is just a physical projection of the
object logical element. If we did this we would solve the
inconsistency without needing to add another element to user models.

*** Consider creating meta-types for transform, chain and context     :story:

These are clearly an established pattern within dogen. It would be
nice to make them visible. Advantages:

- we could have templates that define the class layout.
- we wouldn't have to define "typeable" everywhere, we can make sure
  there are only two archetypes for it.
- it would automatically be set to override.
- we would have different colours for transforms and chains.
- if the users supply the context and model we could automatically
  generate the correct includes, and add these to the apply
  method. This can either be done as a hack (as we do for say
  archetypes, etc) or we could add support for proper operations. It
  could be done via meta-data.

Notes:

- the =transform= meta-type should include dependencies so that we
  could compute the transform graph as part of the generation
  process. We need to rework all chains so that they can be code
  generated.

*** Consider creating a container for profiles                        :story:

At present we have placed all profiles in the profiles model and made
it non-generatable. This means we can use regular packages. However,
if we wanted to place profiles in a model which generates code and if
those profiles were placed in a package we would generate an empty
package. Ideally, we should be able to have a profile-specific
container for profiles which does not have an expression at the
physical level.

*** Consider creating a container for features                        :story:

At present we have regular namespaces containing features. Perhaps it
makes more sense to have a specialised container that stops users from
adding other types? Note that we don't have the same requirement as we
do for profiles, this is just to make things "neater".

*** Allow convert output to =std_out=                                 :story:

It would be nice to be able to supply the injector destination instead
of a full path to destination, and dump the ouptut to =std::out=. Its
painful when trying to create a new converter to have to deal with
files.

*** Consider creating a org-mode based tracing format                 :story:

We've already seen the power of org-mode for representing a code
generation model. This opens an intriguing question: is org-mode also
a good tracing format? At present we are tracing with JSON. This
works OK but we always have to JQ the result and the JSON mode in
emacs is not the most performant. If we could instead look at trace
files from org-mode we would have the full power of org-mode:

- we could create "indexes" with links to all dumps: a top-level org
  file with links to the chains and top-level files, which then link
  to other files.
- we could add clock tables to time each transform (through since we
  are serialising its not obvious if this has any purpose.
- we could use other org-mode tools for analysis such as org-roam.
- we could create tables to organise useful information such as
  timings.
- we could add links to the log file and all other files produced.
- we could add a link to the target model and reference models.

In effect, org-mode would provide us with a navigable (and diffable)
set of documents that provide a complete description of each run.

This work should wait for the PDM refactor as we do not want to have
to create helpers for all of the PDM types (lists, vectors, etc).

This could also be very useful as a logging format if we could
compress it to a single line as we do with JSON, but then expand it to
multi-line via a trivial conversion (e.g. as we do with JQ). For
example, if we escaped newlines, tabs etc, it should be possible to
trivially reconstruct the original org-mode document via simple
elisp. In this case, we could switch IO to use org-mode, as long as
there is an "escape" flag somewhere.

In addition, if we have a pointer, we could replace those with
org-mode links to take us back to the original definition of an
object. We could simply hash the pointer and use it as a property in
org-mode; then for all links, we just hash the pointer and create a
link. In addition, if the user provides suitable annotations, we could
do the same thing for IDs. That is, say we have a primitive
representing the qualified name for an entity; if we could somehow
know that, we could then create a link back to the entity (though,
sadly, not via the hash pointer).

If we had support for sizes, we could also create org-tables giving us
a breakdown in terms of size. This could use a gnuplot block or an R
block for graphical representation.

Links:

- [[https://emacs.stackexchange.com/questions/53416/draw-pie-chart-from-orgmode-table][SO: Draw pie chart from orgmode table]]

*** Consider allowing representation of namespaces in file names      :story:

Languages like .Net represent namespacing using dots rather than
separate folders. Perhaps we should support a mode of operation where
all files are placed in a single folder but have the namespacing
encoded in the file name. For example:

: /a_project/types/a.cpp
: /a_project/io/a_io.cpp

would become:

: /a_project/types_a.cpp
: /a_project/io_a_io.cpp

or, using dot notation, so we can distinguish namespaces from
"composite" names:

: /a_project/types.a.cpp
: /a_project/io.a_io.cpp

We do not have a use case for this yet, but it should be fairly
straight forward to add it. We just need meta-data support to enable
the feature and then take it into account when generating the file
names (e.g. instead of using =/= as a separator, use =.=).

Actually this is _almost_ already possible: we provide a facet folder
meta-data that is always used to generate a new folder. If however
there was a way for it to not generate a folder we could achieve
this. For example, say we had to supply:

: /types/

as the facet folder. Then the user could simply supply instead:

: types_
: types.

And no folder would be created.

Notes:

- see also the story on destinations.
- consider splitting this story into two: one is about how folder
  layout (physical) may need to match namespace layout (logical);
  another is related to allowing users to flatten facet
  directories. They have some connection, but its not obvious how much
  they overlap.
*** Handling of model name is incorrect                               :story:

At present we are handling all containers in one way, and model names
in another. For model names we check for the presence of the model
name on both model modules and simple names. For all other cases,
including containers, we do not have the container name in the
internal module path; we just append the simple name when flattening
or when building a file name. We should be able to use the exact same
logic for model names, but this will likely result in a lot of
breakage. We need to change it once we have finished the locator clean
up.

*** Product family projects                                           :story:

Just like it makes sense to operate at the product level (e.g. add
components, packaging, building, etc) there are certain operations
that may make sense at the product family level. For example, take the
Dogen product family:

- dogen
- C++ and C# reference models
- PDMs (possibly 3 or 4 products at present)

And so forth. A typical use case is that we make a change to one
product such as:

- updates to README structure
- updates to valgrind rules (however, remember that we want to be able
  to compose valgrind rules in the future)
- update to CMakeFiles (for example changing code coverage flags,
  spanning across CMake, CTest and scripts)

In this case we want to be able to make this change, test it in a
product and then regenerate the product family. One way to achieve
this is to say that we have a special git repo that houses the product
family. This has the product family model. When we load the product
family model, we can them operate on all products in one go. This
could be done by having relative product paths:

: product_family_repo
: product_a
: product_b
: ...

Alternatively we could support git modules in the product family
repo. This would allow us to do one single checkout for the entire
family. The downside is that we then start building inside of the
sub-repos (we never want to build at the product family level). This
may not necessarily be a bad thing.

Notes:

- we now have a clear understanding of the complete product model. Its
  hierarchical structure is as follows:

  - organisation
  - product family
  - product
  - component: primary or secondary. Git repos at this level.
  - backend: primary technical space (C++, C#), secondary technical
    spaces (CMake, Visual Studio).
  - part: implementation (src), public headers (include)
  - facet: types, etc.
  - archetypes.

  Its possible for a component and a backend to have facets and/or
  archetypes. Facets are a form of "relative" partitioning of the
  physical space, by "topics".
- the text model will have an instantiation of the MASD product model,
  from product onwards.

Merged stories:

*Introduce dogen projects*

At present we are manually configuring each dogen target, adding each
separately to the build system. Perhaps a better approach is to have a
dogen project file where one can configure all of the targets in one
go. We don’t necessarily have to call dogen directly – perhaps another
command line tool is responsible for invoking dogen? The problem here
is that we’d end up with all dogen models in memory.

At any rate, the project file would contain all models for a given
product. We could possibly run with “all” or “specific” whereby the
user would supply one or more projects to code generate. For all
properties that are common, we’d defined them only once somehow
(common regexes, log level, etc).

One interesting thing is that once we have support for projects we can
make things slightly more efficient:

- cache all system models and other data from filesystem;
- load exomodels only once for all references; first check to see if
  there is a cached version and if not execute the exomodel chain
  again. Actually we may even be able to go up the endomodel chain all
  the way up to merging.

These should be called "product models" rather than projects. We
should also consider how "product families" fit in this architecture.

*** Add enablement test in C#                                         :story:

At present we have probably broken enablement in C# due to the hackery
around physical space expansion. However all tests are green. We need
to define a profile in C# that disables a facet in order to ensure we
test enablement before we start hacking around with the enablement
transforms. It will most likely be red - we need to add the pruning
hack to get rid of disabled artefacts as we do in C++.

*** Investigate goxygen's functionality                               :story:

Seems like an interesting project:

#+begin_quote
Goxygen aims at saving your time while setting up a new project. It
creates a skeleton of an application with all configuration done for
you. You can start implementing your business logic straight
away. Goxygen generates back end Go code, connects it with front end
components, provides a Dockerfile for the application and creates
docker-compose files for convenient run in development and production
environments.
#+end_quote

Links:

- [[https://github.com/Shpota/goxygen][GitHub]]

*** Throw on invalid stereotypes for all model elements               :story:

At present we seem to throw on invalid stereotypes but not for all
modeling elements. We need to ensure we check this for all model
elements. For example, add the following to a =masd::enumeration=:

: some::crap

This will not throw.

*** Consider adding valgrind suppressions to PDMs                     :story:

We have a number of suppressions that are coming from boost log. Its a
bit painful to have to add the same suppressions to all products. It
would be nice if we could supply the suppressions as part of the
utility PDM and then have all products that depend on utility
automatically include it. Valgrind supports multiple suppression
files:

: --suppressions=/path/to/file.supp one or more times.

Links:

- [[http://valgrind.org/docs/manual/manual-core.html#manual-core.suppress][Suppressing errors]]

*** Configuration binds to element types                              :story:

At present we can check bindings for at most the "kinds" of elements
in the logical meta-model. However, in many cases we have features
that only make sense for certain meta-model elements. We could do this
fairly simply:

- add a "tag" to configurations that tell us what meta-model elements
  we have.
- add a "tag" to features that tell us what meta-model elements they
  support.

This approach could be generalised to support all binding types. It
just requires some thinking: =all= must match any tag, =global= must
match only one specific element, etc. We could perhaps have two levels
of tagging, or - even better - have multiple tags. So if all elements
are tagged with all, we could match on all. So effectively the binding
point becomes just a series of strings. This is very powerful because
now we do not have to know anything about the geometry of logical
space and yet we still are able to have very fine-grained checks on
bindings.

Once we have stereotypes in the =ident= model we can then bind to
them.

*** Consider adding cartridge M2T transforms                          :story:

In effect, T2T transforms. At present we are creating CMake scripts to
call ODB. This kind of makes sense because we see ODB as an external
code generator. However, we then have several issues:

- ODB files are not known to dogen. We could create some expectations
  of what these files are and where they will be located, but if the
  user changes anything problems will occur.
- if the user chooses a different build system, we need to add more
  targets to run ODB.

One can imagine that these problems will be common to all external
code generators we add in the future (e.g. protobuf, XSD tool etc). A
slightly different take on this would be to call the external tools
from within dogen. We would add meta-model concepts for these (not
exactly clear what those should be, to be fair). The gist of it is
that we would have a "cartridge formatter", with the following
responsibilities:

- shelling out and calling the tool with the correct environment
  setup;
- gather the generated files and place then in the right directories;
- gather the errors and report them back to the user.

The advantage of this approach is that we can then run other parts of
the pipeline on the generated files (e.g. perform any formatting that
may be required). For each cartridge we would have to teach dogen on
how to invoke it and what the expectations are.

Notes:

- config file should tell dogen about the application location if not
  in path.
- extensions should be registered as archetype kinds in the
  meta-model.
- we could probably decomission the ODB options files, which would be
  nice as they are a bit awkward. We could either generate them on the
  fly or not even bother to generate them at all and supply the
  options directly to ODB's command line. Basically we would need
  meta-model elements to capture the command line invocation for each
  element. We could even allow users to supply overrides to command
  line such as =--include-regex-trace=.
- we could also detect when files need to be rebuilt, although this is
  probably not trivial. Basically, by following the element
  dependencies we could determine if there is a need to call ODB or
  not. Alternatively, we could follow the include graph. Either way we
  need to make sure we rebuild when we should and this is like
  replicating a make inside of dogen. This is where a deep integration
  with build2 would be nice (e.g. via =libbuild2=).
- we can also add this to the new post-processing framework. In this
  case, the ODB post-processor (formatter) will run ODB, generate the
  files in the simplest possible way (e.g. all in the same directory)
  then load them up into the artefact for further processing. These
  files can be generated in a temp directory somewhere. The
  post-processor will generate N artefacts for one file.

Links:

- [[https://www.boost.org/doc/libs/1_72_0/doc/html/process.html][Boost.Process]]: we could use this library to start external
  processes.
- [[https://github.com/rajatjain1997/subprocess][subprocess GH]]: "The aim of this library is to allow you to write
  shell commands in C++ almost as if you were writing them in shell."
- https://github.com/build2/build2

Merged stories:

*Analysis on the modeling of generated files*

We have a principle that states that any file that is a part of a
project must have a meta-model element that models it. However, there
are cases where this break down:

1. stitch templates. In this case we have one stitch template per
   =.cpp= file we generate. It would be cumbersome to have to have 2
   model elements, one for the template and one for the output file
   since they will be near identical. However, if we did have it then
   we could allow associating stitch templates with *any* facet, which
   would be very convenient - not that we have a use case for this,
   mind you. In this scenario, you would create a stitch template as a
   proper template and then create the meta-model element and then
   within the element you would associate the template with the
   element on a given archetype. This is 100% correct, but then we
   will end up with 2N elements. For models such as =m2t.cpp= and
   =m2t.csharp=, this could mean more than 50 additional elements. A
   different way to look at this is to say that there is only one
   modeling element and then there are multiple facets; the stitch
   template is itself a facet. Or perhaps to allow files to live in
   the same directory, we could say there are multiple archetypes. In
   this case, we are saying that an archetype a0 could generate
   another a1 via a function f0. This could also be generalised so
   that we could generate a set A rather than just one file. If this
   could be an external process, we could then call ODB directly
   inside of Dogen. Though not sure this is a good idea.
2. wale templates.
3. ODB files.

Actually both 1 and 2 have now been modeled correctly so the only
problem is now ODB files. These will be handled by the cartridge
story.

Merged stories.

*Ignore ODB files automatically*

At present we are adding the following regular expressions to knitter
whenever we are using ODB with dogen:

:        --ignore-files-matching-regex .*sql
:        --ignore-files-matching-regex .*-odb.*)

We should inject the ODB files automatically into the list of expected
files. For a given element =foreign_key=, we will have a dogen file

: foreign_key_pragmas.hpp

We will also have the following ODB files:

: foreign_key-odb.cxx
: foreign_key-odb.hxx
: foreign_key-odb.ixx

The first file can either be on the =include/odb= directory or on the
=src/odb= directory (it is moved by the ODB target). All other files
are placed in the =include/odb= folder. Note that at present we are
using =cpp= extension rather than =cxx=.

In addition, on a multi-database environment we also have:

- =repository-odb-oracle.hxx=
- =repository-odb-pgsql.hxx=
- ...

Ideally we should also add the ODB include files to the master
includes. However, we probably need a separate master include file
just for ODB files.

One of the amazing side-effects of this approach is that we will
automatically delete any ODB files which are no longer required
(because we will not generate ignores for them). At present we are
manually deleting them.

This also means we can add the ODB files to the visual studio project
even before they get generated.

We should have meta-data configuration that describes the ODB files:

- their extension
- the location of headers and implementation
- the need to move the files

We should generalise this problem so that when we are using other
cartridges such as protobuf/grpc, we can extend it for these use
cases - expected cartridge files?

*** Consider making a separation between userspace and masd space     :story:

Userspace is actually quite a good name to represent the users of MASD
(copied from kernel land). However, we don't really have a
kernel-space. We need an equivalent (and it can't be modeling-space
because user space is also about modeling.

*** Integration of configuration and variability                      :story:

There are several threads going on in different stories that can be
unified. These are related to variability and configuration. We need
to find all of these stories and merge them at some point. The key
finding seems to be that the variability we are experiencing in dogen
as configuration is the same as the configuration all applications
require - e.g. app.config, INI files, etc. Because Dogen has
configuration embedded into models, its not very obvious that we are
doing the same thing. What this means is the following:

- we could, very easily, create a serialisation format for
  features. These would look a bit like the JSON format we had some
  sprints ago. However the key difference is that this time /we would
  generate the JSON files from models/. We would have a new part for
  Dogen: =configuration=. Under this part, we would store the
  serialised format for feature models. Note that these are the
  features, not their instances. The file names could be something
  really simple such as =[MODEL_name].features.[FORMAT]=,
  e.g. =dogen.variability.features.json= or
  =dogen.variability.features.ini= (probably not INI as I think its
  not rich enough to carry the features). We could then aggregate all
  of these files and place them in the dogen package. At start up
  Dogen would look for this directory and if found would load all of
  the features from there. This is exactly the same as having the
  features in code.
- the second aspect is access to configurations. Now, for dogen, the
  configurations live in dynamic instances of variability
  objects. This is fine. We could create a wrapper, very similar to
  what we are doing at present, that reads data from these objects and
  presents the developer with a c++ object. However, the key thing is
  that this is a very special case. In almost all cases, what we
  really have is a config file such as =app.config=, INI etc. The
  profile (as we call it at present) is the instance of the config
  file. The feature model is the type information required to generate
  a class that can read the config file and present an idiomatic
  object to the consuming user. Nothing stops us from allowing users
  to define feature bundles, then using those to generate classes that
  know how to read those features from config sources; and then
  serialising profiles into config sources. Finally, at run time, the
  code would merely read the serialised config source.
- all of this, of course, sounds extremely convoluted for the simple
  case. But the power of it is the more complex cases. We can now
  quite trivially create a consul loader that takes a config file and
  populates the KVP store. Similarly, we could also create a consul
  client that retrieves the config from the KVP store, always getting
  latest. We could even support both, depending on environment
  variables.
- configuration sources allow us to support many configuration
  backends: boost property tree, environment variables, etc. However,
  we can't have these as facets. We probably need to define an
  interface and then multiple implementations, with associated
  factories.
- note that the generation of JSON/INI files for the features is also
  only a dogen requirement. For the "userspace" cases we can define
  the features in a diagram and then create the instantiations in the
  same diagram as profiles and then code-generate the profiles as both
  INI files and c++ code to read the files (e.g. we only need to code
  generate the configuration). However, for dogen this is not possible
  (by design).
- while we're at it, we should also create the notion of
  "configuration sets". These are multiple instantiations of the
  feature model and could be used to model environments.
- one way to flatten and aggregate both features and profiles to make
  them suitable for code generation on a single file is the creation
  of a meta-model element that gathers them. We could create a
  transform that finds all features in a model and populates the new
  meta-model elements with them (like say a feature initialiser at
  present). This would also allow for a clean way to distinguish
  between models that use features without code generation and models
  that require code generation.
- configuration is strictly a KVP model. It should not support object
  graphs.

With this approach we can finally unify the world of code generation
variability with the world of application variability.

Links:

- [[https://github.com/oliora/ppconsul][ppconsul GitHub]]: C++ client for Consul (http://consul.io)
- [[https://github.com/david-antiteum/consulcpp][consulcpp GitHub]]: A C++ library that implements the Consul API
- [[https://github.com/oatpp/oatpp-consul][oatpp-consul GitHub]]: oatpp client for consul https://oatpp.io/
- [[https://www.consul.io/api/libraries-and-sdks.html][Consul - Client Libraries & SDKs]]

*** Rename main Dogen package in Debian                               :story:

At present we seem to have called our package =dogen-applications=:

: $ apt-cache search dogen-applications
: dogen-applications - The Domain Generator - Generates source code for domain driven development.

We should try to call it just =dogen=.

Merged stories:

*Rename debian package*

At present our package is called =dogen-applcations=. Since there will
only be one dogen application/package, this is a confusion name. We
should rename it. Names:

- masd-dogen

*** Rename facets                                                     :story:

We originally called our support for =std::hash= just =hash= and our
support for =boost::serialization= just =serialization=. The problem
is:

- we may want to also support =boost::hash=.
- we may want to support other serialisation types.

We should rename these. Perhaps:

- =std_hash=
- =boost_serialization=: a tad verbose, but quite explicit.

In addition, =io= is very misleading as the facet is not supposed to
do I/O proper (e.g. serialisation) but more pretty-printing or debug
dumping. So perhaps =pretty_print=.

Test data is a strange name. We need something slightly more idiomatic
such as perhaps sequence? We need to look into STL generator
terminology. We should also look into Rx and transducers - these
should be pluggable into these. Ranges also come to mind.

Merged stories:

*Rename the =io= facet*

IO is a very bad name for this facet. Everyone knows this as
"pretty-printing". We should call it pretty-printing and use =pp= as
the short name (folders, etc).

*** Add merging code generation support                                :epic:

#+begin_quote
*Story*: As a dogen user, I want to manually change some code in
generated files so that I can add functionality that is missing in
dogen.
#+end_quote

At present it is not possible to manually add methods to a class that
was code generated; one must stop code generating the class and
maintain the whole class manually. This is made even more painful by
the fact that one cannot add support for IO etc for these types
manually.

However, in a lot of cases it makes sense to have a combination of
manually generated and code generated code:

- value objects need helper methods such as for example boolean
  properties (e.g. =is_empty=) that make use of other properties, or
  simple methods such as population etc that really belong in the
  object rather than an external service;
- services sometimes need state and it would be good if we could
  manage that via code generation.

For this we need a merging code generator: that is, a code generator
that is aware of code that was crafted manually and does not overwrite
it - but instead "intelligently" merges manual with code generated
code.

From the beginning we avoided this because we thought it would be too
complicated for dogen. However, its becoming apparent that this is a
needed feature for the real world - there are many cases where we are
working around this deficiency. A few solutions are possible:

- let the code generator manage the header file and create two types
  of CPP files, one which includes the other: a manual and an
  "automatic" one. This would effectively separate the two types of
  code. For this dogen would have to be able to generate complex types
  in operations (e.g. we'd have to solve the lack of support for
  =const std::string&=).
- use clang to do the merging. this probably means adding some kind of
  attribute to every method - possibly using C++ attribute support
  (e.g. =[ [generated ] ]= and/or =[ [ manual ] ]= (spaces due to org
  mode). We could then say to clang: read current state of the file,
  grab every non-generated method and copy them across to the newly
  code generated file. Merging could be the final stage before
  writing. In addition, we should also have some dynamic extensions to
  determine which files require merging. The dynamic extension could
  be populated automatically (e.g. grep for the manual attribute) or
  manually. Note that using clang to do merging will make things a lot
  slower so we probably want to know up front which files need to be
  merged to avoid doing spurious work.

Notes:

- include management would be a mix of manual versus automatically
  generated. This is not possible because there will be no way to
  determine which one is which. To solve this problem we need to allow
  users to add include files from the dynamic extensions and get those
  processed like all the other includes. In the new world this means
  adding includes to the formatter settings. These are local
  settings. As at present, we cannot identify a use case for adding an
  include file for all types, so there is no need to support this
  feature at the global settings level. Thus this fits nicely with the
  existing settings infrastructure.
- merging could be done without needing clang, which would also make
  it cross language. All that is required is for the language to
  support some kind of meta-data to mark a method as "manually
  generated". This could even be done using comments but this is not
  ideal. The process would then be: dogen would open up an existing
  source file and locate the attribute; then look for a open brackets
  to indicate the start of the method (={=) and then find the matching
  close brackets (=}=). We could keep a counter and increment it when
  a new open bracket is found and decrement it when a close bracket is
  found. When its zero we are done. All the code from the attribute to
  the close brackets would be lifted. A very simple regex matching
  would be done to find the method name - or perhaps some trivial
  parsing could be done, but it should be kept as simple as
  possible. The objective is simply to figure out the method name. The
  method is copied across and stored in the =cpp= model, in the
  correct method. When code generating, if a method is marked as
  "manually generated" and if there is implementation content, we dump
  that; otherwise we generate the skeleton of the method as if it was
  not "manually generated". We could also create a very simple spirit
  parser that only knows of comments, function names and function
  bodies.
- merging could be done as part of yarn, in meta-data. That is, we
  could annotate the merged method into language specific properties
  in the meta-data and then query those in the language specific model
  generation. We could have another yarn workflow to look for files;
  it could use the meta-data for file path. The extension will tell it
  what "function parser" to use. We could literally look through the
  meta-data extensions looking for file path, and for each run the
  "function parser"; it will return a set of "manual" functions. These
  we can then slot into the meta-data and reuse later on. Actually, we
  can't use meta-data for this given the existing convention that
  meta-data is constant. However, nothing stops us from adding the
  required properties to yarn directly (e.g. we could have an
  =operation= which has a language specific container of
  implementations), to be used by formatters. Interestingly, this then
  seems to share some logic with method helpers. That is, if we could
  supply the stitch template as an operation (per facet, per
  language) and if we could annotate the operation somehow as
  "external" to the object, helper methods could use the same
  infrastructure. But perhaps this doesn't make a lot of sense since
  for helper methods we need to run the formatter whereas for merging
  we already have the final form of the code and we just need to carry
  it along to dump it in the formatter.
- the proper technical name for methods that can be manually edited is
  "protected regions". Merging is not a technical term according to
  MDSD at least.
- the act of checking the generated files vs the user edited files is
  called "model synchronisation" and its part of incremental
  transformation (according to [[https://pdfs.semanticscholar.org/7eca/ca8db190608dc4482999e19b1593cc6ad4e5.pdf][Czarnecki and Helsen]])

We should also address the MDSD comment:

#+begin_quote
If, for performance reasons, or because the target language doesn’t
offer any options for consolidating different artifacts, handwritten
code must be inserted into generated code directly, the introduction
of protected areas is inevitable. Please do this only if such
exceptional conditions require this approach!
#+end_quote

- We will need to support multiple merging strategies, depending on
  the use case:
  - simple extension: when we just want to add a method or two to an
    otherwise generated object. For this, protected regions at the
    function level is probably enough.
  - large extensions: when we want to add lots of functionality but
    there is a small component of code generation. For this we want
    some large blocks of protected regions that the user manages
    (includes, anonymous namespace, class, etc. The user can then put
    whatever it wants in those).
  - private inheritance: we could create a private/internal base class
    with all of the generated code and then inherit privately from it
    in a hand-crafted file. Only available on C++.
  - partial classes: one definition is code generated, the other
    hand-crafted. Only available in C#.

- interestingly, it seems that merging is a variation of a wale
  template. Basically we can generate the file, slot in the GUIDs for
  the protected regions as wale keys, then read the protected regions
  from the file system into a wale KVP container and then instantiate
  the generated file as if it was a wale template.

Links:

- [[https://arxiv.org/pdf/1509.04498.pdf][A comparison of mechanisms for integrating handwritten and generated
  code for object-oriented programming languages]]
- [[https://gitlab.com/mtekman/org-tanglesync.el][org-tanglesync GH]]: "A package to pull external changes into an
  org-mode source block if that block is tangled to an external file."
- [[https://github.com/xieyuheng/tangle-rs][tangle-rs GH]]: "A collection of tools to do tangle in rust."
- [[https://emacs.stackexchange.com/questions/45164/does-org-have-any-inverse-tangle-operations-e-g-for-collaborating-with-non-or][Does org have any “inverse-tangle” operations e.g. for collaborating
  with non-org users?]]

*** Consider renaming profiles                                        :story:

The correct MDE terminology for feature selection is configuration. We
have used this term within the variability model with a similar
meaning. We tried to avoid reusing the name so that its meaning is not
dependent on context, but it seems its worthwhile giving end users the
canonical MDE term and using something else internally.

In "Systems variability modeling : a textual model mixing class and
feature concepts", the authors describe the following concepts:

1. Meta-Features Model. Previous researches did not mention the
  Meta-Models Clearly; they mentioned it as features that may contain
  more than one sub features.
2. Features Meta-Model; this model is predefined and domain
   independent. It defines different domain features with their
   relations.
3. Feature Model: Compact model of features diagram and feature
   constrains. It is an instance of the Features Meta Model.
4. Feature diagram; Graphical representation showing each feature and
   its relations with its subs. And
5. Feature’s configuration; Set of selected features producing a
   release in SPL. Configuration is permitted with feature model and
   preserves features’ constrains.

This may mean that we should have a features meta-model and a features
model, therefore resolving the issue of the naming; we are in the
meta-model dimension within the variability model, and in the model
dimension when instantiating a user model.

One way to understand features in Dogen is as follows:

- the variability model is a features meta-model because it defines
  the shape of all features that can be created within Dogen.
- feature bundles and templates are a feature model - that is, they
  are instances of the feature meta-model.
- the configuration model is an instance of the feature model and is
  implemented as profiles and profile templates. From a SPL
  perspective, the configuration model is associated with a product.
- the product component models makes use of the configuration model
  defined for a product. All products must follow dogen's product
  meta-model.

*** Consider renaming feature bundles                                 :story:

It seems that the approach used by OOFM could be used to improve the
terminology in the variability model. We need to read this paper:

V. T. Sarinho and A. L. Apolinario (2010), “Combining feature modeling
and Object Oriented concepts to manage the software variability”, IEEE
International Conference on Information Reuse and Integration (IRI),
pp. 344-349.

*** Consider creating a generic host at modeling time                  :epic:

Instead of just introducing DI as a meta-pattern, we should look into
the entities used in the dotnet core model for hosting and see if we
can extract the entire hosting logic as a meta-pattern. The idea being
that you could then annotate entities with the correct stereotypes and
we would then code-generate plain c++ code which performs the same
role as the library code in C#.

Notes:

- we could note down all constructors on all annotated classes, create
  a graph and then generate a method that creates instances of each
  class, in graph order, supplying parameters to constructors as
  required. We could even detect smart pointers and determine when to
  =new= and when to use move semantics.
- there are a number of services provided out of the box in
  dotnet. For example, kestrel the web service is one of them. We
  could also provide some of these services (gRPC, etc) but as code
  generation. Users would configure via variability in their models
  what services are required and code generation would initialise
  these services.
- the host starts services in order, and stops them in reverse
  order. This is so that if there are dependencies between services
  these are respected.
- =application_lifetime= is an out-of-the-box service provided by the
  framework for other services to listen to start and shutdown
  events.
- =host_lifetime= is an out-of-the-box service that controls when the
  host stops and starts. For example by listening to termination
  events (=SIGTERM=, ctrl-c, etc). Its job is to tell the host to
  start and stop.
- =host_environment= is an out-of-the-box service to read different
  configuration sources such as the environment, config files, etc.
- application versus services: application is the user code. The
  application uses services to implement its functionality. Some
  services are provided out of the box, others can be implemented by
  the user. Services are registered against a DI container, which it
  itself a service.
- service collection is the DI container for dotnet core. You can
  register services with it. You can also request services. This is
  only expected to be done at the top-level, with the remaining
  services (inter-services dependencies) to be resolved implicitly as
  part of the dependency graph. In C++ we have two approaches:

  - the move semantics approach: the user requested =type_x= in the
    constructor. We can just create one on the fly and supply it.
  - the pointer/references approach: the user requested =type_x&= in
    the constructor. In this case we need to create the type at the
    scope of the =host_lifetime= (not necessarily in it, but with the
    same scope or smaller) and then supply the reference. The object
    will have to be a pointer, or else we need to create the object
    even before the user asked for it.

- in dotnet there are four lifetimes available to services in the
  instance collection:

  - instance: you add a specific instance of a service and that
    instance will be returned on each call. You need to manually add
    this one instance. In c++ this would require a pointer.
  - transient: creates a new instance every time the user requests
    this type.
  - singleton: you register the type and the service collection will
    then instantiate it, once. If the type requires other types, the
    DI is responsible for instantiating those as well.
  - scoped: the instance of the object exists within a scope. If the
    scope does not yet exist, DI has to create the scope and place the
    object in it. Anyone requesting objects on that scope will get the
    same object. The classic example for scoped is in HTTP processing,
    where the HTTP context is created for each individual request, and
    within that context you want to reuse created objects.

- ideally you want to make services such as databases, message queues,
  etc also available as part of the DI mechanism. The code generator
  is responsible for creating the glue code that reads the config from
  the config file, instantiating the resource (say database
  connection) and supplying it to the using application as
  required. All of this is performed transparently by generated code
  that can be trivially debugged and is part of the application
  itself, not library code. This code should also use the conventions
  of the platform it instantiates. For example, say you are using
  ODB. The DI will generate code such as:

#+begin_src c++
    using odb::oracle::database;
    std::unique_ptr<database>
        db(new database("northwind", password, "XE", "localhost", 1521));
#+end_src

  However, the database name, password etc is all supplied as part of
  the hosting context, and the hosting context will retrieve it by
  using the configuration chosen at the modeling stage (e.g. JSON
  file, INI file, etc). Similar example but for consul:

#+begin_src c++
using ppconsul::Consul;
using namespace ppconsul::agent;

// Create a consul client that uses default local endpoint `http://127.0.0.1:8500` and default data center
Consul consul;
// We need the 'agent' endpoint for a service registration
Agent agent(consul);

// Register a service with associated HTTP check:
agent.registerService(
    kw::name = "my-service",
    kw::port = 9876,
    kw::tags = {"tcp", "super_server"},
    kw::check = HttpCheck{"http://localhost:80/", std::chrono::seconds(2)}
);
#+end_src

  In this case we need to generate the code for registering the
  service. The port, etc can be obtained from configuration (for
  example from the environment variables which are populated on the
  host environment).

- We can have multiple configuration sources:

  - level 0 or bootstrapping: config file with application. For
    example, this can contain the consul settings.
  - level 1: consul contains the rest of the configuration.

  If we associate the configuration with the service, this will enable
  us to generate the correct code for each service.
- the hosting environment must support an "in-memory" host out of the
  box for cases where you want to hook it into tests.


Links:

- [[https://andrewlock.net/introducing-ihostlifetime-and-untangling-the-generic-host-startup-interactions/][Introducing IHostLifetime and untangling the Generic Host startup
  interactions]]
- [[https://andrewlock.net/controlling-ihostedservice-execution-order-in-aspnetcore-3/][Controlling IHostedService execution order in ASP.NET Core 3.x]]
- [[https://github.com/oliora/ppconsul][GH ppconsul]]: "A C++ client library for Consul. Consul is a
  distributed tool for discovering and configuring services in your
  infrastructure."
- [[https://andrewlock.net/comparing-startup-between-the-asp-net-core-3-templates/][Comparing Startup.cs between the ASP.NET Core 3.0 templates]]

#+begin_src plantuml :file hosted-service.png
@startuml
class host
note top of host
    An host is an object that encapsulates App resources such as:

    - logging
    - configuration
    - hosted services.

    When a host starts, it calls an async start method on all hosted services.

    The main reason for including all of the app interdependent
    resrouces in one object is life time management. It provides control over the
    app start and shutdown.
end note

class host_builder
note top of host_builder

    The job of the host builder is to setup the host. This includes
    reading configuration from all of the supported configuration
    sources (for example XML file, INI file, possibly database,
    etc). It also includes the processing of environment variables. C#
    uses a convention that variables starting with DOTNET_ are
    read. It also processes the command line args supplied to the
    process.

    The host builder is also a DI container.

end note

host_builder o-- host

class hosting_environment
host_builder o-- hosting_environment
host o-- hosting_environment

note top of hosting_environment

    Contains all of the environmental information for a host.

end note

class host_lifetime {
    void wait_for_start
    void stop
}
class console_lifetime extends host_lifetime
class systemd_lifetime extends host_lifetime

host_lifetime o-- host : notifies the host of start/stop events

note top of host_lifetime
    Listens to certain events and uses those to manage the life time
    of the hosted service. For example, can listen to SIGTERM or
    ctrl-c and then relay to the service the termination. Inheritors
    have different policies (listen to different things). In general,
    their objective is to manage the life time of the host.
end note

interface application_lifetime
host_lifetime o-- application_lifetime : propagates lifetime events

note top of application_lifetime
    Manages the life cycle of individual applications running within a
    host. Responsibilities include the management of cancellation
    tokens against which callbacks can be registered to know of when
    the application is going to stop.
end note

interface hosted_service {
    void start_async
}

note top of hosted_service
  A worker service is a long running service in dotnet
  terminology. hosted_service is the implementation of the service.
end note

host o-- hosted_service : manages one or more
class user_hosted_service implements hosted_service

user_hosted_service o-- application_lifetime : registers to receive lifetime events
user_hosted_service o-- hosting_environment : accesses configuration via

@enduml
#+end_src

#+RESULTS:
[[file:hosted-service.png]]

*** Consider generating dependency injection code                      :epic:

If one could mark constructors as =injectable= in a diagram, we could
then generate something like a castle windsor container and do all of
the management of dependency injection from generated code. We also
have access to all interfaces and their implementations so a lot of
the clever logic done at run time by castle/guice etc could be done in
the generated code.

For this to work we need some kind of DI library with a generic
interface; we can then do as we do in C# and have a number of
"installers" per library which add classes to the container, against
an interface. Dogen is basically responsible for:

- creating a top-level installer in the executable which calls all of
  the installers of all dependent libraries (internal and external to
  the current product, but dogen generated). We could have a flag to
  tell us if a model requires installers or not.
- creating each library's installer. For this we need two things: mark
  a type as an interface and split out registration code from the
  hand-crafted class. Instead we should just have all of the classes
  that need registering against an interface doing it in the
  installer, which we code generate. We also need to mark the
  interface as "dependency injectable" so we know we need to generate
  DI code.
- finally, from a consumption perspective, we can then call the DI
  container and request all instances of the interface we require. We
  should not use DI for all construction; just for the cases where an
  interface is used. The binary creates the container and supplies it
  to whomever needs to resolve an interface.

Actually, another way to think of this is that DI is a meta-pattern
and not a pattern; one of the biggest problems with Castle is the
amount of run-time trickery it does, making it really difficult to
understand what is happening behind the scenes and why things are not
working. For the vast majority of use cases, we know all of the
information at compile time and all we want to avoid is to manually
generate the code wiring things up. It would be a much better approach
if dogen generated this code, making it look just like regular code
which can be inspected and debugged, and move the DI to a compile time
phenomenon. We would still require all of the elements of castle, but
now they become meta-entities describing the relationships between
types. We then generate code that instantiates types in the correct
order. We could simply create factories that call the appropriate
constructors and have these factories be completely
code-generated. This also means that users must declare the
constructors correctly in dogen models in order to get the factory to
work; note that we will be doing this mostly for hand-crafted code. We
need to do an analysis of the different components in Castle in order
to model them.

Links:

- [[https://github.com/boost-experimental/di][Boost.DI]]: not part of boost yet [[https://github.com/boost-experimental/di/issues/229][and not clear when it will be.]]
- [[http://wallaroolib.sourceforge.net/][wallaroo]] (more run-time DI)
- [[https://github.com/google/fruit][fruit]] (tutorial [[https://github.com/google/fruit/wiki/tutorial:-getting-started][here]])
- [[https://gpfault.net/posts/dependency-injection-cpp.txt.html][Dependency Injection in C++ Using Variadic Templates]]
- [[https://github.com/ybainier/hypodermic][hypodermic]] (more run-time DI)
- [[https://riptutorial.com/Download/castle-windsor.pdf][Castle Windsor tutorial]]
- [[https://github.com/castleproject/Windsor/blob/master/docs/installers.md][Windsor docs on installers]]. See also [[https://github.com/castleproject/Windsor/blob/master/docs/ioc.md][ioc]] and the docs directory in
  general.

*** Formatters can only belong to one facet                           :story:

Up to know there was an agreement that generation space was
hierarchical and formatters could only belong to one facet. This has
been true until now, but with the addition of CMake support to tests,
we now have an exception: we need to honour both the tests facet and
the cmake facet. If either of them are off, then we should not emit
the CMake file. This means that we need to somehow map one formatter
to multiple facets. For now we just hacked it and used one of the
facets. It means that if you disable CMake but enable testing you'll
still end up with the testing CMake file.

*** Model "types" and element binding                                 :story:

It seems clear that we will have different "types" of models:

- product models, describing entire products.
- component models, which at present we call "models". These describe
  a given component type such as a library or an executable. Thus,
  they themselves have sub-types.
- profile models: useful to keep the configuration separate. However,
  it may make more sense to place them in the product model, since its
  shared across components?
- PDMs: these describe platforms.
- generative models: generate extensions to code generators.

At present there is no concept of model types, so any meta-model
element can be placed in any model. This is convenient, but in the
future it may make things too complicated: users may end up placing
types in PDMs when they didn't meant to do so, etc. What seems to
emerge from here is that, just as with variability, there is a concept
of a binding point at the model level too. That is, meta-model
elements are associated with specific model types (binding element?).

In an ideal world, we should have a class in the meta-model that
represents each model type. We then instantiate this class within one
of the dogen models to register the different model types. Its
code-generation representation is the registration. It also binds to
all the meta-model elements it binds to. This can be done simply by
creating a feature that lists the stereotypes of the elements
(remember that these are then registered too, because we will generate
the meta-class information as we generate the assets model). Then, we
can ask the model type if a given element is valid (check a set of
stereotypes).

Formatters are themselves meta-model elements, and they bind to other
meta-model elements (which raises the question: which meta-model
elements are bindable? we can't allow a formatter to bind to a
formatter...). Perhaps we need another type of model, which is a
"generation model". This is where we can either declare new technical
spaces or add to existing technical spaces; and declare new facets and
formatters. We should be able to add to existing facets and TSs by
allowing users to specify the TS/facet when declaring the
formatter. If not specified, then the user must declare a facet in the
package containing the formatter. Similarly with TSs.

Note also that the formatter binding code is "inserted" directly
during generation into the CPP file. Its not possible to change
it. Same with the includes. This ensures the user cannot bypass the
model type system by mistake. Also, by having a formatter meta-model
type, we can now declare the header file as we please, and ensure the
shape of the implementation. Now, the stitch template can be
restricted to only the formatting function itself; the rest is
code-generated. We no longer need wale templates. This will of course
require the move to PDMs and the removal of the helper code. This also
means that anyone can declare new meta-model elements; they will
register themselves, and correctly expand across archetype
space. However, we do not have the adaption code nor do we have
containers for these modeling elements. We need a separate story for
this use case.

Destinations are meta-model elements too. In the generation.cpp model
we will declare all the available destinations:

- global
- src
- include
- tests

etc. The formaters bind into destinations. Formatters belong to facets
in the archetype space, which express themselves as directories in the
artefact path when we project from archetype space into artefact
space. More generally: assets in asset space are projected into the
multidimensional archetype space. Archetypes are projected into
artefact space, but the dimensions of archetype space are flattened
into the hierarchy of the filesystem.

We also need a concept of artefact types. These mainly are needed for
file extensions, but conceivably could also be used for other
purposes.

Notes:

- the binding should be done at the streotype level, not model
  element.

Merged stories:

*Model types and element binding*

Once we introduce the concept of model types (e.g. product, component,
possibly different types of components), we should also take into
account that some model types don't support some model elements. It
should be possible to declare which model types support which element
types such that if a user tries to use an invalid model type, we get a
sensible error. In effect, this is a binding problem at the meta-model
level.

One of the component model types should be "generative
components". These are used to augment the code generator. We need to
look at the thesis on generative development to look for terminology.

We should also have model types for terraform, etc.

One way to implement this is to add a feature at the element (once we
have a way of expressing dogen meta-elements) that lists the supported
model types for a given element.

*logical models could have a model classification*

Consider creating an enumeration for model classification (e.g. type
of the model):

- relational model
- core domain model
- generic sub-domain model
- segregated core model

This still requires a lot of analysis work. This is kind of a model
level stereotype which can be used by the code generator for example
to determine which models are compatible. It could also be used to
determine what facets can be enabled/disabled.

*Merged with modes of operation story:*

Create "modes" of operation: relational, object-oriented and
procedural. they limit the types available in yarn. relational only
allows built-ins plus relational commands (FK and PK; FK is when
using a model type, PK is a marker on a property). procedural only
allows built-ins plus model types. we will need pointer support for
this. object oriented is the current mode. the modes are validated in
the middle end.

*** Project layout analysis                                           :story:

We should probably look at the layout of a few projects and see if our
meta-model covers these cases.

Links:

- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1204r0.html][Canonical Project Structure]]
- [[https://www.reddit.com/r/cpp/comments/8qzepa/poll_c_project_layout/][Poll: C++ project layout]]
- [[https://www.reddit.com/r/cpp/comments/996q8o/prepare_thy_pitchforks_a_de_facto_standard/][Prepare thy Pitchforks: A (de facto) Standard Project Layout]]
- [[https://github.com/vector-of-bool/pitchfork][Pitchfork is a Set of C++ Project Conventions]]
- [[https://mariuszbartosik.com/directory-structure-for-a-c-project/][Directory Structure for a C++ Project]]

*** Consider moving compatibility mode to feature model               :story:

Given that compatibility mode only really applies to features, we
should just have the flag in the feature model itself?

*** Model SQL scripts as meta-model entities                          :story:

At present we are adding SQL scripts to the relational model under the
=sql= directory. These should be part of the model. We need meta-types
to represent these files. For now they just need to generate an empty
file - or perhaps just the SQL modeline and decoration. They should
also be marked as handcrafted. We also need to add a part for SQL.

*** Model lisp scripts as meta-model entities                         :story:

We are using lisp scripts in the dia and templating projects. These
need to be modeled and generated. Generation can have just decoration.

*** Consider adding descriptions to feature bundles                   :story:

It would be nice if we could add the feature bundle as an entry into
dumpspecs, with an associated description. For example, say for
=masd.generation.decoration=, explaining what a decoration is.

*** Move models into the project directory                            :story:

At present we have a models directory in each component of a
product. However, perhaps it makes more sense to have it as a
subdirectory of the component itself. This is because in an ideal
world, we should create a package for the component with the model and
the header files as well as the binaries, allowing users to consume
it:

- in the Dogen case, it means users can create plugins for Dogen;
- in the PDM case, it means users can make use of the PDM in their own
  models;
- for user models, it means you can consume a product in another
  product by referencing its models.

However, one downside of this approach is that we then need to have
many directories in the include path for models. If we take the
include headers as an example, there are a small number of directories
in the path:

- compiler specific directories
- =/usr/include=
- ...

Maybe we have two separate issues here:

- when creating a product, where should the models be placed? If we
  keep in mind that models are themselves an asset like any other and
  as such require a meta-model representation, it would be logical to
  keep the model with the component it generates (just like we keep
  the product model within the product it generates). This means for
  instance that we could easily initialise a component via the command
  line and create a "template" blank model (in dia or JSON) with a
  number of things already set. We probably also need a way to avoid
  deleting multiple files (e.g. if we have both a dia and a JSON
  model, we need to know to ignore both of them). This means that when
  building a product we need multiple include directories for models,
  just as we do for headers. This work should be done as part of
  adding products to the asset model because models will be in the
  same namespace. The dia and JSON directories are then the facets for
  the model. This also means that we can now add the targets for
  generation, conversion etc directly into each component. So,
  somewhat paradoxically, when we create a model, we need to have a
  model of the model in it (or maybe two models of the model, Dia and
  JSON). Interestingly, now that we have a model of the model, we can
  suddenly move all of the keys that we have placed at the top-level
  into this modeling element. We can aslo associate it with a profile
  via stereotypes, removing the need for
  =masd.variability.profile=. And if we take it to the next leve, then
  perhaps references are themselves also modeling elements. Its not
  clear if this is an advantage though.
- from a "consumption" perspective, perhaps we could have a single
  =shared/dogen/models= directory, just like we will also place all of
  the PDM's includes under =/usr/include= and the SO's under
  =/usr/lib=. We could split it into Dia and JSON if need be.
- the product model itself should be at the top-most directory of the
  git repository. We also need a "models" directory to store models
  which are not expressed as source code (profiles, PDMs, etc). Then,
  for each component, we should have the models at the root directory
  of the component. Whilst this is not in line with our OCD, it is
  required in order for the product model to be able to locate the
  component models. An alternative is to have a convention that we
  always look into a "models" directory (which can be renamed via a
  meta-data parameter) for models, plus any additional directories in
  the "model path". We must inject the model file names to dogen so
  that we do not delete the models.

Notes:

- we need a model element for the model itself.

*** Add support for product skeleton generation                       :story:

Now that dogen is evolving to a MDSD tool, it would be great to be
able to create a complete product skeleton from a tool. This would
entail:

- directory structure. We should document our standard product
  directory structure as part of this exercise. Initial document added
  to manual as "project_structure.org".
- licence: user can choose one.
- copyright: input by user, used in CMakeFiles, etc. added to the
  licence.
- CI support: travis, appveyor
- CMake support: top-level CMakefiles, CPack. versioning
  templates, valgrind, doxygen. For CTest we should also generate a
  "setup cron" and "setup windows scheduler" scripts. User can just
  run these from the build machine and it will start running CTest.
- vcpkg support: add "ports" code? user could point to vcpkg directory
  and a ports directory is created.
- agile with first sprint
- README with emblems.

Name for the tool: dart.

Tool should have different "template sets" so that we could have a
"standard dogen product" but users can come up with other project
structures.

Tool should add FindODB if user wants ODB support. Similar for EOS
when we support it again. We should probably have HTTP links to the
sources of these packages and download them on the fly.

Tool should also create git repo and do first commit (optional).

For extra bonus points, we should create a project in GitHub, Travis
and AppVeyor from dart.

We should also generate a RPM/Deb installation script for at least
boost, doxygen, build essentials, clang.

We should also consider a "refresh" or "force" statement, perhaps on a
file-by-file basis, which would allow one to regenerate all of these
files. This would be useful to pick-up changes in travis files, etc.

One problem with travis files is that each project has its own
dependencies. We should move these over to a shell script and call
these. The script is not generated or perhaps we just generate a
skeleton. This also highlights the issue that we have different kinds
of files:

- files that we generate and expect the user to modify;
- files that we generate but don't expect user modifications;
- files that the user generates.

We need a way to classify these.

Dart should use stitch templates to generate files.

We may need some options such as "generate boost test ctest
integration", etc.

Notes:

- [[https://github.com/elbeno/skeleton][Skeleton]]: project to generate c++ project skeletons.
- split all of the configuration of CMake dependencies from main CMake
  file. Possible name: ConfigureX? ConfigureODB, etc. See how find_X
  is implemented.
- detect all projects by looping through directories.
- fix CMake generation so that most projects are generated by Dogen.
- add option to Dogen to generate test skeleton.
- detect all input models and generate targets by looping through
  them.
- add CMake file to find knitter etc and include those files in
  package. We probably should install dogen now and have dogen rely on
  installed dogen first, with an option to switch to "built" dogen.
- generate git ignore files with common regexes. See [[https://github.com/github/gitignore][A collection of
  useful .gitignore templates]]. We could also model it as a meta-model
  object with associated options so that the user does not have to
  manually edit the file.
- generate top-level CMake, allowing user to enter dependencies and
  their versions (e.g. Boost 1.62 etc) and CMake version.
- inject dogen support automatically to CMake (on a feature switch).
- determine the list of projects by looking at the contents of the
  input models directory.
- user to enter copyright, github URL.
- we probably need to create a kernel for dart due to the
  peculiarities of the directory structure.

We should copy the approach from =dotnet=:

- [[https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-new][dotnet new]]: we could have an equivalent dogen new. We could also
  copy the approach of templates (e.g. CMake template, Visual Studio
  template, etc). We could also supply the packaging sources (though
  this needs better modeling).
- [[https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-sln][dotnet sln]]: we need an equivalent option to specify
  components. List, add, remove.
- we should search predefined directories (=modeling= etc).

*Directory Themes*

It seems obvious no one in C++ will agree with a single way of
structuring projects. The best way out is to start a taxonomy of these
project layouts (directory structure themes?) and add this to the
project generator as a theme. At present there are several already
available:

- [[https://github.com/vector-of-bool/vector-of-bool.github.io/blob/master/_drafts/project-layout.md][Project Layout]]: see also discussion in [[https://old.reddit.com/r/cpp/comments/996q8o/prepare_thy_pitchforks_a_de_facto_standard/][reddit]]. Also: [[https://vector-of-bool.github.io/2018/09/16/layout-survey.html][Project
  Layout - Survey Results and Updates]]
- [[https://build2.org/][Build2]]: the packaging system seems to have a preferred directory
  layout. In particular, see [[https://build2.org/build2-toolchain/doc/build2-toolchain-intro.xhtml#proj-struct][Canonical Project Structure]].
- GNU: gnu projects seem to have a well-defined structure, if not the
  most sensible.
- [[https://www.reddit.com/r/cpp/comments/cvuywh/structuring_your_code_in_directories/][Structuring your code in directories]]
- [[https://api.csswg.org/bikeshed/?force=1&url=https://raw.githubusercontent.com/vector-of-bool/pitchfork/develop/data/spec.bs#src.layout][The Pitchfork Layout (PFL)]]
- [[https://www.boost.org/development/requirements.html#Organization][Boost: Organization]]
- [[https://hiltmon.com/blog/2013/07/03/a-simple-c-plus-plus-project-structure/][A Simple C++ Project Structure]]
- [[https://github.com/golang-standards/project-layout][Standard Go Project Layout]]
- [[https://github.com/CodelyTV/csharp-basic-skeleton][csharp-basic-skeleton]]: "This is a repository intended to serve as a starting
  point if you want to bootstrap a project in C# and dotnet." See also skeletons
  for other languages, e.g. [[https://github.com/CodelyTV/java-basic-skeleton][Java]].

*Product Model*

Actually we have been going about this all wrong. What we've called
"orchestration" is in fact the product model. It is just lacking all
other entities in the product meta-model such as:

- injection/coding models: injection/coding models are themselves
  modeling elements within the product meta-model. However, to avoid
  having to load an entire coding/injection model, a product coding
  model can contain only the key aspects of the injection/coding
  models we're interested in: a) file or path to the model b)
  references c) labels: these allow us to group models easily such as
  say "pipeline" or "injection" etc. d) references: with this we can
  make a product graph of model dependencies. We can also avoid
  rereading models. we can also figure out what packages needed by the
  model graph.
- build systems: visual studio, msbuild, cmake
- ctest
- CI: travis, appveyor.
- kubernetes support, docker support.
- valgrind
- compiler: clang, gcc, msvc, clang-cl. Version of the compiler. This
  is used in several places such as the scripts, CI, etc.
- operative system: windows, linux. used in installation scripts, CI,
  etc.
- dependencies for install scripts; these are sourced from the
  component models.
- manual: org mode, latex
- org agile: product backlog, sprints, vision, etc.

Notes:

- a product may be associated with one or more primary technical
  spaces (e.g. support for say C# and C++ in the same model). This
  would have an impact at the product level.
- a product could have some simple wale templates so that when you
  initialise a product you would get a trivial dia model with a simple
  entry point (for executables) or a library with maybe no types.
- when generating a product we can generate all models (product and
  component), generate just the product, generate a specific component
  or generate a label (which groups components).
- we need a "init" command that initialises a product. It needs a
  product name and maybe some other parameters to determine what to
  add. Maybe it just makes a product model and asks the user to fill
  it in instead.
- there are several types of component models: 1) models that do not
  generate anything at all. these are useful for defining templates,
  configurations, etc. 2) regular component models 3) product
  models. 4) platform definition models that are used to adapt
  existing libraries into MASD.
- in this sense, we have two different models: product and
  component. Both of these need to be projected into artefact space
  (because we have multiple facets in products as well). This means we
  somehow need to use archetypes from both models.
- the product model should have meta-elements describing the component
  models (perhaps =masd::component_model::target=, with a matching
  =masd::component_model::reference= in the component models).
- See aslo the story about directories in dogen: [[*Move models into the project directory][Move models into the
  project directory]].
- we could create separate chains for product and component
  model. This would imply a need for distinct model types. On the
  product model, we would locate all of the meta-elements representing
  a component model, and for each of these, run the product model
  chain. For other meta-model elements we just run their associated
  transforms - hopefully not many as these are expected to be very
  simple elements. We should also make use of injection model caching
  to avoid reloading models.
- as with component models, we should also have templates for product
  models so that we could simply do a "dogen new product" or some such
  incantation and that would result in the creation of a dogen product
  model and possibly its initial generation. One slight problem is
  that if we do a "dogen new component" we still have to manually add
  the component to the product model.
- we need to have a separate injection adapter for product models so
  that we filter out "invalid" meta-elements for the model
  type. Similarly, in the component injection adapter, we should
  filter out product model meta-elements (travis build files, etc).

Links:

- [[https://github.com/bkaradzic/GENie][GENie - Project generator tool]]
- see [[https://github.com/cginternals/cmake-init][cmake-init]] for ideas.
- [[https://github.com/premake/premake-core][Premake: powerfully simple build configuration.]]
- [[https://jgcoded.github.io/CMakeStarter/][CMake Starter]]: "This website is a simple tool to help C++ developers
  quickly start new CMake-based projects. The tool generates an entire
  C++ project with boiler-plate CMake files and source code, and the
  generated project can be downloaded as a zip file."
- [[https://awfulcode.io/2019/04/13/professional-zero-cost-setup-for-c-projects-part-1-of-n/][Professional, zero-cost setup for C++ projects (Part 1 of N)]]:

*** Add support for the static/dynamic pattern                        :story:

A common use case in Dogen is the "static/dynamic" pattern. It must
exist somewhere in a pattern catalogue. It is as follows:

- base class has a virtual method, say =id()=.
- descendants implement the virtual method, but we know at compile
  time what the values should be. So we create a =static_id()= and
  then implement =id()= in terms of =static_id()=.

Ideally we want to code-generate this infrastructure. A twist to this
is when we have some elements where the descendant may not have a
static ID.

Use cases:

- meta-model names.
- formatter names.

*** Add additional reference directories                              :story:
     CLOSED: [2020-04-07 Tue 08:51]

 At present we expect the reference models to be either on the data
 directory (for system models) or on the same directory as the
 target. Presumably, users may also want to have models on other
 directories. For example, if one were to extend Dogen with a different
 project, it would be required to load models from the dogen
 directory.

 We could simply add a command line argument for reference directories;
 if the reference is not found in the target model directory, we would
 then try all available reference directories.

 We should implement this as a =-I= parameter akin to compilers. We
 should also have a command to output the current include path - check
 GCC to see what command they use for this.

 This will be a requirement in order to support PDMs because we shall
 have many directories with models. We will need the concept of "system
 include directories" for this. We need to look into how compilers do
 this. We must also dump these in dumpspecs.

 Merged Stories:

 *Add additional data files directories*

 #+begin_quote
 *Story*: As a dogen user, I want dogen to use my own private data
 libraries so that I don't have to supply them as diagrams.
 #+end_quote

 Users should be able to provide directories for their own JSON
 models. We just need to add a new parameter to the knitter and
 transport it all the way to OM's workflow.

 In the future, when everything is a model, data file directories and
 reference directories will become one and the same.

*** Add version support                                               :story:

 At present we are generating the product version file as part of some
 CMake hackery. In reality, this is really a property of a MASD product
 and possibly component. We could even consider it to be a meta-model
 element.

 Notes:

 - there should be a meta-model element for version. It should be
   usable in either product or component models. In product models, we
   still need to figure out where the file with the details should be
   placed.
 - version can then be used for details such as travis/appveyor
   version, CMake version, DLL/SO version, assembly properties, etc.
 - however, we need to figure out how components will be able to see
   product level elements.
 - this can also be used for [[*Add versioning support][serialisation versioning]].
 - the version is a property of the product itself, so perhaps we
   should just make it a meta-data element. Then, if you would like the
   version to come out as a header file you need to create a "product
   version" element. This will use the product version (in a yet to be
   defined manner). This would allow users to decide where they want
   the header file to be created. A similar approach can be taken for
   components as well.

 Merged stories:

 *Supply dogen version as a configuration parameter*

 At present we are using the =config.hpp= file in several places in
 dogen to determine the dogen version. This makes things complicated
 when we don't actually care about the particular version such as in
 tests. We should supply the version as part of the initial
 configuration and then reuse it from there.

*** Allow dropping facet postfix for an element                       :story:

We sometimes need to suppress the facet postfix. For example, when
outputting tests, at present we have:

: cpp_ref_impl.boost_model/generated_tests/main_tests.cpp

We may want it just to be called:

: cpp_ref_impl.boost_model/generated_tests/main.cpp

However, we don't want all files on that facet to drop their postfix,
just main.

*** Add "is abstract" to profiles                                     :story:

Some profiles are created to be inherited from and are not meant to be
instantiated. We should have an "is abstract" flag on profiles and
error if a user attempts to instantiate a profile.

For now, by convention, we have marked these profiles as abstract in
Dia.

Notes:

- we should let the user know when they create a stereotype against an
  abstract profile, or conversely when they create a non-abstract
  profile which does not have a stereotype.

*** Detect non-configurable fields                                    :story:

Some stereotypes cannot be placed in a configuration. Placing them
there will only cause confusion and hard to debug errors. List:

- =injection.dia.comment=: this is only applicable to the UML note in
  dia.
- =injection.reference=: as we need these to load models, it would
  cause cycles if placed in configuration.
- =injection.input_technical_space=
- all fields needed to load the configuration itself, as it would
  cause cycles.

We should have a property in the field such as "supports
configuration" or configurable or some such. When reading the
configuration, we need to validate that none of the entry templates
contain fields with this value set to false.

Its not quite "supporting configuration", more like " supporting
unbound configurations". All features are by definition configurable.

A related problem is the converse: some fields _can_ be placed on a
configuration. In this case, we should not read the fields prior to
performing configuration expansion. This can probably be detected
quite easily: say we can have a flag that tells us if we have expanded
the configuration. If the flag is false, we should throw when we
attempt to read fields that can be placed in profiles. In effect we
are saying configurations exist in one of two states:

- pre-expansion, in which only fields that are "pre-expansion" can be
  read;
- post-expansion, in which only fields that are "post-expansion" can
  be read.

*** Replace variability enum mapper with lexical casts                :story:

Its not clear what value the mapper adds now we can just lexical cast
enums from strings.

*** Enablement problem is in the variability domain                   :story:

Up to now we have considered the enablement problem as a generation
model problem, but this is incorrect. The enablement problem is
basically the idea that if I set a type to be hashable (for example),
the system should implicitly determine all other types that need to be
hashable too. This means that if I have descendants, they should also
be hashable, and if I have properties, the type of those properties
must also be hashable. In reality this is just a variability
problem. We need to tell the variability model about:

- features that require "propagation across model elements". We need a
  good name for this, without referencing model elements.
- the relationship between bound configurations. This can be copied
  from the model element (the bound configuration has the exact same
  name as the model element).

Then, we can simply build a DAG for the feature model using only bound
configurations (e.g. at present, binding type of "not applicable") and
then DFS the DAG setting properties across this relationship. Call the
relationship R between a and b, where a and b are configurations; all
properties that have the "propagate" flag on will be copied across
from a to b as is (due to R). If done after building the merged model
and after stereotype expansion this will work really well:

- we don't really care how a got into the state it is at present, we
  just copy the relevant properties across.
- there is no solving, BDD, etc. However, R must not have cycles. We
  probably need to first see how many cycles we find with inheritance
  and associations.
- we may need a way to switch this off. Say we really want to
  introduce a cycle; in that case, the bound configurations should be
  ignored.

Note that we will probably need to store pointers to the configuration
in order for this to work, or else we'll end up doing a lot of lookups
and copying around (to get the configurations from the model elements
into variability, the DAG etc and then back into the model at the
end).

Interestingly, this also means that we should not move the
global/local enablement computations into archetypes as we had planned
earlier. Instead, we need to explore if it is possible to generalise
the notion of "local" and "global" configurations, with overrides and
default values. This would work as part of the configuration binding
via implicit relationships - its just that the global configuration is
not really a relationship inferred from the underlying model. We then
need to look at the cleverness that we are using for overwrite as
well. Whilst we only need this logic for enablement, it may be useful
for other fields as well in the future. We also need some kind of way
of declaring certain fields as "cloneable" (for want of a better
term). In this case, we start off with a list of these fields, and if
there is no configuration point for them locally, we take the global
configuration point; if none exists, we take the default value.

Actually its more like "hierarchical copy" because we need to take
into account the hierarchy. In addition, we don't particularly care
about say backend, facet, etc at the element level, we just want the
archetype. So we need to encode these rules as a type of bind. It can
even be hacked as a bind "special" just for this purpose, its still a
better approach.

Another interesting issue is that of "reverse references". That is,
the fact that a model m is referenced by a set of models S; each of
these models may enable facets on elements that are associated with
elements from model m. On a first pass, we need to be able to consider
the configuration requirements as "non-satisfiable". The user
requested a configuration on the target model which cannot be
satisfied unless we alter the configuration of a referenced model. On
a second pass, when we have product level support, we could consider
adding "referenced" models to each model. This means that when we are
building m we have visibility of how m is used in the product and we
can take those uses into account when building the DAG.

We should really read up on OMG's CVL and associated technologies, as
it seems they have done much of the analysis required here.

Merged stories:

*Propagate =fluent= stereotype*

It would be nice to be able to mark an object template called say
Message with =dogen::fluent= and then have all of the classes that
instantiate that template set to fluent.

This is a variation on the general problem of feature propagation
(e.g. hashing, etc).

It would also be nice to have a meta-data parameter to determine if
the "auto-propagation" is on or off.

*Computation of enablement values*

Note: this story is still *very* sketchy.

At present we have a very simple way of determining what formatters
are enabled: if a facet has been enabled by the user then all
formatters on that facet are enabled. This is a good starting point
but results in a lot of manual work:

- if we add a type which does not support all facets, we will generate
  invalid code. Users should be able to mark which facets are
  supported and then the graph of dependencies should do the right
  thing, propagating the disabled status.
- we are enabling all formatters in a facet. For hashing and forward
  declarations, it would make more sense to have a "dependency based
  enablement": if we determine that someone in the model needs that
  feature, we enable it, if not its disabled. Users can always
  override this and force it to be globally enabled.
- if a user creates a "service", all facets other than types are
  disabled. Ideally we should be able to define "enablement profiles"
  and then set an element's enablement profile. Each enablement
  profile is made up of a set of enabled facets. They could be
  supplied as a KVP. In fairness we probably just need "types and io"
  or "default".

One way to think of this problem is to imagine a matrix for each
element in element space. Each matrix is two-dimensional: one
dimension is the facets and the other are "dependent elements". These
are effectively made up of all attributes for each element, with a
name tree expansion. Each value of the matrix can either be 0
(disabled), 1 (enabled) or 2 (not computable). Not computable is a
hack to cope with cycles in the graph of dependencies.

Each value is computed by looking up an element's matrix and looking
for zeros. If there is one or more zero against a facet, the element's
value for that facet is zero. If there is a two we need to do a
two-pass whereby we first compute the matrix ignoring all the two's;
then, for each cycle we create a list of all the elements on that path
and the pair of elements that causes the cycle. We then compute the
enablement for this pair with a simple table (OR the computed
enablement values). We then traverse the cycle in reverse, updating
the twos to real values.

We could start with one large matrix with rows by element and columns
by feature. All values on this matrix are set to 1. We would then
multiply it against the global enablement matrix. We would then
multiply it by the local enablement matrix, for each element. We would
then compute the dependency matrices for all elements only taking into
account facets that are still enabled. We need to find the linear
algebra operation that takes a column with zeros and ones and returns
one if all rows are one and zero otherwise.

This produces the enabled facets. We then need to worry about the
formatters. There are a few sources of information:

- the facet enablement.
- the user local or global decision for that formatter.
- some kind of default formatter property (e.g. disabled by default).
- dependencies.

For these we need to create a "get dependencies" method in
each formatter which returns dependent formatters. For example, the
visitor formatter depends on the forward declarations formatter. This
is a static dependency. The more complex case is where there are
dynamic dependencies. For example, if hashing is detected for a given
type, we then need to enable the hashing facet for the containee. We
should probably hard-code this scenario for now.

We may want to make these computations disableable. For example: a)
all: no computation, everything is enabled b) all supported: all that
is supported is enabled c) by dependencies.

Requested help from FB. Core of the email:

#+begin_quote
Lets start with the simple case. Let G be a DAG. For each vertex of G
there is an associated vector over a field F. Now I would say F is
GF(2), which suits my needs (as you will see below). The objective is
to compute, for each vertex, the value of its associated vector, as
follows:

- first we go through the vertices in any order and setup its initial
  values according to a predetermined heuristic. Different nodes will
  have different values, and the heuristic has no dependency on G.
- then we iterate through G using DFS. If a vertex has no children
  then the final value of its vertex is the initial value. If a vertex
  has children, the value of its vector is obtained by multiplying the
  initial value against the values of the vectors of its child
  vertices. Multiplication under GF(2) is just a logical AND which is
  great for my purposes.

Just to make sure I'm explaining my self correctly, lets look at it in
layperson's terms: if a vertex has a 1 at position zero of its vector,
and all of its children also have a 1 at position zero, then the final
value for position zero will be 1. If there is a 0 anywhere at
position zero then the value is 0. So far so good, this works as
expected.

However! The problem is, G is actually not always a DAG. Sometimes
there may be cycles, which are detectable during DFS. My question is:
is there anything I can do to still perform this heuristic (or some
approximation of it) with a graph that has cycles? For example:

- record the path to the cycle and perform several passes. This seems
  to breakdown when there are several cycles because I seem to hit
  some kind of recursive problem.
- ignore the cycle. Of course, the problem with this approach is that
  if there was a zero at either side of the cycle, I would be
  incorrectly computing the node, but maybe that's the best one can
  do?
#+end_quote

Actually maybe we are looking at this the wrong way. Lets imagine that
for each element there is a vector v in GF(2) called the initial
vector. The objective is to compute u, the output vector. The output
vector is made up of the initial vector of the element, times the
output vectors of all the elements the element depends on. However,
these can be formulated in terms of initial vectors too (e.g. the
initial vector of the depended element times the initial vectors of
the elements it depends on times the initial vectors of the elements
they depend on and so forth). Thus for each element there is an
expansion that just relies on initial vectors. For the cases where
there are cycles: its not a problem since multiplying n times by
the same vector (in GF(2)) produces the same result as multiplying
just once.

It would still be useful to have a graph though, to find all of the
initial vectors for a given element. We just need to stop DFS'ing when
we find a cycle. We can also cache the initial vectors for each
element.

Notes:

- we can greatly simplify this story if we do not allow for cycles. We
  can simply create a graph of all dependencies and then iterate the
  graph from the leaves. Call Ev the enablement vector for each
  element; we can descend the graph and perform an OR of Ev at each
  level. Consider element e0, which is a child of a set of elements E;
  for each entry in the set, we'd OR the element vector of e0 (and of
  all of its descendants). As a result, its values would be the
  superset of all of the enabled values on each leaf element.
- since we do not allow cycles, we should detect them and break with
  an error. We should provide the cycle path to the user and then
  allow users to remove certain types from this computation via
  meta-data. If a type is set not to contribute to the graph, we can
  simply skip it. The user is then responsible for manually setting
  that type.
- since we can only alter generatable types, we should detect when we
  reach an element which is not generatable. If the OR'ing of that
  element does not produce its current enablement vector we should
  simply error and tell the user the current enablement requirements
  are not satisfiable. The user is then responsible for addressing the
  issue by either changing enablement requirements, ignoring types,
  updating reference models manually or providing helper types. To
  make life easier we could state what are the enablement requirements
  that have not been met so that users can quickly decide what to do.
- once we compute the dependency graph we can also check to see what
  types are on it. Any type which is absent can be removed from the
  model. We could also compute the models that are on the graph and
  compare them to the list of references. If the list of references at
  present only includes references of the target model, we can figure
  out any unnecessary references. Sadly we cannot do the opposite:
  (lost the train of thought).
- it would be nice to have "enablement requirements". For example, if
  the user used =std::unordered_map= against a dogen type, it should
  trigger the generation of hash for that type (and all dependent
  types). Similarly, for =std::map= it should trigger the creation of
  =operator<=. If we could declare upfront that a type's types facet
  depends on another facet, this could be computed.

*Formatters need different =enabled= defaults*

We should be able to disable some formatters such as forward
declarations. Some users may not require them. We can do this using
dynamic extensions. We can either implement it in the backend or make
all the formatters return an =std::optional<dogen::formatters::file>=
and internally look for a =enabled= trait.

We need to be able to distinguish "optional" formatters - those that
can be disabled - and "mandatory" formatters - those that cannot. If a
user requests the disabling of a mandatory formatter, we must
throw. This must be handled in enabler.

This story was merged with a previous one: Parameter to disable cpp
file.

#+begin_quote
*Story*: As a dogen user, I want to disable cpp files so that I don't
generate files with dummy content when I'm not using them.
#+end_quote

It would be really useful to define a implementation specific
parameter which disables the generation of a cpp file for a
service. This would stop us from having to create noddy translation
units with dummy functions just to avoid having to define exclusion
regexes.

In some cases we may need a "enable by usage". For example,
it would be great to be able to enable forward declarations only for
those types for which we required them. Same with hash. We can detect
this by looking at the generated include dependencies. However,
because the include dependency only has a directive, we cannot tell
which formatter it belonged to. This would require some augmenting of
the directive to record the "origination" formatter.

*Disable facets on element state*

In certain cases it may not make sense to enable a facet. The main use
case is for testing: we should not bother testing an object if there
are no attributes. This can be achieved with a small hack: add a
container in archetype repository of all archetypes that require
objects to have properties. Then, augment =is_element_disabled= to
perform this check. We just need formatters to supply this information
when building the repository.

A much more robust version would be to have formatters return a
function that takes in the element and returns true or false. We could
default all formatters to just return true. However, we do not have
support for boost/std function so this would mean manually coding the
repository. We'd have a similar problem if we add an interface.

*Add support for facet dependencies*

At present we left it as an exercise to the user to ensure facets are
enabled to meet dependencies. In reality we need a solver for
this. Look for other solver story in backlog. In addition, we also
need to have a way to declare facet dependencies:

- all facets other than types depend on types.
- tests depends on at least types and test data.

Actually what we really need is a model to declare all entities in the
archetype space and their relationships:

- archetypes
- facets
- formatters
- kernels

The annotations model can then depend on this model. It should have
facilities for registration of kerneles, etc. However, note that this
has nothing to do with model to text transforms - its just declaring
the lay of the land for the archetype space. We called this generation
space up to know but generation is concerned with the mapping of
coding entities into archetype space, not with defining the geometry
of that space. We need a good name for this model:

- =masd.dogen.archetypes=

This also makes it clear why annotations had a need for locations in
archetype space: its because the configuration is the configuration of
formatting functions which are responsible for mapping coding elements
into archetypes. Of course we have configuration that is not related
to archetypes as well. We need some kind of way of stating this at the
archetype model level so that we don't have to associate all features
with a location on archetype space when none exists.

*Add support for formatter and facet dependencies*

Once we are finished with the refactoring of the C++ model, we should
add a way of declaring dependencies between facets and between
formatters. We may not need dependencies between facets as these are
actually a manifestation of the formatter dependencies.

These are required to ensure users have not chosen some invalid
combination of formatters (for example disable serialisation when a
formatter requires it). It is also required when a given
facet/formatter is not supported (for example when an STL type does
not support serialisation out of the box).

Note that the dependencies are not just static. For example, the types
facet depends on the hash facet if the user decides to add a
=std::unordered_map= of a user defined type to another user defined
type. We need to make sure we take these run-time dependencies into
account too.

*** Invalid stereotypes outside of objects are not detected           :story:

At present we are only checking for invalid stereotypes (e.g. those
which are neither static stereotypes, nor profiles nor object
templates) on objects and object templates. We need to add a validator
that checks all other element types. This should be easy, if we ended
up with any dynamic stereotypes we should error.

In fact we should generalise this processing: object templates should
mark their stereotypes as bound and then we will check every element
for any unbound stereotypes using the traversal.

We should solve this problem as part of the logical meta-model, by
creating some form of binding between the declared stereotypes and the
meta-model elements.

*** Setting include and source directory to empty                     :story:

At present it does not seem possible to set either the include or
source directories to empty. This probably just requires annotations
to understand empty values, e.g.

: a.b.c=

*** Throw on unsupported stereotypes for specific kernels             :story:

In some cases we may support a feature in one language but not on
others like say ORM at present. If a user requests ORM in a C# model,
we should throw.

If we are in compatibility mode, however, we should not throw.

Note that we are already throwing if a stereotype is totally
unknown. The problem here is that the stereotype is known, but not
supported for all kernels. This is a bit trickier.

We also need to check the existing code in stereotypes transform to
stop trowing if compatibility flag is on.

This means we need some kind of way of attaching a feature to a
technical space and a TS version.

*** Add support for multi-components in a model                       :story:

In the world of cross-model transformations (see story), we need lots
of separate models just because they need to generate their own
libraries or executables. It is a bit of a shame that we need to have
a number of "modelets", each for its own component. An alternative
would be to support multiple components from a single model, but this
would be a bit tricky. Thoughts:

- the model would have a multi-component mode, set at the top. No
  model elements are allowed at the top level.
- each package has a stereotype of =dogen::component= (not the best of
  names given it conflicts with UML component diagrams). Dogen
  generates each of these namespaces as a separate component
  (e.g. shared library or executable).
- the top-level model name becomes the first model name, the package
  name the second model name. Interestingly, this should mean dogen
  will generate all components on the top-level directory without any
  additional work.
- the easiest thing to do in terms of the existing pipeline is to
  create the concept of components at the meta-model level and then
  create a transform that takes a component based model and generates
  one model per component and processes them one at a time with the
  existing pipeline. However, we need to be careful because one model
  will contain all of the business logic whereas the other models are
  simple references to it. This could be addressed by having
  references, based on the existing model references.

*** Create "opaque" kernel and element properties                     :story:

As part of the element container, we can have a set of base classes
that are empty: =opaque_element_properties=. This class is then
specialised in each kernel with the properties that are specific to
it. We probably need an equivalent for:

- kernel level properties
- element level properties
- attribute level properties.

We then have to do a lot of casting in the helpers.

Once we got these opaque properties, we can then create "kernel
specific expanders" which are passed in to the yarn workflow. These
populate the opaque properties.

*** Consider creating a "variant" meta-model type                     :story:

Variants are a pain in the backside to define because they result in
really large elements. It would be much nicer to have a meta-model
element that encapsulates the definition of the variant, where each
attribute is a valid value of the variant (say
=masd::structural::variant=). We could then take one of several
approaches:

- generate a "typedef" for the variant as a file. This is the easiest
  way to fit in the existing framework. It could even include support
  for serialisation, etc.
- "convert" the variant into a name tree and replace the existing
  mentions with it. This is not as nice.

Notes:

- we could also generate visitors for the variant. See [[*Add boost variant visitors][this story]].

*** Consider changing variability value into a variant                :story:

Really all we are doing is adding a lot of infrastructure to be able
to store different types of values. This is what the variant is
designed to do. In addition, we then have all of the complexities
around selection that are already handled by variant.

If we did this then the declaration of feature bundles would have
native types and we would not need to have any mapping. This would
simplify a lot the variability work and also would then mean we don't
need two different kinds of mappings (fixed and extensible). The only
slight concern is how we handle the processing of values at present:
we need to know the type in order to cast it correctly. For most cases
this is simply hard-coded, but for key value pairs and for containers
we need to take the appropriate action. Perhaps we could resolve this
by using SFINAE? Basically provide implementations for each of the
valid values.

*** Come up with a name for models that contain "meta-elements"       :story:

At present we created a number of models called "profiles"; these have
only the profiles for configuring specific products. However,
conceivably we want them to also store the generation markers and
possibly licences, modelines, etc. Basically all of the shared
meta-infrastructure. We need a name that reflects this.

Ideas:

- configuration

*** Use of binding points in profiles                                 :story:

At present we have the concept of a binding point in a feature. This
allows us to determine how a feature can be bound to a modeling
element in a configuration. For example, take feature =X= with a binding
point of =global=; this feature can only be configured in the root
module because it does not make sense to exist anywhere else.

This concept was already present in the old annotations model, where
we checked that a "scope type" of a field matched the scope type of
the element. However, this was present haphazardly in profiles; we had
the notion of a "scope type" on a profile as a property but the
profile hydrator never populated it; in addition, the profiler only
set the annotation scope:

: pc.annotation().scope(scope_types::not_applicable);

We probably started thinking about this but stopped half-way. So, if
we try to retrace our steps logically:

- a profile could conceivably have a binding point. It would be used
  to validate that all profiles it merges against also have the same
  biding point (or similar; say =any= or =module= for =global=). It
  could also be used to validate that the feature templates referred
  to in its configuration point templates are also compatible.

At present we have preserved the old logic of having a binding point
in a profile as a feature, and left the initial feature processing
support in the adaptor transform, but:

- we did not add it to the profile template and profile classes;
- consequently we are missing all of the validation logic defined
  above.

We did some work on this for sprint 25:

- profiles now have binding points in the logical model, and meta-data
  is now being read to obtain the value of the profile.
- we did not hook in the profile conversion when adapting the logical
  profile to the variability profile (in =profile_adapter=) because
  when we tried, it caused all profiles to break. Apparently we have a
  number of inconsistencies in the way we are using features in
  profiles. This requires some investigation. However, this work
  should way until we do the terminology clean up of the variability
  model.

*** Investigate current implementation of the origin transform        :story:

Do we need to have the origin transform? can we not just supply the
origin type to the adapter directly?

Actually this cannot be done. The problem is we still need to
distinguish between dogen models and non-dogen models; we need to
register all dogen models. This is done via meta-data. We cannot use
the meta-data until we have converted into a logical model. We could
consider having a flag at the injection level for this - it is a
concept at this level - but we still need to map it to origin
types. However, it is perhaps cleaner to express this concept at the
logical level rather than the injection level given we are saying
there are two different kinds of injection models: proxyness is a
fundamental property of an injection model. If we do this we can then
do the mapping in flight as we transform from injection to logical
model.

This story is directly connected to "model types".

*** Add =ignore= meta-model element for VCS ignore files              :story:

It would be nice if we had a meta-model element to represent ignore
files for version control. For extra bonus points we should support
multiple version control systems. We should also have out of the box
ignore collections. These could link in via technical spaces. We
should also be able to use composition (e.g. C++ with CMake and Visual
Studio). It should also be possible to generate a user defined
collection.

Links:

- [[https://github.com/github/gitignore][gitignore]]: A collection of useful .gitignore templates. We should
  use this project, unmodified; perhaps by adding a PDM model which
  imports the git ignore files they provide. The downside is having to
  create meta-model elements for each file.

*** Consider creating meta-entity for root module                     :story:

At present we are supplying model properties via a "special" comment
in a model. Items such as model_modules etc are read from this
comment. We then generate the root module and use these properties to
configure model-wide variability. However, according to the rule that
there are no implicit model elements, we should have a meta-model
element representing the root module. The properties of this element
should reflect those on the "special" comment.

Or perhaps a case can be made that the root module is special and it
is the only implicit element.

Actually this can be explained via projections from spaces - e.g. the
model projects as a namespace into the physical model. However, the
only inconsistency is that we have both the notion of a model as well
as the notion of the root module. For consistency we should not
require the root module in the logical domain.

*** Handling of "derived meta-types"                                  :story:

In the past we had the notion of "element extensions". This was
basically a way to allow a meta-element to have more than one
associated representation. For example: many elements can give rise to
forward declarations in C++, so it would be nice to transform these
"source" elements into a second (common) element that reflects this
(say a "forward declaration" meta element). Then we can easily create
a simple template. However there were numerous problems with this
approach:

- we had two distinct elements occupying the same point in modeling
  space. Due to this we made elements contain elements (effectively,
  though it wasn't explicit in the design).
- we then had the notion that an archetype associated with a was no
  longer a "complete" location in archetype space; this is because
  until you know the facet for which you are creating the forward
  declaration for you cannot know your location in archetype space.
- code was rather complex in order to handle all of these special
  cases.

To solve this problem we made forward declarations an archetype,
projected across facets. This made the design much neater but now we
have a second problem: we need to duplicate all of the formatting
logic for e.g. objects, enumerations, etc and worse, for each facet as
well.

The existing state was just too complex so we just "fixed it" at the
time. However, we now have a very similar problem with ODB "object"
options. These are only applicable to a facet, at least, but we still
need them for more than one element (primitive, object).

One possible solution is to use the same approach as we did for
helpers, but in a much simpler way. We could:

- create simple functions that just take an assets type design for the
  template (e.g. forward declaration, element ODB options).
- the element itself would contain these types, which are created as
  part of the transforms.
- the existing facet level templates simply call the helper function,
  supplying the type.

We still have to create all of the duplicate machinery for the
formatter, but once all of that is code-generated, this won't be too
much of a problem. This way there is no duplication of formatting
code, but also no meta-model complexity. Conceptually, these are just
facets, but at the implementation level they are implemented
differently. This also means we do not violate the rule of making
implicit meta-model elements explicit.

*** Make vistor a proper meta-model element                           :story:

At present we use the stereotype of =visitable= to emit a
visitor. This violates the principle of making all types
explicit. Instead, we should:

- create a new stereotype =masd::visitor=. It triggers the creation of
  the visitor meta-model element.
- visitor must have a target via meta-data. This points to the element
  to visit.

We need to make sure we don't break cross model visitation with this
change - i.e. derived models must also explicitly create their own
visitors.

*** Create a README meta-type                                         :story:

Once we have a product generation framework, it makes sense to have
the README as a meta-type, with its contents as markdown. We could
then generate all of the emblems. These can be properties that we
enable or disable in the meta-type as required (e.g. visual studio
version, etc).

This should be done when support for products has been implemented.

*** Schema name in ORM should be transitive                           :story:

At present when we define the schema name on a top-level namespace, we
don't "inherit" it from child namespaces. The problem is compounded by
the fact that we need the schema name in order to output ODB pragmas
(separate bug). It seems more logical to propagate the schema name to
child namespaces.

*** ODB pragmas not populated when schema name is not set             :story:

At present we have a bug whereby not setting the schema name results
in not having most ODB pragmas set. We should always populate them
even if the schema name is not set. To be precise, the problem is not
directly related to the schema name - we just require some ORM
property to be set. AS it happens, it normally tends to be the schema
name, because it makes sense to set it when defining a relational
model. This is why we never bumped into this problem before.

*** Support for cmake components and groups                           :story:

#+begin_quote
*Story*: As a dogen user, I need to integrate the generated models
with my existing packaging code.
#+end_quote

We recently added support for creating multiple packages from a single
source tree. We need generated models to have a new top-level cmake file:

: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/src)
: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/tests)
:
: install(
:     DIRECTORY include/
:     DESTINATION include
:     COMPONENT headers
:     FILES_MATCHING PATTERN "*.hpp")

And the =src= cmake file:

: install(TARGETS dia ARCHIVE DESTINATION lib COMPONENT libraries)

*** Warn on transitive references to models used directly             :story:

At present, when we reference a model B, we get all of the models it
references. However, if model A (the target) is using types of say
model C, it should reference it directly rather than via B. If B
changes, A will break. It would be nice to have a warning for these
types of referencing.

*** Mapping of third-party dependencies (PDMs)                        :story:

System models should follow the physical structure of
dependencies. That is, we should not have a "boost" system model, but
instead a boost-test etc. Each of these can then have mappings
(e.g. vcpkg name, build2 name, etc). Users must declare these
references just like they do with user models. Dogen can then create
code for:

- cmake targets, properly linking against libraries;
- vcpkg install, at product level, by de-duplicating component
  dependencies;
- possibly distro dependencies.

We should only have a mandatory dependency, which is the STL. In
addition, we need different models for each version (e.g. c++ 03,
etc). This makes it easier to include the right types.

Note that each model must have an associated version. The version
should be part of the file name. However, maybe we need to distinguish
between TS version (11, 17, etc) from library version.

One way of solving this regularity problem (e.g. having
masd::std::string is the real name but its annoying to have to have
users typing this) is to support "using" statements at the model
level. If a user could type something like =using masd= and this would
allow us to find all types as if we had typed =masd::= then users
could still type =std::string= and find =masd::std::string=. We'd
solve both the regularity and the "look and feel". The downside is
that this could have important ramifications in resolution:

- how does this work in the presence of merged models? do we merge
  usings? will this find types we're not supposed to find?

Needs some more thinking.

Notes:

- utility model must be split into multiple models: the PDMs should go
  with their own models. The real utilities should stay on a utility
  product. This can then be reused by dogen (for example, if logging
  is enabled it uses the logging setup from this model). Users should
  be able to switch it off though.

Merged stories:

*Platform description models*

We should consider integrating all of the information regarding
"platforms" into platform description models (PDMs). These include:

- all types available in a library (proxy models) and their mapping in
  terms of aspects. We could make the mapping a bit clearer by
  designing platform description models that are not part of
  modeling. We don't really need support for attributes, operations
  etc. However, we need some kind of "adaptor" that extracts all the
  type information (or some other way of making resolution work across
  different model types).
- name of the library, supported language (e.g. for Boost, C++ etc),
  versions of the library. We must associate the types with a version
  (e.g. introduced on version X, deprecated on version Y) so that when
  the user is using a given version it errors if a type is not
  available.
- packaging support: mapping to the name used on most common packaging
  systems such as DEB, RPM etc. Also, mapping on language specific
  package managers such as build2, vcpkg, conan, nuget, etc. User can
  decide what package manager to use overall or for a specific library
  (e.g. possible to mix-and-match package manager). It should also
  have a mapping for CMake support that includes "in CMake library
  from version X" and "available on an external source" with a URL. If
  the user selects the latter, Dogen can download the CMake file (or
  maybe Dogen should include the external CMake files to guarantee a
  stable behaviour).

Notes:

- with this we can now move away from the bad modeling used with proxy
  models, where it was not quite clear what they were. We can create
  a different file format (e.g. *.pdm* )that can share some
  similarities with existing JSON models but is not stuck with all the
  baggage needed to represent user models.
- however, PDMs are just regular models. Users should be able to make
  use of the PDM stereotypes to define PDM types in a regular UML
  model. Interestingly, a PDM from this perspective is similar to a
  named configuration. The difference is that it introduces a new type
  into the type system rather than a new stereotype. But with this we
  can now make use of named configurations, making PDMs a lot less
  verbose (e.g. define a named config and share it across multiple
  types, like SmartPointer etc). We could even share it across
  multiple platform models!
- PDMs should be organised by language (e.g. folder for cpp, etc). Top
  level directory is PDM rather than library.
- users declare references to PDMs just like normal models. PDMs have
  an attribute for "auto-loading". If on, we load regardless. Else it
  must be referenced. This is useful for say STL, hardware types.
- users can supply their own PDMs, although they are encouraged to
  submit them if they are general enough.
- if users choose CMake as the build system, we automatically add all
  the boilerplate required to add the library to CMake. In an ideal
  world, the PDM should contain the CMake snippets, with "macros"
  where required (e.g. version, etc). This would mean we wouldn't have
  to change templates when new libraries are added. We could also have
  a "standard" CMake snippet that works for most libraries.
- different libraries may have different types of support (e.g. a
  library may not exist in a package manager, etc)
- we could now create an installation script that sets up all
  dependencies (e.g. DEB/RPM). It cannot be a target because the build
  would fail (e.g. CMake would not find all dependencies).
- now that =library= can be renamed to =pdm=, we could call the
  top-level directory =library=.

*Adding linking libraries is not handled*

#+begin_quote
*Story*: As a dogen user, I want to link against libraries without
having to manually generate CMakeFiles.
#+end_quote

At present whenever a model requires additional link library targets
we need to disable CMake generation and do it by hand. However:

- for well-known dependencies such as boost we could create a
  convention (e.g. assume/require that the CMake boost libraries flags
  are set via find boost). Alternatively, the types should contain
  meta-data that has information about linking requirements; e.g. if
  you use a type from a boost model, it should provide you with
  linking information in its meta-data. Each boost type could have
  different information depending on which boost library they come
  from.
- for user level dependencies we should add dynamic extensions at the
  model level. Also, references provide sufficient information to link
  against other dogen models.

*** Add support for object templates that work cross-model            :story:

We've implemented support for cross-model inheritance in sprint 87 but
we did not cover object templates. Most of the approach is the same,
but unfortunately we can't just reuse it.

Tasks:

- we need a refines field which is a text collection.
- we need refinement settings, factory etc.
- update parsing expander.

Merged Stories:

*Cannot make qualified references to concepts*

At present it is not possible to consume concepts defined in a
referenced model, nor is it possible to refer to a concept in a
different module from the module in which the element is in, e.g.: say
concept C0 is declared in module M0; all types of M0 can have C0 as
stereotype and that will resolve. However any types on any other
module cannot see the concept.

One suggestion is to allow scoped names in stereotypes:
=module::Concept=.

The heuristic for concept resolution is then:

- external modules are never part of the scoped name;
- on a scoped concept with M names, we first start by assuming that
  the first name is the model module and M-2 is/are the internal
  module(s). We try this for all names in M-2, e.g. first two names
  are model modules and M-3 names are internal modules and so forth.

*Add support for using object templates across models*

At present it is not possible to make use of a concept across models
or even modules in the same model. We have two problems:

- resolution will probably fail;
- even if resolution does work, the elements of the concept will be
  invalid. This is because they will have relative references to types
  (e.g. =Nameable= concept in =yarn::meta_model= assumes it is in
  that module and so makes use of =name= rather than
  =yarn::meta_model::name=).

For this to work we could:

- create a flat resolution for concepts, whereby all concepts are
  resolved from the stereotypes just by the simple name rather than
  the qualified name. This means we cannot have two concepts with the
  same name across all loaded models. The alternative is to force
  users to fully qualify concepts,
  e.g. =yarn::meta_model::Nameable=. In which case we should probably
  stop using the upper case convention,
  e.g. =yarn::meta_model::nameable=.
- actually the second problem may even be moot; if concept resolution
  is performed before we merge attributes, all references will be
  fully qualified by then. We need to test this.

Final conclusion: if somehow one was able to fully qualify concepts as
part of the stereotype, this would work out of the box.

Notes:

- we did some work on this previously: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/v1/sprint_backlog_00.org#concepts-cannot-be-placed-inside-of-packages][Concepts cannot be placed
  inside of packages]]. This solved some of the problems such as
  allowing concepts to be placed in packages. We just need to test
  this. It may be that we are not merging object templates.

*** Rename validators to checkers                                     :story:

In the literature, model validators seems to be called
"checkers". There are global checkers and local checkers. We need to
go through our validation logic and see if we can follow the pattern
of the literature. In particular we need a definition for what a
checker is and what global/local means. Also, should there be checkers
associated with model elements? For example, check that an enumeration
has enumerators, that these have distinct names, etc.

We should have a documented set of rules for this, like compiler
errors. We need to classify these by severity:

- warning
- error
- information

The result of the checker should be a list of "events" at different
levels which can then be handled by the user interface - e.g. for the
command line output errors like clang/gcc so that we can integrate
with emacs.

Checkers also include meta-data validation, such as missing fields,
fields with incorrect types, etc.

We also need to understand the difference between model checking and
model verification.

We also need to look at EVL: [[https://www.eclipse.org/epsilon/doc/evl/][Eclipse Validation Language]]. See Chapter
4 of the [[https://www.eclipse.org/epsilon/doc/book/][Epsilon book]].

*** Add stereotypes support at the attribute level                    :story:

At present dia does not have stereotypes in attributes. This means
things like ORM primary keys etc are being supplied as tagged values;
in reality, its more natural (from a UML perspective) to supply them
as stereotypes. We could add some meta-data that creates a tagged
value for stereotypes.

This also lines up with the story we had where we question our use of
attributes. In reality we have created a notion of "property". We need
a stereotype for this.

*** Rename types that clash with reserved keywords                    :story:

When we added the new validation rules, yarn did not pass
validation. This is because =module= and =concept= are reserved on
current C++ TS's. Since we know these features will land in C++ sooner
or later, we should rename these types to avoid problems. Namestorm:

- module: package (clashes with java?)

*** The =types= facet should always be on                             :story:

At present users are given the option to enable or disable the
=domain= facet; this is not very wise because all facets depend on
it. It must always be on. We should remove these options.

In addition the facet is incorrectly named: when we performed the
rename of =domain= to =types= we left the command-line facet. We
should rename it to =types= too.

We should probably create a notion of "mandatory" facets to make this
more general.

Actually, we did find [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-c-itself-as-a-front-end][one use case]] where types needs to be off.

In general, this is a variation of the problem of the graph of
dependencies between facets at the element level.

*** Improving use of exceptions                                        :epic:

Exceptions in dogen are not providing enough contextual
information. The problem is that we just see the information available
at the throw site, not the context regarding the processing
pipeline. Boost exception has the required tooling for this, but we
are not making use of it correctly. Ideally we need to know:

- model currently being processed
- element
- attribute
- name tree
- name in name tree
- phase in the yarn pipeline (e.g. injection, etc)
- phase in the quilt.cpp pipeline (e.g. building properties,
  formatting, etc).
- file and line where the element was defined in the frontend.

Not all of these make sense at the same time. What would be ideal is:

- create a base exception for each model with all the attributes we
  need to capture.
- add support for automatic generation of Boost Exception tags for
  each attribute.
- add support for exception logging via IO.
- change all throw paths to fill in the attributes by catching
  exceptions and augmenting them. This seems quite involved
  overall. In some cases, we probably just need to locate a few select
  entry points (pipeline etc), but for others such as attributes etc
  this will be hard.
- log the exception only at the final catch point. We no longer need
  to log at throw because the catch will have all the information (and
  more).

Merged stories:

*Provide contextual error messages during validation*

As per the validator story, we need to ensure the model we are
processing is valid. However, the validator must provide contextual
validation error messages:

: error 1: properties must have a non-null name
: in model 'my_model' (Dia ID: O0)
: in object 'my_object (Dia ID: O0)
: property 'my_property' has empty name.

We should also try to make this compatible with compiler errors so
that we can go to the file where the error occurred:

: file.dia:5:10: properties must have a non-null name

For this to work we need to label everything with a file, column and
line. We can't call this class a =location= since we already use that
name for positions in model space. We could probably call it
=source_location= like [[http://clang.llvm.org/doxygen/classclang_1_1SourceLocation.htlm][clang does]]. We can use [[https://developer.apple.com/library/prerelease/content/documentation/Darwin/Reference/usr_APIs/tree/index.html][XML_GET_LINE]] to get
information about the line number of dia documents. We need something
similar for JSON.

The context should be provided using Boost Exception diagnostics and
dumped into =std::cerr= at the very top.

*Improve file importer errors*

It would be nice if one could know the file, line and column where an
error has occurred when importing any imported file. Even for Dia,
this would make troubleshooting much easier. We need to add some
information to the meta-model to keep track of the source of the
types: file, line start, line end perhaps.

Links:

- [[https://www.gnu.org/prep/standards/standards.html#Libraries][GNU: 4.4 Formatting Error Messages]]

*** Attributes versus properties                                      :story:

At present we have assumed that all attributes in objects should be
generated as properties. This is not quite the right thing to do; one
may actually want to generate a member variable which is not a
property. One solution would be to create a dynamic extension at the
class level that defaults all attributes to properties (or to member
variables). This could be the default for objects but not for
services. The right thing would be to have attribute stereotypes but
dia does not support these.

We would have to extend assets to understand member variables as well
as properties.

Actually we should just have a meta-model concept for a masd property,
which knows about getters and setters; these are associated with masd
object's and object templates. Attributes, if required, need their own
meta-model representation. This also means that we should determine
the getter/setter settings within coding (for example, if immutable
disable setter etc).

We can then create the notion of a "pojo", where all attributes are
automatically mapped to properties.

*** Replace boost property tree with real JSON support                :story:

Once we support JSON fully we should go through all of the uses of
JSON we have at present and replace them with the JSON serialised
version of the types.

*** Handling of inclusion constants needs reviewing                   :story:

At present we hard-coded "inclusion_constants" in C++ formatters for
common includes such as string, etc. However, what we are really
trying to say is that a given model type for a given facet should have
a resolver level dependency against a type on a PDM.

This is not so straightforward: up to now the idea was that you'd
declare the types level dependency between types. In effect, we have
one big upfront association between types at the coding level and
then we use that to determine what it means in terms of associations
between facets. Now, we already have some hacks to deal with the
relationships between facets:

- types needs types;
- most other facets need themselves and types, but not all.

This is handled via the =inclusion_dependencies= functions in each
formatter, which also injects additional includes via
inclusion_constants. However, the problem with this approach is that
we have a silent dependency against models such as boost model and
system model. The right solution would be something like:

- formatters know what models they require and at the beginning of
  processing we inject all model references from the formatters.
- coding has a new type of association: implicit dependencies. These
  are functions with a name and a facet that map to a facet. Example:
  serialisation requires (=std::string=, types). The formatter injects
  all of these relationships to each object (by meta-type).
- this container is taken into account when computing the includes.
- it is also taken into account when resolving names.
- coding could provide an interface against which the formatters could
  register to provide this information. This is somewhat similar to
  what we do with dynamic transforms.
- logging (if really required) is another special case. This is
  because for each model we'll have a different logging
  implementation. We need to somehow map to it.

*** Consider adding an indent JSON transform                          :story:

Once we start making use of a proper JSON library, we should output
indented JSON models as part of conversion. We always have to indent
manually anyway. For extra bonus points, it would be nice if the
indent could cope with our invalid JSON (not deleting duplicate keys).

We could even expose it as an activity/command so that we could indent
external files without going through conversion; this would be useful
for library models.

*** Update copyright notices                                          :story:

We need to update all notices to reflect personal ownership until DDC
was formed, and then ownership by DDC.

- first update to personal ownership has been done, but we need to
  test if multiple copyright entries is properly supported.

*** Remove empty default values                                       :story:

At present we have a number of default values in feature bundles set
to the empty string =""=. It makes more sense not to have a default
value and have the client code handle its absence.

Once we do this, we can then remove the spurious quotes we added to
all feature templates and do that directly in code-generation. It was
added only because we needed to distinguish between the empty case and
the "empty" string.

*** Add logging support to generated tests                            :story:

At present generated tests are not writing to the log file. This is
because we wanted to keep them clean so that users could generate
tests for their models without having to pull in dogen
headers. However, for dogen tests this is a bit painful; if a test
fails we can't just look at the log file to see why. We could have a
flag to generate tests with logging.

The other problem is we need to move utility into its own library as a
PDM before we can do this because otherwise the logging will be in
different locations (i.e. dogen vs reference model).

We should add some meta-data flag to suppress/enable logging.

*** Move ORM camel-case and databases into assets                     :story:

We should handle this property at the ORM level, rather than at the
ODB level.

Similarly, we should move the ODB databases into yarn and make that a
ORM-level concept.

** Feature Heap

New features for this release. These are more nice to haves rather
than mandatory work.

*** Licence compatibility errors                            :reviewing:story:

If we correctly annotate all PDMs with a licence, as well as all model
code, we could define "compatibility relationships" between
licences. For example we could say that a model M with a proprietary
licence including a PDM P with GPL licence is an error. This could
come up as any other error would. We could have a "compatible with"
field in licences for this.

*** Add stereotype for IoC containers                                 :story:

At present we are marking IoC containers with either handcrafted or
header only. In reality, they should have their own stereotype and
colours as, in the future, we want to code generate them. However, we
can only do this once we get rid of the initialisers because they are
also a form of IoC containers, but with different requirements.

Proposed stereotype: =masd::ioc::container=.

Actually, this is not quite right. We are not creating the IoC
containers themselves, but the wiring code that sets up these
containers. We need to figure out the correct term for
these. Suggestions:

- [[https://github.com/avao/Qart/blob/master/Src/Qart.CyberTester/Bootstrapper.cs][bootstrapper]], with a =Bootstrapper.CreateContainer= method;
  e.g. =masd::ioc::bootstrapper=.

Links:

- [[https://github.com/autofac/Autofac][Autofac GH]]: Autofac is an IoC container for Microsoft .NET. It
  manages the dependencies between classes so that applications stay
  easy to change as they grow in size and complexity. This is achieved
  by treating regular .NET classes as components.

*** Primitives are not comparable                                     :story:

Our wrapping code around primitives means we can no longer perform
arithmetic operations on them or comparisons. This may be what is
intended (e.g. adding or multiplying =customer_id= does not make
sense) but it also means we can't delete ranges from the database for
example. It would be nice if there was some meta-data we could add to
primitives to make this possible:

- =comparable=
- etc

With this we would generate the appropriate operators by delegating to
the underlying type.

We probably need some way of knowing if the underlying type supports
comparisons. A meta-data flag used to annotate proxy models would be
sufficient.

*** Exception classes should allow inheritance                        :story:

#+begin_quote
*Story*: As a dogen user, I need to generate object graphs for my
exception classes so that I can model my domain better.
#+end_quote

We need to have a form of inheriting from a base exception for a given
model. We also need to be able to inherit from other exceptions in a
model. At present exceptions are not objects so the dependency graph
support is not there.

When we do this we need to split relatable into "associatable" and
"generalisable" and get exceptions to model generalisable.

*** Add ORM type mapping support to primitives                        :story:

At present we can setup a type map in ORM as follows:

: #DOGEN masd.orm.type_override=postgresql,JSONB

However, this is specific to attributes. It would be really nice to
have this at the primitive level as well so that we can map once and
reuse the mapping.

More generally, we now have three mechanisms for mapping, which cannot
possibly be a good thing:

- extensible mapping: for LAM. Extensible, but not really, as users
  need to physically change PDMs.
- fixed mapping: extremely limited use, just for the variability
  types. Similar in logic to the ORM mapping, but not as flexible (end
  users need not be aware of its existence at all).
- ORM mapping: at present we are using the default mapping on the
  underlying technology (e.g. ODB). In truth though, some LAM like
  mapping is happening, we just don't see it. That is, there exists a
  SQL model with types, on a per relational database level, and we are
  mapping C++ types to these types. The overrides are there for the
  case where the user is not happy with this mapping.

We need to create a general mapping mechanism that can handle all
three use cases, including the overrides/extensibility. It could
signal when a mapping cannot be overriden (e.g. "final" mapping) or
when it is extensible. Users should then somehow be able to declare
overrides in their own models.

*** Allow stereotypes in object templates                             :story:

At present we need to use inheritance to "merge" object
templates. This has served us well, but has one limitation:
composition has to be tree-like. In practice, we have use cases where
composition is more haphazard, not allowing us to draw a clean
inheritance diagram. For example, we have the "properties-like
elements" in coding, that all have:

: Documentable, Annotatable, Configurable, Nameable

These could easily be packaged into a object template, but we can't
because its not possible to have two "kinds" of inheritance graphs -
we'd end up with lots of lines intersecting each other. However, a
natural way to solve this problem is to allow dynamic stereotypes in
object templates. These are mapped to parents and processed exactly as
if we had the inheritance relationship. From a practical perspective
this makes a lot of sense, but we need to make sure this is not
frowned upon from a theoretical perspective.

The other problem as well is that we need to mix and match dynamic and
static stereotypes (e.g. we need =masd::object_template= as well).

Another example of where this feature would come in handy is with
features and feature templates, which could share an object template
but can't because we also need to inherit from "Documentable,
Configurable, Nameable".

*** Keep track of which transforms touch which elements               :story:

It would be nice to be able to know, for a given modeling element,
which transforms modified it. We can add a property to element in
assets that is a list of transforms and guids, so that we can
distinguish invocations. This way we can then easily ask the database
for details.

Notes:

- add a "modified by" container to element. Its a pair of transform ID
  and GUID. Add it to models as well.
- update every transform to record this information.

*** Allow property level bindings to primitives                       :story:

We should map any variability bindings that happen at the
property/attribute level to the anonymous attribute that all
primitives have. This is because its not possible to address the
anonymous attribute in a diagram. At present we are hacking these
features to the =any= scope just so that we can move on. Affected
features:

: masd.orm.type_override
: masd.orm.type_mapping

In order for this to work, when we are building configurations from
the original meta-model element, we need to "override" the
type. However, this raises the question: what if we do want to bind
something at the primitive level? What if something binds both to the
property and the primitive?

The alternative is to make the attribute explicit. We need to think
about the consequences of this as well.

*** Default model modules from filename                               :story:

It would be nice to be able to not have to supply model modules when
its obvious from the filename.

Update hello world to demonstrate this. We basically want to make the
entry use case as simple as possible, requiring little to no
meta-data.

*** Handcrafted templates                                             :story:

At present we generate constructors, swap, etc. for handcrafted
classes. Ideally users should be able to create a profile that enables
the things they want to see on a template and then associate it with a
stereotype. For this we will need aspect support.

A more interesting approach would be to combine wale (or its proper
replacement, a mustache based solution) with the meta-model: if one
could create *any* text file that can behave like this kind of
template, we could arbitrarily extend dogen for trivial use cases:

- main, entry point.
- interface.
- other uses users may find. Because they can bind templates against
  elements, this would make extensibility easier.

However, this is not a replacement for stitch: it is only helpful for
trivial cases and its not even clear it would work for all - e.g. how
would one loop trough all attributes in an object?

Actually, we probably already have enough for this to work, at least
for a few simple cases:

- interfaces: wale template with correct constructors, destructors,
  etc. For extra bonus points check operations.
- trivial main.

We just need to use the wale template to create the first "draft" and
then set overwrite to false.

*** Make use of association relationships                             :story:

When we start having to create elements such as visitor etc., it would
be nice to rely on the association between visitor and visitable to
figure out what the visitor is visiting. This and other simple cases
can be inferred simply by looking at the end points of the
association. However, we should still allow supplying this
meta-parameter as meta-data because it may not be practical to have
the association. And we need a way to express this in JSON as well.

*** Consider allowing UML inheritance of object templates             :story:

It would be nice if we could infer that an object inherits an
interface from an object template via the "implements" generalisation,
and so does not require the stereotype.

*** Consider inheriting annotations and stereotypes                   :story:

At present we need to define the stereotype
=masd::cpp_artefact_formatter= on every formatter. However, we know
that all classes inheriting from =artefact_formatter_interface= are
all going to have this stereotype (and thus the same
configuration). It would be nice if it was possible to somehow put
this information into the interface itself and then anyone
implementing the interface would automatically inherit the
stereotypes. However, this should not be done automatically because we
probably have lots of cases where we use inheritance but don't expect
this to happen. Perhaps we could have some specific meta-data for
this:

: descendant_stereotypes=xyz
: implementer_stereotypes=xyz

Or perhaps "propagate across inheritance". This is in effect, a variation
of IPC (implicit presence conditions).

*** Consider creating shared pointer typedefs for leaves              :story:

It would be nice to be able to enable the creation of shared pointer
typedefs for leaves, to simplify code such as:

: void operator()(boost::shared_ptr<coding::meta_model::structural::object_template> ot) {

to maybe:

: void operator()(shared_object_template ot) {

Users should be able to decide which shared pointer implementation to
use (maybe as a parameter to the meta-model element?). We could use
the forward declarations to define the typedef. Alternatively we could
define the typedefs with the leaf class itself, but then we'd be
pulling in the shared pointer into the header.

*** Getter by reference of pointee                                    :story:

A useful use case is, whenever we have a property which is of
pointer-like type (shared pointer, etc), is to return the type pointed
to by const reference. We should be able to configure the generator
for this:

- we can already detect if the type is a pointer type;
- we would need some meta-data at the property level (generate
  de-refenced const/non-const setter). If this is used but the
  property type is not a pointer then we should throw.
- the generator would look for the meta-data, if enabled it would add
  additional setters.
- we may even want to suppress the pointer getters as well.

*** Consider adding stereotypes into profiles                         :story:

It would be nice to be able to define a profile called "ormable" and
then have it enable:

- types
- ODB
- the appropriate ORM stereotype, e.g. =orm::object=, etc.

However, at present there is no way to associate a stereotype with a
profile. Note carefully that we do not mean allow binding profiles
against elements via stereotypes (this is what we do
already). Instead, we are saying is: allow defining stereotypes as a
configurable field within a profile and then when applying the
profile, set the stereotype of the element it binds against.

For this to work correctly we probably need to be able to at least
validate that the element to which we applied the profile is of the
expected type. Our binding at present is just "element" so it cannot
distinguish between enumerations, etc.

As far as just associating stereotypes with profiles, this could
actually be done fairly easily: we could have a dynamic field called
stereotypes and just apply these when expanding profiles. However, we
probably need to think about this very carefully as it could introduce
all sorts of weird and wonderful problems such as cycles in stereotype
resolution (what happens when we apply a stereotype which is actually
a profile name? is that just invalid or should we redo profile
expansion?). Unless we have a very important use case, we probably
should not allow this.

*** Consider adding "deprecated feature names"                        :story:

One way to allow the renaming of features across releases is to keep
track of the old name; we could create a property in a feature called
"deprecated feature name" or some such and when we fail to find a
feature, we could try to see if the deprecated feature names match. If
they do we could bind to the old name, issue a warning and
continue. By removing the name from deprecated feature field we'd
officially stop supporting the old name.

Alternatively we could just process deprecated features in exactly the
same way but issue a warning to users, just like compilers do. We
could also add support for a message deprecation message.

Links:

- [[https://en.cppreference.com/w/cpp/language/attributes/deprecated][C++ attribute: deprecated]]
- [[https://stackoverflow.com/questions/295120/c-mark-as-deprecated][SO: C++ mark as deprecated]]

*** Add string view to dogen exception constructor                    :story:

At present we cannot build an exception if the string passed in is a
string view.

*** Mask sensitive fields in io                                       :story:

Certain types contain fields that should not be logged by default. For
example, passwords, salt/seeds, etc. It should be possible to mark
these fields as "sensitive" such that when one dumps an object to the
logger the fields are masked out with say =****=. It should also be
possible to set an environment variable such as
=MASD_DO_NOT_MASK_SENTIVE= and get the actual values printed.

To implement this we need:

- a feature for marking fields as sensitive. Add a sensitive default
  for each primitive type, e.g. =****= for strings, =1234= for
  numbers, etc.
- a new manipulator in the shared library: =masd::unmask_sensitive=.
- update io for fields marked as sensitive; by default output the
  sensitive default unless =masd::unmask_sensitive= - in which case
  output the real value.

Notes:

- consider adding a warning for fields with certain names such as
  "password": mark this field as sensitive?

Merged stories:

*Consider adding a global configuration for io*

It would be nice to have some kind of configuration for IO that could
be accessed globally for the current process. There we could set
things such as floating point display, etc.

Actually maybe the right thing to do is to have masd specific
manipulators that you can check for in the streaming functions. We
need to read up on manipulators.

Links:

- [[http://www.two-sdg.demon.co.uk/curbralan/papers/WritingStreamManipulators.html][Writing your own stream manipulators]]

*** Add models for executables                                        :story:

We have now updated all of dogen so that everything is a model,
including the executables. However, we still haven't added high-level
concepts to generate the right targets for both executables and
libraries. We also need a template for entry point.

Interestingly, it seems that a dogen component generates multiple
components:

- tests executable
- library, executable
- in a library per facet, multiple libraries

We need to take this into account when we implement this.

Merged stories:

*Allow generating executables from dogen*

At present dogen always assumes we want to generate a static
library. It would be nice to be able to generate an executable too,
with all of the cmake infrastructure generated.

This falls under the "model types" work, though perhaps its not clear
how it maps to libraries/executables. The problem is that at present,
models generate multiple targets: tests, library/executable.


*Previous understanding*

At present the executables are all hand-crafted. However, as we want
to move the options into each executable we need them to be in a
model.

When we tackle this we should take into account testability as well.
At present we have some hacks around binaries to allow us to test the
code. We manually create a static library that excludes main so that
the tests can include it. We should make this the code-generated
approach when we start generating CMake files for binaries.

This is all made much simpler if we use the Build2 approach of merging
all files into a single directory and have multiple targets per
project. However, we may need to manually add files to each target or
at least ensure there is a good way of doing this via regex. For
example, all test files can be =.test.cpp=. We can then use a regex to
exclude these from the main binary, and exclude =main.cpp= from the
tests. Or it may make sense to always create a static or shared
library for all files excluding =main.cpp= and then link against it.

Notes:

- stereotype: classes annotated with =masd::entry_point= will only
  have a =.cpp= and have main on it.
- users can choose "themes"; for example masd orchestration theme
  creates a main that includes orchestrator and executes it. Validator
  ensures that there is such a class.

Tasks:

- tailor generates names with extensions so we have yarn.dia.json. We
  need to drop the .dia part.
- references have extensions on them as well. We need to drop all
  extensions and then be clever and look for files ending on any of
  the supported extensions. If more than one, error.
- add meta-data for "model type": library or executable. Defaults to
  library. When library, users can make use of the CMake machinery to
  determine if static or shared.
- create a model for each executable and add the options to the model;
- add meta-data to generate an executable instead of a library in
  CMake.
- generate a main skeleton if one does not exist.
- remove options project.

*** Support for multiple "generation strategies"                      :story:

It seems we have a requirement to support different kinds of
generation:

- full generation: this is what we currently associate with an
  =pbject=, =enumeration=, etc.
- partial generation: this would be useful when associated with
  =object= so that we could add methods. For this we can make use of
  protected regions.
- skeleton and stub generation (on-off): we use this for handcrafted
  types; create the initial structure of the class, but then let the
  user manually code it.
- no generation: we don't have this at present, but in some cases we
  may want to just give the user an empty file.
- round-tripping generation: it would be great if the user could mark
  a type as round-tripped and then we would automatically update the
  model with any manual changes and generate new methods, stubs etc as
  the user changes the model. This is conditional on having really
  good clang integration. Users should be aware that when using
  round-tripping problems may occur (code gets deleted etc).

What is interesting here is that we may need all of these kinds of
generation depending on the particular use case; one model may require
all code generation types. Thus this is not a property of the code
generator but of the model.

We could probably call this the "generation strategy". It could be a
property of the model element. However, given that we have multiple
facets, it seems that the generation strategy is also a function of
the facet; certain facets may only support a subset of the generation
strategies.

Links:

- [[https://link.springer.com/chapter/10.1007/978-3-319-27869-8_7][Integration of handwritten and generated object-oriented code]]

Merged stories:

*Add support for "extensions"**

Until we have proper merging support, one feature which would be quite
nice is to mark an element as "extensible"; that would automatically
generate hpp/cpp/cs with an appropriate prefix
(e.g. =TYPE_extensions=) so that the user can add "extension
methods". In C# this would map to real extension methods, in C++ to
just helper functions.

Notes:

- this could be a stereotype.
- we could inject a type with the appropriate name, but then we need
  to ensure it uses the handcrafted profile.
- for extra bonus points: it would be nice if the extensions could be
  made a class with access to private properties in the "extended"
  object. This would allow us to encapsulate state.

*** Additional extraction model post-processing chain transforms      :story:

The following transforms can be done after generation of the
extraction model:

- clang format
- protected regions: read the file on disk, replace contents of the
  protected region with the data read from disk.

Note that we need artefacts to have an associated language so that we
can use the correct clang format configuration. If a language is not
supported by clang format (e.g. c#) we should just skip the files. The
text model could group files by language.

*** Consider adding collections at the meta-model level               :story:

A very common pattern we've observed is the creation of "container"
classes that have a member which is a container (say
=std::list<some_type>=) and a set of properties associated with that
container (name, description, etc). We then expose the underlying type
to users directly. However, perhaps it would make more sense to create
a new meta-type (collection? container?) with associated meta-data; in
C++, it would translate to the creation of a class obeying C++
collection concepts such as iterators etc.

Once this is in place we get code that expresses the high-level domain
concepts but its still pluggable into the STL machinery such as
algorithms, ranges, etc.

This also addresses somewhat the annoying issue with pretty printing
of STL containers because most of the time we would have a domain type
for the STL collection. The approach is akin to C#'s idea of
inheriting from collections, except here we use composition.

This also means we can transparently change the underlying collection;
though it would force a recompile, no code should break (e.g. from
list to vector or array).

In a way this is very similar to what we already do for primitives. We
should try to unify both approaches, though of course there are
significant differences. The idea is to allow users to model a
container after an existing STL container, but exposing a domain
interface.

*** Add support for "directory mode" in conversion                    :story:

The real use case we have for conversion is to point it to a directory
with models and give it a destination "type" (e.g. json) and a output
directory, and then have it convert all models to that type and place
them in the output directory.

A second but related use case is to point it to a model, supply a
destination "type" and then output it into a directory, without having
to supply a destination file.

In effect, this is a common use case for all commands (generate and
weaving as well). We could probably deduce it: if the user supplied a
directory as a target, we should do it in directory mode.

*** Add model sources and sinks in Dogen                              :story:

At present we are reading and writing to files inside the
workflows. This means that if we want to use databases in the future
we will have to update the entire code base to cope with this. A
better approach is to perform IO via some interface, which can be
implemented to target either the filesystem or a database, cache, etc.

Notes:

- add workflow that takes in a string, path, etc and creates a model
  set. It will need to read references and language from the model
  annotations.
- add model source into injection
- add model set into injection, with target and references
- add model set into coding.
- add a new model: coding.injection. Create a class that converts from
  one model set to another.

*** Generate a single type                                            :story:

Sometimes, when a model is in a bit of a state, it would be really
useful to be able to generate a single type (or maybe set of
types). We could easily do this by forcing the overwrite flag to false
to all artefacts other than those that belong to this set of
types. This could be a transform performed just before we generate the
extraction model: loop through all elements and those that are not
in this set, set their overwrite flags to false - or maybe even remove
them from the model?

Of course, we still need for the model to be in a state good enough to
build. We should wait for a proper use case for this feature.

*** Add aliases to enumeration string conversions                     :story:

We often need to cast a enum from a string but the name is not exactly
like the original enumeration. For example, we use enums for
stereotypes but we cannot have the same namespacing structure on the
enum. For these cases it would be nice to be able to supply an alias.

The only slight problem is that if we use this approach, the cast will
still work when you supply the simple or qualified enumeration.

Perhaps we can have two concepts:

- alias. Everything else will still work.
- overrides. Only the override is considered valid. Conversions will
  now always use the override.

*** Consider adding a =to_string= facet                               :story:

We originally added the boost lexical cast facet, but that requires
boost. The new  C++ approach seems to be to use the conversion methods
=to_string=. However, there is no equivalent "from string". We could
add it though.

Links:

- [[http://www.cplusplus.com/reference/string/to_string/][to_string]]

*** Consider extending lexical casts, =to_string= to any type         :story:

Perhaps it is a useful thing to dump an object into a string (via
possibly JSON). We could simply have a default behaviour of if lexical
cast is enabled for an object, dump JSON. This could possibly be
overridable - e.g. supply a parameter that determines the type of
conversion to apply. This is also useful for enums because we could
have several variations: qualified, unqualified.

*** Add support for ignoring types and models                         :story:

#+begin_quote
*Story*: As a dogen user, I want to ignore certain types I am working
on so that I can evolve my diagram over time, whilst still being able
to commit it.
#+end_quote

Sometimes when changing a diagram it may be useful to set some types
to "ignore", i.e. make dogen pretend they don't exist at all. For
instance one may want to introduce new types one at a time. It would
be nice to have a dynamic extension flag for ignoring.

We should probably have some kind of warning to ensure users are aware
of the types being ignored.

Isn't this just using "enable=false" for all formatters?

In a world where we can define bundles of meta-types, and import them
from system models, we could possibly just define a bundle called
=Ignored= with all formatters set to false.

This should be a stereotype to make it really visible. Also, the type
should have a colour that is easy to spot like light gray. Actually
this is ok, we can just add a profile.

This is also useful at the model level so that the model is not used
either for tests or for generation. This is more than just formatting:
its basically "act as if this type was not defined in the input" or
"this model did not exist". It should result in resolution failures if
anyone is referring to the model/type.

*** Consider making fully generated files read-only                   :story:

We could add emacs/vi tags to make fully generated files read-only -
as opposed to partially generated files such as services, which are
expected to be modified by the user. Example:

: /* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4; buffer-read-only: t -*-

There must be a vi equivalent. There is =view= but its not clear how
to set it into a modeline. The alternative is to write the files as
read only.

: /* vim: tw=60: ts=2: view=t: set ro: */

Requires changes  to =.vimrc=:

: set modeline

It would be even better if we could make parts of a file read only, so
that only the protected regions could be written on.

Links:

- [[https://stackoverflow.com/questions/20023363/emacs-remove-region-read-only][emacs remove region read-only]]
- [[https://www.emacswiki.org/emacs/FoldingMode][Folding mode]]

*** Pre-includes defines                                              :story:

For boost test, we need the ability to define a macro before any of
the includes: =BOOST_TEST_MODULE=. At present, the decorations have
both the licence and the includes, and its not possible to place
something in between the two. So we are doing the easy thing and
adding the define before the preamble. In an ideal world we should be
able to inject pre and post includes defines. These can be done by the
formatter or even by the user.

Actually the right thing to do is to have a meta-model attribute to
represent this. We should avoid words such as "includes" and "defines"
and try to model this in a more general way. For example we could
create a macro meta-element that enables users to define any
macro. This then maps to language specific constructs like C++ macros.

Links:

- [[https://www.boost.org/doc/libs/1_69_0/libs/test/doc/html/boost_test/utf_reference/link_references/link_boost_test_module_macro.html][BOOST_TEST_MODULE]]

*** Add support for inlining                                          :story:

We should be able to set a model-wide property that tells dogen to
generate inline methods for properties. We could be more flexible and
allow inlining at class level or just for a single property. We don't
have any use cases for these at present.

*** Create org-mode documentation for =cli=                           :story:

We need a way to associate documentation with a model. The ideal
output is something like =dotnet=:

- https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-sln

In an org-mode world, it would be really nice if we could have the
documentation and the definition of the command line options next to
each other, in the same org-mode model.

*** Consider generating program options code                          :story:

If there was a syntax to describe boost program options, we should be
able to generate most of the code for it:

- the code that initialises the options;
- the domain objects that will store the options;
- the copying of values from program options objects into domain
  objects.

This would mean that creating a command line tool would be a matter of
just supplying an options file. We could then have a stereotype for
this (name to be yet identified). Marking a type with this stereotype
and supplying the appropriate meta-data so one could locate the
options file would cause dogen to emit the program options binding
code.

A similar concept seems to exist for python: [[http://docopt.org/][docopt]]. We should keep
the same syntax. We just need to have a well defined domain object for
these. The aim would be to replace config.

For models such as these, the dia representation is just overhead. It
would be great if we could do it using just JSON.

Actually even better would be if we could have a text file in docopt
format and parse it and then use it to generate the code described
above.

Actually maybe we are just making this too complicated. We probably
just need some very trivial meta-data extensions that express the
required concept:

- create a yarn element to model this new meta-class. We basically
  need to model the structure of program options with option groups
  and options.
- define a stereotype for the new yarn elements, say
  =CommandLineOptionGroup=.
- for types facet we simply generate the regular c++ code. But in
  addition, we also generate a new facet that: a) injects the
  propertties into boost program options b) instantiates the c++
  objects from boost program options.
- this means that instead of creating a new meta-type, we need to
  augment =yarn::object= with command line options stuff.

Notes:

- create stereotypes for options group, options; allow users to define
  members of type options in options group. Or should the options just
  be member variables? In which case we could have
  =command_line::options= as the stereotype.
- generate the options classes.
- inject a hand-crafted validator or consider generating the validator
  given the meta-data supplied by the user (mandatory, at most X
  times, etc).
- generate an options builder that takes on the building
  responsibilities from the parser.
- generate a parser that hooks the builder and copies data from the
  options map into the options.
- allow users to supply the help text and the version text as
  parameters; these should probably be done in a similar way to what
  we do with the modeline etc.
- allow users to set default values in the options attributes and set
  them in generated code. This is probably just adding default value
  support to dogen, for which we have a separate story.
- one very useful way in which to use program options is via
  projections. That is a given model M0 defines the configuration and
  a second model M1 defines the options parsing. In this case the
  options defined in M0 already has the required shape:
  - there is a top-level class housing all options, traditionally
    called "configuration";
  - the top-level class contains meta-data with the product blurb;
  - attributes of that class can be annotated as "modes", "groups" or
    nothing. A mode will result in a modal CLI interface. Groups
    result in top-level groupings of options. Nothing means the
    attribute must be of a simple type and will be a global option
    (e.g. =help=, =version=, etc).
  - attributes have a description, etc associated as meta-data. They
    also have other useful annotations such as optional, mandatory
    etc. These are used in validation. Interestingly this may mean we
    can also automatically generate a validator.
  - dogen generates in M1 a set of chained program option parsers
    (assuming a modal interface; otherwise just one) which generate
    the M0 options.
  - in M1, users define a class with attribute
    =masd::command_line_options=, associated with an options class.
  - users can choose the "backend": boost program options, etc. Each
    is implemented as a separate template.
  - dogen generates a parser with an associated exception
    (parser_validation_error). The exception is simply injected as a
    type.
- we should have a look at the existing syntax for defining command
  line options and extract the meta-model elements for them. These can
  then be injected into our logical model under a suitable namespace.

Links:

- [[https://github.com/abolz/CmdLine2][CmdLine2]]: alternative library to program options.
- [[https://github.com/p-ranav/argparse][argparse]]: Argument Parser for Modern C++

*** Code-generate a "one-shot" serialisation API                      :story:

For the dia model we manually generated a class called
=diagram_serialization_helper=. It provided a simple API to read/write
the dia model:

: static void to_xml(std::ostream& s, const diagram& d);
: static diagram from_xml(std::istream& s);

It would be nice to have this code-generated and also to cover the
other two archives (text, binary).

Users should be able to mark types with some property so that we know
we need to generate these wrappers.

Another common use case is to serialise from and to string. It would
be nice to have helpers for strings too.

These APIs can then be reused by the caching layer.

We could have a stereotype that marks a class as requiring this kind
of serialisation, such as "serialisation entry point".

We should code-generate this API. They are equivalent to persisters as
well.

Merged stories:

*Generalise persister and remove serialisation helpers*

With the move of the knit persister into each model, it became obvious
that users need a way to hydrate and dehydrate certain types by just
supplying a path. The ideal setup would be where each supported
serialisation mechanism registers a number of extensions with an
hydrator / dehydrator and the user can supply a path; the path gets
dispatched to the correct serialisation. In this world we wouldn't
need the XML serialisation helper (in utilities) because we would code
generate a complete serialisation solution. This only works for files
(and not for streams) because we infer the format from the
extension. Having said that, if there was a way to supply an enum or
such-like with the stream, we could create a class for streams and
then implement the file one as an adaptor to the stream class.

*Persisters only support XML*

Persister should support all archive types. At present it always
outputs in XML; it should respect the archive type requested by the
user.

*Persisters should throw on invalid archive types*

At present we are checking to see if the archive type is invalid, and
if so ignoring it:

:     if (at == archive_types::invalid)
:        return; // FIXME: should we not throw?
:
:    const auto& dp(create_debug_file_path(at, p));
:    sml::persister persister;
:    persister.persist(m, dp);

We should:

- pass the archive type into persister;
- throw if the archive type is not supported.

*** Add visibility to coding attributes                               :story:

We need to be able to mark yarn attributes as:

- public
- private
- protected

*** Add support for default values                                    :story:

 #+begin_quote
 *Story*: As a dogen user, I want to set default values for certain
 built-ins so that I can model my domain more accurately.
 #+end_quote

 It would be nice to be able to add a default value in Dia and have it
 set on the default constructor, if the type is a built-in or a
 =std::string=.

 These could be added as dynamic extensions; however, they should not
 be model specific - the same defaults should apply to all
 languages. However, note that in certain cases the default values may
 be language specific (e.g. =0.0f= for float, etc). We should have a
 language independent way to express defaults and then interpret them
 in language specific ways at the template level.

 Note also that there are two levels of default values: the type-level
 (all built-ins of type =x= default to =y=) and the property level
 (instances of type =x= in class =z= default to =y=). The meta-data can
 be applied at each level to achieve the desired effect.

 Defaults are also useful with const properties, both if the class is
 immutable, or if just the defaulted property is immutable.

 Validation rules:

 - property can only have a default value if built-in
 - property default value must be castable to built-in type.

 Notes:

 - we have now implemented support for values at the dia and injection
   level (not JSON). We just need to propagate it into coding and
   express it on the model to text transforms.

*** Add support for interfaces                                         :epic:

It would be great to be able to mark a type as an interface, add all
methods that the interface has, and have dogen fully code-generate the
interface, marking all methods as pure virtual, override, etc. We
would want to distinguish this from the abstract base class case,
because we may want to add one or more concrete methods to the ABC.

We need support for operations for this to work. We also need support
for pointers, references and const.

Merged stories:

*Add =interface= meta-model element*

Even though we can't generate much outside of plain types, we should
already have support for a stereotype of =interface=. In the future we
may be able to code generate the interface. This should be implemented
in coding as a type on its own right.

- add an interface which is: element, operatable, relatable. Not
  stateful. We should also have a "is abstract" flag
  somewhere. Perhaps in relatable?

*** Add support for private and protected attributes                  :story:

#+begin_quote
*Story*: As a dogen user, I would like to define properties as
protected so that I only expose them to derived types.
#+end_quote

We need to distinguish between public and protected attributes when in
the presence of inheritance. If not, issue a warning.

Another interesting use case for protected and private attributes is
extensions. This is not yet supported by C++, but one can envision a
time where it will be possible to extend the code generated classes
with manually crafted code; these extensions may have access to
protected/private state, thus allowing for encapsulation.

*** Add support for error info augmenting of exceptions               :story:

When we declare a boost exception, we some times need to augment it
with additional data. This is explained [[http://www.boost.org/doc/libs/1_58_0/libs/exception/doc/tutorial_transporting_data.html][here]]. We should be able to add
these tags to the exception class and have dogen generate the
=error_info= boilerplate in the exception header.

Note that we can have many tags associated with a given
exception. They all need to have meaningful names. They should all
have globally unique names, unless the name makes sense in the context
of more than one exception. We could make the tags a property of the
exception (e.g. inner class). Or we could simply have a global class
that declares all of the application tags in one go. If most tags are
exception specific, then the first approach is better. If many tags
are shared, the latter is preferable.

In terms of dia/frontend, we can simply add these as attributes of the
exception and have the code generator do the right thing for
C++. Other languages would/could have different implementations.

Once we implement this we need to go through all exceptions and add
all the needed attributes; in many cases we are being lazy and logging
one message but throwing a less informative one. This should really be
captured in the exception type.

We should probably support some meta-data to enable/disable this
behaviour, in case someone wants real attributes in an exception.

*** Add support for deprecation                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to mark certain properties, classes
or methods as deprecated so that I can tell my users to stop using
them.
#+end_quote

We should be able to mark classes and properties as deprecated and
have that reflected in both doxygen and C++-11 deprecated attributes.

Note that at present nothing stops the users from adding the marker
themselves.

Perhaps we should add general support for attributes. This would be
useful for languages like C# and Java, to control serialisation, etc.

We should make this approach consistent with deprecation of features
as per [[*Consider adding "deprecated feature names"][this story]].

** Technical debt and refactoring

Stories that deal with cleaning up the code base.

*** Detect duplicate attributes via validator                         :story:

At present if we add two attributes with the same name, we get a weird
exception in the guts of the logical model:

: 2020-10-01 11:25:07.331325 [TRACE] [logical.helpers.configuration_model_set_adapter] Extracting: tests_directory_name (dogen.physical.entities.project_path_properties.tests_directory_name)
: 2020-10-01 11:25:07.331328 [TRACE] [logical.helpers.configuration_model_set_adapter] Extracting: tests_directory_name (dogen.physical.entities.project_path_properties.tests_directory_name)
: 2020-10-01 11:25:07.331331 [ERROR] [logical.helpers.configuration_model_set_adapter] Duplicate qualified name for configuration: dogen.physical.entities.project_path_properties.tests_directory_name

It would be more user friendly if we had a validator, probably even at
the codec level, that throws if there are attributes with duplicate
names.

*** Use GCC to code generate tests in nightly                         :story:

At present we seem to be using the clang binary to code generate the
code for the gcc build (somehow). This means that if clang fails, GCC
will behave strangely (loss of all generated tests, massive drop in
code coverage). We should use GCC for GCC code generation.

*** Do not throw when getting attribute value                         :story:

At present in =processed_object_factory= we are getting the attribute
value like so:

: template<typename AttributeValue, typename Variant>
: boost::optional<AttributeValue> try_get_attribute_value(const Variant& v) {
:     AttributeValue r;
:     // FIXME: we should probably replace this with a static visitor.
:     try {
:         r = boost::get<AttributeValue>(v);
:     } catch (const boost::bad_get&) {
:         return boost::optional<AttributeValue>();
:     }
:     return r;
: }

This is really not ideal and means we can't just do a

: catch throw

in gdb when looking for "unexpected exceptions". In general, a =catch
throw= should only show real problems and not "expected"
circumstances. We need to clean up this code and any other that shows
up when using this command.

*** Resolution needs to segment names                                 :story:

At present we are resolving any name in modeling space, regardless of
modeling element type. This is not correct. For example:

- PIM models should not refer to non-PIM types.
- features should only refer to types in the =masd.variability= model.
- archetypes should be able to only refer to logic-less templates.

In addition:

- resolution of licences, modeline groups etc is probably using some
  bespoke logic or suffers from exactly the same problems.

What we really need is to group "referrable names" by meta-model type
(or groups of meta-model types, more accurately) and then resolve only
using types in those groups. This should only be done after we
simplified name resolution by moving to the [[*Rewrite name resolution in terms of lists][list-based approach]].

Merged stories:

*LAM name resolution should be PIM only*

With the current approach, we are not resolving any names until after
we've mapped LAM models into a PSM. This possibly means that users can
provide a model with non-LAM types. Ideally we should not allow
this. Check to see if we are validating this in the present mapping
code.
*** Rewrite name resolution in terms of lists                         :story:

Even since we did the external modules / model modules change we broke
code generation; this is because we do not go up the model modules
during name resolution. We did a quick hack to fix this but it needs
to be done properly.

Let's walk through a simple example:. Name cames in as:

- model module: =probing=
- simple: =prober=

We are in model:

- model module: =dogen.external=

Expected behaviour is to try all combinations of model modules:

- =dogen.external.probing=
- =dogen.probing
- =probing=

This highlights a fundamental problem with resolution: we view the
{external, model, internal} modules as if they are separate entities
but in reality, for the purposes of resolution, there is only one
thing that is relevant: the module path. If it matches because of
{external, model, internal} modules, well that is not relevant to
resolution. Other users of =name= do need to know this information
(for example to generate directories or file names) but not the
resolver.

Interestingly, because we are only looking for an id, it doesn't
really matter how we get to it (in terms of the internal composition
of the name), as long as it matches bitwise. This means we can look at
the process slightly differently:

- start off with the name as the user provided it. Extract all strings
  from it to create a list, in order: external, model, internal,
  simple. Try to resolve that. Call it user list.
- then create a second list from model / context: external, model,
  internal. Call it model list.
- try concantenating model list and user list, pretty printing and
  resolving it. If it fails, pop model list and concatenate again. Try
  until model list is empty.

Tasks:

- first add a quick hack just to get the code generator working
  again. For example, take the first model module of the model and try
  resolving with that. Then worry about fixing this properly. This has
  been done with a big hack.
- split the conversion of name into list from pretty printer. Printer
  should merely take a string or list of strings and do its thing. We
  need to find a good location for this method, since (for now) we
  cannot place it in the right location which is the name class
  itself.
- change resolver to obtain the lists as per above. The to list
  machinery can be used for this, though we need to handle model names
  somehow. We can copy the =model_name_mode= logic from printer.
- drop all of the logic in resolver at present and use the list logic
  as per above. Do not check references, etc.

Notes:

- there are a few useful functions here:
  - subtraction: given a base list, subtract another list. Fro
    example, given =masd::dogen::annotations::annotation=, subtract
    =masd::dogen::annotations=. This is useful when determining the
    right qualification inside a class.
  - addition: concatenate a list with another.
  - combination: given a base list, create all possible permutations
    for a second list. For example: =masd::dogen::annotations= and
    =some::type=, we want =masd::dogen::annotations::some::type=,
    =masd::dogen::some::type=, =masd::some::type=, =some::type=. We
    are iterating upwards the first list.
  - make id: given a list, generate an ID. This was we don't even need
    to go though the whole "name building" exercise, we simply go from
    lists into ID's and check the containers.
- we probably should introduce a type for this: =flat_location=?
  something that can be converted from a =location= (but not the
  opposite) and has the properties defined above. Or we could have a
  "location flattener" that performs these actions, but this is less
  clean as we now need a few of these helpers.
- there are two fundamental concepts: a path (which is what we call a
  location) and an address (which is what we call an ID). Path implies
  an hierarchical space, which is what modeling and generation space
  are. Address is flat and unique. There is a function to go from
  paths to addresses but not vice-versa. Given two paths we can
  generate all possible addresses by performing a "climb" in the
  hierarchical space.
- we could make addresses URIs, and preserve almost all of the
  information: =masd://some.model.name/a/b.c=. The problem is we
  cannot tell the difference between model modules and external
  modules. However, we could simplify this and say model modules and
  external modules are all the same thing; users can choose to express
  external modules as part of the file name or not. (e.g. "express
  full path" or some such flag). We can also choose to express
  external modules as directories or as a dotted path. URIs may not be
  the best of ideas because models exist in contexts (workspaces,
  servers, users) rather than in one universal space. However, we
  could use URLs as a way to identify resources once we clear up the
  REST story.
- another very interesting point is that the separation of parsing and
  resolution may not be a great idea. At present we parse "unparsed
  types" in attributes and create "proto-names"; these have guesses as
  to what the different components should be (model module,
  etc). However, a slightly different way to look at this is, we could
  just create the lists off of the parsing and then send them to the
  resolver; the resolver would either resolve them into a name or
  fail. There would be no need to have this in-between state of
  half-filled names. However, if we do want to separate parsing from
  resolution, perhaps the right solution is to create a parallel
  structure to the name tree which is made up of nested lists. The
  parser populates these and the resolver uses them to create the name
  trees. Even better: we can move all of this work into injection. The
  parsing into lists can happen in injection and then we just need to
  map these into model types in coding (resolution). The creation of
  the name tree structure is resolution.
- if we do go for the lists approach, perhaps we should also have two
  types of name trees: a list based name tree (call these parsed
  types) and a names based name tree (resolved types?). List trees are
  not really name trees I guess, but they are in the right format for
  the resolver to use. This would mean the parser could be simplified,
  and the whole "lets guess what this scope is" could be removed. The
  job of the parser is just to produce these lists. The resovler then
  resolves them into actual names. If we go for this approach, this
  should be part of injection: we'd have "value trees" in injection
  and "name trees" in assets. Resolution is in effect the conversion
  of these two tree types. However, the only slight problem is that,
  at present, resolution is done in the middle of the assets pipeline,
  after merging. For this to work, we'd need to, somehow, move it to
  the injection to assets adaption phase. This is not entirely
  inconceivable: we already have all elements with their correct names
  at this stage. If we could view the entirety of the model set rather
  than just the model when resolving, it would work. In other words,
  if we built a single map with all of the element IDs across the
  model set and used it to resolve, according to a well-defined set of
  rules, it should work. However, the set of rules themselves would
  involve some processing: we need to know about module containment
  (external, model module and internal). Nonetheless, this is probably
  already available at this stage - i.e. the owning element should
  have this information as part of the adaption process. And in truth,
  this is the only thing we should be using in resolution. That is,
  the original lists as obtained from injection, plus the owning name;
  we then perform a traversal/climb using the well-defined rules
  above, and for each point in the climb, do a look-up in the map. If
  found, use the name pointed to by the element. If not found, keep
  climbing. The only slight problem we now have is we created a
  circular dependency: in order to perform resolution we need access
  to all the names in the model set, but these are only generated as
  part of the adaption of the injection model to the asset model. We
  need to somehow have a two pass approach: first build all the names,
  then adapt the elements. This could be done in engine, as part of
  the injection to assets chain.

*** Create tests for variability overrides                            :story:

We've added all the functionality needed to override meta-data, but we
did not update any of the test models to exercise all of its
permutations: update model, element, attribute.

*** Simplify implementation of boost serialisation registrar          :story:

at present we are using templates on the cpp file, and it is not clear
if the hackery we did is not causing problems. We seem to have trouble
with some of the tests on windows which appear to be related to
registration problems. This could be because of the weird and
wonderful C++ rules around how the linker removes unused code and
templates, etc across different compilers. If possible, we should have
a really simple implementation for each archive type, that is
guaranteed to work on any compiler and any c++ version. Or we could
just create a simple header file for this (see [[https://www.boost.org/doc/libs/1_70_0/libs/serialization/doc/serialization.html#registration][registration]]). In
addition, it seems we now have a new way of exporting classes which is
very simple: [[https://www.boost.org/doc/libs/1_70_0/libs/serialization/doc/serialization.html#export][Export]]. We need to see when this was made available in
boost though.

Notes:

- we need to check that we are calling register types for all
  references. It seems that, at present, we assume it will be
  present. We should check to see that the registrar type exists on
  the referenced model first.

*** Merging of profiles and configurations is non-intuitive           :story:

As per comments in profile binding:

#+begin_quote
Finally, merge against the configuration. This must be done in order:
first the accumulating_profile, the base layer. This ordering is
*highly* non-intuitive. It derives from the fact that, on a merge, lhs
takes precedence over rhs. If we merge the base layer first, as it is
logical, this would mean that the "overrides" would fail to override
for all of the features that the base layer has already set. Clearly
base layer is not a good name here; its more of a "default feature
configuration" or something of the sort.
#+end_quote

*** Handling of forward declarations on generated types               :story:

At present, if we disable forward declarations globally (in a profile,
say), the code fails to build with errors on visitors. This is because
we need forward declarations for:

- the visitable type;
- all of its descendants;
- the visitor.

This is a hard requirement because, without these the code does not
make sense. We need some way of "forcing" enablement for some features
where there is such a hard dependency. This is probably something we
need to look at when we implement "computable enablement". We then
need some way of telling the system about these dependencies:
e.g. visitor requires enablement x, y, z. This should done in archetypes
by declaring facet depencies.

A second problem is that, at present, there is no way to manually
enable (force) forward declarations on visitors. We can enable them on
all model elements but not on the generated type. Because of this we
are generating forward declarations for all types, for no reason.

The more general problem here is the declaration of dependency between
formatters and the ability to take these into account when resolving
enablement.

*** Add "is abstract" to assets                                       :story:

With C# we have started deciding if a class is abstract or not on the
basis of whether its a parent, etc. The right thing to do is to have a
"is abstract" property which is populated on the guts of assets (using
the current logic of parents are abstract).

We then need to review the C++ templates and figure out where we were
also inferring "abstractness", and use the new flag.

Actually we already have an is abstract flag, we just need to check
its been used correctly across all kernels.

*** Implement CLI configuration validator                             :story:

At present we are not performing any validation on the new
configuration classes we created in the API model.

*** Create a theme for element colours based on an algorithm          :story:

It would be nice if we could automatically determine the colours for
new modeling elements. This could be done by choosing some kind of
base colour for each group and then incrementing by a colour step for
each element. This can easily be done in the python script. The
question is whether the result would look sensible. This needs some
experimentation. We also need to make sure that the colour step
results in colours that are distinct enough so that we can recognise
the elements. We also need to make sure the elements are readable.

Links:

- [[https://stackoverflow.com/questions/214359/converting-hex-color-to-rgb-and-vice-versa][Converting hex color to RGB and vice-versa]]
- [[https://stackoverflow.com/questions/43001349/convert-numpy-array-of-rgb-values-to-hex-using-format-operator][Convert numpy array of RGB values to hex using format operator %]]
- [[https://www.gnu.org/software/emacs/manual/html_node/emacs/Colors.html][emacs: Colors for Faces]]: list RGB colours in emacs.
- [[https://jiffyclub.github.io/palettable/][Palettable GH]]: Color palettes for Python
- [[https://www.colour-science.org/][colour GH]]: "Colour is an open-source Python package providing a
  comprehensive number of algorithms and datasets for colour science."
- [[https://seaborn.pydata.org/tutorial/color_palettes.html][seaborn colour palettes]]

*** Improve error messages for mistakes in meta-data enums            :story:

At present when one makes a mistake in meta data the errors are not
particularly enlightening:

: FAILED: projects/dogen.models/dia/CMakeFiles/generate_dogen.engine.dia
: cd /work/DomainDrivenConsulting/masd/dogen/integration/build/output/clang9/Release && /work/DomainDrivenConsulting/masd/dogen/integration/build/output/clang9/Release/stage/bin/dogen.cli generate --target /work/DomainDrivenConsulting/masd/dogen/integration/projects/dogen.models/dia/dogen.engine.dia --output-directory /work/DomainDrivenConsulting/masd/dogen/integration/projects/
: Error: bad lexical cast: source type value could not be interpreted as target

This was caused because we put in an invalid binding point:

: #DOGEN masd.variability.default_binding_point=entity

We should trap the lexical cast exception and provide a proper error
given the context. We could also calculate the levenshtein distance
against the existing enums when the errors.

*** Tracing backend is not defaulted                        :reviewing:story:

Not supplying a tracing backend results in the following error:

: FAILED: projects/dogen.models/dia/CMakeFiles/generate_dogen.dia
: cd /work/DomainDrivenConsulting/masd/dogen/integration/build/output/clang9/Release && /work/DomainDrivenConsulting/masd/dogen/integration/build/output/clang9/Release/stage/bin/dogen.cli generate --target /work/DomainDrivenConsulting/masd/dogen/integration/projects/dogen.models/dia/dogen.dia --log-enabled --log-level trace --tracing-enabled --tracing-level detail --tracing-guids-enabled --reporting-enabled --reporting-style org-mode --output-directory /work/DomainDrivenConsulting/masd/dogen/integration/projects/
: Error: Tracing backend is unsupported: { "__type__": "tracing_backend", "value": "invalid" }

We need to add a sensible default value.

*** Naming of DLL entry point does not follow existing convention     :story:

With the introduction of =entry_point=, users can now declare an
entity representing the component's entry point. It is projected to
the different facets:

- for types, it should mean =main.cpp= for executables and
  =dllmain.cpp= for shared objects.
- for tests it should mean =main.cpp=.

At present we do not have a way to define a "prefix" for a
formatter. Thus it is not possible to generate a file called
=dllmain.cpp=. We need to investigate the naming conventions for these
files.

*** Boost test module is named after the entity                       :story:

At present we have:

: #define BOOST_TEST_MODULE cpp_ref_impl::boost_model::entry_point

In reality, the test module name should be more something reflective
of the model itself, such as cpp_ref_impl::boost_model::tests.

*** Use profile overrides reduce the number of test models            :story:

At present we have a number of test models in C++ reference
implementation that are variations on the same theme: testing the
enablement of a single facet. This could easily be achieved with a
single model by using the new profile override machinery that was
developed for nightlies.

Notes:

- we need to somehow also override the model modules.
- we could create separate CMake targets for these special cases.
- this may also apply to cases such as delete extra, etc. We need to
  do an inventory of all models that have exactly the same composition
  but different configurations.

*** Improve error message for missing files                           :story:

At present when we are expecting a file but the file is missing, the
tests state:

: Unexpected file (remove): CSharpRefImpl.CSharpModel/SequenceGenerators/AssistantSequenceGenerator.cs
: Unexpected file (remove): CSharpRefImpl.CSharpModel/Dumpers/AssistantDumper.cs

We should really say "Expected file". Think a bit more about the right
wording for these messages.

*** Add "structural" to structural stereotypes                        :story:

At present all masd stereotypes are namespaced correctly, according to
the matching meta-model elements, except for the structural ones. That
is, we do =masd::entry_point= instead of
=masd::structural::entry_point= etc.

*** Mop-up nested namespaces using legacy syntax                      :story:

It seems we still have a number of places in the templates where we
are using the legacy nested namespaces. Its probably only in
serialisation, given that's the only place where we've hard-coded the
namespaces and they are more than one level deep (we have a lot of
=std= but that's not affected):

: namespace boost {
: namespace serialization {

We need to wrap these in if's for C++ 17 and add nested namespaces.

*** Tidy-up profile names                                             :story:

At present we have two types of profiles:

- the "enable just one facet" profiles such as typeable, etc. These
  are meant to allow for composition: e.g., =dogen::typeable,
  dogen::pretty_printable=. in order to allow composition, the type is
  based on profile "disable all facets".
- the "handcraft this type" profiles such as =handcrafted_typeable=,
  =handcrafted_pretty_printable= etc. These are also meant for
  composition. Again, their base type is using "disable all facets".

There are several problems:

- the names aren't great. Unless you know Dogen you don't know what
  most of these profile names mean.
- We now have a third use case: enable types without disabling all
  other facets. This is needed because we want the entry point in CLI
  to work for tests with profile overrides. For now we need to do this
  manually.

*** Add inheritance via meta-data to profiles                         :story:

For objects we can use meta-data to declare inheritance relationships:

: #DOGEN masd.generalization.parent=assets::meta_model::element

However, this is not honoured for other element types. We should
extend this to any case where there is inheritance. At present these
are:

- object templates;
- profiles.

Interestingly we seem to support a different kind of "inheritance" via
 =masd.variability.profile=, e.g.:

: masd.variability.profile = dogen.profiles.base.disable_all_facets
*** Consider renaming =origin_types=                                  :story:

We created an enumeration called =origin_types= to distinguish between
models of different types:

- target model
- reference model:
  - proxy reference: a PDM really.
  - non-proxy reference: a regular dogen model we reference.

However, this is not really the model's "origin". It is more like "the
model's purpose in this context". We need to think more about the
meaning of this enumeration.

*** Consider merging all "registrar-like" entities                    :story:

At present we have a registrar in C++ generation model which is used
to generate the code to register types for boost serialisation. This
principle is fairly universal, and probably all serialisation
technologies will require something of this kind in presence of
inheritance. In addition to this, we have the related notion of
"initialisers". These are used to register types such as:

- formatters
- feature templates

The notion here is that we need to know of a set of related
types. Whilst registration happens at run time, in reality all is
known at compile time and we generate all of the required code up
front. If we attempt to generalise this idea, it seems we have:

- a set of types that need to be registered, according to some
  condition. Examples: the meta-type (feature template, etc), the fact
  that the type is a base class/derived class, etc.
- a class responsible for registration.
- a container of some kind that remembers what was registered. The
  type of the data stored is a parameter. It could be that we want
  pointers to an interface (e.g. formatters) or just remember some
  strings.
- a set of initialisers that talk to the registrar to register the
- there is a top-level class that creates the repository and then
  passes it through to the initialisers to get it populated. Further
  transforms may then be made to the repository.

We should look into registrar terminology to see if all of these
actors have names.

Notes:

- the construction of the type may be non-trivial. We may need to
  supply some arguments, etc. We need to catalogue all of the
  initialisers and see the comonalities.
- bundle initialisers introduced the notion of "scope" whereby an
  initialiser will look for types in the same package and worry only
  about those. Then, a higher level initialiser would call all of the
  lower level initialisers.
- we need to somehow create different initialisers per type or else
  these are going to become too complicated.
- the problem of the serialisation registrar is that, instead of using
  actual data stored in a repository, it works at the template
  level/compule time, with each type being registered against an
  archive. We need to look into the boost serialisation documentation
  and see if there is a way to make the code look a bit more like the
  other instances of this pattern.

*** Code generation of registrars for static registration             :story:

We are using a lot of static registration and we have converged into
what appears to be a useful pattern. It would be nice to be able to
mark a type as registrable and to have the registrar automatically
generated for it, with a name configurable via dynamic extensions. We
would also need to configure the registration method (name,
arguments - we may want to register against a string or just have a
list of registered types).

The next logical step would be to code-generate the static
initialisers too. For this we would have to be aware of all types to
register in a given model (perhaps by looking at inheritance across
models) and for each of these generate the appropriate initialiser
code. This is more tricky but it would be really useful. Actually
given the templatisation of formatters, this is not useful any longer.

Tasks:

- create a registrar type in yarn and associated stereotype.
- add keys to link types to a yarn type. This should be generic so
  that we can use it for other model elements. We could either do it
  the hard (static) way where we manually list the types, or in a more
  dynamic way by providing a base class and letting yarn determine all
  of the descendants.
- add formatter interfaces and a stitch template for registrar.

Note: the sample principles probably apply for code generating the
container.

*** Update the MASD UML profile to reflect the latest changes         :story:

The UML profile is now a fair bit out of date. Take advantage of the
down time waiting for builds to sync it.

Merged stories:

*Add =structural= namespace to core elements*

We've created a namespace inside the coding meta-model for the core
entities but we did not update the MASD profile.

Actually structural is not a very good name - all of the meta-model
elements are structural elements, really. We need to find a good name
before we update the stereotypes.

We should update the MASD profile only after we have moved all
entities into the assets meta-model and finished refactoring
generation.

*** Update all exception names to match framework guiidelines         :story:

Exceptions should end with "exception".

*** Fix pictures in old release notes                                 :story:

Many of the pictures we used in the past were in panoramio. These are
now gone. We need to replace them with other pictures.

*** Current implementation of vcxproj has many problems               :story:

Problems:

- bug: startup project is hard-coded in both c++ and c#:

: StartupItem = CppModel.vcxroj
: StartupItem = CSharpModel.csproj

- we should create separate groups for ODB files, etc.
- its not possible to supply versions (VS version, tools version,
  etc).

Links:

- [[https://github.com/Teivaz/vcxproj-generator/blob/master/generate_vcxproj.py%0A][vcxproj-generator]]
- [[https://docs.microsoft.com/en-us/visualstudio/msbuild/itemgroup-element-msbuild?view=vs-2019][ItemGroup element (MSBuild)]]
- [[https://docs.microsoft.com/en-us/visualstudio/msbuild/item-element-msbuild?view=vs-2019][Item element (MSBuild)]]
- [[https://docs.microsoft.com/en-us/cpp/build/reference/vcxproj-file-structure?view=vs-2019][.vcxproj and .props file structure]]
- [[https://docs.microsoft.com/en-us/cpp/build/reference/project-files?view=msvc-160][Project Files Example]]

Merged stories:

*Generate visual studio c++ projects*

No =vcxproj= for c++ and no way to add code-generated files. Ideally one
should be able to include a code-generated file into project with list
of items.

*** Create a meta-model mapping type for ORM                          :story:

At present we are hacking the mapping of types in ODB by adding them
next to the class using it. Ideally we should create a separate header
file with all the mappings in a model and include it as required. The
inclusion logic probably requires a fair bit of cleverness (.e.g is
type in map?).

Notes:

- create a modeling element called =orm::type_map=. It has entries
  with the fields of an ODB type map:
  - database: optional
  - source type
  - destination type
  - to: optional
  - from: optional
- we can use the attributes to represent entries, and name for source
  type and value for destination type. The rest is supplied as
  meta-data.
- the name of the element will give raise to the name of the
  file. There can be more than one map per model.
- for each type with type overrides, check to see if the type name is
  in any of the maps. If so, add an include. At the meta-model level
  this can be captured as a dependency.
- we should add a database of "any" or "all" - this would allow us to
  add references that are not specific to a database engine.
- we should obtain a list of the core types in ODB and check if a type
  is n the list. If not, we should tell the user a type map needs to
  be created.

: #pragma db map sqlite:type("JSON_TEXT") as("TEXT") to("json((?))")
: #pragma db map pgsql:type("JSONB") as("TEXT") to("to_jsonb((?)::jsonb)") from("from_jsonb((?))")

*** Improve handling of stereotypes                                   :story:

At present we can add any string as a stereotype. If anyone binds to
that string, we will do "something" if no one binds, we will do
"nothing". This is not ideal:

- its not easy to tell what stereotypes are available and what they
  do.
- if a user is expecting some functionality to come out based on a
  stereotype, they won't know why it didn't.
- more than one consumer may exist for a single stereotype - e.g. a
  stereotype may have more than one meaning by mistake.

Ideally we should have:

- a central registry of stereotypes with associated descriptions and
  modeled by a meta-model concept of a stereotype.
- a validation check that all stereotypes match registered stereotypes
  and a fatal error if not (perhaps overridable?)
- a command-line parameter to dump available stereotypes and their
  descriptions so that users know whats available.
- a check that a stereotype has not yet been registered so only one
  consumer can bind to it.

*** Single reporting format option                                    :story:

At present we have different options for report format for each
format. It makes more sense to have a single option, so that for
example we use org-mode for tracing and reporting. Note also that the
byproducts dir does not have =cli=.

*** Clean up injection element properties                             :story:

When PDMs were deemed a hack, we did a number of quick hacks to
provide missing information directly in the JSON:

- =can_be_primitive_underlier=
- =in_global_module=
- =can_be_enumeration_underlier=
- =is_default_enumeration_type=
- =is_associative_container=

And maybe more. These were added as attributes of the JSON and placed
directly in the injection element. This means its not possible to set
them from Dia. The right solution for this is as follows:

- add a transform in injection that reads these properties and sets
  them in the element. This is now possible because we have proper
  annotation support.
  - move the attributes to meta-data on all JSON models.

*** Multiple entries of the same key is invalid in JSON               :story:

The gist of it is that we have invalid JSON at present on our JSON
models:

:    "masd.injection.reference": "cpp.builtins",
:    "masd.injection.reference": "cpp.std",
:    "masd.injection.reference": "cpp.boost",
:    "masd.injection.reference": "masd",
:    "masd.extraction.ignore_files_matching_regex": ".*/CMakeLists.txt",
:    "masd.extraction.ignore_files_matching_regex": ".*/test/.*",
:    "masd.extraction.ignore_files_matching_regex": ".*/tests/.*",
:    "masd.extraction.delete_extra_files": "true",
:    "masd.extraction.delete_empty_directories": "true",

The duplicate keys in an object are not valid, which means that they
are filtered out when we indent. A quick fix for this is to allow
users to supply arrays in JSON but then internally map them back to
flat key value pairs. This should be really easy to do. We should also
at some point add some validation to stop users from adding silly
things such as objects as values, etc.

One interesting caveat is conversion from dia to JSON. If the keys on
the Dia model are not all together, the converter needs to be clever
enough to locate all keys with the same name and group them into the
same container, else we will end up having exactly the same
problem. However, we should try to preserve the order of the keys as
much as possible because this makes troubleshooting much easier.

Merged stories:

*Support containers correctly in annotations*

At present we are allowing users to enter the same key multiple times
to represent a container:

: #DOGEN yarn.output_language=cpp
: #DOGEN yarn.output_language=csharp


This was an acceptable pattern from a Dia perspective, because we had
control of the KVP semantics. However, when we copied the pattern
across to the JSON representation things did not work out so
well. This is because the following JSON:

:     "yarn.output_language": "csharp",
:     "yarn.output_language": "cpp",

Is interpreted by a lot of JSON parsers as a duplicate, and results on
only a single KVP making it. We could try to solve a lot of problems
in one go and standardise all of the meta-data on JSON:

- use start and end markers to enclose the JSON when in dia. Story:
  [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-adding-a-start-and-end-dogen-variable-block-in-dia][Consider adding a start and end dogen variable block in dia]]
- this would also solve the problem with pairs (or at least part of
  it). Story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_99.org#add-a-new-annotation-type-of-pair][Add a new annotation type of “pair”]]
- we could allow users to keep the JSON externally. Story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_99.org#add-support-for-one-off-profiles][Add support
  for “one off” profiles]]
- the JSON would also work nicely with the concept of a dogen
  project. Story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_99.org#introduce-dogen-projects][Introduce dogen projects]]

However, before we embark on this story we need to perform a lot of
analysis on this.

Notes:

- yarn.dia.comment is no longer necessary, just look for the
  markers.
- we should only allow arrays of simple types.
- the fragment used inside Dia should be identical to the file
  supplied as argument for the one-off profile and it should also
  identical to a fragment inside a project. Do we need to support both
  projects and one-off profiles?

Sample:

#+begin_src
  "annotation": {
    "yarn.dia.comment": true,
    "yarn.dia.external_modules": "dogen::test_models",
    "annotations.profile": "dogen",
    "yarn.input_language": "language_agnostic",
    "yarn.output_language": [ "csharp", "cpp" ]
#+end_src

This error has been picked up by codacy too:

- [[https://www.codacy.com/app/marco-craveiro/dogen/commit?cid%3D79696432&bid%3D3493157&utm_campaign%3Dnew_commit&utm_medium%3DEmail&utm_source%3DInternal][Commit 91886c6]]

*** Check if enable kernel directories is on extraction               :story:

When we moved the kernel logic into yarn from quilt, we did not rename
the traits.

*** Add additional checks in generation formatters                    :story:

We are not validating very much in the formatters. Checks we could be
doing for boilerplate formatter:

- anonymous namespaces outside of c++ are invalid.
- ensure TS is supported.
- include guards outside of c++ are invalid.
- include guard must obey a regex.
- c++ dependencies are expected to be enclosed in quotes or angle
  brackets.
- c# dependencies are expected to be valid identifiers.

*** Operations to add when we have code merging support               :story:

This story lists all of the operations that should be added to objects
once we can merge handcrafted code with generated code.

- add =size()= to model and element repository, computed by adding the
  size of all element containers.
- add =merge()= (join?) to models.
- add =id()= to names or maybe to elements (or both).

*** Shared pointers have getters and setters with references          :story:

We should really pass shared pointers by value instead of reference.

*** Improve LAM test models                                           :story:

At present LAM is very basic. We need to ensure that all features work
correctly for LAM:

- inheritance, composition
- dynamic arrays, static arrays
- enumerations
- exceptions
- cross-TS handcrafted profiles

We need to look at the core C++ and C# models and see what else we may
be missing.

*** Create an element builder                                         :story:

At present we are manually populating the core properties of
element. This means every time a new one is added, we need to go and
find all the places where element is being created. We need a template
based builder for element that takes care of these:

- populate implicit properties, such as configuration whenever name is
  populated.
- hide name factory inside of builder.
- to determine the builder API, see all use cases where we are
  manually creating the element.
- populate the origin type. At present we are doing this on a
  transform but the problem is we keep forgetting on adding new
  meta-model types to it. Ideally we just want the types to be
  populated straight on adaption. We just need a flag to be supplied
  to the injection to coding transform (is target?), if we don't have
  one already.

*** Improve model mapping support                                     :story:

Our original mapping implementation was very tentative. It had a
number of half-baked ideas that were not fully implemented. We tried
to preserve some of it, so that when we return to fix it we can
continue from where we left of. The two key ideas that are very
undeveloped are:

- mapping to native types. For example, we want a way to say
  =lam::array<int>= and have that map to =int[]= on C++ and C#. This
  is not entirely trivial because it requires some intelligence at the
  mapping level; we could for example map =array= to setting a bit on
  the =int= type to signify an array.
- mapping to types that are not required on a given technical
  space. For example, =lam::pointer= maps to a =shared_ptr= in C++,
  but to a normal type in C#. We need some kind of "type erasure" or
  again treat this as a native type. This is probably the cleanest
  solution.

Other problems:

- at present there is no way to tell if the mapping support is
  complete or not. For example, we could have forgotten to add a
  reference to a C++ model, which would cause the mappings to fail. We
  have no way of knowing what reference is missing. However, we could
  "grep" a library and determine all of the potential mappings and
  then tell the user that it is missing a reference. This includes
  models that are not referenced. In fact this is useful in general,
  we could have a "light weight model" that just gets the IDs and
  mappings of all injection models and is used on resolution failures
  and mapping failures.
- mapping only allows a single target per element, e.g. =std::string=
  can only be mapped to a single LAM element. This has two
  limitations: first, it is entirely possible we have a LAM type
  mapped to more than one element (for example say we could have two
  string-like types in LAM that are implemented by
  =std::string=). Secondly, users cannot define their own mappings if
  the destination has already been mapped. Both of these limitations
  would be addressed if we made the destination =text_collection=
  rather than just =text=. In theory it should just work.
- we can only have one mapping set. Originally we had envisioned that
  users could supply multiple mapping sets, mapping different types as
  required to LAM types. However, the new setup does not allow for
  this. It could be possible via some weird and wonderful
  parameterisation of mapping targets, but we are yet to have a use
  case for such a complex scenario so its probably not worth worrying
  about it.
- mappings and overrides is a complex issue. Some of the problems we
  have are:
  - assume you can define mapping overrides; how do you determine
    their scope? Do the overrides work only for one of the loaded
    models or all the loaded models? Can you apply overrides to a
    single package? What happens when you have a model set with
    multiple overrides?

The fundamental problem appears to be that we need a good use case for
model mapping. At present we haven't got one so there are too many
possibilities for the implementation. If we do decide to go forward
with the clean up, we should first read up on the literature and then
settle on a subset of functionality that we can reasonably "make up" a
use case for. For example, its useful not to have to define test
models for all languages we support. It would be much nicer if we
could have the "core" test model, testing inheritance, association,
composition and the like, defined in LAM and reused across all
supported output technical spaces.

Notes:

- perhaps the correct solution is to have two different kinds of
  modeling elements related to mapping:
  - the "abstract" element which is in effect a place holder and must
    belong to an abstract technical space such as LAM;
  - the "mapping" element, which is defined against a concrete
    technical space such as C++. Each attribute of the mapping element
    is a mapping between an abstract element and a concrete element.

Merged stories:

*Allow users to override mapping sets at the element level*

Sometimes we may want to use a different mapping just for a particular
element. For example, by default =lam::linked_list= binds to
=std::list= for C++; once Dogen supports =std::forward_list=, one may
want to override this for a partial number of elements. It would be
nice if one could have a meta-data tag at the attribute level that
would override the mapping. The one slight wrinkle is that we would
not be able to supply a breakdown of:

- simple name
- model name
- internal modules

and so forth. So this may cause issues for resolution. We'd have to
test it and see what breaks. Actually this may just work, The current
mappings in LAM take the form of:

: "masd.mapping.target": "masd.lam.numeric.integer8",

This is placed in the source of the mapping. We could allow mapping
overrides locally by letting users define both the source and the
target of a mapping in the context of the attribute. Then, during
mapping, we could use these overrides only in the function that maps
the attribute: e.g. read the local attribute override, override a
local copy of the map, then apply the mapping. The changes are only
local to the override. The problem of global overrides must be tackled
elsewhere.

If this fails, the alternative is that the mapping is by id, and we'd
resolve it internally using the mapping container, e.g.:

- create a map of names for each language by id
- user supplies the id for a given language, we look it up and
  retrieve the name.

*** Improve resolution errors when user fails to reference model      :story:

We could have a "light weight model" that just gets the IDs and
mappings of all injection models that have not been referenced and
uses it for resolution failures and mapping failures.

*** Move to .net core in C#                                           :story:

At present we are using .net framework, but everyone has moved on to
.net core. Update projects to use .net core.

Links:

- [[https://kimsereyblog.blogspot.com/2018/08/sdk-style-project-and-projectassetsjson.html][SDK-Style project and project.assets.json]]

*** Use C# notation in JSON C# models                                 :story:

At present we are generating C++ style types in JSON models (at least
for LAM):

:   "type": "masd::lam::text::character"

If we change this to

:   "type": "masd.lam.text.character"

We get a parsing error. We need to detect the output technical space,
use the correct convention for module separation and ensure it is
valid from a parsing perspective.

*** Consider renaming LAM                                             :story:

We originally created the Language Agnostic Model (LAM). This may not
be the best of names, especially now we moved over to technical
spaces. It should really reflect its PIM (platform independent model)
nature. Names: =masd::lannguage_agnostic=, =masd::la= or maybe
=masd::pim=? LAM is not a MDE term either.

Actually, now that we are moving towards Technical Spaces, the name
should reflect the fact that it belongs to an abstract TS.

*** Remove empty types in ==injection.json= models                    :story:

At present we are adding type to the converted model, even when its
empty. For cases such as enumerations this is just confusing:

:     {
:       "name": "meta_model::static_stereotypes",
:       "documentation": "Lists all stereotypes defined in the masd UML profile.\n",
:       "stereotypes": [
:         "masd::enumeration"
:       ],
:       "fallback_element_type": "masd::object",
:       "attributes": [
:         {
:           "name": "object",
:           "type": ""
:         },
:         {
:           "name": "object_template",
:           "type": ""
:         },
:         {
:           "name": "exception",
:           "type": ""
:         },

It would be much easier to read this if we ignored empty types. We
need to check that the hydrator is not expecting this field.

*** Consider creating top level exceptions                            :story:

There are a number of exceptions that have been repeated across
projects. For example:

- transform error
- building error

We should consider having these in the dogen API and removing them
from each project. However, we need to consider if end users should
necessary know about these exceptions.

*** Add dependency checks to transforms                               :story:

Check the backlog as we may already have this story. At present we do
not have a way to determine if a given transform dependency has been
met. Say for example transform A depends on transform B; we are aware
of this only because the comments on the transform chains tell us
so. It would be nicer if we could declare transform dependencies (and
reasons) as part of the transform "interface" and then check that a
given transform has indeed been applied to a model. This also fits in
with the idea that we keep track of which transforms have affected a
given model. In this sense we need to capture:

- what transforms have been applied;
- whether the transform has modified anything or not.

*** Modeling elements should not have =profile= setup                 :story:

We expected =masd.variability.profile= to be only applicable at the
global scope, but when overriding it manually no error occurs and no
override happens either. Check the binding properties of this
field. We need to ensure that users cannot add this field to profiles
or else we will end up with very weird error messages and/or
behaviours.

Actually this is probably not right - we have stereotypes on elements
and these map to profile binds. The only real problem is when using
profiles on profiles probably. This requires some thinking.

*** Profile overriding may cause link errors                          :story:

We've added profile overriding to solve the nightlies problem:
generating all facets. However, there was a consequence we didn't
notice at the time: the code must always link against *all* the
libraries required by full code generation. That is, if any facet
requires a third-party library, and that facet is normally disabled
but it is enabled on the nightly, we need to link against the library
even when not using the facet.

How we noticed this problem: we removed boost serialisation from our
list of libraries and now the nightly is failing to link with lots of
errors like these:

: /work/DomainDrivenConsulting/masd/vcpkg/masd/installed/x64-linux/include/boost/archive/basic_xml_oarchive.hpp:99: error: undefined reference to 'boost::archive::basic_xml_oarchive<boost::archive::xml_oarchive>::save_start(char const*)'

The right solution is to have "facet specific" link libraries, that
kick in when the facet is enabled. This will not happen for a long
time.

*** References to ODB types in different namespaces fails             :story:

By mistake we created a type which was meant to be in the relational
namespace, but due to dia's peculiarities around copy and pasting,
ended up in the model namespace. The generated ODB files had includes
that placed the referenced ODB files in "types" rather than in
"odb". It seems there is some kind of regex error.

*** Schema name propagation is not handled correctly                  :story:

The logic around the propagation of schema names is very dodgy. Not
only do we rely on overwriting the schema name (e.g. first we take the
containing module schema name, then we check to see if its overridden
at the element level) but we also do not take into account recursive
composition. We need to revisit this.

*** Test ORM support for table name and column name                   :story:

At present it seems we can supply a table name, but no corresponding
ODB pragmas are generated for it. The same seems to be the case with
column name.

Actually column name seems to work because we are using "raw" odb
pragmas:

: #DOGEN masd.orm.odb_pragma=column("LASTNAME")

We should really have an ORM level concept of column name and table
name.

*** Conversion does not output static stereotypes                     :story:

At present we only output dynamic stereotypes. However, there is no
point on fixing this until we move to the new JSON format. This could
also resolve the elment fall back issue in JSON: check to see if any
static stereotypes with element definitions have been supplied, and if
not, use the fallback element type.

Actually we shouldn't even have a fallback element type at the JSON
level; if an injector does not provide a type, we should just default
to say object.

Merged stories:

*Rename =fallback_element_type=*

Our JSON uses a very strangely named attribute to carry the meta-type:

:       "fallback_element_type": "masd::object",

Its not at all obvious what this is meant to do. It should just be the
=element_type=.

We introduced this because users can set the stereotype,
e.g. =masd::object= - but don't always have to (e.g. when converting a
model from Dia). In this case, the fallback element type is
used. Perhaps we can keep the "fallback" logic internally, but just
call it element type?

One possible solution is to simply populate the stereotypes with the
inferred metamodel type. For this we need to check against a list of
metamodel types ("has the user already defined a stereotype?") and if
not, use the default one. This means our conversion will not roundtrip
without differences, but at least it produces more sensible models.

We directly mapped KVPs in UML to JSON, e.g.:

: #DOGEN masd.injection.model_modules=Masd.CSharpRefImpl.CSharpModel
: #DOGEN masd.injection.input_language=csharp
: #DOGEN masd.injection.reference=csharp.builtins
: #DOGEN masd.injection.reference=csharp.system.collections.generic
: #DOGEN masd.injection.reference=csharp.system.collections
: #DOGEN masd.injection.reference=csharp.system
: ...

maps to:

: {
:  "tagged_values": {
:    "masd.injection.dia.comment": "true",
:    "masd.injection.model_modules": "Masd.CSharpRefImpl.CSharpModel",
:    "masd.injection.input_language": "csharp",
:    "masd.injection.reference": "csharp.builtins",
:    "masd.injection.reference": "csharp.system.collections.generic",
:    "masd.injection.reference": "csharp.system.collections",
:    "masd.injection.reference": "csharp.system",
: ...

However, we cannot have duplicate keys in JSON, resulting in problems
when we indent models: the indenter removes all duplicate keys but
one. This means we have to massage models post indentation every
time. Solutions:

- use a JSON container for container keys. The problem with this is
  that our internal representation does not have a container but a
  list of KVPs. We need to somehow convert to and from this container
  representation. We also need to be able to dynamically determine if
  the value is a container or just a plain value when deserialising
  from JSON. If it's a container, we need to flatten it.
- actually, now that we added annotations to the injection model, we
  can first perform the annotations transform; this would convert the
  keys to the right types. We can then convert to JSON using the
  annotations. However, the one downside of this approach is that the
  JSON representation of injection would be at a higher level of
  abstraction.
- the final solution for this is to make the map a container of
  pairs. In effect that is what the container is in the first place,
  we just mapped it incorrectly into JSON. So instead of

:  "tagged_values": {
:    "masd.injection.reference": "cpp.builtins",
:    "masd.injection.reference": "cpp.std",
:    "masd.injection.reference": "cpp.boost",

  we'd have:

:  "tagged_values": [
:    { "masd.injection.reference": "cpp.builtins" },
:    { "masd.injection.reference": "cpp.std" },
:    { "masd.injection.reference": "cpp.boost" },

*** Colours test model is invalid at present                          :story:

We should probably generate this model; at the moment, we have many
missing elements/meta-data, causing dogen to choke. However we need a way
to make the model generate nothing.

*** Copyright holders is scalar when it should be an array            :story:

At present its only possible to specify a single copyright holder. It
should be handled the same was as odb parameters, but because that is
done with a massive hack, we are not going to extend the hack to
copyright holders.

This functionality has been implemented but requires tests in the test
model.

*** Check support for decoration configuration overrides              :story:

At present we have hard-coded the decoration configuration to be read
from the root object only. In an ideal world, we should be able to
override some of these such as the copyrights. It may not make sense
to be able to override them all though.

This functionality has been implemented but requires tests in the test
model.

*** Location of =--byproduct-directory= not respected on conversion   :story:

It seems that at present we are not honouring the directory supplied
by the user. This seems to only happen on convert mode.

*** Generated tests fail when model has nothing to test               :story:

In cases where we are generating tests for a model which has nothing
to test - from a dogen generated code perspective - the tests will
fail with an error. This is because boost test expects to have at
least one test registered. In the past we have solved this by adding
"fake tests" which kept the test suite green until we added real
tests. However, for generated code, we need to somehow determine when
the fake tests should be injected.

This is not a trivial thing to do because we need to ensure that the
test template did not emit a single test for a given entity, and then
look at all entities and see if there is at least one test or not.

Example fake tests:

#+begin_src c++
#include <boost/test/unit_test.hpp>
#include "dogen.utility/types/test/logging.hpp"

namespace {

const std::string empty;
const std::string test_module("dogen.tests");
const std::string test_suite("fake_tests");

}

BOOST_AUTO_TEST_SUITE(fake_tests)

BOOST_AUTO_TEST_CASE(test) {
    SETUP_TEST_LOG("test");
}

BOOST_AUTO_TEST_SUITE_END()
#+end_src

The current workaround is to disable generated tests whilst there are
no types to test. This is not ideal because if we introduce testable
types we will probably forget to remove the disabling, but its
arguably better than adding a fake type.

*** Add meta-data to "force" parent                                   :story:

At present we can force a class not to be final:

: #DOGEN masd.generalization.is_final=false

However, this still does not create the methods for a parent such as
virtual destructor, equals etc. We need something to trigger those
methods as well.

*** =CMakeFiles= do not reference dogen models                        :story:

At present we cannot test cross-model referencing because our
CMakeFiles are not adding the linking references to these models. This
needs to be fixed before we can test cross model serialisation.

Notes:

- in order to map references to models, we need to create a modeling
  element for a reference. For this we have two cases: for proxy
  models/PDMs, we need to read from the meta-data the name of the lib
  the model generates. For dogen models we can create it from the
  model name.
- this is a variation of the "exports and imports" pattern: we import
  a set of libraries (these can either be macros or actual library
  names) and we export (for now) a single library. When we support
  facets in libraries, we may need to export more than one, so we
  should cope with this scenario now. We need to keep track of the
  exports for a reference, and then use those as the imports for the
  model.
- in an ideal world, all imports come via this mechanism. However,
  this means we now have to create PDMs/proxies just to setup the
  imports. For example, for LibXML we will not need to define any of
  the types, but we need the import. However, If we do force the
  definition of the PDM, the advantage is that we now have the right
  place to put the definition, and is done only once and shared by all
  models.

*** Cannot create classes with pointers to base                       :story:

At present it is not possible to create a class (call it A) with a
shared pointer to a base class (B). This is because A defines the
equality operator, but B does not. We need to have some kind of "has
equality" but in a more generalised form so that we can reuse it for
other things like comparisons, etc.

Then, we suppress the generation of the equality operator in the
presence of any members which do not have support for it.

*** Detect unqualified stereotypes                                    :story:

If a user enters say =enumeration= instead of =masd::enumeration= we
are providing an unhelpful error message:

: Error: Attribute type is empty: structured

This is because we validate the class as if it was an object and then
figure out that there are no types against the attributes. One easy
way to make things more useful is to detect unqualified stereotypes
and error straight away with a more useful message such as "did you
mean yarn::xyz?".

We could also do the same if the stereotype is blank ("did you mean
enumeration?").

- for extra bonus points use the [[http://en.wikipedia.org/wiki/levenshtein_distance][Levenshtein distance]] for spelling
  suggestions. See stories on this by searching for Levenshtein.

*** Using =std::set<std::string>= causes compilation errors           :story:

 In theory sets of strings (and any other type that has =operator<=
 should work out of the box, even though we do not support sets of
 dogen types. However, when we tried to use a set of strings we got a
 whole load of compilation errors in serialisation, etc.

*** Line endings could cause rewrites                                 :story:

At present if we git clone a repo with UNIX line endings and then
re-run dogen on Windows, even though nothing has changed in the model,
all generated files should get rewritten with windows line endings. We
should have a setting that enforces one set of line endings at the
model level. Interestingly, at present almost all extraction and
generation tests are green, implying we do not see any diffs. This is
very puzzling.

*** Character member variables are not tidied up on io                :story:

At present there is no code to convert non-printable chars into
something acceptable in JSON. We probably never noticed this before
because test data generates printable chars. Code generated is as
follows (all built-in model):

: << "\"char_property\": " << "\"" << v.char_property() << "\"" << ", "

We need a "tidy-up char" function to handle this properly.

For now we've hacked this and set =remove_unprintable_characters= to
false to keep backwards compatibility with legacy.

*** Lists of strings are not properly tidied up on io                 :story:

In the log file, when we dump include dependencies we see invalid
JSON:

: [ "<iosfwd>", ""dogen/sml/types/merger.hpp"" ]

This implies we are not calling =tidy_up_string=. This can be tested
by creating a container of =filesystem::path=.

*** Use of disabled facets in non-generatable types                   :story:

#+begin_quote
*Story*: As a dogen user, I want to know when I try to use a disabled
facet in a non-generatable type so that I don't generate
non-compilable code.
#+end_quote

It would be useful to set facets to disabled on non-generatable types,
when there are generatable types that depend on them. For example, if
we create some non-generatable types for which there is only a =types=
facet, we may still want to create generatable types that make use of
them. In this case, we would like Dogen to automatically disable all
facets except for =types=. Also, if a type is non-generatable, all
facets should be automatically disabled and its up to the user to
enable the ones he is interested in manually.

*** Type with the same name as the project does not compile           :story:

It seems that if we create a type with exactly the same name as the
model, we get strange compilation errors:

: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.4/stage/bin/dogen_examples/source/hello_world/include/hello_world/test_data/hello_world_td.hpp:37:13: error: ‘hello_world::hello_world::hello_world’ names the constructor, not the type
:     typedef hello_world::hello_world result_type;
:             ^

We should do a test case for this and fix the errors.

*** Allow for generation of class with the same name as package       :story:

At present its not possible to generate a class inside a package with
the same name of that package, if the package documentation is being
generated. This is because they will both have the exact same file
name.

*** Partial matching of built-ins doesn't work for certain types      :story:

We introduced a fix that allows users to create types that partially
match built-ins types such as =in= or =integer=. The fix was copied
from the spirit documentation:

[[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/doc/html/spirit_repository/qi_components/directives/distinct.html][- Qi Distinct Parser Directive]]
- [[http://www.boost.org/doc/libs/1_52_0/libs/spirit/repository/test/qi/distinct.cpp][distinct.cpp]]

Seems like the thing to do here is to create a keyword parser and nest
it with the existing parsers:

- [[http://stackoverflow.com/questions/21960167/prevent-the-boost-spirit-symbol-parser-from-accepting-a-keyword-too-early][Prevent the Boost Spirit Symbol parser from accepting a keyword too early]]
- [[http://www.boost.org/doc/libs/1_53_0/libs/spirit/repository/doc/html/spirit_repository/qi_components/directives/kwd.html][Qi Keyword Parser Directive]]

The new parser has a fix for these problems but was not completed so
cannot yet replace the legacy parser.

*** Returning optional of base class results in invalid code          :story:

When defining a model with a type with a field of =boost::optional<x>=
where =x= is an abstract base class, we get compilation errors in test
data. The problem appears to be that our test data factories try to
instantiate =x= rather than go through the abstract base class
machinery. We need to build a test model for this and fix the code.

We should also question if this is a valid scenario - if not we must
add it to the validation rules.

*** Investigate current support for =std::set= and =std::map=         :story:

It seems that we do not support sets and maps at present. When we
tried to use a set, we got errors in the guts of test data generation:

: /usr/bin/../lib/gcc/x86_64-linux-gnu/4.9/../../../../include/c++/4.9/bits/stl_function.h:371:20: error: invalid operands to binary expression ('const dogen::sml::qname' and 'const dogen::sml::qname')
:       { return __x < __y; }

This could be a bug (e.g. we are placing the =operator<= in the wrong
place etc). Or it could be that we just never needed ordered maps and
sets so we never added proper support.

*** Format doubles, floats and bools properly                         :story:

At present we are using IO state savers but not actually setting the
formatting on the stream depending on the built-in type.

Ideally we should pass in some dynamic extensions to determine the
formatting. We should also consider using =boost::format= for this.

*** Private properties should be ignored                              :story:

At present we treat private properties as if they were public; we
should ignore them. We need to go through all the models and change
the private ones to public before we do this.

We should also log a warning.

*** Identifiable needs to use camel case in C#                        :story:

At present we are building identifiables with underscores.

*** Inserter for enumerations shouldn't throw                         :story:

We only use the inserter for debug dumping and it could happen that we
are about to write the message for an exception when we decide to
throw. Instead we should just print unexpected/invalid value and cast
it to a numeric value in brackets.

*** Assignment operator seems to pass types by value                  :story:

The code for the operator is as follows:

:         stream_ << indenter_ << ci.name() << "& operator=(" << ci.name()
:                << " other);" << std::endl;

If this is the case we need to fix it and regenerate all models.

Actually we have implemented assignment in terms of swap, so that is
why we copy. We need to figure out if this was a good idea. Raise
story in backlog.

: diff --git a/projects/cpp/src/types/formatters/types/class_header_formatter.stitch b/projects/cpp/src/types/formatters/types/class_header_formatter.stitch
: index f9f91af..663f0ac 100644
: --- a/projects/cpp/src/types/formatters/types/class_header_formatter.stitch
: +++ b/projects/cpp/src/types/formatters/types/class_header_formatter.stitch
: @@ -253,7 +253,7 @@ public:
:  <#+
:                  if (!c.is_parent()) {
:  #>
: -    <#= c.name() #>& operator=(<#= c.name() #> other);
: +    <#= c.name() #>& operator=(<#= c.name() #>& other);
:  <#+
:                  }
:              }
: diff --git a/projects/cpp_formatters/src/types/class_declaration.cpp b/projects/cpp_formatters/src/types/class_declaration.cpp
: index c2eeb3c..534ab69 100644
: --- a/projects/cpp_formatters/src/types/class_declaration.cpp
: +++ b/projects/cpp_formatters/src/types/class_declaration.cpp
: @@ -457,8 +457,8 @@ void class_declaration::swap_and_assignment(
:
:      // assignment is only available in leaf classes - MEC++-33
:      if (!ci.is_parent()) {
: -        stream_ << indenter_ << ci.name() << "& operator=(" << ci.name()
: -                << " other);" << std::endl;
: +        stream_ << indenter_ << ci.name() << "& operator=(const " << ci.name()
: +                << "& other);" << std::endl;
:      }
:
:      utility_.blank_line();
: diff --git a/projects/cpp_formatters/src/types/class_implementation.cpp b/projects/cpp_formatters/src/types/class_implementation.cpp
: index 5c9fe50..9276701 100644
: --- a/projects/cpp_formatters/src/types/class_implementation.cpp
: +++ b/projects/cpp_formatters/src/types/class_implementation.cpp
: @@ -456,8 +456,8 @@ assignment_operator(const cpp::formattables::class_info& ci) {
:          return;
:
:      stream_ << indenter_ << ci.name() << "& "
: -            << ci.name() << "::operator=(" << ci.name()
: -            << " other) ";
: +            << ci.name() << "::operator=(const " << ci.name()
: +            << "& other) ";
:
:      utility_.open_scope();
:      {

*** Shared pointer to vector fails to build                           :story:

If one has a property with type
=boost::shared_ptr<std::vector<std::string>>=, we get the following
error:

: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp: In function ‘boost::shared_ptr<std::vector<std::basic_string<char> > > {anonymous}::create_boost_shared_ptr_std_vector_std_string_(unsigned int)’:
: /home/marco/Development/kitanda/output/dogen/stage/bin/demo/demo_20/sprint_20/src/test_data/my_class_td.cpp:47:50: error: ‘create_std_vector_std_string_ptr’ was not declared in this scope

This is because the generated code is not creating a method to new
vectors:

: std::vector<std::string> create_std_vector_std_string(unsigned int position) {
:    std::vector<std::string> r;
:    for (unsigned int i(0); i < 10; ++i) {
:        r.push_back(create_std_string(position + i));
:    }
:    return r;
:}
:
:boost::shared_ptr<std::vector<std::string> >
:create_boost_shared_ptr_std_vector_std_string_(unsigned int position) {
:    boost::shared_ptr<std::vector<std::string> > r(
:        create_std_vector_std_string_ptr(position));
:    return r;
:}

*** Add tests for immutability on an inheritance tree                 :story:

We are using the full constructor for immutability, but its not clear
how that would work on a inheritance tree. Ensure we have test cases
for this.

In particular, have a look at the test data generator with
immutability - it appeared wrong.

*** Equality in floating point numbers is incorrect                   :story:

At present we are blindly comparing floating point numbers. In
all_builtins test model:

:         double_property_ == rhs.double_property_ &&
:         float_property_ == rhs.float_property_;

Links:

- [[http://realtimecollisiondetection.net/blog/?p%3D89][Floating-point tolerances revisited]]

*** Fix northwind tests on OSX and Windows                            :story:

Get the tests to compile and run on windows. At present they are
failing to link. It seems there is some kind of mismatch between debug
and release, at least on MSVC.

Building with linking errors is available [[https://ci.appveyor.com/project/mcraveiro/cpp-ref-impl/builds/22859591][here]]. For now we've disabled
postgres:

: diff --git a/.appveyor.yml b/.appveyor.yml
: index 3f9fc6e..c265e24 100644
: --- a/.appveyor.yml
: +++ b/.appveyor.yml
: @@ -41,7 +41,6 @@ services:
:  before_build:
:    - SET PGUSER=postgres
:    - SET PGPASSWORD=Password12!
: -  - SET POSTGRES_SERVER_SETUP=1
:    - psql -f %APPVEYOR_BUILD_FOLDER%\build\scripts\setup_postgres.sql -U postgres
:    - if DEFINED msvc_setup_path call "%msvc_setup_path%" %msvc_setup_arg%
:    - cd %APPVEYOR_BUILD_FOLDER%

At present we are building northwind on all platforms, but the tests
are being excluded on OSX and windows, so we are not really testing
the linking, just the compilation. One of the problems is that we
conflated the running of the tests (for which we need a postgres
server) with the building of the tests (which we should always do
whenever we find all the required dependencies). However, it seems a
bit silly yo have to have two flags for this.

At present we have linking failures on both OSX and windows. It is not
entirely clear what is causing these failures. We need to revisit this
when we clean up the linking across dogen.

*** Do not output JSON markers for primitives                         :story:

We should just output the underlying primitive.

*** Parent without descendants in current model errors                :story:

 At present it is not possible to generate a parent in a model which
 does not have at least one descendant in that model. This is because
 we do not generate the base class methods. We need a meta-data
 parameter to force a class to become a parent.

 For now we just hacked it by adding a fake descendant.

 Interestingly, we already have a "is final" meta-data parameter. We
 could possibly check this; if its not final then we could assume it is
 a base class.

*** C# inheritance requires ordering                                  :story:

When we implement C# inheritance we must make sure we place the
interfaces at the end after any base classes or else we will have
compiler errors. This means we need to be able to distinguish
interfaces from other types (e.g. =interface= stereotype). We must
also make sure there is only at most one base class, as multiple
inheritance is only supported on interfaces.

*** Add meta-data to trigger inclusion/generation of =stdafx=         :story:

No setting to add include for precompiled headers: =stdafx.h=. In
addition, this should be a formatter specific property. It must look
at some model state (e.g. "generate windows files").

*** Immutable types cannot be owned by mutable types                  :story:

When we try to create a mutable class that has a property of an
immutable type, the code fails to compile due to the swap
method. This is because immutable types do not provide swap.

*** Add DateTime and related types to C#                              :story:

At present we do not have date types in C#. We will need helpers,
etc. We should do this after the move to external PDMs.

*** Using underscores with C# results in invalid code                 :story:

When building in LAM, if one uses underscore notation we create code
like so:

:        public int prop_0 { get; set; }
:        public class_0(int prop_0)
:        {
:            prop_0 = prop_0;
:        }

C# thinks we're assigning the parameter to itself rather than making
use of the property.

The right fix for this is to support the "camel case mode" where we
will interpret underscores and generate camel case identifiers.

For now we should warn users when they try to use lower case
attributes in C#.

*** Not setting output language results in weird errors               :story:

When setting the input language to language agnostic and not setting
the output languages, we get the following error:

: /dogen/projects/yarn/src/types/legacy_name_tree_parser.cpp(123): Throw in function std::__cxx11::string {anonymous}::grammar<Iterator>::scope_operator_for_language(dogen::yarn::languages) [with Iterator = __gnu_cxx::__normal_iterator<const char*, std::__cxx11::basic_string<char> >; std::__cxx11::string = std::__cxx11::basic_string<char>]
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::yarn::parsing_error>
: std::exception::what: Invalid or unsupported language: { "__type__": "languages", "value": "language_agnostic" }
: [tag_workflow*] = Code generation failure.
: [owner*] = <dogen><test_models><all_path_and_directory_settings><package_0><package_0_1><class_2>
: unknown location(0): fatal error: in "workflow_tests/all_path_and_directory_settings_generates_expected_code_dia": std::runtime_error: Error during test
: /home/marco/Development/DomainDrivenConsulting/dogen/projects/knit/tests/workflow_tests.cpp(213): last checkpoint

*** Add cross-model support to C#                                     :story:

At present we do not have any tests that prove that cross-model
support is working (other than proxy models). We need to create a user
level model that makes use of types from another model. In theory it
should just work since we are using fully qualified names everywhere.

*** Do not include algorithm if swap is disabled                      :story:

At present we always include =algorithm= in types' class header - both
in new and old world. However, it is there for swap, so we should only
include it if we are going to generate swap. This could be achieved
with:

: if ((!c.all_properties().empty() || c.is_parent()) && !c.is_immutable()) {

As per stitch template. We should probably add a "is swappable" flag
at the yarn level for this.

This is a bit more relevant now we are generating wale templates
because we are including algorithm all over the place on the generated
templates.

*** Multiple inheritance and profiles do not work predictably         :story:

The current inheritance logic is fine for single inheritance or even
multiple inheritance when two parts of the inheritance tree do not
define the same types; but it fails when there is overlap. For an
example, see the previous attempt to define "disable odb cmake" in terms
of "disable odb" and "disable cmake". This fails because disable odb
inherits from enable all facets; when we merge against "disable cmake"
we do not know that cmake was enabled via "enable all facets" and so
this takes priority.

We could add some validation that checks to see if we'd "revert"
changes depending on order. This is not very easy because it may
actually be the user's intention. We need to find some useful but
simple heuristic that indicates problems. Example:

- changing a given knob more than twice?
- updating a given knob in more than one place at the same level of
  the hierarchy? This is a good candidate because its easy to detect.

*** Reference to non-existent features produce unhelpful errors       :story:

When renaming fields, we get the following dogen errors:

: 2016-01-09 22:54:27.703708 [ERROR] [dynamic.workflow] Field definition not found: cpp.odb.class_header_formatter.inclusion_required

This is not particularly helpful. We should state:

- that the field instance is in the user model but does not exist in
  the library;
- the type in which the field instance was used;
- for extra bonus points use the [[http://en.wikipedia.org/wiki/levenshtein_distance][levenshtein distance]] for spelling
  suggestions. See story on this.

In addition this also depends on the field. For example, while
renaming =dia.comment= to =yarn.dia.comment=, we had no errors at all,
but then all fields defaulted. We should have gotten an error message
stating that the field did not exist.

Links:

- [[https://github.com/Martinsos/edlib][edlib]]: Lightweight, super fast C/C++ (& Python) library for sequence
  alignment using edit (Levenshtein) distance.
- [[https://github.com/cschanaj/levenshtein-distance][levenshtein-distance]]: C++ Functions for Levenshtein Distance
- https://gist.github.com/TheRayTracer/2644387: A simple C++
  implementation of the Levenshtein distance algorithm to measure the
  amount of difference between two strings.
  Computation with Generic Types
- [[https://github.com/schuyler/levenshtein][levenshtein]]: Fast string edit distance computation, using the
  Damerau-Levenshtein algorithm.

*** Using =std::unordered_map<my_enum, ...>= fails equality           :story:

In the past we created an unordered map of an enumeration and then
suddenly the equality tests started to fail. Since we use unordered
maps for strings quite a lot, it may be related to the fact that we
used an enum? Add a test case on the test models and see if we can
reproduce it.

*** Recursive structures result in crashes                            :story:

If one defines a tree node with a parent and children (such as =node=
in =logical= model's helpers) dogen generates code that recurses
inifinitely. This is because the structure contains a parent and we
loop through the parent back to itself and so on. To stop this from
happening we need to tell dogen to exclude certain fields. For
example, we could mark =parent= as a cycle. This is then interpreted
by the io feature as a "do not follow the pointer" (just dump its
memory address). We could have a manipulator that tells the
=boost::shared_ptr= io to skip its payload, much like we do when the
pointer is empty.

In summary:

- add a tag to mark a property as circular. Do not confuse this with
  name tree cycles which are at the type level.
- create a manipulator that is set when a circular property is
  found. Set it appropriately.
- on all pointer code (io, comparisons, etc) check for the
  manipulator; if set, do not dereference the pointer. For equality do
  a pointer comparison, for io dump the address, etc.

*** Hydrators provide no context when errors occur                    :story:

We tried to parse a JSON file using the INI parser and got the
following errors:

: 2015-03-27 15:16:05.291132 [DEBUG] [formatters.modeline_group_hydrator] Reading file: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../data/modeline_groups/emacs.json
: 2015-03-27 15:16:05.291215 [ERROR] [formatters.modeline_group_hydrator] Failed to parse INI file: : <unspecified file>(1): '=' character not found in line
: 2015-03-27 15:16:05.291933 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/formatters/src/types/modeline_group_hydrator.cpp(172): Throw in function dogen::formatters::modeline_group dogen::formatters::modeline_group_hydrator::hydrate(std::istream &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10formatters15hydration_errorEEE
: std::exception::what: Failed to parse INI file: <unspecified file>(1): '=' character not found in line
: [P12tag_workflow] = Code generation failure.

The exception provides no context to the file being parsed. We need to
catch the exception and augment it with the file name.

This is probably related to using boost property tree for JSON
parsing. Now that we have good vcpkg support we should just move to a
decent JSON library and ensure it reports errors at the line and
column level.

*** Using types of non-referenced models produces bad error messages  :story:

By mistake we made a reference to =dynamic::object= in the schema
model, during the =dynamic= to =schema= refactoring. This resulted in
the following, non-obvious, error message:

: 2015-03-09 12:56:00.920766 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/sml/src/types/merger.cpp(120): Throw in function void dogen::sml::merger::update_references()
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen3sml13merging_errorEEE
: std::exception::what: Cannot find target dependency: dynamic
: [P12tag_workflow] = Code generation failure.

What this is trying to say is that the =dynamic= model is not being
referenced. We should make this a bit more obvious because it would be
very difficult for the user to figure out what type is bringing in
this dependency. It would make more sense to say "type X requires
model Y, which is not part of the list of reference models" or
something along these lines. However, for this we need to load all the
available models - however this is defined. We could also take an
approach similar to compilers:

: The type or namespace name 'Tasks' does not exist in the namespace 'System.Threading' (are you missing an assembly reference?)

And for models:

: Could not load file or assembly 'TreemapControl, Version=1.0.1.38, Culture=neutral, PublicKeyToken=3f6121a52ebf7c82' or one of its dependencies. The system cannot find the file specified.Could not load file or assembly 'TreemapControl, Version=1.0.1.38, Culture=neutral, PublicKeyToken=3f6121a52ebf7c82' or one of its dependencies. The system cannot find the file specified.

We could also add the search path, e.g. looked in directories X, Y and
Z.

*** Improve error messages on empty meta-data                         :story:

Consider a meta-data entry without a value, in a dia diagram (model
note):

: #DOGEN dia.comment'

At present the following error is triggered:

: 2014-09-27 10:07:32.761795 [ERROR] [dia_to_sml.comments_parser] Expected separator on KVP.

This provides very little context of what went wrong. Also, we should
consider allowing for features that have no value, where the value is
assumed to be true. For cases like comment it would make life
easier. This may be good for boolean types with implied value
(e.g. presence means true). However on read we should capture the fact
that no value was supplied and then further down the processing
pipeline handle the defaulting to true.

*** Improve error message for blank types                             :story:

#+begin_quote
*Story*: As a dogen user, I want a clear error message when I forget
to supply a type for a property so that I don't spend ages searching
the diagram for the missing type.
#+end_quote

If the user does not supply a type at all in Dia, dogen spits out a
message that is not very informative:

: Error: Failed to parse string: .

The log file is not much better:

: 2014-09-06 16:11:54.143249 [ERROR] [dia_to_sml.identifier_parser] Failed to parse string:
: 2014-09-06 16:11:54.150595 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dia_to_sml/src/types/identifier_parser.cpp(198): Throw in function sml::nested_qname dogen::dia_to_sml::identifier_parser::parse_qname(const std::string &)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml13parsing_errorEEE
: std::exception::what: Failed to parse string:
: [P12tag_workflow] = Code generation failure.

We should instead mention that the string was empty or blank. We also
need to provide the property and class that contained this string. To
reproduce this problem create an enumeration but remove the
=enumeration= stereotype. This is a very common error when creating
enumerations (forgetting to set the stereotype). We should supply some
kind of clue ("did you mean to set the stereotype to enumeration?").

*** Error in log files when reading in Dia model                      :story:

For some reason the log file is full of errors like this:

: 2014-01-20 18:28:31.219549 [ERROR] [dia_to_sml.processor] Did not find expected attribute value type: composite

Presumably the errors are not fatal as code generation still
works. Investigate the errors and tidy-up the log. Since the errors
are not fatal we should at least downgrade them to warnings.

*** Improve error messages for unconnected objects                    :story:

#+begin_quote
*Story*: As a dogen user, I want to know exactly which object is not
connected correctly so that I can fix it.
#+end_quote

At present when a Dia object is not connected we get the following
error message to std out:

: Error: Expected 2 connections but found: 1. See the log file for details.

The log file is a bit more verbose but still not particularly helpful:

: 2014-01-23 08:25:28.115363 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2014-01-23 08:25:28.118718 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen10dia_to_sml16processing_errorEEE
: std::exception::what: Expected 2 connections but found: 1
: [P12tag_workflow] = Code generation failure.

We should try to at least name the object that has the one connection
to make the user's life easier.

*** Code coverage does not work for C#                                :story:

It seems that using NUnit and OpenCov does not work. The main reason
appears to be the use of shadow copying, which is no longer optional
on NUnit 3.

Links:

- https://github.com/Ullink/gradle-opencover-plugin/issues/1
- https://github.com/codecov/example-csharp/blob/master/appveyor.yml
- https://www.appveyor.com/blog/2017/03/17/codecov/

*** ODB linking is incorrect for generated code                       :story:

At present we are adding the libraries in the test instead of the
model using ODB. The following should be part of the generated code:

:    ${Boost_LIBRARIES}
:    ${ODB_PGSQL_LIBRARIES}
:    ${ODB_SQLITE_LIBRARIES}
:    ${ODB_BOOST_LIBRARIES}
:    ${ODB_LIBODB_LIBRARIES}
:    ${PostgreSQL_LIBRARIES}
:    ${OPENSSL_LIBRARIES}
:    resolv
:    ${SQLite3_LIBRARY}

The same problem applies to boost linking.

*** Inheriting from oneself causes segfault                           :story:

If you set an object to inherit from itself, say via metadata:

: #DOGEN masd.generalization.parent=in_memory_weaver

Dogen segfaults due to recursion. We need to test this via UML
inheritance as well.

*** Nested external model path results in strange references          :story:

Note: we have probably already implemented a solution for this, need
to check the resolver.

The external model path does not contribute to path resolution in a
model. Up til now that has actually been a feature; it would have been
annoying to have to dype =dogen::= on every type for every
model. Instead, we refer to say =dogen::a::b= as simply =a::b= in all
models that use =a=. However this masks a deeper problem: this is not
the desired behaviour at all times. We saw this problem when we
created multiple models under dynamic: =dynamic::schema= and
=dynamic::expansion=. In this case, users of these models referred to
them as =schema= and =expansion= respectively, and this was not
ideal. In general:

- external module path should contribute to references just like
  internal module path does - there should be no difference;
- dogen should be clever enough to determine if two models share a
  top-level namespace (regardless if it was obtained from the external
  or internal module path) that there is no need to have an absolute
  path. So in the case of =dogen=, since every model has =dogen= as
  their external module path, according to this rule we should not
  have to type it.

*** Facet enablement and model references is buggy                    :story:

At present we are processing enablement as part of the
post-processing. This means that we are using the target model's
annotation profile in order to determine the facet enablement. This
can cause problems as follows: say we enable hashing on a model via
the model profile of M0. We then consume that model as a reference and
disable hashing on M1. When processing types from M0 for M1 we will
disable hashing for them as well. Thus, no includes for hashing will
be generated even if a hash map is used.

Actually this is not quite right. We are expanding annotations at the
external model transform level; this means the enablement on the
reference must be correct. However, somehow we seem to be looking at
the element on the target model when deciding to include the hash file
from reference model.

*** Serialisation support for C++-11 specific containers              :story:

We can't add =std::array= or =std::forward_list= because there is no
serialisation support in boost 1.49. A mail was sent to the list to
see if this has changed in latter versions:

http://lists.boost.org/boost-users/2012/11/76458.php

However, it should be pretty trivial to generate serialisation code by
hand at least for =std::array= or to use a solution similar to
=std::unordered_map=.

*** Shared pointers to built-in types                                 :story:

At present we do not support shared pointers to built-in types. This
is because they require special handling in serialisation. See:

http://boost.2283326.n4.nabble.com/Serialization-of-boost-shared-ptr-lt-int-gt-td2554242.html

We probably need to iterate through all the nested types and find out
if there is a shared pointer to built-in; if there is, put in:

: // defined a "special kind of integer"
: BOOST_STRONG_TYPEDEF(int, tracked_int)
:
: // define serialization for a tracked int
: template<class Archive>
: void serialize(Archive &ar, tracked_int & ti, const unsigned int version){
:     // serialize the underlying int
:     ar & static_cast<int &>(ti);
: }

*** Generate ORM tests                                                :story:

We do not seem to be testing the generated ODB code. We don't need to
test ODB per se, but we should at least have some sanity checks that
test CRUD functionality.

Notes:

- for this we need a "masd database".
- tests should only trigger if postgres or some other relational
  database is detected.
- if foreign keys are used we need to detect them and ensure we
  populate the data accordingly.

*** Add warning for unused references                                 :story:

It would be nice if we could figure out if the user has added some
references to a model that are not required. This could be done as a
byproduct of resolution. However, we need to be careful when we
introduce two-passes as we may use a model for say profiles or
concepts but nothing else. One way of addressing this is to have a
container of "used references"; whenever we find a type from a
reference, we add that reference to the list. In the end we diff that
container against the global refs.

*** Additional validation rules for coding                             :epic:

Now that we have introduced the basic validation infrastructure
([[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_95.org][sprint 95]]), we should start adding more and more checks. This story
keeps track of all ideas around validation. We should convert these
ideas into stories and add them as we go along.

*Dia level checks*:

- exceptions and enumerations can't inherit
- exceptions shouldn't have properties.

*Injection level checks*

- ensure all attributes are unique, all element names are unique
  (qualified with the package).

*Intermediate model checks*:

- validate feature template names: We need to ensure the template
  names are valid identifiers in C++.
- ensure all models part of a model set have compatible versions. For
  example, referencing a C++ 98 model from a C++ 11 model (or
  vice-versa) should result in an error.
- detect cycles in references. Done, just needs warnings/errors framework.
- detect references to self. Done, just needs warnings/errors framework.
- if a model is making use of boost serialisation and it has leaves, a
  warning should come up stating that a type registrar is needed. The
  check should also take into account if a model is referencing models
  which have registrars.
- if a meta-model element is requested for a technical space which has
  no formatters, we should issue a warning. For example, registrar on
  a C# model.
- For C# models: attribute that start with lower case will cause
  problems because the property name will match the argument name,
  resulting in warnings in the complete constructor (argument assigned
  to itself). A simple validation check is to error if a user add an
  attribute that starts with lower case in C#.
- enumeration values should be valid according to its type. Basically
  cast to int.
- if =use_implementation_defined_underlier= is true then
  =underlying_element= must be empty and vice-versa
- primitive =underlying_element= must not be empty.
- if =use_implementation_defined_enumerator_values= is true then
  enumerator's value must be empty.
- enumeration must have at least one enumerator. warning?
- enumerations cannot sometimes define values and sometimes leave them
  empty. They must all be either provided or all left blank.
- external module path of the model matches all objects, etc in
  current model.
- documentation does not have non-printable characters.
- number of type arguments is consistent with objects type.
- Test relationships between objects and other meta types: We should
  validate that objects are only related to other objects - e.g. they
  cannot inherit from exception or enumeration or vice-versa. Add
  tests for this.
- Its not possible to be immutable and fluent.
- it is not possible to be immutable and be in an inheritance
  relationship. FIXME: why is that?
- user models cannot have stereotype of "builtins". Actually do we
  even need to validate this since it would do nothing.
- we don't support generic types so we should throw if a user attempts
  to use them.
- a type marked as final cannot have descendants.
- types in global namespace must have an empty location.
- if model module path is empty, location must also be empty.
- check the number of type parameters in the type definition and
  ensure that all name trees have the expected number of type
  parameters.
- properties of types in other models result in dependencies.
- concepts that don't refine must have at least one property (or
  method). Maybe just warn?
- issue error when a property is a value of an abstract class
- properties exist in merged model.
- vistor is only supported at the base class level: due to
  implementation constraints, we only support visitable at the base
  class level. Add an exception if users attempt to use visitable
  stereotype in a class that has parents. Note: is this true? We are
  using derived visitable in C++ model.
- there must be at least one element for a given id with
  =is_element_extension= set to false.
- for interfaces, etc: if a user overloads a method, check to see if
  arguments to the method with the same name are in the same order for
  all overloads (framework design guidelines p. 124).

Existing validation code:

:    if ((o.is_parent() || o.is_child()) && p.is_immutable())  {
:        BOOST_LOG_SEV(lg, error) << immutabilty_with_inheritance
:                                 << o.name().id();
:
:        BOOST_THROW_EXCEPTION(
:            transformation_error(immutabilty_with_inheritance +
:                o.name().id()));
:    }

: BOOST_AUTO_TEST_CASE(inheritance_with_immutability_throws) {
:     SETUP_TEST_LOG_SOURCE("inheritance_with_immutability_throws");
:     auto c(mock_context(model_name));
:
:     const auto po(mock_processed_object_factory::make_generalization());
:     const auto con(po[0].connection());
:     BOOST_REQUIRE(con);
:     const auto parents = std::list<std::string> { con->first };
:     c.child_id_to_parent_ids().insert(std::make_pair(con->second, parents));
:
:     transform(c, {po[1]});
:
:     auto po1(po[2]);
:     po1.stereotype(immutable_stereotype);
:     const auto op1(mock_profile(po1));
:     contains_checker<transformation_error> cc(immutability_inheritance);
:     BOOST_CHECK_EXCEPTION(transform(c, po1, op1), transformation_error, cc);
:
:     c.child_id_to_parent_ids().clear();
:     auto po2(po[1]);
:     po2.stereotype(immutable_stereotype);
:     const auto op2(mock_profile(po2));
:     BOOST_CHECK_EXCEPTION(transform(c, po2, op2), transformation_error, cc);
: }

- enumerators with values:
  - they are unique and valid according to enumeration type;
  - they do not class with invalid value.

*ORM*

- if there are ODB options, check that there are common ODB options
  too.

*Decoration elements*

- modeline groups must be defined at the top-level.
- modeline groups cannot contain any modeling element other than modelines.
- modelines must be defined inside of modeline groups.
- licences and generation markers must be defined at the top-level.

*Serialisation*

- if using boost serialisation with a model that has inheritance, the
  registrar should be present.

*Other*

- Check that all header guards generated for a given model are
  unique. We've had cases where a class with a prefix (say
  =variability_x=) and a class in a namespace (say =variability::x=)
  resolve to exactly the same header guard (say
  =VARIABILITY_X_HPP=). We could easily obtain all headers in a model,
  sort them and unique them. If there are duplicates, we should error
  or warn.

*Features*

- that formatter fields are not duplicated on simple name.
- fields are not duplicated on qualified name.

Merged stories:

*Add warning support for validation*

Once we implement a validator, we will soon run into warnings: cases
where the user has done something silly but we still want to code
generate. These are best handled as warnings rather than errors.

This story keeps track of things we think should be a warning.  List
of known warnings:

- unconnected dia object
- comment greater than 80 columns

We will probably soon need a way to enable/disable warnings. We could
use a similar scheme as GCC: =-Wname= and =-Wno-name=.

*Warn if value or entity has methods*

We should issue a warning if a user defines methods in value or entity
objects as its most likely by mistake.

*** Report validation errors like compilers do                        :story:

It would be very useful to output validation errors like a compiler,
referencing the original file from where the meta-element was defined
and any other additional information the user may find useful. We
could also have "reporting themes" such as visual studio or
GCC. Finally, we should support validation warnings, using the same
conventions as compilers (-Wall, etc). Though perhaps these are better
defined as part of the metamodel rather than command-line arguments.

We should implement this via =boost::error_info=, by adding file name,
line and column everywhere for all validation rules.

*** Check for incompatibility between input language and kernels      :story:

At present it is possible to have a model with input language of say
C++ but with the C# backend enabled. We should throw if the input
language is incompatible with the enabled kernels.

This explanation has bit rotted a bit and needs to be revisited:

Sadly this is not trivial. This is because quilt only sees the mapped
models; thus as far as knit is concerned, we ask for the input
language (e.g. c++) and there is an enabled kernel for it. We don't
look at it from the enabled kernel's perspective (e.g. "C# is enabled,
why is there no input language for it?"). We could have a method in
quilt that returns all enabled kernels; we could then look at all
models we are going to build and if there is a mismatch we can
throw. But extracting the =configuration_factory= out of quilt
workflow is not going to be easy without screwing up the API.

*** Improve comments on reference implementation                      :story:

At present it is very difficult to understand what each model and/or
each type does in the reference implementations. We need to add some
comments to make it more obvious.

*** Spirit: Improve error reporting                                   :story:

# Overview

At present when the parser fails we give the user no idea as to why it
failed; we just return the failed state. In theory we should be
reporting errors:

#+begin_example
        on_error<fail>
            (
                type_name,
                std::cout << val("Error! Expecting ")
                << _4                             // what failed?
                << val(" here: \"")
                << construct<std::string>(_3, _2) // iterators to error-pos, end
                << val("\"")
                << std::endl
                );
#+end_example

In practice this never does anything.

Requirements:

- get the =on_error= expression to trigger when an error occurs
- raise an exception with the contents that we are sending to
  std::cout at present.

*** Spirit: replace the legacy parser with the Klaus parser           :story:

Klaus created a new parser for us, but it was never integrated and
replaced the legacy parser. It should handle errors a bit
better. However, IIRC he tried to make a parser that was able to
handle more of the C++ type definition, which is no longer a
requirement - we just need to be able to support native arrays. We
need to review his code and integrate it.

*** Spirit: Add support for "native" arrays                           :story:

At present our parser does not support arrays such as:

#+begin_example
int[50];
#+end_example

This should result on a name with a size associated with it. The
parser cannot cope with this. Ideally one should be able to declare a
const for the size on the class too:

#+begin_example
            static const int maxLimit = 50;
            int objects[maxLimit];
#+end_example

So that we'd just recall the size parameter as a string in the name
tree rather than just expect it to be an integer.

*** Add test with smart pointer in base class                         :story:

At present we have the following helper formatters registered against
SmartPointer:

:      {
:        "quilt.cpp.types.class_implementation_formatter": [
:          "<quilt.cpp.types><smart_pointer_helper>",
:          "<quilt.cpp.io><smart_pointer_helper>"
:        ]
:      }

This should have caused something to break. It didn't because we don't
seem to have a test case with a smart pointer on the base class. This
raises the interesting point: do we ever need more than one helper for
a given family and a given file formatter? If so, we should change it
from a list to a single shared pointer.

Interestingly, for AssociativeContainer we have:

:    "AssociativeContainer": [
:      {
:        "quilt.cpp.types.class_implementation_formatter": [
:          "<quilt.cpp.io><associative_container_helper>"
:        ]
:      },
:      {
:        "quilt.cpp.io.class_implementation_formatter": [
:          "<quilt.cpp.io><associative_container_helper>"
:        ]
:      },

*** Improve error reporting around JSON                               :story:

At present when we break the JSON we get errors like so:

: Error: Failed to parse JSON file<unspecified file>(75): expected object name.

These are not very useful in diagnosing the problem. In the log file
we do a bit better:

: 2015-03-30 12:02:12.897202 [DEBUG] [dynamic.schema.json_hydrator] Parsing JSON file: /home/marco/Development/DomainDrivenConsulting/output/dogen/clang-3.5/stage/bin/../data/fields/cpp.json
: 2015-03-30 12:02:12.897216 [DEBUG] [dynamic.schema.json_hydrator] Parsing JSON stream.
: 2015-03-30 12:02:12.897450 [ERROR] [dynamic.schema.json_hydrator] Failed to parse JSON file: <unspecified file>(75): expected object name
: 2015-03-30 12:02:12.897515 [FATAL] [knitter] Error: /home/marco/Development/DomainDrivenConsulting/dogen/projects/dynamic/schema/src/types/json_hydrator.cpp(226): Throw in function std::list<field_definition> dogen::dynamic::schema::json_hydrator::hydrate(std::istream &) const
: Dynamic exception type: N5boost16exception_detail10clone_implIN5dogen7dynamic6schema15hydration_errorEEE
: std::exception::what: Failed to parse JSON file<unspecified file>(75): expected object name
: [P12tag_workflow] = Code generation failure.

But it requires a lot of context to know whats going on. We need to
append more details to the exception. We should look at this after we
move to RapidJSON.

Merged stories:

*Duplicate fields in JSON result in non-intuitive errors*

By mistake we added the same field twice in JSON:

:            "extensions" : {
:                "cpp.types.class_header_formatter.inclusion_directive" : "<boost/property_tree/ptree.hpp>",
:                "cpp.serialization.class_header_formatter.inclusion_directive" : "<boost/property_tree/ptree_serialization.hpp>",
:                "cpp.io.class_header_formatter.inclusion_directive" : "<boost/property_tree/json_parser.hpp>",
:                "cpp.io.class_implementation_formatter.inclusion_directive" : "<boost/property_tree/json_parser.hpp>",
:                "cpp.io.class_implementation_formatter.inclusion_directive" : "<boost/property_tree/json_parser.hpp>",
:                "cpp.hash.class_header_formatter.inclusion_required" : false

The resulting error message was not particularly helpful:

: 2015-06-17 13:56:06.658500 [DEBUG] [sml.json_hydrator] Processing type: <boost><property_tree><ptree>
: 2015-06-17 13:56:06.658519 [ERROR] [dynamic.field_instance_factory] Expected at most one element

*** Failed facet dependencies should be treated as errors             :story:

#+begin_quote
*Story*: As a dogen user, I want to know when I try to use a
non-supported facet from a system type so that I don't generate
non-compilable code.
#+end_quote

if a facet is not supported in a system module and the user tries to
make use of it, we should error. The user must then go and disable
explicitly the facet on the affected object via the meta data. We
should not silently disable facets.

*** Ensure an error info tag is not set already before we set it      :story:

Now that we will start making use of error info tags in anger, one
problem we have spotted is that some times we reset the same tag:
i.e. when an exception has been thrown, we do not want to set the same
tag multiple times because it overwrites it. There should be a way to
either allow multiple bits of information to be associated with the
same tag, or a way to check if the tag has been already set. But if it
is, what then? We can't throw.

*** Add more validation to stitch                                     :story:

Missing validation:

- check that directive an has end marker.
- start control block marker inside of an inline control block.
- profile cannot have the same type instantiated more than once.
- type definition cannot define the same type more than once.
- directives cannot be used in lines with mixed content;
- variables cannot be used in lines with mixed content;

*** Add tests for external and model modules                          :story:

At present we do not have tests exercising different combinations of
external and model modules.

Tests:

- 0-3 levels of external modules
- 1-3 levels of model modules

*** Add test for parent with no derived classes on the same model     :story:

We need to check that when a parent that has no leaves on its own
model generates correct code. The most likely problem is that the
parent will not be marked as abstract.

*** Add tests to inheritance test model                               :story:

We should make sure types' use of IO kicks in via the inheritance test
model. For this we need a base class with associative containers, etc
and a derived class.

*** Add tests to association model                                    :story:

We need a test for composition / recursion.

*** Add tests to identifier parser with invalid names                 :story:

We need to handle properly the following cases:

- totally blank name.
- template with angle brackets but nothing inside: =a<>=.
- template with angle brackets, type and then a comma: =a<b,>=.

*** Add tests for empty objects                                       :story:

This was mainly in the context of IO but could be useful for other
facets. Example:

: class empty_model_generator {
: public:
:     typedef dogen::sml::model result_type;
:
: public:
:     result_type operator()() {
:         dogen::sml::model r;
:         return r;
:     }
: };
: ...
: BOOST_AUTO_TEST_CASE(validate_io_for_empty_objects) {
:     SETUP_TEST_LOG("validate_io_for_empty_objects");
:
:     /* ensure we generate valid JSON for empty model. test was added
:      * because empty property trees were not correct, but its valid on
:      * its own right as we always use populated objects when testing
:      * JSON.
:      */
:     // test_io<empty_model_generator>();
: }

*** Cross package referencing tests                                   :story:

Scenarios:

- object in root refers to object in package: A => pkg1::B;
- object in root refers to object in package inside of package: A =>
  pkg1::pkg2::B;
- object inside of package refers to object inside of the same
  package: pkg1::A => pkg1::B (must be qualified);
- object in package refers to root object: pkg1::A => B;
- object in package refers to object in other package: pkg1::A =>
  pkg2::B;
- object in package refers to object in package in package: pkg1::A =>
  pkg1::pkg2::B;
- object in package refers to object in other package in package: pkg1::A =>
  pkg2::pkg3::B;
- object in package in package refers to object in package in package:
  pkg1::pkg2::A => pkg3::pkg4::B.

*** Add test model for disabling XML                                  :story:

At present we are not testing model generation with XML disabled.

*** Add tests for disconnected connections                            :story:

We should throw if a diagram has a disconnected inheritance or
composition relationship.

At present the error message for an inheritance object in dia which
has less than two connections is less than helpful:

: 2013-06-26 22:58:50.236488 [ERROR] [dia_to_sml.processor] Expected 2 connections but found: 1
: 2013-06-26 22:58:50.236917 [FATAL] [dogen] Error: /home/marco/Development/kitanda/dogen/projects/dia_to_sml/src/types/processor.cpp(166): Throw in function dogen::dia_to_sml::processed_object dogen::dia_to_sml::processor::process(const dogen::dia::object&)
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::dia_to_sml::processing_error>
: std::exception::what: Expected 2 connections but found: 1
: [tag_workflow*] = Code generation failure.

We should really try to detail which object ID failed, as well as
details of the connected object if possible, etc.

*** Add run spec targets for each test                                :story:

We could piggy back on the ctest functionality and add a target for
each test so one could =make enable_facet_domain= and =make
run_enable_facet_domain=. The targets need to be prefixed with module
name and test suite.

*** Add specification comments to tests                               :story:

We started off by adding a technical specification as a doxygen
comment for a test but forgot to keep on doing it. Example:

: /**
:  * @brief It shall not be possible to create more terms than those
:  * supported by a finite sequence, using std::generate_n.
:  */

This helps make the purpose of the test clearer when the name is not
sufficient.

This may not be required once we move over to Catch since the specs
become very readable.

*** Create model with invalid built-in type                           :story:

At present we are validating that all built-in types work but we don't
check that an invalid type doesn't work.

** Infrastructure

Stories related to infrastructure improvements that we may or may not
achieve.

*** Update the contributing to dogen doco                             :story:

The MSSQL tools project has a great set of points on contributing. We
should merge most of them into our contributing doco. We should also
port it to org-mode.

Links:

- https://github.com/microsoft/sqltoolsservice

*** Test cases show up as zero in report.ci                           :story:

The [[https://report.ci/reports/gh/MASD-Project/dogen][report.ci]] tool written by Klemens Morgenstern is now showing zero
test cases. According to Klemens:

#+begin_quote
I just checked, and the issue is that boost.test doesn't contain
passed test cases, only the suites. Since I now manually count them
(bc other test frameworks don't have it as a numerical value at the
end), it looks like it's zero. This is bad, but I don’t want to spam
the report with "unnamed test case #42" either, so I'll probably need
to add a few more things, so it might take a bit.

In the meantime, you can add more information to your tests by setting
the log_level & report_level of boost, report.ci will then also
generate annotations for passing
tests. https://www.boost.org/doc/libs/1_73_0/libs/test/doc/html/boost_test/utf_reference/rt_param_reference.html

You can set a level to the badges (experiment here:
https://report.ci/badges), the current default is `cases`, but you can
alternatively set it to `suites`, which gives you 8/8 or `checks`,
which counts the assertions (1901/1901).

Hope that helps, I also created an issue
https://github.com/report-ci/scripts/issues/16
#+end_quote

*** Consider adding support for kcov                                  :story:

In the past we used kcov and were very happy with it. However,
coveralls and codecov gave us a few problems so we ended up going to
llvm-cov. If we do decide to go back to kcov, look at commit
6cb01a132d.

*** Add support for stack traces                                      :story:

At present when we throw we have very little context around the
exception. With recent versions of boost its possible to output a
stack trace. It would be nice to be able to do this on debug builds at
least.

Links:

- [[https://www.boost.org/doc/libs/1_72_0/doc/html/stacktrace/getting_started.html#stacktrace.getting_started.exceptions_with_stacktrace][Exceptions with stacktrace]]

*** Consider adding =ClangBuildAnalyzer=                              :story:

Interesting clang tool:

#+begin_quote
Clang C/C++ build analysis tool when using Clang 9+ -ftime-trace. The
-ftime-trace compiler flag (see Aras' blog post or Clang 9 release
notes) can be useful to figure out what takes time during compilation
of one source file. This tool helps to aggregate time trace reports
from multiple compilations, and output "what took the most time"
summary[.]
#+end_quote

Links:

- https://github.com/aras-p/ClangBuildAnalyzer

*** Add new c++ warnings to compilation                               :story:

- =-Wunused-private-field=: Seems like this warning is not part of
  =-Wall=
- =-Winconsistent-missing-override=: new clang warning, probably 3.6.
- =-Wdocumentation=: clang doxygen warning.
- =-WExtra=
- =-save-temps= =-Wextra= =-Wfloat-equal=

See also:

- [[http://releases.llvm.org/5.0.0/tools/clang/docs/ReleaseNotes.html][Clang 5.0.0 Release Notes]], section "Improvements to Clang’s
  diagnostics".
- [[https://kristerw.blogspot.co.uk/2017/09/useful-gcc-warning-options-not-enabled.html][Useful GCC warning options not enabled by -Wall -Wextra]]
- [[http://www.productive-cpp.com/hardening-cpp-programs-stack-protector/][Hardening C/C++ Programs: Part I – Stack Protector]]
- [[https://developers.redhat.com/blog/2018/03/21/compiler-and-linker-flags-gcc/][Recommended compiler and linker flags for GCC]]
- [[https://www.reddit.com/r/cpp/comments/grpux6/which_gcc_flags_do_you_use_often/][Which GCC flags do you use often?]]
- [[https://tobias.hieta.se/llvm11-release][LLVM 11.0.0 Released - Here are some highlights for C/C++
  developers]]: section on new warnings.

Merged stories:

*Consider enabling =-Wshadow=*

We make use of shadowing on occasion so maybe this is why we disabled
this warning. Enable it and check to see what breaks.

:    # definition shadows another
:    # FIXME: causes too many problems.
:    # set(warning_flags "${warning_flags} -Wshadow")

*Enable =maybe-uninitialized= warning*

This warning caused build breaks. The main problem seems to come from
boost variants using model types, which then rely on the variant's
swap function. This uses the move constructor. For some reason, the
compiler does not think the default move constructor is initialising
the member variables correctly. Not obvious why that would be.

*** Investigate =modern-cpp-template=                                 :story:

This project seems to supply a very good base to use on our C++
projects: "A template for modern C++ projects using CMake,
Clang-Format, CI, unit testing and more, with support for downstream
inclusion."

In fact it seems there are quite a few of these. We need to find out
the differences between them.

Link:

- [[https://github.com/filipdutescu/modern-cpp-template][modern-cpp-template GH]]
- [[https://github.com/lefticus/cpp_starter_project][cpp_starter_project GH]]: A template CMake project to get you started
  with C++ and tooling
- [[https://github.com/TheLartians/ModernCppStarter][ModernCppStarter GH]]: Kick-start your C++! A template for modern C++
  projects using CMake, CI, code coverage, clang-format, reproducible
  dependency management and much more.

*** Run ODB on nightlies                                              :story:

At present we are code-generating all code on dogen nightlies but we
are not running ODB. This means that if we have broken ODB pragma
generation, we will not find out until we manually run it. We should
run it as part of the "generate everything" approach in nightly.

*** Check OSX packages                                                :story:

We need to ensure OSX packages are still working. At present, the
build says:

: FATALcould not find C++ runtime

For this the problem appears to be that we are trying to add the C++
run time and not finding it. We need to test the OSX packages and see
if Dogen still works. If it does, remove this.

:   if(EXISTS "/usr/local/lib/i386/libstdc++.6.dylib")
:       set (cpp_runtime_libs
:           ${cpp_runtime_libs} "/usr/local/lib/i386/libstdc++.6.dylib")
:       set (cpp_runtime_libs
:           ${cpp_runtime_libs} "/usr/local/lib/i386/libgcc_s.1.dylib")
:   else()
:       message(FATAL "could not find C++ runtime")
:   endif()

*** Move from doxygen to standardese                                  :story:

We should try to use standardese to generate the documentation for
dogen. Seems easier to use and CMake friendly. Also, it seems more c++
compliant because it uses libclang.

Once the move is done, we should update dogen to generate comments in
either markup via a meta-data parameter (documentation markup?).

Links:

- https://github.com/foonathan/standardese

*** Build dogen from docker                                           :story:

- fix references to gcc etc
- run:

: docker exec -it zen_euclid env TERM=dumb
: /home/marco/Development/DomainDrivenConsulting/dogen/build/scripts/build.linux.sh
: Release 4 gcc /home/marco/local

Merged stories:

*Build dogen docker images from travis*

We should build docker images with each commit into master.

Links:

- [[https://devblogs.microsoft.com/cppblog/using-multi-stage-containers-for-c-development/][Using multi-stage containers for C++ development]]
- [[https://github.com/GoogleCloudPlatform/cloud-builders/blob/master/bazel/Dockerfile][cloud-builders]]
- [[https://sebest.github.io/post/using-travis-ci-to-build-docker-images/][Using Travis.ci to build Docker images]]
- [[https://github.com/OpenSourceRisk/Engine/tree/master/tools/docker][ORE docker script]]
- [[https://giorgos.sealabs.net/build-and-test-against-docker-images-in-travis.html][Build and Test against Docker Images in Travis]]
- [[https://medium.com/travis-on-docker/triple-stage-docker-builds-with-go-and-angular-1b7d2006cb88][Triple-Stage Docker Builds with Go and Angular]]
- [[https://medium.com/travis-on-docker/multi-stage-docker-builds-for-creating-tiny-go-images-e0e1867efe5a][Multi-Stage Docker Builds for Creating Tiny Go Images]]
- [[https://github.com/gliderlabs/docker-alpine/issues/24][What is the alpine equivalent to build-essential?]]
- [[https://caveofcode.com/2017/03/continuous-delivery-to-kubernetes-with-travis-ci/][Continuous delivery to Kubernetes with Travis CI]]

*** Create a docker image for builds                                  :story:

GCP's cloud builde uses the idea of cloud-builder images. These
contain all of the basics required to build for a given language. Now,
we can't use their images directly as they do not have CMake, but it
does give us an interesting approach: we can create a MASD base build
image with:

- clang, gcc
- cmake

And then with it build vcpkg and build and install all of the deps. We
need to be careful not to push the build files (over 13 GB!). We can
then use this image for both Travis and the nightlies, as well as, one
day, cloud build.

While we're at it, we should also consider the points in the article below.

Links:

- [[https://github.com/GoogleCloudPlatform/cloud-builders/blob/master/bazel/Dockerfile][Bazel's cloud builder]]
- [[https://www.toptal.com/devops/better-google-cloud-continuous-deployment][A Better Approach to Google Cloud Continuous Deployment]]
- [[https://www.reddit.com/r/googlecloud/comments/9i093f/google_cloud_build_is_there_an_easier_way/][google cloud build is there an easier way?]]
- [[https://cloud.google.com/cloud-build/docs/configuring-builds/create-basic-configuration][Creating a basic build configuration file]]
- [[https://cloud.google.com/blog/products/devops-sre/cloud-build-brings-advanced-cicd-capabilities-to-github][Cloud Build brings advanced CI/CD capabilities to GitHub]]
- [[https://cloud.google.com/cloud-build/docs/quickstart-docker?hl=en_GB&_ga=2.136964915.-2065564734.1572530049][Quickstart for Docker]]
- [[https://github.com/GoogleCloudPlatform/cloud-builders/blob/master/bazel/Dockerfile][Dockerfile]]

*** Linux and OSX binaries are not stripped                           :story:

At present our Linux and OSX build is much bigger than our windows
builds (3.8 MB on Windows vs 31 MB OSX and 15 MB on Linux). The
problem appears to be that we are not stripping the binaries on Linux.

We tried manually stripping:

:     # strip the binaries in release
:    set(CMAKE_C_FLAGS_RELEASE "${CMAKE_C_FLAGS_RELEASE} -s")
:    set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} -s")

However clang does not support this.

This may be related to the CMake build type of MinRelSize. Try doing a
build with this and see if the binaries are smaller. Actually this
does not work. We also tried:

: CMAKE_INSTALL_DO_STRIP

Which seems to have some effect but not exactly the same as a command
line =strip=. Supposedly this is a install level strip.

The only solution that appears to work is to add a custom command to
all targets in the build to strip:

: add_custom_command(TARGET ${target} POST_BUILD
:        COMMAND ${EMBREE_SIGN_FILE} $<TARGET_FILE:${target}>)

However we need to be careful because stripping shared libraries may
cause problems. Also this is done for every build.

Links:

- [[https://www.technovelty.org/linux/stripping-shared-libraries.html][Stripping shared libraries]]
- [[https://cmake.org/pipermail/cmake/2012-March/049741.html][make install/strip does not strip static libraries]]

*** Code coverage does not include odb files                          :story:

The latest code coverage reports are excluding ODB files. We need to
understand why kcov is ignoring those.

Links:

- [[https://codecov.io/gh/MASD-Project/cpp_ref_impl/tree/master/projects/cpp_ref_impl.northwind][cpp_ref_impl.northwind in codecov]]

*** Investigate the emblems used by Bit7z                             :story:

This project uses emblems for version, platform and compiler:

https://github.com/rikyoz/Bit7z

This may or may not be useful to dogen.
*** Mine =common-universal-cmake= for ideas                            :epic:

This project seems interesting:

- [[https://github.com/polysquare/common-universal-cmake][common-universal-cmake]]

They seem to have added support for conan, appveyor, etc in an
extensible way. We need to figure out if anything can be nicked for
our infrastructure. This is also a useful template for when we
code-generate products.

Notes:

- nice idea to create text files with all of the dependencies for the
  different OSs (packages and repositories).

* V3 Release

Release goals:

- define the Dogen ecosystem: /better tooling/. This involves all the
  tooling around Dogen, including LSP support for both stitch and all
  supported injectors (Dia, JSON, org-mode?).

** Candidates

*** Add support for SPDX licences                                     :story:

We should try to create a model with all the SPDX licences, using
their naming conventions, format etc.

Links:

- [[https://spdx.org/licenses/][SPDX License List]]

*** Tight integration of Dogen and =build2=                           :story:

In an ideal world, we should have a single command for managing the
entire life-cycle for a product. As luck would have it, all of the
packaging and building are already taken care of by Build2:

- https://build2.org/doc.xhtml

Build2 has support for both the build system as well as the package
management. Were we to consume it as a library, we could integrate
this functionality into Dogen such that we wouldn't even create build
files at all. We would have dogen commands such as:

: dogen new
: dogen build
: dogen test
: dogen package

And so on. We should copy these from =dotnet=. Then, internally, we
would call all of the required build2 commands to execute these. Dogen
would merely orchestrate build2 using the meta-information about the
product. Since not all libraries are on build2, we would only place
dogen libraries in build2 and use =vcpkg= as the supplier of all other
libraries. Actually, the disto could also supply these. The general
idea is that there are external packages in system directories. You
need to tell dogen about these. Everything else comes from Dogen's
integration with build2 (PDMs, etc). This also has the advantage that
build2 has support for modules.

The product transforms would then involve calls to download packages,
build, publish etc. Build2 does not know how to generate MSIs, DEBs
etc so there is still some work required to handle the final
packaging. Perhaps these can be encoded as post-processors within
Dogen. We could copy the code from CMake for this. Ideally you want a
C++ library for each type of package. We also need to take into
account the docker use case. Alternatively we could simply look at the
commands one runs to generate packages normally and literally encode
these into dogen as shell commands via boost process. Finally we
should publish the result into GitHub. The API keys etc should be
supplied to Dogen as environment variables. This approach is very
specific to C++ so we need to be careful we are not hard-coding too
many things that will not work for other languages.

Notes:

- we should also include building the documentation.
- it should be possible to build docker images that have nothing
  installed on it other than the product.
- the build steps should take care to regenerate the components when
  requested or to warn if the model is out of date if regeneration is
  not requested.

Links:

- [[https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-new][dotnet new]]
- [[https://docs.microsoft.com/en-us/dotnet/core/tools/custom-templates][Custom templates for dotnet new]]

*** Tighter integration of Dogen and Emacs                            :story:

There are a couple of areas where we could take advantage of code
generation and emacs:

- running of tests: generate a org-mode test file which has a tree for
  all configurations (Debug, Release) and compilers (gcc8, clang7,
  etc). Then, for each of these we could have the test suites and the
  tests. It runs all the tests and builds org-tables with the total
  times. The runs are babel snippets. It updates the entries with the
  status of passed, failed or not run. We need to find out how to run
  all code snippets for a given subsection. We can use org properties
  to supply the boost test variables such as log level etc. Because
  Dogen knows of all of the test suites (e.g. these are declared as
  model elements), it can parse them. We can also mark tests and test
  suites as "generated" or "handcrafted".
- files: Dogen could also output a generation model set for all types
  in a product using a lisp like serialisation format. This could
  somehow be read by emacs to give it visibility of all files dogen
  knows off. Its still not clear what we'd do with it though. Some
  kind of integration with projectile?
- bookmarks: we could generate a org-mode file of bookmarks for all
  key directories in a product such as say "coding include types",
  etc. This can also use traditional emacs bookmarking format. It
  should also include links to models.
- run affected test suites: create a graph of includes per file such
  that when you change a file, you can ask to run affected test
  suites. This is computed based on the include graph. Won't work for
  all technical spaces though.
- build: it would be great if we could have the build scripts as
  babel. However, instead of executing inside of org-mode, we want to
  call the compilation buffer with it. This means we could have
  bookmerks for all of the usual targets and even target combos such
  as generate, convert and indent. For extra bonus points, once we
  have code merging, we could have our own compilation targets added
  manually.

Links:

- [[https://www.youtube.com/watch?v=dljNabciEGg][Literate Devops with Emacs]]
- [[https://github.com/wlbr/cl-marshal][cl-marshal]]: Simple and fast marshalling of Lisp datastructures.
- [[https://www.cliki.net/serialization][cliki]]: Libraries to translate data structures to/from a format which
  can be stored/retrieved
- [[https://github.com/rxi/fe][fe GH]]: "A tiny, embeddable language implemented in ANSI C"


*** User interface generation                                          :epic:

we have not really thought about it that deeply but it should be
possible to use dogen to generate some basic uis, for example for wt.

It would be extremely interesting to see if we could encode these
meta-model elements within a org-mode document.

The gist of the approach is to add new meta-model elements to logic
that are on the UI domain. User models then instantiate those. We need
to look at the

links:

- [[http://plantuml.com/salt][salt]]: plantuml wireframe language.
- [[https://en.wikipedia.org/wiki/Glade_Interface_Designer][Wikipedia: Glade Interface Designer]]: GtkBuilder is the XML format
  that the Glade Interface Designer uses to save its forms.
- [[https://gitlab.gnome.org/GNOME/gtk/blob/7601bca7587dd29bde3d1554e644a10ae6dafc18/gtk/gtkbuilder.rnc][RelaxNG schema for GTK builder XML]]

Merged stories:

*Add support for GtkBuilder / Glade XML files*

There is nothing stopping us from using a GtkBuilder / Glade XML file
to do the boiler plate setup of the UI. With a bit more work one could
potentially even generate the bindings for a presentation model.

*** Investigate support for automatic model updates                    :epic:

For classes that are manually generated, it would be really nice if we
could update the properties of the class in the diagram from the
source code. This would work as follows:

- user creates a class =x= and marks it as non-generatable; executes
  dogen.
- dogen creates the initial file and adds as much boilerplate as
  possible. For instance if the user manually added properties or
  operations to the class, dogen generates skeletons for these.
- once the file exists, dogen will no longer touch it (see also the
  merging code generation story, for a different take on this).
- the user runs a second tool (the diagram updator, in need of a name)
  which uses clang internally; it reads the diagram and looks for all
  of the non-generatable classes; for each of these, it updates the
  dia class with the properties found in the source file. Everything
  else is left untouched.

This feature would be extremely powerful when in presence of many
other features such as mocking, remote method invocation, etc - the
user would have no effort at all in generating the
code. Implementation-wise we'd have to:

- create an XML writer;
- add write support for the dia model and ensure we generate valid dia
  models;
- integrate clang libraries with dogen;
- create tool - or perhaps we should just have an "update diagram"
  mode in dogen?

*** Code generation as a service                                       :epic:

*Latest Understanding*

- create a website using Wt: single screen with a few radio buttons to
  upload files or edit/paste JSON input; command line options
  available as text boxes.
- create a service that receives requests for code generation and
  returns a tarball/zip with the generated code: =jersey=. Archiving
  libraries:
  - https://github.com/libarchive/libarchive
  - https://github.com/saboorian/moor
  - https://github.com/do-m-en/libarchive_cpp_wrapper/
- consider creating a simple HTTP wrapper around =jersey= just to see
  how it works. =hem=.
- two modes of operation: 1) upload a set of files, marking one as the
  target 2) simple input box where user can type in JSON and code
  generate.
- tick boxes for main options.
- potentially display a sample of one of the types.
- =jersey= receives a code generation request with the files as part of
  the payload. This means we need to either save them down to a temp
  directory and then pass paths over to knitter or we should have
  support for different kinds of inputs such as streams in knitter.
- save all requests and responses to a Postgres database for analysis
  later (performance, bugs, etc). It would also be nice to have user
  comments/complaints, perhaps linked to a user voice-like interface.
- a more useful version of this service would allow: 1) ability add
  types to existing models such as std and boost, or even create new
  system models altogether. Some minimal UI for model editing would be
  required, including meta-data support (includes, etc). Ideally, each
  of these models should have an associated test model which would
  automatically be updated with the new types so users could get
  compilation feedback on their changes. 2) the ability to edit stitch
  templates on the site or even the ability to add new facets or new
  features for existing facets, and to code generate using those. Also
  the ability to add helper methods for a given facet to a given
  type. The testing for this could be achieved via a docker container
  with an incremental build and simple validation (time out, etc). If
  the user submits dodgy changes only the container would be
  affected. Once built, the container is used for code generation. It
  would be really nice to be able to test the template as one is
  editing it, perhaps by choosing a type in a model; the user could
  press "generate" at any time and see the file for that feature and
  facet. Behind the scenes this is stitching, building, code
  generating and then displaying the result for that one type. 3) the
  ability to submit PR from within the site. Users log in using their
  github accounts. They make their changes to a remote git repo. Using
  github magic - in a similar way to how gitter forks and creates
  PRs - when the user is happy enough with its changes, their are
  submitted as a PR.
- note that this advanced version would probably require having the
  test models in JSON so that we could edit them via the site; it
  would be cumbersome to add new types to a dia model
  programmatically.
- for extra bonus points, we should integrate with clang-format
  generation sites such as [[http://zed0.co.uk/clang-format-configurator/][this one]] and allow users to add their own
  styles dynamically.
- it should be possible to add more modelines/modeline groups,
  licences, and even fields. This only makes sense if the fields are
  totally dynamic such that the formatter could make use of the new
  field directly. For this we would have to supply the ownership
  hierarchy from within the stitch template.
- we should create interesting end points such as: dia to json
  conversion; merged model generation; merged model to code
  generation, with language options; formatting; etc. Each is a
  distinct end point.

Notes:

- we must cache system models so we don't reload them for every
  request.
- we should load per language (references?)
- we need to verify the JSON input to avoid large blobs, buffer
  overruns, etc.
- support for workspaces so that users can save references.
- system data: load once, reload on changes. Data "dirs" can be
  stored/cached on redis or DB.
- we need to move XML reader to memory.
- hydrators need a data store scoped to the user.

*Previous Understanding*

One way of testing new functionality added to dogen is to try to
exercise it as part of the code generation itself. We have been doing
this with the bootstrapping, but there were limitations on
functionality such as ODB and EOS where we couldn't see any obvious
use for it in code generation. However, there is one way of exercising
this and a lot more of these sort of features: to create a Web-based code
generation service, along the lines of Web Sequence Diagrams or
YUML. We could create a simple bootstrap based website that forwards
requests to a set of end-points, all done within the dogen project.

We'd create a casablanca REST layer with a simple interface, with
functionality such:

- create workspace: returns a UUID and creates some kind of internal
  storage area.
- upload target: uploads a Dia or JSON model to be used as the code
  generation target.
- upload reference: uploads a Dia or JSON model to be used as a
  reference.
- set options: which facets to generate, which languages, etc.
- codegen: runs the code generation and returns a tarball with
  generated files and the log file; or returns a set of code
  generation errors.
- we could integrate with google drive to load the files from there.

As a further layer we could create an ASIO service that is queried by
the casablanca REST. This would exercise all of the messaging
infrastructure. Internally it would create the engine and run code
generation. It could also exercise ODB by writing session information
to a database and keeping track of the historical usage of the
service, log files etc.

This stack would allow us to continuously exercise pretty much every
feature we need out of dogen. As an added bonus, when we get to the UI
we could also exercise that (Wt, GTK).

Finally, this would also allow us to play with Docker, and place each
service in their own container, create load balancing etc.

Links:

- [[http://codeplanet.io/principles-good-restful-api-design/][REST API Design]]
- [[http://www.drdobbs.com/tools/json-and-the-microsoft-c-rest-sdk/240164821][Using Microsoft REST SDK]]
- [[https://git.gnome.org/browse/dia/tree/doc/diagram.dtd][Dia DTD]]: make sure all dia diagrams are valid according to DTD. Same
  for JSON documents. Make sure there are size limits for names,
  locations, documents, etc. See also [[https://security.stackexchange.com/questions/416/if-an-xml-document-is-not-validated-as-well-formed-or-checked-against-a-schema][If an XML document is not
  validated as “Well Formed” or checked against a schema, what are the
  risks?]]

*** Consider replacing out libxml bindings with RapidXML              :story:

We rolled our own libxml bindings for reading the dia XML. However, it
may make more sense to use [[https://github.com/dwd/rapidxml][RapidXML]] instead. It seems basic but our
needs are also very basic.

*** Org-mode schemas                                                  :story:

Check to see if there is any concept of an org-mode schema, whereby
certain outlines, etc are mandatory in order for the document to be
compliant.

Links:

- [[https://github.com/tgbugs/laundry][laundry GH]]

*** Dogen Modeling Studio                                              :epic:

Assorted set of ideas of what an "integrated environment" for modeling
within Dogen would look like. Very random.

- model management, concept of projects.
- integration with git in order to link model versioning with version
  control. References should point to specific versions (tags or
  commits).
- ability to view revisions for a given model set. That is, at model
  version X, code looks like Y. Linkage between generated code and
  model.
- configuration for all models and for each models - e.g. product
  families, component families.
- load all revisions with model changes: given a git repo, show me all
  the commits that changed a model and see these as a log for the
  model. We should also be able to see the comments on the
  commit. We can then visualise the model and see the diffs across
  revisions somehow.
- ability to config the git repo, model location, output
  directory. Performs a clone of up to X depth. Then reads history of
  model files.
- it should be possible to select branches for a git repo as well.
- for injectors with text formats, it would be nice to be able to edit
  the models within the IDE.
- ability to download a zip with generated code for the model at a
  specified version.
- support for tracing within the IDE so that we can browse and
  navigate all of the generated tracing information. Visualisation of
  JSON as a expandable tree.
- ability to load/save models into a relational database.
- it would be nice to have endpoints with different versions of
  Dogen. That way we could generate code and diff it. We can also
  check if a model will error in the future/past versions of
  Dogen. There should also be an endpoint of "latest" which is always
  the latest version.
- once we have warnings and validation, we can generate github
  badges. Perhaps a =.dogen= file at the top could be used to describe
  the settings (API tokens, location of models, etc). Then we could
  provide the following information:

  - is the model green (no errors, no warnings).
  - if its not green, what warnings and errors is it generating.
  - is the code out of sync with the model?
  - model health reports, based on complexity measures, etc.

  The idea is that we would provide feedback with each commit so that
  users can be on top of it. Also, users can choose to point to latest
  just to see whenever their models break with latest changes.
- this story has a lot in common with [[*Consider adding a wt frontend][Consider adding a wt frontend]].

*** dogen as a github integration                                     :story:

Perhaps there are some useful services dogen could provide to users in
terms of dogen integration. If, with every commit, we could regenerate
the model and read the current state in github, we could then provide
a status report:

- the model does not build; red emblem. Some changes were made to the
  model (or to dogen) that make the model invalid. User should take
  action.
- the model builds but generates files that are different from what's
  checked in on github. yellow emblem. Provide a report with the
  diffs. This can either be because the code generator has changed or
  the user changed the model.
- the model builds and generates exactly the same code; green emblem.

With this approach we have two advantages:

- we do not need to add projects as part of the dogen tests; the
  service takes its place. We can still add a few as the core tests,
  but we don't need to expand it much beyond reference implementation
  and dogen itself.
- we exercise dogen itself as well as the rest endpoint generation
  code in a way that is actually useful to end users; it would be nice
  to know immediately when something breaks.

Notes:

- we'll need some kind of way of dealing with tokens and secrets in
  order to support private GH projects.
- with the addition of overrides, we can also have another check:
  build with overrides. When setting up the project, the user can
  define the override profile to use. The report will also provide
  details on what fails with overrides.

Links:

- [[https://report.ci/][report.ci]]

*** LSP as a form of roundtrip support                                :story:

Once we have LSP delegation, one thing we could consider doing is use
this information for roundtripping. It would not be a complete
roundtrip support, but instead a "localised" roundtrip:

- for hand-crafted elements, we could keep track of any operations
  that the user may have defined on the model elment. We could also
  warn if the user did not define an operation. The problem with this
  is that we do not want to issue a lot of warnings. If we had an
  injection model for which we were confident we could make changes
  without breaking anything (such as org-mode), we could try to
  automatically inject the properties and operations into the element
  but we need to make sure this is not going to generate a lot of
  noise and/or result in losing user data. It would be nicer to have
  this as a controlled operation (e.g. run dogen on
  "update"/"roundtrip" mode) so that we could bring in the diffs
  manually and inspect them.
- for elements which are mainly generated (but may or may not have
  merge support), this is not very useful. This is because we only
  expect

*** LSP daemon for Dogen                                               :epic:

We should be able to create a simple LSP daemon that helps with dogen
models and stitch templates. For dogen models:

- provide completion of meta-data keys
- model validation (flycheck)
- completion of model elements, context sensitive.
- completion of references (assuming a "reference directory"
  parameter)
- help hints for meta-data features: description of the feature and
  example usage. This should be added to every feature.
- help hints for types based on their documentation.

See also the MOP story for more ideas. In terms of implementation, we
probably need to know the current line of the model so that we can
figure out what modeling element we are sitting on. With this context,
we can build an assets model and extract all of the required
information.

For stitch templates:

- mainly meta-data hints and help.
- find a way to call the LSP daemon for c++ with the expanded
  template, and then interpret the resulting errors in terms of
  template lines (we have a story for this).
- find a way to call the LSP daemon for c++ and obtain its view of
  completion etc. This is slightly harder because we need to map the
  current line to the expanded template line. It may be even
  impossible if in a for loop etc.

Notes:

- we should consider "LSP delegation". That is, our LSP could call a
  technical space specific LSP for the stitch fragments and for model
  fragments. We just need to figure out if its possible to send the
  current file contents to the LSP server rather than have it read
  from the filesystem because otherwise we would have to regenerate
  the model every time the user changes a fragment or a
  template. Ideally we should regenerate in memory (in the LSP server
  itself) and then send the new file to the LSP server such that the
  actual file in the filesystem is not touched.
- LSP delegation would work extremely well when combined with an
  org-mode front-end. We could then allow users to edit fragments of
  code, but map these to the LSP errors and completion coming out of
  the delegated server. For errors in lines coming outside of this
  fragment we could map these to some other location.
- LSP delegation also means we can intercept *any* errors coming out
  of the remote LSP server and use them to highlight model
  elements. However, its not clear how this would work as normally we
  need to notify the LSP server that we are working on a given
  file. Perhaps here we need to integrate with the plain LSP server
  somehow. However, we do have a list of files in the model so we can
  easily filter for those and if there is a match, use that to display
  an error. The error could be just pointing out to the model element
  in question and stating there is an error. If the element is not
  code-generated we could tell the user to go to the file.
- we could also consider adding LSP for Dia. There is nothing about
  LSP that makes it text-mode specific. We could easily monitor a dia
  XML file from Dogen, resolve it into a model and then provide
  answers for requests on the Dia front-end:
  - completion of types
  - errors and warnings (make font or backgroun red?)
  - documentation on hover
  - etc.

Links:

- [[https://www.reddit.com/r/cpp/comments/eov5fh/critique_my_project_libclsp_a_c17_library/][Critique my project. Libclsp, a C++17 library]]
- [[https://github.com/otreblan/libclsp][libclsp]]: A C++17 library for language servers.
- [[https://github.com/kuafuwang/LspCpp][LspCpp GH]]
- [[https://microsoft.github.io/language-server-protocol/implementors/sdks/][SDKs for LSP]]
- [[https://microsoft.github.io/language-server-protocol/implementors/servers/][LSP servers]]: we could look at the C++ servers to see if any offer an
  LSP library.

*** Dogen studio: builds and binaries for users                       :story:

It would be nice if when a user code-generates a model in the dogen
studio site, we could do a travis build with the generated code. The
build would then produce static/shared libraries, assemblies, etc so
that the model is ready to use. We could even generate a debian
package. Dogen would produce all the required plumbing (CMake,
msbuild, etc).

We could even get away with having a single project in github/travis,
and doing a single commit that erases all previous state and instates
the new model. We would also need a queue telling users where they
are, so that all builds are serialised. For C#, it would be even
cooler if we could directly upload the model into nuget after the
build and supply the user with the nuget commands required to get the
model. However, the only problem may be that we'd end up with a lot of
dodgy code in nuget, so maybe this is not ideal.

Actually given we want to move away from Travis, it makes sense for
this to be done using regular cloud functionality (e.g. docker, etc).

*** Add support for element renaming                                  :story:

This story is still in its early days. We could probably create a
simple tool that renames types. There are several things we may want
to achieve from this:

- dia updating tool that receives new and old name and updates the
  diagram. This makes renames easier because we keep missing places
  where types are used.
- dogen support: with access to new name and old name we can now
  rename files which are not code generated. This avoids the usual
  loss of work, git reverts etc.

The problem we have is that if we do not update the diagram
automatically, the rename is not that useful. However, updating the
diagram is not entirely trivial. We probably could record XPaths
against elements and then use those XPaths to update the XML
tree. This would be a multi-step process:

- first we'd generate the final Yarn model;
- then use the old ID to locate the element to rename and all of its
  usages;
- then update the XML based on the XPaths obtained from the renamed
  elements and save it to file (maybe as =.new=?).
- we can then reload the yarn model from the new and generate from
  there.

A slightly more useful approach would be to integrate clang with
dogen. It would be a multi-step process:

- update the input format (JSON, DIA)
- run clang rename across all affected models; for this, we need to
  know the references. This is more problematic across products, so
  for now lets just imagine an update within a product. We would read
  all models in a product to create a model dependency graph, then use
  that to obtain a list of files that depend on the item to be
  renamed. This is done by separating the generated and non-generated
  files. The generated files we can infer dependencies from the
  metamodel. The non-generated files we can infer dependencies from
  the list of includes (for C++).
- then we'd generate clang tool code to rename all of the dependent
  files.
- finally we'd code generate.

This process shares a lot of commonalities with finding deprecated
(not used) elements.

We should also take into account model renaming. This is probably
easier; we could add a renaming activity to CLI. This requires a bit
of thinking.

*Automatic Renames*

This could be potentially done automatically if we could access say
the existing version of the diagram and the new version; we could load
both models and then do a diff. This could be a bit dangerous though
if there are other changes, etc.

*Attribute Renames*

Another useful use case, is the renaming of attributes.

Merged stories:

*Detect moved files*

It would be nice if we could detect files that are non-generatable and
have been moved, so we could move them across. For this we need to
know:

- that the file is not generatable: e.g. service, etc.
- that the new file name is equal to the previous file name, just in a
  different directory.

We could then just replace the empty file with the contents of the
previous file. Of course, we would still need updating namespaces,
etc.

Another way of doing this is to have UUIDs associated with each
type. The UUID is preserved into the file (ideally into a language
attribute, queryable by clang but could also be a comment). Before we
write the file we check to see if a file already exists with the same
UUID. If it does, we simply rename it to the new name matching the
UUID.

We should also issue an error if a file hasn't got a matching UUID and
at least to start off with force users to manually update it. Ideally
we should be able to use clang to update the UUID but for this we need
merging code generation support.

*** Consider C++ itself as a front-end                                 :epic:

One can imagine a clang-based front-end that reads C++ code suitably
annotated, perhaps with =masd::generatable= or some such attributes -
basically all attributes required to build a sensible logical
model. The frontend will parse the code and generate an injector
format. We can then generate serialisation, hashing etc for the
hand-crafted code.

One very good use case for this is for legacy code bases. One could
benefit from adding serialisation and IO support to an existing legacy
code base. This raises the interesting problem that we would need to:

- disable the types facet;
- inject types created from the c++ code with the types facet
  properties populated (namely include and file path).
- to be totally non-intrusive: generate a shared library that provides
  dogen code and requires the presence of the user code library.
- we may also need to somehow add a whole load of supporting types in
  order to be able to resolve references. So there would be a "target
  model" that we need to somehow identify but there would also be the
  supporting libraries (all other types the model references that we
  do not wish to code generate). This will probably be tricky. If the
  user needs to add these types manually it will not be
  practical. However, it may not be trivial to distinguish between the
  two.

This feature requires a bit of thinking as the architecture does not
support any of these.

The c++ frontend would also open some interesting possibilities:

- *read c++ and generate JSON*. This is a great way to import types into
  dogen; given a set of libraries returns a basic JSON model for dogen
  for them, with as much filled in as possible such as include
  directories, etc. This would save us a lot of time instead of
  manually adding these. The story for this was: As a dogen user, I
  want to generate system models automatically so that I don't have to
  create them manually.
- *read c++ and generate dia*. This would allow creating diagrams of
  existing code bases. Not trivial because the layout of the diagram
  would be quite hard to get right. We should create a separate tool
  for this.
- *update existing diagrams*. See [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#investigate-support-for-automatic-diagram-updates][this story]].

It would be important to be able to filter the classes. This could be
achieved by either having to supply the header files to read or
probably better by supplying a list of class names one is interested
in. We will probably need a compilation database to instantiate clang
so this would mean we'd have visibility of the entire project.

Actually the right way to do this is to have an LSP-to-Dogen
approach. This would also allow us to support any LSP-compliant
lanugage in one go.

Links:

- For clang source code transforms, see [[https://github.com/flexferrum/flex_lib][flex_lib]] and this
  presentation: [[https://www.youtube.com/watch?v%3DaPTyatTI42k][Automatic C++ source code generation with clang -
  Sergei Sadovnikov]]

*** Consider compiling generated code with clang                      :story:

It make sense to perform some kind of minimal sanity checks on the
code generated by =knit= to provide a heads up to users. A full
compilation may be beyond the scope (database compilation may not be
setup, etc) but it would be nice to pickup basic syntax errors.

This story needs to be updated in light of LSP.

*** Compile stitch template output with clang                         :story:

Once we have integrated clang for formatting stitch output, it would
be great to also integrate it for compilation. It would work as
follows:

- start creating compilation databases when we do a regular build with
  clang;
- within stitch, get clang to detect the compilation database "near"
  the template; presumably clang has some infrastructure to do so
  already - if not we could perform a file look up. Actually we should
  just have a stitch command line parameter for the compilation
  database as well as for the clang-format settings.
- create a mapping of template lines to output lines during output
  generation.
- run clang with the compilation database options against the template
  output. Use the mapping to provide a cross reference between the
  template output and the template. We could use a scheme similar to
  GCC (included from, etc).
- apparently its possible to compile from a memory buffer. See [[http://lists.cs.uiuc.edu/pipermail/cfe-dev/2015-July/044029.html][this]]
  thread.

This story needs to be updated in light of LSP.

*** Splitting facets out of a component                               :story:

It is not always desirable to generate a facet as part of the main
model. For example, say we want to support coherence, or a relational
representation. We don't want to generate a single shared object with
the model and also include coherence in it because this means everyone
will have to link against the coherence libraries. Instead, one would
like to create "project facets"; that is, to be able to somehow create
a top-level project just for that facet (or perhaps group of facets)
such that they would end up in a different project and thus different
shared object. For example:

: project_a/cpp/types
: project_a/cpp/coherence

would become

: project_a/cpp/types
: project_a.coherence/cpp/coherence

Or some such approach. Now that we have support for dot model names,
this is easier to do. We should have a meta-data element that
determines if a given facet is "built-in" ("shared"?) or if it is
"external".

This is not exclusive to facets like coherence. For example, we may
not need test data in production, particularly when we start
supporting test data types because we would then include a lot of data
in the binary (or start requiring data files to be distributed with
the model). This is useful to create system tests, but not ideal for
the production binary.

We may not need separate projects. Having different directories
complicates facet management a great deal. If we take the [[https://build2.org/build2-toolchain/doc/build2-toolchain-intro.xhtml#proj-struct][build2
approach]], in that a single project can create multiple binaries, this
would make life *much* easier: we could then simply generate static or
dynamic libraries if the facet is not built-in. We could also
automatically add the dependency between the facet library and the
main library, or even between facet libraries because we have access
to the facet dependency graph. The user should just classify the facet
as "external"; it is up to CMake to determine if its a static or
shared library.

* Future Releases

Stories that have not yet been allocated a release, by broad themes.

** Technical debt

Ideas to improve the code base.

*** Add support for temporal entities                                 :story:

It would be nice to declare an entity "temporal" and then have
automatic temporal database support.

Links:

- [[https://github.com/nearform/temporal_tables][temporal_tables GH (as a function)]]: "This is an attempt to rewrite
  the postgresql temporal_tables extension in PL/pgSQL, without the
  need for external c extension."
- [[https://github.com/arkhipov/temporal_tables][temporal_tables GH (as an extension)]]: "A temporal table is a table
  that records the period of time when a row is valid. There are two
  types of periods: the application period (also known as valid-time
  or business-time) and the system period (also known as
  transaction-time)."
- [[https://news.ycombinator.com/item?id=26748096][HN: Temporal Tables PostgreSQL Extension]]
- [[https://github.com/xocolatl/periods][periods GH]]: "This extension recreates the behavior defined in
  SQL:2016 (originally in SQL:2011) around periods and tables with
  SYSTEM VERSIONING. The idea is to figure out all the rules that
  PostgreSQL would like to adopt (there are some details missing in
  the standard) and to allow earlier versions of PostgreSQL to
  simulate the behavior once the feature is finally integrated."

*** Create a test model with SHA1 hash in generation marker           :story:

At present we cannot test this feature because we are using all models
in both JSON and Dia generation. When we do this, the original model
produces different SHA1 hashes. We need to create a test for a model
which has only one injection type.

*** Logging at info is very messy                                     :story:

Ideally when logging at info we want to see the spine of the program,
showing all transforms with their start and finishes and some
information about what they are processing (e.g. model).

*** Check all of the dogen warnings in lgtm                           :story:

We have [[https://lgtm.com/projects/g/MASD-Project/dogen/context:cpp][a number of warnings in lgtm]], some of which are actually
relevant. Even the FIXMEs should be dealt with - either create a story
or just make it a comment. The data has been saved as a file under:

- https://github.com/MASD-Project/dogen/tree/master/doc/lgtm.json

*** Convert utility exceptions into dogen exceptions                  :story:

At present the utility model has a number of hand-crafted
exceptions. We need to convert them to dogen exceptions. We also need
to get rid of the invalid enum exception and use the
=std::argument...= exception instead.

This should wait until the utility model is moved out of dogen.

*** Change order of includes according to Lakos major design rule     :story:

Lakos says:

#+begin_quote
The .c file of every component should include its own .h file as the
first substantive line of code.
#+end_quote

We decided to include it as the last line. However, Lakos approach has
the side-effect of automatically detecting headers that are missing
includes. We used to do this manually by generating =.cpp= files that
just included the header but then had to remove it because it was
slowing down compilation. With Lakos approach we get the best of both
worlds.

We need to update the generated code to follow this approach. This
will require some thinking. We should create two include blocks to
make this distinction clear, e.g.:

: // MAIN_HEADER
: #include "XYZ"
:
: // DEPS
: #include "ZZZ"

This means the include container should have two different containers
to match this structure.

*** Consider making =disabled= a trace/log level                      :story:

At present we have two knobs to control tracing/logging:

- enabled
- level

According to the rule of making invalid states unrepresentable, we
should just have a log level of disabled, so that its not possible to
set the trace/log level when logging is disabled.

*** Consider renaming log level =trace=                               :story:

Its slightly confusing given that we also have =tracing=.

*** Consider replacing the associations against object templates      :story:

Object templates are really a higher level concept when compared to
objects, etc. We should not be using associations to denote the notion
of an object instantiating an object template. Perhaps the "implements
an interface" relationship is more appropriate. Check the UML books.

*** Use DI in injection model                                         :story:

We need to use the boost.di injector in the injector model to register
the encoding and decoding transforms. This means we need to receive
lists of codecs in the main transform and then setup the reigstrar
internally. It also means we need state, which means we need to
propagate the DI pattern all the way to the top.

*** Use =std::filesystem= instead of =boost::filesystem=              :story:

Now that we have upgraded to C++ 17 we can start to make use of
=std::filesystem=. Given our very limited use, all of the required
functionality should be available.

*** Use =std::shared_ptr= instead of =boost::shared_ptr=              :story:

It seems it is now possible to serialise =std:shared_ptr= using
regular boost serialisation. We should start to move our pointers to
standard ones. The only slight problem is that we do not have boost
serialisation support for shared pointers.

Links:

- [[https://stackoverflow.com/questions/8115220/how-can-boostserialization-be-used-with-stdshared-ptr-from-c11][How can boost::serialization be used with std::shared_ptr from C++11?]]

*** Consider renaming =object=                                        :story:

We started off by having the notion of "value objects" but this was
subsequently changed to just objects. Value objects as understood by
DDD are really for things with value semantics. What we were looking
for was something akin to:

- [[https://en.wikipedia.org/wiki/Plain_old_Java_object][POJO]]
- [[https://en.wikipedia.org/wiki/Plain_old_CLR_object][POCO]]
- [[https://en.wikipedia.org/wiki/Passive_data_structure][Passive data structure]]
- [[https://en.wikipedia.org/wiki/Data_transfer_object][Data transfer object]]

However, in addition to these basic behaviours we also may have others
such as IO, test data etc. We need a name that reflects this. Ideas:

- aggregate object
- record

Interestingly, we could then have two levels: struct (no behaviour),
non-struct (behaviour).

In addition to this, we have made a mistake by transforming attributes
into getters and setters without any stereotyping. We need something
like =property= for this. Then we can say that for these kinds of
objects and their templates, the default attribute stereotype is
=property=. When not set, we should just code generate the attribute
with the correct visibility. This also means that the code templates
will be much easier, and support for real classes too (useful when we
can make use of protected regions).

*** Cross-model transformations                                       :story:

Once we have interfaces, there are at least two very useful
transformations we could do:

- remotable transformation: take a service and convert it into a set
  of messages - possibly in an IDL (e.g. protocol buffers, grpc, brpc,
  thrift), possibly in a c++ representation. For this we could have a
  meta-model element (=dogen::remotable=?) and a meta-data parameter
  pointing to the interface to make remotable, as well as the
  implementation (internal implementation to be named with boost,
  beast, etc (=mutu=?), gprc, protobuf, etc). Users can create a model
  and customise globally these parameters, then "import" the required
  services. When dogen spots a meta-model element of type remotable,
  it locates the original service interface and then for each method
  it creates messages. These are then processed depending on the
  enabled facets (for example if protobuf is enabled, generates the
  IDLs, etc). It is not possible to mix transport layers in a
  model. The generated code also contains a wrapper interface for the
  client which implements the original interface in terms of the
  transport layer. Dogen should automatically enable the required
  serialisation mechanisms (e.g. JSON, etc).
- interop transformation: take a service and generate a SWIG wrapper
  for it. Interestingly, if we had access to ports, connect,
  disconnect, etc (probably not much more) the SWIG wrapper would also
  be able to cope with transport layer implementations as well,
  allowing us to invoke remote services. However, this is probably not
  important because we can either use HTTP or protobuf etc from the
  scripting languages.

  Links:

- [[https://github.com/apache/incubator-brpc][brpc GH]]: An industrial-grade RPC framework used throughout Baidu,
  with 1,000,000+ instances(not counting clients) and thousands kinds
  of services. "brpc" means "better RPC".
- [[https://github.com/rpclib/rpclib][rpclib GH]]: "rpclib is a RPC library for C++, providing both a client
  and server implementation. It is built using modern C++14, and as
  such, requires a recent compiler."
- [[https://github.com/msgpack/msgpack-c][msgpack-c GH]]: "MessagePack is an efficient binary serialization
  format, which lets you exchange data among multiple languages like
  JSON, except that it's faster and smaller. Small integers are
  encoded into a single byte and short strings require only one extra
  byte in addition to the strings themselves." See also [[https://github.com/msgpack/msgpack-c/tree/cpp_master][cpp_master]].

*** Add support for "ad-hoc" tracing                                  :story:

We have a set of inputs supplied to the tracer called "initial
input". This is not ideal. We need a way to generalise the "initial
input" dumping. In effect, what we are really saying is that within a
transform we may need to dump more state than just the initial
inputs. We need a way to express this in the probing API.

*** Split registrar into two classes                                  :story:

At present we do not distinguish between the setting up of the
registrar and the usage of the registrar. Up to know this is not a
major issue, although its a bit of a smell that we have to call
validate at some arbitrary point.

However, with the new parts/builder setup, this becomes even more of a
problem because we only want to build the parts once we have
registered all of the formatters. The right thing would have been to
have:

- a registrar builder, used during registration;
- a build step which returns the (validated) registrar. Once build is
  called, we should throw if anyone attempts to add more formatters.

This makes it hard to misuse the API.

Notes:

- how does this affect plugins? will it still be possible to register
  formatters from a shared library?
- actually this will happen anyway as we move towards using Boost.DI.

Tasks:

- create a registrar builder with most of the existing registrar
  interface. On build it computes the parts, generates the repository,
  etc and then supplies that to the registrar. The registrar itself is
  no longer static, just a member of the workflow.

*** Use element ids for associations                                  :story:

There doesn't seem a need for having entire names for associations;
these are used to find information by ID anyway. We should try to
convert them to element id's instead and see what breaks.

- transparent, opaque associations
- base, derived visitor
- contained by

We can't do this for:

- visitor: we use the name in the formatter.

Actually there is a reason for this: we use the names to build the
file paths and the includes. We need to add some comments.

*** Tidy-up "is floating point"                                       :story:

We should introduce "point type" enumeration to replace "is floating
point":

- none
- floating
- fixed
- exact

*** Clean up comment formatter                                        :story:

Comment formatter is now a mess of ifs and boolean variables. We need
to create a proper state machine describing its internals and then
implement it.

*** Language namespaces and modeling element locations                :story:

When we designed Dogen's meta-model yarn, we created a separation from
"physical space" and "modeling space". That is, a modeling element
living in modeling space does not know of any implementation specific
details such as serialisation or test data generation. Those are
concerns left to the kernels that implement "physical space" such as
the C++ kernel and are normally implemented as separate facets. Again,
facets are a "physical concept" and have no equivalent in modeling
space.

Facets normally tend to have a folder associated, originally
envisioned as a way keep the code a bit more manageable. If we take
the [[https://github.com/DomainDrivenConsulting/dogen/tree/master/projects/yarn/include/dogen/yarn][yarn model itself]] as an example:

- types: domain types
- hash: support for std::hash
- io: iostreams support
- serialization: boost serialisation support
- test_data: test data generators

Crucially, modeling space is not aware at all of these folders and
thus they are not related to the modeling space concept of modules. So
it is that the domain type, housed in the types folder, is [[https://github.com/DomainDrivenConsulting/dogen/blob/master/projects/yarn/include/dogen/yarn/types/enumeration.hpp][defined as]]:

#+begin_src
...
namespace dogen {
namespace yarn {

/**
 * @brief Defines a bounded set of logically related values for a built-in type
 * or a string.
 */
class enumeration final : public dogen::yarn::element {
...
#+end_src

And so forth (note the absence of "types" in the namespace
declaration). This worked well for C++. However, this approach may
cause problems for C# and will certainly cause problems for Java. This
is because in these languages, folders are supposed to correspond to
namespaces. In C# this is largely optional, but in Java it is
mandatory. Thus we need some way of injecting the facet directories as
internal modules before we code generate.

Actually this is non-trivial; all references to types will now have to
concern themselves with the facet. For example, say test data
generator is referring to the domain type; this now needs to be
qualified correctly, as they are in different namespaces. This
requires quite a bit of thinking in order to generate compilable
code.

On further thought, perhaps its not that bad. We just to be able to
distinguish proxy from non-proxy types (in order to know whether to
apply the "fake" facet namespace); then, we either apply the current
facet (say test data) or types. We don't refer to a third facet. In
addition, we can also use the facet folder as the fake namespace. So,
before we make use of a name, we need to call the assistant to inject
the fake internal module, either with the current facet or types; this
is done for all non-proxy names. The "is proxy" property needs to be
added to names.

In addition, we could define =using= declarations for all dependent
facets at the top of each file as we know these up front. Thus all
types are declared on a namespace specific to the facet, but each
facet includes dependent facets so that the templates themselves are
not affected (other than the using statement). Actually this will not
work if there are several referenced models which happen to have types
with the same name.

Tasks:

- add a meta-data flag to enable/disable this feature.
- in assistant, during code generation, provide a function which
  injects the internal module.

*** Rename formatting assistant                                       :story:

With the introduction of assistants in C#, we now have overloaded the
term. We need to find another name to refer to the formatting
assistant in C++ so avoid confusion.

*** Remove unused elements from logical model's final model           :story:

We could mark all used elements during resolution and then during the
transformation into the final model, we could drop all unused
elements. This could be done by a "reducer" transform.

This makes even more sense once we move all of the dependency
generation code from the kernels into assets, because then only
generated types are required. This means we have two levels of
removal:

- types which are not referenced during resolution: slightly more
  complicated now that we resolve in multiple places. However the key
  is to be able to mark a type as "used" during any part of the
  transform pipeline.
- types which are not generatable. This can be removed as we finish
  the parts of the pipeline that require them (e.g. variability, etc).

We could create transforms for each of these.

Merged stories:

*Filter out unused types from final model*

When we finished assembling the model we should be able to determine
which supporting types are in use and drop those that are not. This
can be done just before building the final model (or as part of that
task).

We should have a class responsible for removing all types from a model
which are not in use. This could be done as part of model assembly.

One way this could be achieved is by adding a "usages" property,
computed during resolution. Resolver could keep track of the
non-target names that are in use and return those.

*** Model should contain set of built-in id's                         :story:

We are computing the set of all built-in id's in generation but this
should really be part of the logical model.

We need to double-check this - we have a built-ins model. This story
may have bit-rotted.

*** Identifier parser has hard-coded built-ins                        :story:

Instead of using the hardware model, we have hard-coded all of the
built-ins. In addition, there are some built-ins which are C++
specific (=wchar_t=), as well as others which are only valid in
certain cases such as =void=. This needs a bit of thinking.

We could look for all built-ins in the global namespace. Or we could
have a tag in the types that describes them in a way that we can
filter: =hardware_type= flag? The problem is that we need the
identifier parser in order to load models and we need the loaded
models in order to locate these types.

One solution for this problem is to move the properties expansion to
later on after the front end workflow has finished executing. Once we
have a merged model we can then easily take the built-in container
and inject that into the identifier parser. The only slight problem is
that we need to know of the top-level modules for a given model in
order to use the identifier parser. This means we need to expand
unparsed types before merging. There is a circular dependency here.

We somehow need a first pass to obtain all the built-ins and a second
pass to parse.

*** =always_in_heap= is not a very good name                          :story:

What the name is trying to say is: I have a type parameter and that
type parameter is always allocated in the heap. But it does not quite
convey that at all - it seems like the type itself is always in heap
the way we use it in resolver.

*** "current" is not the best of names in name tree                   :story:

We need to find a slightly more meaningful name for the "current" name
on the name tree. It was just about alright and then we went and
introduced =is_current_simple_type=, which is unintelligible.

We could call it just "data" and drop "current" from flag.

*** Rename methods parsing name trees                                 :story:

We have a variety of names for the methods parsing name trees
recursively. The best one seems to be =walk_name_tree=. We should use
this name consistently.

*** Create utility methods for =__type__= etc                         :story:

At present we've hard-coded the field name for =__type__= and so forth
in each formatter. This is not ideal. Create a simple utility method
that returns it and update all formatters to use it instead. List of
hard-coded things:

- =__type__=
- =<empty>=
- =data=
- =value=
- =memory=
- string helper variables: =<new_line>=, =<quote>=
- =tidy_up_string=

*** Improve container details in JSON dump                            :story:

#+begin_quote
*Story*: As a dogen user, I would like to know how many elements
containers have so that I don't have to count it manually.
#+end_quote

It would be nice to have the container type and size in the JSON
output. In addition, it seems we are not outputting all containers
correctly. For example, for associative containers we have:

:  "elements": [
:    [
:      {
:        "__type__": "key",
:        "data": "<std><unordered_map>"
:      },
:      {
:        "__type__": "value",
:        "data": {
:          "__type__": "boost::shared_ptr",

We should really be outputting the container type, as well as the key
and value types:

:  "elements": {
:     "__type__": "std::unordered_map",
:     "count": 10,
:     "entries": [
:         {
:             "__type__": "std::pair",
:             "first": "<std><unordered_map>", ==> NOTE: just a string
:             "second": {
:                 "__type__": "boost::shared_ptr",
: ...

And so forth. The only problem with this approach is with simple
types. If we have a key

*** Consider removing filtering ostream                               :story:

Originally we added a boost based stream to handle
indentation. However, since we moved over to stitch, there probably is
no need to use it any longer. We need to investigate if the formatters
model is making use of it (generating comments, namespaces, etc). If
not, remove it.

*** Sequences that support multiple postfixes                         :story:

In C# we encountered what is probably a new use case for sequences: a
single sequence with two different postfixes, depending on the element
we're processing. We solved the problem by using two separate
sequences, with lots of copy and paste and duplication. However, a
more elegant solution is to allow "indices" for the configuration,
e.g. instead of:

:                dogen::formatters::sequence_formatter sf1(o.local_attributes().size());
:                dogen::formatters::sequence_formatter sf2(o.local_attributes().size());
:                sf1.element_separator("");
:                sf2.element_separator("");
:                sf1.postfix_configuration().not_last(", true/*withSeparator*/");
:                sf2.postfix_configuration().not_last("   ");
:                sf1.postfix_configuration().last("");
:                sf2.postfix_configuration().last("");

We'd do:

:                dogen::formatters::sequence_formatter sf(o.local_attributes().size());
:                sf.element_separator("");
:                sf.postfix_configuration(0).not_last(", true/*withSeparator*/");
:                sf.postfix_configuration(1).not_last("   ");
:                sf.postfix_configuration(0).last("");
:                sf.postfix_configuration(1).last("");

And then, we'd use it as:

: sf1.postfix(0)

and so forth.

We lost the C# use case for this though, as we managed to make the API
symmetric for both use cases.

*** Attribute types are always fully qualified                        :story:

When we code generate non-built-ins attributes we always fully qualify
them even if they are on the same namespace as the containing
type. This should be easy to fix by extending the resolver to take in
the internal module path of the context. We could even recurse up the
internal module path, allowing for references to types in containing
modules.

*** Using default value with text collection throws                   :story:

We don't support default values with text collection, but if the user
tries to use it, the error that comes out is not particularly helpful:

: Invalid or unsupported value type:
: { "__type__": "value_types", "value": "text_collection" }

This is because we attempt to instantiate the field value in the
hydrator, but there is no support for text collection there:

: 2016-08-05 08:10:03.749580 [ERROR] [knitter] Error: ../../../../projects/dynamic/src/types/json_hydrator.cpp(150): Throw in function boost::shared_ptr<dogen::dynamic::value> dogen::dynamic::json_hydrator::create_value(dogen::dynamic::value_types, const string&) const
: Dynamic exception type: boost::exception_detail::clone_impl<dogen::dynamic::hydration_error>
: std::exception::what: Invalid or unsupported value type: { "__type__": "value_types", "value": "text_collection" }
: [tag_workflow*] = Code generation failure.

The right thing to do is to throw a more sensible exception such as
"default value is not supported for text collections".

Once we have a use case for default values in text collections we
should add it.

*** Perform lexical casts once only for error reporting               :story:

There are a number of places in the code where we do lexical casts for
enumerations for the exception part:

: BOOST_LOG_SEV(lg, error) << unsupported_formatter_type << ft
:                          << " name: " << o.name();
: BOOST_THROW_EXCEPTION(workflow_error(unsupported_formatter_type +
:    boost::lexical_cast<std::string>(ft)));

We should just do the lexical cast once at the top and use it for both
logging and the exception message.

In addition we should be using =string_converter= for qnames now
instead of io'ing them directly.

*** Consider moving the mock factories into the test_data directory   :story:

There is no good conceptual reason to split the mock factories from
the test_data generators. However, we did it because we don't have a
good way to give dogen visibility of the existence of these files: we
could add regexes but then its not very maintainable and not visible
from the project diagram.

The correct solution for this may be to have some tags that state that
an object only has representations in certain facets. This is captured
by this story: [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-adding-facet-specific-types][Consider adding facet specific types]].

*** Use consistently the American spelling for license                :story:

We have a mix of American and British spelling of license (e.g. data
file folder is called licence. For details on the subject see [[http://www.future-perfect.co.uk/grammar-tip/is-it-license-or-licence/][this
article]].

We are going to take the easy approach as we did for serialisation and
make all the code artefacts American. Documentation etc is not that
important.

*** Clean up coding resolver tests by extending mock factory          :story:

Now that the mock factory has the concept of "stages" of processing,
we need to create a "stage" for merged but unresolved models and
remove the merger from the resolver tests. The flag for this has been
added, we just need to go through the different scenarios and add
handling code for them.

*** Refactor coding mock factory method names                         :story:

We have a zoo of naming conventions, some starting with =build_=, some
starting with =object_= etc.

*** Validate assets mock factory on its own tests                     :story:

At present we have a lot of code that ensures that the output of mock
factory actually corresponds to expectations. However, this validation
is in the tests that use the mock factory, resulting in duplication
and possibly missing coverage. We should really just have a mock
factory test with this validation.

*** Split floating point stream settings from double                  :story:

We had a problem where the output of floating point numbers was being
truncated due to scientific notation being used. A quick fix was to
just update the properties of all streams which use either doubles,
floats or _bools_ with precision etc settings. The real fix is to
distinguish between the two such that we only enable =bool= related
settings when dealing with bools and floating point settings when
dealing with =double= or =float=.

*** Split is floating point like from int like in view model          :story:

At present we only have a single test data generator helper method for
any numeric type: =is_int_like=. This works ok, but it means we are not
generating useful test data for doubles, e.g: =1.0= instead of a
slightly more useful =1.2345= or some such number.

We need a =is_floating_point_like= method to be able to distinguish
between them, and then the associated changes in the generators to
create floating point numbers.

*** Inline comments in =comment_formatter= are a hack                 :story:

We need to tidy-up comment formatter. We had introduced
=documenting_previous_identifier= rather than "comment inline" but in
reality we do need to distinguish these two use cases. If there are
several lines, we want to finish each line with a new line. However,
if thee is just one line, as is the case with enumerations, we do not
want to add any new lines. This is because the stitch template will
add new lines so we end up with too many of them.

*** Assignment operator should be protected in ABC                    :story:

As per MEC 33. We should probably do the same for the move and copy
constructors.

*** Full constructor parameter comments                               :story:

#+begin_quote
*Story*: As a dogen user, I want the complete constructor to be
documented automatically so that I don't have to do it manually.
#+end_quote

We could use the comments in properties to populate the comments for
the full constructor for each parameter. This would require taking the
first line of the documentation of each property and then stitching
them together for the full constructor.

*** Header only models shall not generate projects                    :story:

#+begin_quote
*Story*: As a dogen user, I want to generate models with just headers
that do not result in full blown projects.
#+end_quote

A project with just exceptions does not need a make file, and fails to
compile if a makefile is generated. We need a way to not generate a
makefile if there are no implementation files generated.

Actually we still need a CMakeFile. We need to figure out what the
conventions are for header only projects.

*** IO header could depend on types forward declaration               :story:

At present we are depending on the types header but it seems we could
depend only on the forward declarations.

*** Improve streaming of empty expressions in stitch                  :story:

We have a problem with empty expressions:

: <#= #>

Results in:

: s << <<

We need to ignore empty expressions.

*** Rename sequence formatter                                         :story:

The =sequence_formatter= is actually not a formatter, but a helper or
assistant.

*** Create more "utility" members for formatters                       :epic:

One way of making the templates a bit more manageable is to avoid
having really complex conditions. We could simplify these by giving
them intelligible names and making them properties of the
formattables - mainly class info as that's where the complexity seems
to stem from. For example:

: if ((!c.all_properties().empty() || c.is_parent()) && !c.is_immutable()) {

could be replaced with =has_swap=, or perhaps even =has_public_swap= /
=has_protected_swap=.

*** Investigate the integration of =boost::log= with throw exception  :story:

At present we write a lot of code like this:

: BOOST_LOG_SEV(lg, error) << object_not_found << qn;
: BOOST_THROW_EXCEPTION(indexing_error(object_not_found +
:     boost::lexical_cast<std::string>(qn)));

This is to ensure we log the fact that an exception occurred to make
debugging problems easier. However, it leads to a lot of duplicated
code. We need to figure out a way of simplifying this, most likely
through a macro.

Actually an even easier way would be to generate IO for exceptions; we
could simply create the exception, populate all data values, dump it
to the log file and then throw the exception.

We can then create a helper in utility to throw exceptions that
performs both the throwing and the logging. It could be a function
that assumes lg to be in the global namespace, as that is how we
normally use it. We just need to populate the exception up front.

*** Change stitch's standard control block start marker to match t4   :story:

For some reason we used =<#+= as the start marker for standard control
blocks; t4 uses =<#=. We should use the same as t4. One disadvantage
of t4's choice is that we now need to ensure we are not in the
presence of a expression block. We could check for expression blocks
first and if not, then check for standard control blocks.

*** Improve stitch's processing of inline statements                  :story:

At present we have very different handling for the different kinds of
inline statements:

- directives
- expression blocks
- standard control blocks

However, they follow the same pattern and could be implemented with
largely the same algorithm:

Generic processing of an inline statement:

- check start and end markers: =validate_start_and_end_markers=.
- strip start and end markers: =strip_start_and_end_markers=.
- check for any marker, if present error: has_markers=.
- if directive, check for kvp form.

There is no need for looping etc.

*** Consider moving =add_model_module= to flags                       :story:

When we implemented support for =add_model_module= in yarn mock factory
we added the flag to all relevant methods. We could have added it to
the flags instead. The downside of this approach is that we have
static factories in specs, so all tests will have the same set of
flags. Still, intuitively it sounds like all tests should have it
either on or off for a given class being tested. Patch:

: @@ -82,7 +82,8 @@ public:
:              const bool resolved = false,
:              const bool concepts_indexed = false,
:              const bool properties_indexed = false,
: -            const bool associations_indexed = false);
: +            const bool associations_indexed = false,
: +            const bool add_model_module = false);
:
:      public:
:          /**
: @@ -139,6 +140,14 @@ public:
:          void associations_indexed(const bool v);
:          /**@}*/
:
: +        /**
: +         * @brief If true, adds a module for the model.
: +         */
: +        /**@{*/
: +        bool add_model_module() const;
: +        void add_model_module(const bool v);
: +        /**@}*/
: +
:      private:
:          bool tagged_;
:          bool merged_;
: @@ -146,6 +155,7 @@ public:
:          bool concepts_indexed_;
:          bool properties_indexed_;
:          bool associations_indexed_;
: +        bool add_model_module_;
:      };

*** Add a configuration class to logical model's mock factory         :story:

Every time we need to extend the mock factory we are finding we need
to modify every single function. This is particularly painful due to
the fact we rely on defaults. For example, we can't easily add an
external module path because we need to modify every single method. We
need to look into patterns for this. One option would be to create a
factory configuration class that has the super set of all parameters
required and pass that configuration to each function.

We did add the flags to the constructor, but it would be better if we
could pass in the configuration for each method invocation rather than
for the entire factory.

*** Consider using boost pointer container for formatters             :story:

At present we are using a container of shared pointers to house the
different formatter types. These are then encased on a "container"
class. However, in reality we are passing around references to that
container class; it seems we do not need shared pointers at all. We
should look into using a [[http://www.boost.org/doc/libs/1_57_0/libs/ptr_container/doc/ptr_container.html#motivation][boost pointer container]]. We do not have dogen
support for this so we would have to add it first.

*** Clean up stitch terminology using markup fundamentals             :story:

We came up with a number of quick definitions for stitch because we
needed them for our use cases. However, the names we chose were fairly
random. We should look into the theory around markup languages to name
these things properly.

Links:

- [[https://en.wikipedia.org/wiki/Markup_language][Wikipedia's page on Markup Languages]]

*** Consider adding stitch support for class feature control blocks   :story:

T4 supports an additional type of control blocks called class
features. These permit declaring methods on the class (external
functions for us) that can then be called by the standard control
blocks.

From a stitch perspective we don't necessarily need these, because the
stitch template is not bound to a given function (such as the
transform function in the T4 case); rather, one has to declare the
function that wraps the template within the template itself.

At any rate, this story is a place holder for further analysis on this
in case there is a sensible way to use these class feature control
blocks within stitch.

One interesting twist to this is that class feature blocks can also
contain text blocks. This means we would have two separate cases of
mixed blocks.

Links:

- [[http://www.olegsych.com/2008/02/t4-class-feature-blocks/][Understanding T4: Class Feature Blocks]]
- [[https://msdn.microsoft.com/en-us/library/bb126478.aspx][Writing a t4 template]] (section Class feature control blocks).

** Unclassified

*** Consider adding support for Click House                           :story:

#+begin_quote
ClickHouse® is an open-source column-oriented database management system that
allows generating analytical data reports in real-time.
#+end_quote

Links:

- [[https://github.com/ClickHouse/ClickHouse][ClickHouse GH]]
- [[https://github.com/ClickHouse/clickhouse-cpp][clickhouse-cpp]]: C++ client for ClickHouse.

*** Consider adding support for KDab reactive programming             :story:

#+begin_quote
Reactive programming & data binding in C++

From plain C++ you get:

- Signals + Slots.
- Properties templated on the contained type.
- Property bindings allowing reactive code to be written without having to do
  all the low-level, error prone plumbing by hand.
- Lazy evaluation of property bindings.
- No more broken bindings.
- Totally stand-alone "header-only" library. No heavy Qt dependency.
#+end_quote

Links:

- [[https://github.com/KDAB/KDBindings][KDBindings GH]]
- [[https://www.kdab.com/signals-slots-properties-bindings/][Introducing KDBindings]]

*** Stuttering in naming                                              :story:

Apparently, when one repeats the same name its called stuttering. From [[https://github.com/facebook/folly][Folly]]:

#+begin_quote
At the top level Folly uses the classic "stuttering" scheme folly/folly used by
Boost and others. The first directory serves as an installation root of the
library (with possible versioning a la folly-1.0/), and the second is to
distinguish the library when including files, e.g. #include <folly/FBString.h>.

The directory structure is flat (mimicking the namespace structure), i.e. we
don't have an elaborate directory hierarchy (it is possible this will change in
future versions). The subdirectory experimental contains files that are used
inside folly and possibly at Facebook but not considered stable enough for
client use. Your code should not use files in folly/experimental lest it may
break when you update Folly.
#+end_quote

We need to add this term into our vocabulary, for example for configuration
options.


Links:

- [[https://michaelwhatcott.com/go-code-that-stutters/][Go code that stutters]]

*** Handling empty primitives                                         :story:

In certain cases we may want to represent the notion of an empty /
optional primitive. This could easily be achieved by having an
optional of the primitive. However, maybe the primitive itself could
have support for optionality? Given we use primitives for very simple
types, we could probably "optimise" it instead of having pointers to
ints etc.

*** Add equals operator to primitives                                 :story:

At present we need to extract the value in order to make comparisons.

*** Org-mode and rountripping                                         :story:

With org-mode we will be able to sync selected elements from the
source code:

- get properties from manually crafted classes. No use case for this
  yet.
- get all test names. This can be used to generate the org-mode test
  file with org-babel.

** Feature Heap

Features for which there is no urgency but we'd like to see them
implemented at some point. They may require some alterations to the
core, but not fundamental rethinking.

*** Consider adding support for zip serialisation                     :story:

#+begin_quote
A modern C++20 binary serialization library, with just one header file.

This library is a successor to zpp::serializer. The library tries to be simpler
for use, but has more or less similar API to its predecessor.
#+end_quote

Links:

- [[https://github.com/eyalz800/zpp_bits][zpp_bits GH]] (C++ 20)
- [[https://github.com/eyalz800/serializer][zpp serializer]] (C++ 17)

*** Add support for citation files                                    :story:

#+begin_quote
What is a CITATION.cff file?

CITATION.cff files are plain text files with human- and
machine-readable citation information for software (and
datasets). Code developers can include them in their repositories to
let others know how to correctly cite their software.

This is an example of a simple CITATION.cff file:

: cff-version: 1.2.0
: message: "If you use this software, please cite it as below."
: authors:
:   - family-names: Druskat
:     given-names: Stephan
:     orcid: https://orcid.org/0000-0003-4925-7248
: title: "My Research Software"
: version: 2.0.4
: doi: 10.5281/zenodo.1234
: date-released: 2021-08-11
#+end_quote

We could easily create a metamodel element with the data required to
generate this file. However, we should also generate the bibtex entry
from the same format and use configuration to decide what to
create. For this we need a new facet (=citation=?) with multiple
archetypes (=cff=, =bibtex=, etc.)

Links:

- [[https://citation-file-format.github.io/][GH]]
- [[https://news.ycombinator.com/item?id=28246899][HN]]

*** Consider adding support for Dr. Mock                              :story:

#+begin_quote
DrMock is a C++17 testing and mocking framework for Windows, Linux and macOS.

Features

- Unit test framework
- On-the-fly mock object source code generation at compile time
- State machine-like mock objects
- Qt5 integration
#+end_quote

Links:

- [[https://github.com/DrCpp/DrMock][GH]]

*** Add docker support                                                :story:

When we create a product, we should be able to enable docker and get
the docker files generated, etc.

Links:

- [[https://thewagner.net/blog/2021/02/25/building-container-images-with-nix/][Building container images with Nix]]
- [[https://news.ycombinator.com/item?id=28240748][HN: Building Container Images with Nix]]
- [[https://sandervanderburg.blogspot.com/2020/07/on-using-nix-and-docker-as-deployment.html][On using Nix and Docker as deployment automation solutions:
  similarities and differences]]

*** Consider adding support for =Hypodermic=                          :story:

#+begin_quote
Hypodermic is a non-intrusive header only IoC container for C++. It
provides dependency injection to your existing design by managing the
creation of your components and their dependencies in the right order,
sparing you the trouble of writing and maintaining boiler plate code.
#+end_quote

Links:

- [[https://github.com/ybainier/Hypodermic][Hypodermic GH]]

*** Consider adding support for =strong_type=                         :story:

#+begin_quote
Very much inspired by @foonathan's type_safe library, but aim is
slightly different. Limit scope for type safety only. No runtime
checks. Also strive for a higher level abstraction of the needed
functionality. The idea is to suffer no runtime penalty, but to
capture misuse at compile time (for example accidentally subtracting
from a handle, or swapping two parameters in a function call) while
still being easy to use for inexperienced programmers.
#+end_quote

Links:

- [[https://github.com/rollbear/strong_type][strong_type GH]]
- [[https://github.com/foonathan/type_safe][type_safe GH]]

*** Add support for C++ modules                                       :story:

We need to wait until clang and gcc support this.

Links:

- [[https://devblogs.microsoft.com/cppblog/moving-a-project-to-cpp-named-modules/][Moving a project to C++ named Modules]]

*** Consider adding support for open telemetry                        :story:

#+begin_quote
An observability framework for cloud-native software.

OpenTelemetry is a collection of tools, APIs, and SDKs. You can use it
to instrument, generate, collect, and export telemetry data (metrics,
logs, and traces) for analysis in order to understand your software's
performance and behavior.
#+end_quote

Links:

- [[https://opentelemetry.io/][main site]]
- [[https://github.com/open-telemetry/opentelemetry-cpp][OpenTelemetry C++ GH]]

*** Consider using PMR strings                                        :story:

We should create a report that provides a size distribution of files
in Dogen. This would enable us to figure out if there is a "common
size" that most files have. If so, it may make sense to create PMR
strings to represent files so that we do not require dynamic
allocation for string concatenation when generating files. However, we
should probably only look into this after considering a move to =fmt=.

Even if we don't use PMR, this could still be useful if there is a way
to pre-allocate these strings like one can do with vectors. We should
also try to use the content string for the formatting.

Links:

- [[https://blog.feabhas.com/2019/03/thanks-for-the-memory-allocator/][Thanks for the memory (allocator)]]

*** Consider adding attributes to meta-model                          :story:

Programming languages such as C#, Java and C++ support annotating
types with attributes. This should be really easy to do via
meta-data. We just need to capture these as attributes in the
meta-model and then code-generate the appropriately. We should also
allow attributes in properties.

*** Consider making conversions a generation concept                  :story:

Strange idea, and one for which we haven't given a lot of thought, but
captured just so we don't forget. At present we have Dia model files
side by side with JSON model files (well, not exactly side by side but
in proximity). Which makes one wonder: what if the conversion into
different injection formats was supported directly in the dogen
meta-model? That is, imagine a world where there is a meta-model
element representing the model itself. Its not a big stretch to think
that we could also have a second representation for the model, with
some feature indicating a conversion is required and a pointer to the
model (it could a meta-type specific for this, such as
=model::conversion= or some such and with the appropriate
"destination" injector). Then dogen would run a conversion of that
model. This would make our life easier because every time we generated
a model, we would automatically convert it into any of the types we
set the model to convert to. If the tests were then clever enough to
test all supported formats then we wouldn't even have to remember to
add models.

*** Consider generating model generation tests                        :story:

At present we are manually adding model generation tests (e.g. the
tests that check if a model's output is the same as what is currently
in the file system). This is a bit painful and we end up forgetting to
add new models. Ideally we want dogen to generate these tests. We also
want dogen to automatically test all the supported injectors (e.g. if
there are files for injection X we want the tests to pick that up
dynamically and test them).

One slight problem though is that we don't want to generate all
generated tests on regular builds; we just want to generate these
tests. Then, on the nightly we want to generate all tests. This
requires some thinking.

*** Consider computing hashes only once for immutable types           :story:

if a type is immutable the hash should be computed once at
construction and then cached. Not quite sure how this would be
implemented though since we have hashing as a totally separate aspect
from types.

*** Dump transforms as serialisable data                              :story:

Now we have detailed transform information, it would be great if we
could also load the serialised data. At present this is not possible
because we are using IO output so that it is easily diffable. However,
we could either:

- add rapid JSON support, so its both readable and serialisable;
- add command-line options to determine the output format, so that we
  can dump data as boost serialisation instead.

The second is low-hanging fruit.

Once we have this, we can then write simple tests that read the
serialised data and use it to reproduce bugs, narrowed to a specific
transform. This would be extremely useful once we have
code-generationa as a service.

Merged stories:

*Add a file format parameter to tracing for model dumps*

At present we are dumping all models in probing as JSON. It would be
nice to be able to dump them as boost serialisation so we can plug
them into tests or to reproduce some problem. It would be even nicer
if we could plug that data back in to dogen but its not obvious how
that would work; we need to have some kind of concept of "stages", and
then supply the inputs and the stage so that dogen could continue from
there.

One slight downside of this is that we'd need serialisation support
for all core models. But its still very useful.

*** Add a new annotation type of "pair"                               :story:

It would be nice to be able to declare a annotation type with a value
type of "pair" or "key value pair" and have the annotations
automatically perform the splitting. The separator should not be
equals, since we already use that for annotations kvps, but it could
be comma, pipe, etc. The API would be augmented to return a
=std::pair= with key and value.

One slight snag: the value could be of any type:

- boolean
- string
- enumeration (when we support these)
- even text collection

We can start by just supporting strings, but probably worthwhile
having a think on how to specify the type.

*** Add support for "colour themes" to dogen                          :story:

At present we arbitrarily colour coded the UML elements according to
our preferences, mapping stereotypes to colours. The extensible way to
do this is to add a feature to a named configuration called
colour. Then once we have a python API we could ask python to retrieve
the mapping between stereotypes and colours and apply it (as per
current script). If we make the colours standard RGB, they can also be
used in other frontends. We must also include entries for metamodel
elements such as =dogen::enumeration=.

We should have a metamodel element for this. The keys could be simply:

: masd.METAMODEL_ELEMENT.colour=RGB

Then, we could expose the metamodel element from SWIG into python.

Actually this is even more useful in the context of Plant UML. We just
need to create a "table" of identifiers, either by meta-model element
name or by stereotype. The meta-model element should work like
profiles allowing inheritance. Or at least dogen could define a base
"table" for elements which can be overriden by the user.

*** Consider adding inheritance support to modelines                  :story:

Though it is probably overkill, it would be nice to be able to
inherit from modelines; then we could define all the common fields
on a parent.

A slightly different take is that modelines, enums, etc could benefit
greatly from templates just like objects. However the only problem is
we can't use the exact same approach, or we'd end up with 2N
elements. We need some kind of way of declaring a template (what we
will in the future call traits) and associate it with a meta-model
element.

*** Create a mock factory facet                                       :story:

A very common pattern we have in dogen is that for a small number of
types we need to create a mock factory. This is normally made in a
test folder and the file must be hand-crafted. It would be nice to
have dogen code-generate the skeletons for the mock factories. For
this we would need a meta-element for mock factories (with the new
ideas on meta-model transparency, we can't just add a switch to the
target element any longer). Ideally we would also have an associated
wale template to give us a good head start.

*** Consider making editor a meta-data element                        :story:

When we create meta-data elements for technical space, facet, etc we
should also consider making one for editors. We probably only need a
few properties such as start of modeline, etc.

*** Add support for multiple profile binds per modeling element       :story:

At present we can only bind an element to one profile. The reason why
is because we've already expanded the profile graphs into a flat
annotation and if we were to apply two of these expanded annotations
with common parents, the second application would overwrite the
first. Of course, we bumped into the exact same problem when doing
profile inheritance; there it was solved by ensuring each parent
profile is applied only once for each graph.

One possible solution for this problem is to consider each model
element as a "dynamic profile" (for want of a better name; on the fly
profile?). We would create a profile which is named after each of the
profiles it includes, e.g. say we include =dogen::hashable= and
=dogen::pretty_printable= for model element e0. Then the "on the fly
profile" would be:

: dogen::hashable_dogen::pretty_printable

It would be generated by the profiler, with parents =dogen::hashable=
and =dogen::pretty_printable=, and cached so that if anyone shows up
with that same profile we can reuse it. Because of the additive nature
of profile graphs this would have the desired result. Actually we
could probably have a two pass-process; first identify all of the
required dynamic profiles and generate them; then process them. This
way we can rely on a const data structure.

This will all be made easier when we have a two-pass pipeline because
we can do the profile processing on the first pass, and we can even
generate the "dynamic profiles" as real meta-model elements, created
on the fly.

Note that this is a very complex story with the potential to cause a
lot of problems. We should only start to tackle it when there is a
very good use case.

*** Consider adding text transform details to generation marker       :story:

It would be nice if we could tell which formatter generated which
file. We could add this to the generation marker:

: add_model_to_text_transform_details

In reality, since the formatter binds to the archetype, we could just
print the fully qualified physical location. We need to understand how
that would work in the existing boilerplate formatter - we need an
additional field to be added.

*** Add a "info" activity to dogen                                    :story:

This is somewhat similar to the dry run mode and diff mode so we need
to think about it properly. The gist of it is that we want a mode that
dumps lots of details about a model:

- all the models it will load and their SHA1 hashes and git SHA1
  hashes
- whether generation would work or what errors it found.
- details on the objects found in each model.
- whether generation would modify any files and if so which files
  (just the names).

Note that we've already have a dumpspecs activity. We need to make
sure there is a clear distinction between the two so users don't get
confused.

*** Consider improving dry-run mode                                   :story:

This article has some good suggestions for a dry-run implementation:

- [[https://www.gresearch.co.uk/article/in-praise-of-dry-run/][In praise of --dry-run]]

Check to see if we are using this approach on our implementation.

*** Allow user supplied enumerator values                             :story:

Now that we have value support in injection, it should be fairly
straightforward to allow users to supply their own enumeration
values. When this happens we need to check that:

- they are unique and valid according to enumeration type;
- they do not class with invalid value.

Actually we have implemented this but using meta-data for some
reason. We need to remove the meta-data support and use the value
field instead.

#+begin_example
    {
      "name": "features::enumerator",
      "documentation": "Parameters related to enumerators.\n",
      "stereotypes": [
        "masd::variability::feature_bundle"
      ],
      "tagged_values": {
        "masd.variability.default_binding_point": "property",
        "masd.variability.archetype_location.kernel": "masd",
        "masd.variability.template_kind": "instance"
      },
      "attributes": [
        {
          "name": "masd.enumerator.value",
          "type": "masd::variability::text",
          "documentation": "Value to use for this enumerator. Must be unique for an enumeration.\n",
          "tagged_values": {
            "masd.variability.qualified_name": "masd.enumerator.value",
            "masd.variability.is_optional": "true"
          }
        }
      ]
    },
#+end_example

*** Consider adding support for JWT                                   :story:

JSON Web Tokens (JWT) are used in web apps.

Links:

- [[https://github.com/Thalhammer/jwt-cpp][jwt-cpp]]: A header only library for creating and validating json web
  tokens in c++.
- [[https://github.com/arun11299/cpp-jwt][cpp-jwt]]

*** Investigate the uses of =nameOf= in Dogen                         :story:

Seems like someone ported =nameOf= from C# into C++. This is a useful
idea: instead of using a string to copy across a name, it enforces
that the string and the name are in sync. For example:

: const std::string book = "book";

The problem is that you can rename the variable =book= to a different
name and this would then result in silly errors. With =nameOf=:

: const std::string book = nameOf(book);

Now if you change =book= then this will not compile.

Links:

- [[https://github.com/Neargye/nameof][nameof GH]]

*** Consider adding support for secure string                         :story:

Interesting project to port the secure string ideas from C# to C++:
"SecureString is a C++ class that does not save data as plain-text in
memory."

Links:

- [[https://github.com/alex-caelus/SecureString][SecureString]]

*** Update static strings to string views                             :story:

Now we're on C++17 we can start making use of its new features. One
low hanging fruit is string view. We use static strings quite a lot
for logging etc. We can just replace these with string views.

Example:

: #include <string_view>
: constexpr std::string_view foo("abc");

We started implementing this story but it will take a while to
complete it across the entire codebase.

Problems:

- cannot do XML text reader because we do not have a good way to
  convert string_view to cstr. See [[https://stackoverflow.com/questions/48081436/how-you-convert-a-stdstring-view-to-a-const-char][How you convert a std::string_view
  to a const char*?]]

Links:

- [[https://www.bfilipek.com/2018/10/strings17talk.html][Let's Talk About String Operations in C++17]]
- [[https://developercommunity.visualstudio.com/content/problem/24487/constexpr-stdstring-view-from-string-literal.html][constexpr std::string_view from string literal]]
- [[https://www.reddit.com/r/cpp/comments/cw35kk/best_practices_for_efficient_string_constants/][Best practices for efficient string constants]]

*** Add string view to dogen exception constructor                    :story:

At present we cannot build an exception if the string passed in is a
string view.

*** Add support for the "badge" pattern                               :story:

We should be able to implement this pattern in Dogen.

Links:

- [[https://awesomekling.github.io/Serenity-C++-patterns-The-Badge/][Serenity C++ patterns: The Badge]]
- [[https://www.reddit.com/r/cpp/comments/bzjbu1/serenity_c_patterns_the_badge/][reddit discussion]]

*** Add support for =fixed_storage_arrays=                            :story:

This is a library with some fixed size arrays. We need to check to see
if we support array syntax at present.

Links:

- [[https://github.com/recatek/fixed_storage_arrays][github]]

*** Consider exceptions with stream to build exception message        :story:

Consider adding some functionality to exceptions whereby we could
create the exception with a stream:

: exception e;
: e << "some blurb" << 123;
: throw e;

Or maybe we need an exception builder that takes in a type T:

: exception_builder<exception> b;
: b << "some blurb" << 123;
: throw b.build();

This is a really common use case.

Actually we just need to understand boost exception better. It seems
it already supports all of these use cases out of the box.

Links:

- [[https://www.boost.org/doc/libs/1_72_0/libs/exception/doc/tutorial_transporting_data.html][Transporting of Arbitrary Data to the Catch Site]]
- [[https://theboostcpplibraries.com/boost.exception][Chapter 56. Boost.Exception]]

*** Add support for proper JSON serialisation in C++                  :story:

We need to add support for JSON in C++. It will eventually have to
roundtrip to JSON in C# but that will be handled as two separate
stories. We need to support multiple available backends because there
are so many c++ libraries available. Different users may have
different needs (e.g. if you are already using boost spirit it may
make sense to use it, if you have casablanca you may want its support
etc.). This does complicate things for testing. We should limit
ourselves to libraries that have vcpkg ports. Tests can roundtrip
between different implementations.

We will not replace the current IO implementation; it should continue
to exist as is, requiring no external dependencies.

We should consider supporting multiple JSON libraries: instead of
making the mistake we did with serialisation where we bound the name
=serialization= with boost serialisation, we should call it by its
real name, e.g. =json_spirit= etc. Then when a user creates a
stereotype for a profile such as =Serializable= it can choose which
serialisation codecs to enable for which language. This means that the
same stereotypes can have different meanings in different
architectures, which is the desired behaviour.

We should create a serialise / deserialise functions following the
same logic as boost:

#+begin_src c++
void serialize(Value& v, const object& o);
void serialize(Value& v, const base& b);

void deserialize(const Value& v, object& o);
base* deserialize(const Value& v);
#+end_src

Or perhaps even better, we can make the above the internal methods and
use =operator<<= and =operator>>= as the external methods:

#+begin_src c++
void operator<<(Value& v, const object& o);
void operator>>(const Value& v, object& o);
#+end_src

Notes:

- create a registrar with a map for each base type. The function
  returns a base type pointer.
- when you deserialize a base type pointer, you call the pointer
  deserialize above. Same for when you have a pointer to an object. It
  will internally call the registrar (if its a base type) and get the
  right function.
- this means we only need to look at type for inheritance. Although we
  should probably always do it for validation? However, what happens
  if we want to make a model so we can read external JSON? It won't
  contain type markings.
- =operator>>= will not be defined for pointers or base classes.
- this wont work for the case of =doc << base=. For this we need a map
  that looks up on type_index.

Links:

- [[https://bitbucket.org/sobjectizerteam/json_dto-0.2/src/default/#markdown-header-what-is-json_dto][JSON DTO]]: "json_dto library is a small header-only helper for
  converting data between json representation and c++ structs."
- [[https://github.com/Tencent/rapidjson][RapidJSON]]
- [[https://github.com/nlohmann/json][JSON]]
- [[https://github.com/open-source-parsers/jsoncpp][JSON CPP]]
- [[https://github.com/Donerkebap13/DonerSerializer][DonerSerializer]]
- [[https://www.codeproject.com/Articles/20027/JSON-Spirit-A-C-JSON-Parser-Generator-Implemented][JSON Spirit]]
- [[https://github.com/miloyip/nativejson-benchmark][nativejson-benchmark]]: project comparing JSON libraries.
- [[https://github.com/dropbox/json11][Json11]]
- [[https://github.com/beached/daw_json_link][daw_json_link]]: more "static" JSON library. Also has its own code
  generator: [[https://github.com/beached/json_to_cpp][json_to_cpp]].
- [[https://github.com/simdjson/simdjson][simdjson]]: parsing gigabytes of JSON per second
  https://simdjson.org. See also [[https://lemire.me/blog/2020/03/31/we-released-simdjson-0-3-the-fastest-json-parser-in-the-world-is-even-better/][this post]].
- [[https://github.com/awangk/ujson][ujson]]: µjson is a a small, C++11, UTF-8, JSON library
- [[https://github.com/ultrajson/ultrajson][ultrajson]]: Ultra fast JSON decoder and encoder written in C with
  Python bindings

Merged stories:

*Rapid JSON support*

For the previous attempt to integrate RapidJson see this commit:

b2cce41 * third party: remove includes and rapid json
fc4edeb416 integration generation.cpp: remove rapid json remnants

*Add support for JSON serialisation*

We should have proper JSON serialisation support, for both reading and
writing. We can then implement IO in terms of JSON.

*Raw JSON vs cooked JSON*

If we do implement customisable JSON serialisation, we should still
use the raw format in streaming. We need a way to disable the cooked
JSON internally. We should also re-implement streaming in terms of
this JSON mode.

*Add serialisation support for JSON*

It seems we do not yet have a story for this. The idea is to have some
basic JSON roundtripping support. The user can configure the backend.

*** Consider adding support for Hypothesis                            :story:

This seems like a very interesting property based testing
framework. Perhaps we can understand the basic concepts and port it to
Dogen.

Links:

- https://hypothesis.works/: main site
- [[https://hypothesis.works/articles/intro/][articles on hypothesis]]
- [[https://hypothesis.works/articles/what-is-hypothesis/][What is Hypothesis?]]
- [[https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python][GH hypothesis python]]

*** Investigate =machine.specifications=                              :story:

Yet another approach to testing:

#+begin_quote
Machine.Specifications is a Context/Specification framework geared
towards removing language noise and simplifying tests. [...] MSpec is
called a "context/specification" test framework because of the
"grammar" that is used in describing and coding the tests or "specs".
#+end_quote

Links:

- [[https://github.com/machine/machine.specifications][GH]]

*** Finish adding support for Language Agnostic Models (LAM)          :story:

Tasks:

- add the missing types to LAM.
- add optional to the list of types. This is actually quite
  complicated because for some types in C# you want to map it to
  nullable, to others just a pointer will do.

LAM type map:

| Type                            | C++                              | C#                                                |
|---------------------------------+----------------------------------+---------------------------------------------------+
| lam::byte                       | unsigned char                    | uchar                                             |
| lam::character                  | char                             | char                                              |
| lam::integer8                   | std::int8_t                      | sbyte                                             |
| lam::integer16                  | std::int16_t                     | System.Int16                                      |
| lam::integer32                  | std::int32_t                     | System.Int32                                      |
| lam::integer64                  | std::int64_t                     | System.Int64                                      |
| lam::integer                    | int                              | int                                               |
| lam::single_floating            | float                            | float                                             |
| lam::double_floating            | double                           | double                                            |
| lam::boolean                    | bool                             | bool                                              |
| lam::string                     | std::string                      | string                                            |
| lam::date                       | boost::gregorian::date           | System.DateTime                                   |
| lam::time                       | boost::posix_time::time_duration | System.TimeSpan                                   |
| lam::date_time                  | boost::posix_time::ptime         | System.DateTime                                   |
| lam::decimal                    | std::decimal                     | System.Decimal                                    |
| lam::dynamic_array<T>           | std::vector<T>                   | System.Collections.Generic.List<T>                |
| lam::static_array<T>            | std::array<T>                    | System.Collections.Generic.Array<T>               |
| lam::unordered_dictionary<K, V> | std::unordered_map<K, V>         | System.Collections.Generic.Dictionary<K, V>       |
| lam::ordered_dictionary<K, V>   | std::map<K, V>                   | System.Collections.Generic.SortedDictionary<K, V> |
| lam::unordered_set<K>           | std::unordered_set<K>            | System.Collections.Generic.HashSet<T>             |
| lam::ordered_set<K>             | std::set<K>                      | System.Collections.Generic.SortedSet<T>           |
| lam::queue<T>                   | std::queue<T>                    | System.Collections.Generic.Queue<T>               |
| lam::stack<T>                   | std::stack<T>                    | System.Collections.Generic.Stack<T>               |
| lam::linked_list<T>             | std::list<T>                     | System.Collections.Generic.LinkedList<T>          |
| lam::pointer<T>                 | boost::shared_ptr<T>             | <erase>                                           |

*Previous Understanding*

When we start supporting more than one language, one interesting
feature would be to be able to define a model once and have it
generated for all supported languages. This would be achieved by
having a system model (or set of system models) that define all the
key types in a language agnostic manner. For example:

: lam::string
: lam::int
: lam::int16

Each of these types then has a set of meta-data fields that map them
to a type in a supported language:

: lam:string: cpp.concrete_type_mapping = std::string
: lam:string: csharp.concrete_type_mapping = string

And so on. We load the user model that makes use of LAM, we generate
the merged model still with LAM types and then we perform a
translation for each of the supported and enabled languages: for every
LAM type, we replace all its references with the corresponding
concrete type. We need to split the supplied mapping into a QName, use
the QName to load the system models for that language, look up the
type and replace it. After the translation no LAM types are left. We
end up with N yarn merged models where N is the number of supported and
enabled languages.

Each of these models is then sent down to code generation. This should
be equivalent to manually generating models per language - we could
use this as a test.

Once we have LAM, it would be great to be able to exchange data
between languages. This could be done as follows:

- XML: create a "LAM" XML schema, and a set of formatters that read
  and write from it. This is kind of like reverse mapping the types
  back to LAM types when writing the XML.
- JSON: similar approach to XML, minus the schema.
- POF: use the coherence libraries to dump the models into POF.

Tasks:

- create the LAM model with a set of basic types.
- add a set of mapping fields into yarn: =yarn.mapping.csharp=, etc
  and populate the types with entries for each supported language.
- create a notion of mapping of intermediate models into
  languages. The input is the merged intermediate model and the output
  is N models one per language. We also need a way to associate
  backends with languages. Each model is sent down to its backend.
- note that reverse mapping is possible: we should be able to
  associate a type on a given language with it's lam type. This means
  that, given a model in say C#, we could reconstruct a yarn lam model
  (or tell the user about the list of failures to map). This should be
  logged as a separate story.

Links:

- [[http://stackoverflow.com/questions/741054/mapping-between-stl-c-and-c-sharp-containers][Mapping between stl C++ and C# containers]]
- [[http://stackoverflow.com/questions/3659044/comparison-of-c-stl-collections-and-c-sharp-collections][Comparison of C++ STL collections and C# collections?]]

*** Add support for "naming rules"                                    :story:

The framework design guidelines has a number of suggestions for how
naming should be done for different meta-types such as enums, etc. We
could have "naming profiles" that validate that the user is following
those guidelines. We could also have a "dictionary" that checks the
spelling, looking for things such as log_in vs login etc.

We should have numbers (or names) for each of these warnings and add a
convention such as =-W= and =-Wno= so that users can enable and
disable warnings. These can be placed globally or locally on elements.

*** Add knobs to control output of constructors and operators         :story:

At present we are outputting all of the default constructors and the
operators in the handcrafted templates. Ideally it should just be the
class name. We need a way of controlling all of the default
constructors and all of the operators in one go so we can set it on
the handcrafted profile.

*** Add support for type_index                                        :story:

At present we cannot create containers using =std::type_index= as a
key because we do not have hashing, IO and serialisation support. This
will require PDM support.

Links:

- [[http://stackoverflow.com/questions/36219532/serializing-stdtype-index][Serializing `std::type_index`]]
- [[https://www.boost.org/doc/libs/1_69_0/doc/html/boost_typeindex.html][Boost typeindex]]

*** Support only specific attributes for certain facets               :story:

Whenever an object has a unique identifier, it may make sense to make
use of it for:

- hashing
- equality
- less than

And so forth. For example, names and name trees don't really require
comparing the entire state of the object. We need a way to mark
properties against each facet in the meta-data. This should ideally
join in with the ORM concept of a primary key. We need to extract that
notion out of ORM and generalise it. Users can then decide if they
want to use all attributes or just the primary key. For less than we
need to know that all attributes in the key support less than. This
must be supplied by the PDM.

*** Consider supporting non-boost exceptions                          :story:

It should be fairly trivial to disable the use of boost exception when
generating exceptions. This would allow us to create a model that is
totally independent of boost. We could do this via meta-data flags. We
probably should have several "types" of exceptions:

- user defined (no dependencies)
- inherits =std::exception=
- inherits =boost::exception=

Merged stories:

*Consider supporting user supplied exception base class*

For models that interface with other models, it may make sense to have
an exception class that is derived from a user defined class. We could
easily support an exception base class supplied via meta-data. The
user would have to expose the type via JSON.

*** Serialisable and ioable exceptions                                 :epic:

#+begin_quote
*Story*: As a dogen user, I want to send exceptions across the wire so
that I can report errors to remote users. I also want to dump
exceptions to the log file.
#+end_quote

At present we only generate the types facet for exceptions. However,
there is nothing stopping us from adding serialisation support for
exceptions. This would be useful for example for services to convey
errors on the remote end point. The same logic applies to io.

This should be fairly straightforward since exceptions are simple
types. We haven't got a use case for it yet though.

*** Use error codes in exceptions                                     :story:

Avoid breaking tests every time the exception text changes by creating
a error code property in exceptions.

After some investigation it was found that boost already supports this
approach in =system=, as per [[http://en.highscore.de/cpp/boost/errorhandling.html][boost book]]. We could define a new
category per model and then create an enumeration of all error codes
in dia, for which the values would be the strings to use for the
error. The user could then create an exception and pass in the error
code in the constructor.

We should also make use of string tables to define all the error
messages.

Could we just have an exception factory that handles all of the
machinery of creating an exception with the right code, message etc?
it could also be responsible for appending more content to an existing
exception so that we'd have the tags all in one place.

Alternatively we could have each exception define the supported error
codes. This would allow us to code generate them. The only problem is
if multiple exceptions share an error code, but this should probably
not happen?

*** Control the emission of pragma once with meta-data                :story:

At present we are always adding =#pragrma once= to the header guard:

: #if defined(_MSC_VER) && (_MSC_VER >= 1200)
: #pragma once
: #endif

This should really be optional and controlled via dynamic extensions,
probably at the c++ model level.

*** Consider adding =with= support for fluent properties              :story:

It seems the java guys have decided to add the prefix =with= when
using fluent interfaces, e.g.:

: x.with_property_x(false).with_property_y(true);

We could easily add this via dynamic extensions, e.g.: fluent prefix.

*** Consider creating a netty like builder for fluency                :story:

An alternative to fluent properties is to have a fluent builder, as
used by Netty quite extensively. This allows the class itself to
remain immutable; the builder just calls the complete constructor at
the end. We could easily have a "buildable" stereotype that generates
a builder just like we do for visitor.

See [[https://github.com/netty/netty/blob/master/example/src/main/java/io/netty/example/echo/EchoServer.java][this example]] (ServerBootstrap in particular).

The slight problem though is that we would then require the builder to
propagate down through all objects the object has attributes of (and
their objects and so forth). So we'd have to create a lot of builders
by just marking one type as buildable. Alternatively, it would be up
to the user to mark those types individually and we just expect the
user to give us a value for a given type (probably much more
sensible).

Stereotype: =masd::buildable=.

*** Make test data generator more configurable                        :story:

#+begin_quote
*Story*: As a dogen user, I want to configure test data generation so
that I don't have to handle corner cases manually.
#+end_quote

One thing that would be useful is to have a way to attach lambdas to
test data generator. Let =a= be a class with a property =prop= of type
string. It would be nice to be able to do:

: a_generator g;
: g.prop([](const unsigned int seed) {
:     std::ostringstream s;
:     s << "my property " << seed * 10;
:     return s.str();
: });

And so on, for all member variables. The generators would have some
default behaviour, but it could be overridden at any point by the
user. With this, test data generator would be a great starting point
as a way of generating random data for test systems.

See also [[http://www.json-generator.com/][JSON generator]].

*** Use coroutines in test data generators                            :story:

We should look at these articles for ideas on how to create generators
that use coroutines.

- [[https://kirit.com/How%2520C%252B%252B%2520coroutines%2520work/Generating%2520Iterators][Generating Iterators]]
- [[https://github.com/lewissbaker/cppcoro/blob/master/include/cppcoro/generator.hpp][generator.hpp]]: sample generator for C++ 20.
- [[https://github.com/MiSo1289/asiochan][asiochan GH]]: "This library provides golang-inspired channel types to
  be used with ASIO awaitable coroutines. Channels allow bidirectional
  message passing and synchronization between coroutines. Both
  standalone and boost versions of ASIO are supported. See the
  installing section on how to install and select the ASIO
  distribution used."
- [[https://github.com/bloomberg/quantum][quantum GH]]: "Quantum is a full-featured and powerful C++ framework
  build on top of the Boost coroutine library. The framework allows
  users to dispatch units of work (a.k.a. tasks) as coroutines and
  execute them concurrently using the 'reactor' pattern."

*** Add support for =std::forward_list=                               :story:

We have been using =std::list= quite liberally. However, on hindsight,
for the vast majority of cases, we don't require a full blown list; a
simple forward list would do. Problem is Dogen does not support
forward lists just yet. We need to add support for these, including
solving the missing boost serialisation problem.

We seem to have partial support for this at present; the type is in
library.

*** Add support for boost and/or std tuple                            :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of tuples in dogen so
that I don't have to manually generate code for types that use it.
#+end_quote

It would be nice to be able to use =std::tuple= and/or =boost::tuple=
from dogen. The processing would be rather similar to containers. It
would be even nicer if one could associate an enumeration to a tuple
so that the gets would be more meaningful, e.g.:

: std::get<my_field>()

rather than

: std::get<0>()

Using =std::tuple= would mean we'd have to create our own serialisers
for it most likely.

*** Add support for posix_time_zone                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of boost posix_time_zone
so that I don't have to manually generate code for types that use it.
#+end_quote

At present we need to use std::string to convey time zone
information. We should be able to use the time zones available in
boost date time library.

See boost documentation: [[http://www.boost.org/doc/libs/1_53_0/doc/html/date_time/local_time.html#date_time.local_time.posix_time_zone][Posix Time Zone]]

*** Add support for structs                                           :story:

Even in C++ it is useful sometimes to define a type as a plain struct,
with no getters and setters and all members public. We could still
generate IO etc as per usual (well, almost as the API would be
different).

For this we would need a different stereotype. We need to do more
research on POCOs/PODs/POJOs.

Links:

- [[https://en.wikipedia.org/wiki/Plain_old_Java_object][POJO]]
- [[https://en.wikipedia.org/wiki/Plain_old_CLR_object][POCO]]
- [[https://en.wikipedia.org/wiki/Passive_data_structure][Passive data structure]]
- [[https://en.wikipedia.org/wiki/Data_transfer_object][Data transfer object]]

*** Code generate C# models using msbuild                             :story:

At present we did a quick hack to code generate in C#: a simple bash
script that runs dogen. However, this is not how we expect the end
user to consume it; there should be a msbuild target that:

- detects the code generator;
- contains the configuration (e.g. options, location of models);'
- runs the code generator - possibly every time models change;
- has a tailor target to generate JSON.

*** Add support for package installation scripts                      :story:

Once it is possible to declare dependencies and map them to different
packaging systems such as vcpkg, the next logical step is to generate
batch files for each platform to build the product dependencies. These
should be standardised.

Notes:

- generate the script with a well defined name.
- create the entire vcpkg workflow (download, build, export, upload to
  dropbox).
- create cmakefile / script to download export from dropbox.

*** Consider adding support for iguana                                :story:

Iguana is yet another serialisation framework.

Links:

- https://github.com/qicosmos/iguana

*** Consider adding support for ormpp                                 :story:

Yet another ORM framework. It would be nice to target ODB and ormpp
using exactly the same metamodel constructs.

Links:

- https://github.com/qicosmos/ormpp

*** Consider adding support for Apache Arrow IPC                      :story:

Yet another serialisation format.

Links:
- https://github.com/apache/arrow/tree/master/cpp/src/arrow/ipc

*** Consider adding C#/C++ attributes                                 :story:

It may be a good idea to have attributes for generated and hand
crafted code, e.g.:

: [[masd::generated]]
: [[masd::handcrafted]]

This would allow us to build some clang tool that determines if all
generated code is used from handcrafted code or not. Presumably
something similar can be done for C# and Java.

We need to compile with some kind of ignore flag
(=-Wattributes=). Once these attributes are in, we could do queries in
LSP to determine what types are used. For example, we could
find-references for a given type and then build some kind of DAG and
determine if any of that code is accessed by non-generated code.

Links:

- [[https://en.cppreference.com/w/cpp/language/attributes][Attribute specifier sequence]]
- [[https://www.bfilipek.com/2017/07/cpp17-in-details-attributes.html][C++17 in details: Attributes]]

*** Consider adding a swagger frontend                                :story:

Once we have HTTP API support, we can consider having a frontend for
swagger. The idea is that the user submits one or more swagger JSON
files (one per supported version of the API, e.g. =v1=, =v2= etc)
possibly with a dogen model in JSON or dia. Dogen then converts the
swagger specification into elements. We can then code-generate:

- all the data objects;
- all the routing code;
- an interface that the user can implement;
- bridge into the HTTP API support for the Rest SDK/Beast code that
  binds the HTTP processing with the routing;

The generated project should be a binary with the static library
generation enabled so we can test the web code. However, users should
be allowed to choose if they want a static or shared library
instead. This is a bit tricky if there is no dogen model because we
have no way to supply meta-data.

We need to ensure we do not hard-code the OpenAPI implementation
against a specific HTTP library such as Boost.Beast or Casablanca. It
should be possible to plug it in with different libraries (even if at
generation time only, since users don't really need more than one
library at the same time).

In addition, in keeping with the move towards meta-elements to
represent all concepts, we should either implement the OpenAPI
concepts as meta-elements or as meta-data parameters. However, in this
case we probably want to keep the input in YAML (parsing it using
something like [[https://github.com/jbeder/yaml-cpp][yaml-cpp]]) but then instantiate meta-model elements that
model OpenAPI. These are then used by the templates to generate code
(after possibly some enrichment). Interestingly, with OpenAPI we have
all that is required in order to create a service definition. The only
slight problem is that we cannot bind back to the underlying library,
even given a interface definition, because there is such a big
mismatch between the swagger API and a regular programming language
interface. From this perspective, it perhaps makes more sense to have
swagger metamodel elements instead of relying on YAML because we could
automatically generate them from the interface declaration (what is
hard is the opposite).

It appears there already exists a framework to add swagger support to
C++: [[https://oatpp.io/][Oat++]]. The good news is the support looks pretty
comprehensive. The bad news is they did not build it on top of ASIO
and Best/Boost, but instead coded everything from scratch including
coroutines etc. It also means we cannot use existing Dogen JSON
support - they have their own DTO. This may not be an ideal end-state
but it seems it is really easy to get something up and running
quickly. We could just create the Swagger/OpenAPI yaml parser and then
generate Oat++ code first. The parser would be useful for other
backends. The templates themselves are probably easy to do (for
Oat). The integration with the dogen interface should be very similar,
regardless of the swagger backend (dispatch of the data coming in from
HTTP and out of the service). The only difference is that we'd be
relying on their DTO objects so we'd probably need two representations
for each object - one as a proper domain model, and a second one as a
oat DTO. We could conceivably code-generate conversion wrappers, given
that the objects are identical from a meta-model perspective. Or we
could talk to the Oat people and figure out if its possible to
integrate other DTOs with Oat. It is also not clear how JWT would be
handled - is it Oat's responsibility or the lower layers.

Finally, it may also be worthwhile trying to "port" oat++ into
Beast. That is, try to figure out how their [[https://github.com/oatpp/oatpp-swagger][swagger infrastructure]]
would look like under Beast and copy across as required. They have
already done all of the artwork, HTML etc so much can be reused. The
good thing about the Dogen approach is that we do not need any
run-time support (e.g. reflection, etc). All meta-data can be
generated at compile time from the meta-model. In addition, we can
place objects in the stack as much as possible and pre-generate JSON
and reuse it.

It would also be interesting to generate a Wt based swagger-like UI to
test the API (as well as org-mode). Ideally this would be generic and
it would use any OpenAPI JSON and render it into a Wt website. The
advantage over using the traditional swagger codegen is that it would
not require javascript and so could be accessed by regular browsers
like eww.

Links:

- https://github.com/oatpp/oatpp
- [[http://docs.servicestack.net/swagger-api][ServiceStack Swagger integration]]
- [[http://radar.oreilly.com/2015/09/building-apis-with-swagger.html][Building APIs with Swagger]]
- [[http://petstore.swagger.io/v2/swagger.json][Swagger JSON example]]
- [[https://github.com/OAI/OpenAPI-Specification][OpenAPI]]: Swagger has been renamed to OpenAPI. See also [[https://swagger.io/specification/][this page]].
- [[https://github.com/eidheim/Simple-Web-Server][Simple-Web-Server]]: alternative to beast.
- [[https://github.com/Stiffstream/restinio][RESTinio]]: another HTTP/websockets framework based on asio.
- [[https://github.com/seemk/WebUDP][WebUDP GH]]: "WebRTC datachannel library and server"
- [[https://github.com/swagger-api/swagger-codegen][Swagger codegen]]: tool for code generation of swagger APIs. See also
  [[https://swagger.io/tools/swagger-codegen/][this link]].
- [[https://github.com/OAI/OpenAPI-Specification/tree/master/examples/v3.0][OpenAPI examples]]: YAML documents with examples.
- [[https://github.com/matt-42/silicon][silicon]]: yet another C++ HTTP framework.
- [[https://github.com/qicosmos/cinatra][cinatra]]: another HTTP framework. Example site: http://purecpp.org/
- OpenApi code generator examples: [[https://github.com/OpenAPITools/openapi-generator/tree/master/samples/server/petstore/cpp-pistache][petstore/cpp-pistache]], [[https://github.com/OpenAPITools/openapi-generator/tree/master/samples/server/petstore/cpp-restbed][cpp-restbed]]
- [[https://github.com/jbeder/yaml-cpp][yaml-cpp GH]]: "yaml-cpp is a YAML parser and emitter in C++ matching
  the YAML 1.2 spec."
- [[https://github.com/OpenAPITools/openapi-generator][GH openapi-generator]]: "OpenAPI Generator allows generation of API
  client libraries (SDK generation), server stubs, documentation and
  configuration automatically given an OpenAPI Spec (both 2.0 and 3.0
  are supported)."

*** Factory method instead of complete constructor                    :story:

Perhaps it is useful to give users the option to create a static
factory method on a class instead of (in addition to?) a complete
constructor. This would be specially useful for classes that are meant
to be used as pointers. We need metadata for the given variation
points (return pointer, return object, etc). We could also then demote
the complete constructor to private so no one else can make use of
it. This would be a metadata parameter of the complete constructor.

Users can then package up this feature configuration into a sensible
"profile" such as =FactoryMethodConstruction=. It would be great if
the users could set the package to "transitive" or "non-transitive",
causing all associated objects to also have it or not.

*** Null assertion on pointers                                        :story:

For languages such as C# and Java it would be nice if setters could
check if the pointer is null. This is also useful for C++. This could
be a meta-data parameter, either global or local.

Users may want to provide their own assert function which throws the
adequate exception for their use case.

Actually these should be meta-data parameters for when using a pointer
type:

- check for null
- exception to throw

This also means we should get the constructors to call the setters
instead of setting variables directly. We can also do this for
immutable types, but make the setters private.

*** Add support for primitive validation                              :story:

When defining a primitive, it may be useful to define associated
validation. For example:

- integers in a certain range (e.g. < 1000, etc).
- strings that follow a regex

It would be nice if these validation expressions could be supplied as
meta data with the primitive in the model and then validation code
would be generated for them. We could then trigger the validation code
in the constructor.

Merged stores:

*Add additional primitive validation*

The classes that we are thinking about adding for test data are useful
in other ways such as for validation. We could have out of the box
validators for each class (where it makes sense, e.g. IP address,
etc). This is mostly useful when converting from an "untyped type"
like std::string to a stronger type such as IP Address.

*** Add test data "types" for generated data                          :story:

At present we are creating really "dumb" test data, just a prefix for
strings and a simple counter for numbers. A much better approach is
the one used by mockaroo. They have the idea of "classes", that is a
"type" for the field which is in addition to the programming type. For
example:

- ip address
- first name
- last name
- email address

and so forth (more examples below). We could easily create a set of
classes such as these ones, including constraints (e.g. maximum
characters, etc) and then encode them into dogen. These are just
configuration parameters for the test data facet. For each type we
could have an authoritative source which is just a text file. Users
can then submit new sources. Or perhaps even better: we could create a
separate open source project that is responsible for maintaining the
test data bundles. Any user can submit and make use of these
bundles. Periodically dogen snapshots the bundles and includes them
with its releases. This way we benefit from anyone who wants to help
out. The bundles should be split by language so that we can have
translations. We should use the exact same approach as regular
[[https://developer.wordpress.org/themes/functionality/localization/][translation files]].

Once we have the types, the next logical thing is to create mock
endpoints. This can be done very easily because of remoting. In
reality, we just implement the remote end point as a "fountain" using
the test data generator to produce the data. We know all of the
endpoint details due to remoting so this should be fairly trivial.

Another option is to have "local fountains". Users can supply a path
to a fountain file with data and some kind of "process" to describe
how to read the data (e.g. linear, uniform random, etc). Users can
associate the path with a type (possibly via primitives, or directly
on an attribute). Dogen creates static data structures with the data
and accesses them at run time. This exact same approach can also be
applied to the bundles described above.

We should also have an option to mark each entry as "valid" or
"invalid" so that test data sets can also include invalid data. There
should be a comment as to the purpose of the test (e.g. invalid
because). These can then be used to generate unit or system tests. For
example, for usernames we should take into account the comments in
[[https://www.b-list.org/weblog/2018/feb/11/usernames/][Let’s talk about usernames]] and look at [[https://github.com/ubernostrum/django-registration/blob/1d7d0f01a24b916977016c1d66823a5e4a33f2a0/registration/validators.py#L25][django validators]]. We can then
perhaps create generators that are either "valid" or "invalid".

We could even have a "combined" generator that generates a percentage
of "valids" and "invalids" as supplied by the user.

In effect, instead of one generator, we would have many:

- trivial generator: prefix for strings, with postfix; numbers in a
  range, etc. Random strings with configurable sizes (readable and
  unreadable characters).
- valid realistic generator: valid entries from a domain.
- invalid realistic generator: invalid entries from a domain
- realistic generator: valid and invalid entries, according to
  percentages.

Each generator group is associated with a class.

Actually maybe we can have multiple bundles for a given item. For
example, take company names: we may want fortune 500 or just small
companies (certain patterns with names may only appear in say small
businesses). So company names is the class, with a default bundle, but
there may be multiple bundles.

*More ideas*

Fountains are code generated from CSV files, themselves created from
massaging external project files. The fountains could have a structure
like:

- =masd/core/fountain/phone_number_fountain.hpp=

Each fountain could have additional meta-data configuration,
e.g. country, partial phone numbers, etc. These are supplied by users
as meta-data when they make use of a fountain. For example, consider
primitive =phone_number= with underlying type string associated with
fountain =phone_number_fountain=. It would have meta-data setting the
country, etc. Or maybe we should just allow specialised fountains
(full phone number, partial phone number, invalid phone number,
etc). This means we could even create a composite fountain that takes
in a set of fountains and a random number distribution and then reads
from each fountain using the distribution (e.g. uniform, etc). We
could also handle countries this way (e.g. use iso codes for phone
numbers, =en_phone_numbers= etc.).

*More Ideas*

Actually a better take on this would be to create an actual
product. We could have one repository which is made up of simple test
data files, with some form of organisation. Then we could add support
to dogen to generate data generators based on those files. These could
be for any language that dogen supports. Then we'd create a product
for each language that export these generators, with a PDM mapping
that exports these so that models could consume them. This means
anyone could use the product outside of dogen and dogen binds to
it. It could live in the MASD umbrella or perhaps have its own
organisation (since we will have many languages it may just pollute
MASD). Interestingly, there is some kind of weird correlation between
"regular" PDMs and the generators. For example:

- the boost PDM exports a type such as date time.
- the generator could contain dates, say birthdays, holiday calendars,
  etc. These then can be expressed in a number of types, depending on
  the library you are using. Therefore in an ideal world, the boost
  PDM library should have a dependency (as far as test data generation
  goes) on this new product. We could use a simple interface for this
  (for example all dates are exported in some type supported by the C
  library). Then the PDM uses a wrapper on top of the original
  exporter to generate data in the boost format:

  C time -> boost time

  Of course, users may then want dates as strings, etc. The key thing
  is to allow for arbitrary composition of these data streams (a-la Rx
  event streams, in a way).

*Fuzzing*

Interestingly, the same principles could be used for fuzzing. We could
somehow have a fuzz fountain for each type which generates values from
a corpus. These could be sensitive to the type.

Links:

- https://mockaroo.com/
- [[https://github.com/Devskiller/jfairy/tree/master/src/main/resources][jfairy]]: seems like there is an existing open source project for
  this. See also [[https://medium.freecodecamp.org/how-our-test-data-generator-makes-fake-data-look-real-ace01c5bde4a][this post]].
- [[https://ieeexplore.ieee.org/abstract/document/8004406/][Automated Generator for Complex and Realistic Test Data]]: paper on
  the subject. Tool they created: [[https://github.com/mrfranta/jop-discontinued][jop]]
- [[https://gitlab.com/daamien/postgresql_anonymizer][Postgresql Anonymizer]]: similar idea but at the database level. We
  need to mine their approach for ideas.
- [[https://github.com/minimaxir/big-list-of-naughty-strings][Big List of Naughty Strings]]: The Big List of Naughty Strings is an
  evolving list of strings which have a high probability of causing
  issues when used as user-input data.
- [[https://github.com/dariusk/corpora][corpora]]: A collection of small corpuses of interesting data for the
  creation of bots and similar stuff.
- [[https://github.com/joke2k/faker][faker]]: Faker is a Python package that generates fake data for you. See also
  PHP [[https://github.com/fzaninotto/Faker][Faker]], Kotlin [[https://jworks.io/datafaker-1-1-0-released/][datafaker]].
- [[https://github.com/schoentoon/cxxfaker][cxxfaker]]: basic C++ faker.
- [[https://github.com/vrok/randodo][randodo]]: random strings generator
- [[https://github.com/emirozer/fake2db][fake2db]]: Generate fake but valid data filled databases for test
  purposes using most popular patterns.
- [[https://devarea.com/python-understanding-generators/#.XKX5tXVKiV4][Python – understanding generators]]
- [[https://picsum.photos/][Lorem picsum]]: Lorem Ipsum but for photos. Could be used for pictures
  for profiles, etc. Also: [[http://lorempixel.com/][lorempixel]]. For splash screens:
  [[https://source.unsplash.com/][unsplash]]. [[https://placeimg.com/][placeimg]] for images of places.
- [[https://llvm.org/docs/LibFuzzer.html][LibFuzzer]]
- [[https://blog.trailofbits.com/2019/05/31/fuzzing-unit-tests-with-deepstate-and-eclipser/][Fuzzing Unit Tests with DeepState and Eclipser]]
- [[https://generated.photos][Generated Photos]]: AI generated realistic pictures of people. We
  could download a fixed set or use the API.
- [[https://github.com/minimaxir/big-list-of-naughty-strings][GH big-list-of-naughty-strings]]: The Big List of Naughty Strings is a
  list of strings which have a high probability of causing issues when
  used as user-input data.
- [[https://github.com/eliabieri/blnscpp][GH blnscpp]]: Single header C++ API for the Big List of Naughty
  Strings.
- [[https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words][GH List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words]]: List of
  Dirty, Naughty, Obscene, and Otherwise Bad Words
- [[https://voice.mozilla.org/en/about][Mozilla Common Voice]]: Common Voice is part of Mozilla's initiative
  to help teach machines how real people speak. In addition to the
  Common Voice dataset, we’re also building an open source speech
  recognition engine called Deep Speech.
- [[https://gitlab.com/dalibo/postgresql_anonymizer][PostgreSQL Anonymizer]]: postgresql_anonymizer is an extension to mask or
  replace personally identifiable information (PII) or commercially sensitive
  data from a PostgreSQL database.

Fantasy:

- [[https://www.mithrilandmages.com/utilities/StreetNames.php][Street and Road Name Generator]]
- [[https://www.mithrilandmages.com/utilities/ModernBrowse.php][List of Modern Names]]
- [[https://www.mithrilandmages.com/utilities/BankNames.php][Bank and Financial Services Name Generator]]
- [[https://www.mithrilandmages.com/utilities/CityNames.php][City & Town Name Generator]]
- [[https://www.namegenerator2.com/country-name-generator.php#generator-bookmark][Country Name Generator]]

*Realistic Model Generation*

Another topic of interest is the generation of dogen models. If we
could somehow capture some basic notions on the fountains such as:

- class names
- attributes related to a class
- relationships

We could generate realistic models. The models don't have to be
realistic in the sense of "making sense" for a given domain, they just
need to have similar characteristics to real models such as:

- number of classes
- number of attributes per class
- diversity of those attributes

**** Example Types

- Animal Common Name
- Animal Scientific Name
- App Bundle ID. Three part app bundle id (com.google.powerflex, com.microsoft.prodder)
- App Name
- Avatar. Random avatar image url.
- Base64 Image URL. data:image/png;base64,iVBORwoA...
- Binomial Distribution.
- Bitcoin Address
- Blank. Always generates a null value
- Boolean
- Buzzword
- Car Make
- Car Model
- Car Model Year
- Car VIN. A random car VIN number, not correlated to other car fields.
- Catch Phrase
- Character Sequence
- City
- Color
- Company Name
- Country
- Country Code
- Credit Card number
- Credit Card Type. visa, mastercard, americanexpress
- Currency. Dollar
- Currency Code (USD, EUR, MXN)
- Custom List. Picks items randomly or sequentially from a custom list of values
- Dataset Column. Pick records randomly or sequentially from one of my datasets
- Date. 07/04/2013
- Department (Corporate). Human Resources, Accounting, Engineering
- Department (Retail). Grocery, Books, Health & Beauty
- Digit Sequence. Create simple sequences of characters, digits, and symbols
- Domain Name. google.com
- Drug Company. Eli Lilly and Company, Novartis Pharmaceuticals Corporation
- Drug Name (Brand). Cialis, Nexium, Lipitor
- Drug Name (Generic). Naproxen Sodium, Selenium Sulfide, Acetaminophen
- Dummy Image URL. Image url from dummyimage.com. http://dummyimage.com/250x100
- DUNS Number. Randomly generated DUNS numbers
- EIN. Randomly generated employer identification numbers
- Email Address
- Encrypt. Simulates encrypted text
- Exponential Distribution. Generates numbers based on an exponential distribution with a specific λ rate.
- Fake Company Name. Morar Group,  Stark-Glover, Sawayn and Sons
- Family Name (Chinese)
- FDA NDC Code
- File Name. lobortis.pptx, erat_volutpat.csv, TortorSollicitudin.docx
- First Name. Jim, Mark, Sasha
- First Name (European). Choose language? Görel, Marie-josée, Hélène
- First Name (Female). Susan, Jessica, Sasha
- First Name (Male). Mark, Bob, Tim
- Formula. Compute a value based on data from other columns
- Frequency. Never, Once, Seldom, Often, Daily, Weekly, Monthly, Yearly
- Full Name. Nancy Smith, Tim Fisher, Al Jones
- Gender. Male, Female. Missing new genders. Should have "classical"
  and "modern" gender.
- Gender (abbrev). M, F
- Geometric Distribution. Generates numbers based on a geometric distribution with a specific probability of success.
- Given Name (Chinese)
- GUID. 36 character hex guid
- Hex Color. #142a0b
- IBAN. FR73 5960 2948 07N1 L9TC PVYX E17, SE85 4302 3680 7231 4238 1624
- ICD10 Diagnosis Code. ICD10 diagnosis code. Source: cms.gov.
- IP Address v4. 121.150.202.132
- IP Address v4 CIDR. 188.245.97.43/27
- IP Address v6. 770:44c0:1c4:9996:2fd:6907:3045:9627
- IP Address v6 CIDR. 9ea4:2b0b:11ba:47a3:47a8:ede4:2ddd:c5f8/115
- ISBN. 574398570-7
- Job Title. Design Engineer, General Manager, Help Desk Technician
- JSON Array. Generates an array of objects in json format.
- Language. German, English, Spanish
- Last Name. Smith, Jones, Miller
- Latitude. 48.52469361225269, 72.26886762838888, -12.592370752117404
- LinkedIn Skill. Algorithms, Sports Nutrition, Payroll
- Longitude. -45.15259533671917, 115.70563293321999, 81.9426325226724
- MAC Address. 2C-D6-9B-77-E5-0B, 2C:D6:9B:77:E5:0B, 2c:d6:9b:77:e5:0b
- MD5. Random hex encoded MD5 hash
- MIME Type. text/plain, image/png, application/pdf
- Money. $3.00, £12.94, €127,54
- MongoDB ObjectID. Globally unique identifiers for MongoDB objects
- Movie Genres. Action | Suspense, Thriller, Comedy
- Movie Title. Goodfellas, Titanic, Silverado
- Naughty String. Strings which have a high probability of causing issues when used as user-input data.
- NHS Number. 10-digit NHS number with mod11 checksum
- Normal Distribution. Generates random numbers in a normal distribution using the Box-Muller algorithm
- Number. 0.25, 5.2, 1000
- Paragraphs. Chosen randomly from lorem ipsum
- Password. A random string of 6-12 characters. bnTZ28AFZ,
  u1vn6SIM5KBA
- Phone. 8-(598)633-6672
- Plant Common Name. Abietinella Moss, Silver Fir, Sedge
- Plant Family. Thuidiaceae, Pinaceae, Cyperaceae
- Plant Scientific Name. Abietinella abietina, Abies alba
- Poisson Distribution. Generates numbers based on a Poisson distribution with a specific mean value.
- Postal Code. Region-specific postal codes (not available for all locations).
- Product (Grocery). Tomato - Green, Spinach - Baby, Avocado
- Race. Filipino, Venezuelan, Asian
- Regular Expression. Generate random data based on a regular expression
- Repeating Element. Repeats an XML element a specified number of times
- Row Number. 1, 2, 3
- Scenario. Generates a number using a saved scenario
- Sentences. Chosen randomly from lorem ipsum
- Sequence. Generates a sequence of numbers with adjustable step and repeat options.
- SHA1. Random hex encoded SHA1 hash
- SHA256. Random hex encoded SHA256 hash
- Shirt Size. S, M, L
- Short Hex Color. #14b, #a32, #926
- Slogan. Randomly generated marketing slogans
- SSN. 678-59-9455, 312-20-4597, 684-62-5799
- State. State/Province names, US and worldwide
- State (abbrev). Two character state/province abbreviations, US and worldwide
- Stock Industry. Semiconductors, Major Banks, Oil & Gas Production
- Stock Market. NYSE, NASDAQ,
- Stock Market Cap. $33.03B, $54.29M, $41.02M
- Stock Name. Microsoft Corporation, NetApp, Inc., The Bancorp, Inc.
- Stock Sector. Technology, Capital Goods, Finance
- Stock Symbol. MSFT, NTAP, TBBK
- Street Address. Street number, name, and suffix. 6449 Pine View Drive
- Street Name. Street name and suffix. Pine View Drive
- Street Number. A street number between 1 and 5 digits, 6449
- Street Suffix. Drive, Terrace, Street
- Suffix. Jr, Sr, III
- Template. Concatenate values from several columns into one
- Time. 3:30 PM, 15:30
0 Time Zone. America/Los_Angeles, Europe/Budapest, Pacific/Fiji
- Title. Mr, Ms, Dr
- Top Level Domain. com, edu, org
- University. The Johns Hopkins University, Pepperdine University, University of Texas
- URL. https://facebook.com, http://google.com/path?foo=bar, /foo/bar
- User Agent. A user agent string from a popular web browser or bot.
- Username. jdoe, twilliams, jfang
- Words. Chosen randomly from lorem ipsum

*** Add support for RPC                                               :story:

It would be nice to be able to mark a type as a service and then have
dogen generate all the comms for it:

- generate a service interface with appropriate async such as using
  ==boost::future= or =std::future= depending on user options. User
  then has to implement this interface.
- generate a new model called comms. For each method =M= in the
  service, create two new classes named =M_request= and
  =M_response=. These classes are composed of the types from the
  method. Also, they will have additional fields such as security
  token, status, error message, etc.
- in comms we need to create a server. This is made up of several
  layers: 1) socket setup: create a socket, open it, etc. 2)
  serialiser: convert from/to raw serialisation format. 3) dispatcher:
  given a message, call the appropriate method on the service and
  construct the appropriate response. The server implementation owns a
  service, receives messages, unpacks them and calls the service; then
  takes the result packs it and returns the response.
- similarly, we also need a client with the responsibility of calling
  the server over a socket. We need to be able to support both "raw"
  sockets as well as HTTP.
- in an ideal world, we would also generate console client and server
  binaries. These would automatically have the command line options
  setup for them given the service request/response. They would make
  use of comms.
- validation of the security token must be done by the generated code
  somehow, as must session ID management and message IDs. Generated
  code must do all the logging of received messages (requests and
  responses) with their correlation IDs. Composition of services must
  be done by routing (e.g. assume [[https://github.com/envoyproxy/envoy][envoy]] is used).
- name of the comms model must be "original model" + =.comms=. Similar
  approach for all dependent models.
- since dogen has been hard-wired to generate one model, we need some
  kind of command line parameter that we can supply to generate
  RPC. Or alternatively, if there is RPC for a given model, we just
  automatically generate all dependent models. Users can
  enable/disable client/server etc via meta-data.
- stereotype: =dogen::rpc::service=. We then manually create objects
  with the dependent stereotypes, e.g. =dogen::rpm::message=,
  etc. Note that creating these objects manually or automatically
  should result in exactly the same behaviour, e.g. first implement
  them manually. The automatically generated models still follow the
  same approach for facets such as =types=, etc. They just have more
  types such as dispatcher, serialisers, etc. These can be injected
  into a model manually (e.g. =dogen::rpc::dispatcher=, etc).
- because the service is async (or can be), the server needs to chain
  to a future for the processing. Similarly, the client provides an
  async interface, with a coroutine. We must use the exact same
  approach for HTTP and raw socket (ideally even the same class).
- for HTTP, users may need to set the method (e.g. PUT, POST,
  etc). This could be done as meta-data. But however, we should have
  an RPC stereotype that maps to these, so that we can use the same
  information for both "raw" sockets and HTTP. Dia supports
  stereotypes in methods, so we could have something like
  =dogen::rpc::put= as a stereotype.
- we have a story for type framing. We should implement this feature
  first and then make use of it. Basically, the entire RPC
  infrastructure is made up of discrete dogen features. These can be
  used stand-alone.
- we should automatically add the async and sync infrastructure. The
  user can choose one or both, and we generate the
  interfaces. However, user needs to manually implement the service
  methods.
- by having a single library for comms it means we will ship both
  client and server on it (as well as raw and HTTP). This is not ideal
  since it means clients will link to server code and also means we
  will pull in the domain library. However, in practice its probably
  OK because in most cases we will need to make use of the domain
  library anyway unless the service is trivial. The alternative is to
  have a ==comms.server= and a =comms.client= project. But since they
  will share a lot of code, this sounds like overkill.
- we need a way to set the available serialisation formats. This can
  be done via meta-data. The service could also have meta-data for RPC
  or non-RPC.
- in order for this to work with the current transform chains, we need
  some way to supply the target and obtain the RPC models; then we
  start the existing code generation chain. However, the chains at
  present are not design to receive a model as an input. We need to
  wire them differently in order to cope with this use case. In
  addition, we may not need all of the post-processing we have in the
  text model chain at present (e.g. visitor, etc) given that we know
  exactly what the generated types will be. Perhaps we can have chains
  designed specifically for this use case?
- we need to copy a lot of the model properties for the generated
  models such as licence, copyright, etc, but not model name. In
  theory we should be able to just override that one and copy all the
  other ones. We also need to inject references.
- If the user has enabled both C++ and C#, this should still work
  (assuming we have RPC support for C#).
- we need to add support for executables, operations, program options,
  main, etc etc before we can satisfy the client/server use cases. We
  need to break down this epic into all of its dependent stories.
- ServiceStack like API interfaces should be generated automatically
  if HTTP is enabled. See ServiceStack/swagger story for
  details. Service comments are used for swagger comments.
- we need to manually inject command line options such as host name,
  port, log level etc. Remaining options are taken from the operations
  in service. Comments provided by users on the operations can be used
  for the command line description.
- it would be great if the client could have a =--interactive= option
  in which case it would then use readline and allow users to send
  commands. For simple types this is trivial (perhaps quotes for
  strings) but for non-simple types it would require a bit of
  thinking.
- before we start defining stereotypes, we need to read: "Towards a
  UML Profile for Service-Oriented Architectures" by Reiko Heckel,
  Marc Lohmann and Sebastian Thöne.

Merged stories:

*Remote method invocation*

Look for type framing, Model and type enums stories.

It seems fairly straightforward to add remote method invocation to a
few select types. The following would have to be done:

- create a new stereotype like =dispatchable=, =remotable= or suchlike
- for languages which support this natively, we could map to their
  technology (e.g. Beans in Java, etc).
- create a new stereotype: interface.
- add support for interface code generation.
- validation: model must have a model ID, thought to be unique across
  models.
- validation: types must be marked as both =remotable= and
  =interface= and have a unique type ID in the model.
- validation: types must have at least one public method
- injector: if at least one type is =remotable=, a new facet is
  created: =rmi=.
- injector: a system enumeration will be created with all the
  supported serialisation types. actually, we should create this
  anyway in serialisation or reflection.
- rmi will contain one class that represents a "frame". this
  frame will be composed as follows: model ID, type ID, serialisation
  type, raw buffer. we need to look at RMI terminology to come up with
  a good name for this frame.
- messages: for each method that exists in each dispatchable service,
  a message class will be created with a name following some well
  defined convention such as =CLASS_NAME_METHOD_NAME=. we need
  examples to make up a sensible convention. or perhaps an
  implementation specific parameter can override the class name. the
  message class is a data object and has as attributes all of the
  parameters of the method.
- a dispatcher class will be created in dispatching. it will have as
  constructor arguments references to all the dispatchable
  services. when passed in a frame, it will hydrate it and dispatch it
  to the correct service.
- a "framer" class will be created in dispatching. it will be
  configured for a given serialisation type. it will take a message
  object, serialise it and frame it.
- we could support the notion of callbacks. for this we need to be
  able to serialise stubs as references such that when the other end
  receives it, it calls a registrar to activate a client stub.

Now we just need a way of creating some generic interfaces that take a
wire client and a wire service and plug the framer and the dispatcher
into it.

Notes:

- we need basic support for operations so that we can convert the
  operations into messages.
- we need some wire format support such as flat buffers.

*** Add support for dependency graphs and complexity data             :story:

It would be nice to incorporate some of the complexity measures found
in Lako's Large-Scale C++ Software Design. We could copy across some
of the code of TomTom's [[https://github.com/tomtom-international/cpp-dependencies][cpp-dependencies]] and make use of it against
the dogen model.

Links:

- [[http://gamesfromwithin.com/physical-structure-and-c-part-1-a-first-look][Physical Structure and C++ – Part 1: A First Look]]
- [[http://gamesfromwithin.com/physical-structure-and-c-part-2-build-times][Physical Structure and C++ – Part 2: Build Times]]
- [[https://gitlab.com/jas/pmccabe][pmccabe GL]]: "McCabe-style complexity and line counting for C and
  C++"
- [[https://github.com/metrixplusplus/metrixplusplus][GH metrixplusplus]]: Metrix++ is an extendable tool for code metrics
  collection and analysis.

*** Add support for configuration as environment variables            :story:

One of the principles in the [[https://www.12factor.net/][12 factor app]] is using environment
variables for all [[https://www.12factor.net/config][config]]. This can then be [[https://www.nginx.com/blog/microservices-reference-architecture-nginx-twelve-factor-app/][managed externally]]. Dogen
can help with the process by allowing users to define configurations
as meta-model elements. These can then be obtained from factories,
using =std::getenv=. They can have all the right types (e.g. perform
casting as required) and can even have built-in validation. For this
we can rely on the taxonomy made for test data (this will be on a
separate story). In addition, this will make mocking easier: we can
either create an interface for the factory with a mock implementation,
or the factory could receive a map on one of its constructors.

We should also take into account the story on config, as well as the
support for env variables built in to [[https://www.boost.org/doc/libs/1_68_0/doc/html/program_options/overview.html#id-1.3.31.5.10.3][program options]]. The good thing
about using program options is that we'd also have support for command
line arguments as well as env variables. We'd also need a way to
describe the env variable name separately from the command line option
name, e.g. =hostname= vs =MY_APPLICATION_HOSTNAME=.

Configuration should be read by a singleton, which is initialised at
program start up. Initialisation validates config. All other calls
return the cached configuration.

Links:

- [[https://en.cppreference.com/w/cpp/utility/program/getenv][std::getenv]]
- [[https://stackoverflow.com/questions/44819569/how-to-extract-environment-variable-with-boostprogram-options][How to extract environment variable with boost::program_options?]]

Merged stories:

*Create a base options class across all tools*

At present we are copying and pasting a bit of code related to general
options across all the command line tools (knitter, darter, stitcher,
tailor). We could create a base class that has the common options and
then have a factory that populates the boost program options
associated with that class.

Ideally we should also have a log initialisation class that uses those
common options.

*** Consider adding support for SBE                                   :story:

There is an efficient serialisation protocol: Simple Binary
Encoding. It supports multiple languages. We could add a facet for it.

Links:

- https://github.com/real-logic/simple-binary-encoding

*** Consider adding support for strong enum libraries                 :story:

There are a number of different libraries that provide support for
strong enums in C++. This story keeps track of the ones we bumped
into. These are useful for two reasons:

- add support to Dogen for these libraries as platforms.
- look at the features of the libraries and see if we can replace them
  with code-generated code.

We could write a short paper surveying the libraries.

Links:

- [[https://github.com/Domiran/enum_values][enum_values GH]]: "Yet another post-build step and class to bring
  reflection to C++ enumerations!"

Merged stories:

*Consider adding support for magic enum library*

Yet another enum library: https://github.com/Neargye/magic_enum. C++
17 only.

*Consider adding support for the =enum.hpp= library*

See the github repo.

Links:

- https://github.com/BlackMATov/enum.hpp

*Consider adding support for Meta Enum library*

Meta Enum provides a number of features for enumerations. We could
simply declare enums using their library. A second useful thing is to
copy all of their features and offer them as plain code generation
features.

Links:

- https://github.com/therocode/meta_enum

*Consider adding support for Better Enums library*

Interesting library that wraps enumerations.

Links:

- https://github.com/aantron/better-enums

*Consider adding support for FWK enum library*

Yet another enum library.

Links:

- https://github.com/nadult/fwk_enum

*** Consider adding support for bitsery serialisation                 :story:

There is yet another modern c++ serialisation library: bitsery. It
seems pretty trivial to add support for it as a facet.

Links:

- https://github.com/fraillt/bitsery/

*** Consider adding support for the spaceship operator                :story:

It seems C++ 20 will introduce a new operator: =<=>=. We should be
able to support it within Dogen. However, it will require knowing the
C++ version to ensure we do not generate it when using C++ 17 or
below.

Links:

- [[https://blog.tartanllama.xyz/spaceship-operator/][Spaceship Operator]]

*** Consider adding support for OutOfLine                             :story:

OutOfLine seems like an interesting pattern to improve cache
locality. It would be nice if we could make it a meta-pattern so that
we could annotate classes (e.g. =out_of_line=?) and then generate the
code to match the solution below.

Links:

- [[https://blog.headlandstech.com/2018/08/15/outofline-a-memory-locality-pattern-for-high-performance-c/][OutOfLine – A Memory-Locality Pattern for High Performance C++]]

*** Consider adding support for GraphQL                               :story:

GraphQL is a DSL for RESTful APIs. Its largely based on JSON and a
JSON like format for queries:

: getUser(id: "user_123") {
:  currency,
:  email,
:  subscriptions
: }

Users can request specific attributes in objects from an object graph
rather than get massive objects and then pick the attributes of
interest. It raises several questions:

- the meta-data describing all objects must be auto-generated. This
  can easily be done by creating a new facet for GraphQL that contains
  the meta-data for each object of interest.
- the JSON serialisation of objects must only serialise fields of
  interest as per query. The JSON serialiser must have some kind of
  flags at the field level for each object which it queries to
  determine if a field is enabled or not. However, we do not want to
  do this on all cases or else it will slow down all JSON
  serialisation. So we need two different kinds of JSON
  serialisers. In addition, we will not be able to deserialise from
  these JSON objects (given they are incomplete).
- we should also develop a "querying" layer that is independent of
  GraphQL so that we could reuse it for other backends. However, there
  are trade-offs to be made here. We do not want arbitrary complex
  queries (querying by primary key or foreign key is sufficient for
  most cases) whereas GraphQL probably supports very complex queries
  including joins etc.
- complete support for GraphQL may be difficult as the DSL appears to
  be very large. Perhaps we could start by supporting the basic
  functionality once there are use cases.

Actually maybe we should see GraphQL as an injector rather than just
as a run time thing. In other words:

- user creates a schema using GraphQL notation. [[https://github.com/graphql/libgraphqlparser/blob/master/test/schema-kitchen-sink.graphql][Example schema]].
- dogen has an injector metamodel for graphql that implements facebook
  representation in json of graphql. that is we use their library to
  parse graphql schemas and generate json objects. we also create
  entities (meta-model) for their json entities. [[https://github.com/graphql/libgraphqlparser/blob/master/test/schema-kitchen-sink.json][Example schema after
  parse]]. Note that the schema is parsed and interpreted "at compile
  time"; that is, we generate code for the model described by the
  schema.
- at runtime we take instance documents of the schema ([[https://github.com/graphql/libgraphqlparser/blob/master/test/kitchen-sink.graphql][example]]) and
  convert these into JSON ([[https://github.com/graphql/libgraphqlparser/blob/master/test/kitchen-sink.json][example]]). Note that the generated
  meta-model must be able to represent instances of the queries.
- at this point we have a query representation in memory. The system
  then needs to go and execute the query (domain code). Once the
  results come back, we then convert them into the generated model and
  "somehow" supply the query as parameterisation for JSON
  serialisation; that is, the query states which fields we should
  serialise out.

Either way, it seems there are two approaches:

- we can start with a GraphQL schema and treat it as a model
  (injector); or
- we can start with a model and generate its GraphQL schema.

In either case, we then need to have some model that represents
GraphQL queries dynamically, and the ability to only serialise out a
subset of elements in a given object as per the query.

Links:

- https://brandur.org/graphql
- https://github.com/graphql/libgraphqlparser
- https://github.com/microsoft/cppgraphqlgen
- http://graphql.github.io/learn/queries/
- https://github.com/DavidUser/GraphQL-Cpp
- https://github.com/caffeinetv/CaffQL
- https://github.com/chentsulin/awesome-graphql

*** Consider adding support for compile time mocking                  :story:

In a post in HN, a user [[https://news.ycombinator.com/item?id=17504197][suggested the following]]:

: mehrdadn 1 hour ago [-]
:
: I'm just saying do conditional compilation instead of virtual dispatch, since
: you generally shouldn't really need both the test and production
: implementations to run inside the same program. So if you have reason to
: require
:   // widget.h
:   class Widget { virtual void throb(); };
:   // widget.cc
:   class WidgetImpl : public Widget { void throb() { ... } };
:   class WidgetTest : public Widget { void throb() { ... } };
: then, instead of that, just do
:   // widget.h
:   class Widget { void throb(); };
:   // widget.cc
:   void Widget::throb() { ... }
:   // widget.test.cc
:   void Widget::throb() { ... }
: where you only compile widget.cc for the production build, and only compile
: widget.test.cc for the test build.

Dogen could automatically support this by having a "compile time mock"
stereotype. The targets for tests and for the binary would be
automatically generated adding the correct object files. However, this
could easily cause problems if we're not careful. For example, what if
a library A wants to use mocks from library B. Library A should only
include its mocks when it is built in "test mode" but not in
production mode, so this means we need two different builds of the
library. However, if we officialised these builds, e.g.:

- A.mock.so
- A.so

And created targets specifically for these then perhaps there would be
no confusion. The other problem is that when we mock we must mock
everything (as there is only one mock build for the library). Thus we
cannot test selectively. We should also not inline any functions in
the header. For this we could put comments/attributes marking the
class as "compile time mockable" so that developers are aware.

Links:

- [[https://www.fluentcpp.com/2018/08/21/default-parameters-mocking/][Integrating Mocking With C++ Default Parameters]]: article with
  techniques to simulate mocking without a mock framework.

*** Consider integrating a sqitch like approach                       :story:

It would be nice to have a way to manage relational schemas, etc from
within a dogen model. Sqitch seems to have an interesting approach,
which may even suit ODB.

Links:

- https://metacpan.org/pod/sqitchtutorial

*** Add support for entity framework                                  :story:

It seems there is a C# entity framework which is some kind of ORM.

Links:

- [[https://docs.microsoft.com/en-us/ef/core/][Entity Framework Core Quick Overview]]

*** Add dogen specific binary serialisation                           :story:

Given we know the internals of the model types to serialise, there is
nothing to stop us from serialising types directly instead of relying
on third party serialisation APIs.

Links:

- [[https://legacy.gitbook.com/book/arobenko/comms-protocols-cpp/details][Guide to Implementing Communication Protocols in C++ (for Embedded
  Systems)]]

*** Add support for MARISA                                            :story:

This seems like a useful data structure:

- [[https://github.com/s-yata/marisa-trie][marisa-trie]]

*** Consider using Boost.PolyCollection in model                      :story:

We have a collection of elements at present in model. It appears there
is a more efficient way of storing collections of base types by using
Boost.PolyCollection. We just need to expose these types. We also need
to figure out their serialisation support.

Links:

- [[https://www.boost.org/doc/libs/1_67_0/doc/html/poly_collection.html][Boost.PolyCollection]]

*** Add support for units                                             :story:

It would be good to add support for the different units libraries:

- [[https://github.com/nholthaus/units][units]]
- [[http://www.boost.org/doc/libs/1_65_1/doc/html/boost_units.html][boost units]]

*** Add support for boost UUID                                        :story:

It seems boost has a proper UUID type. We should use this instead of
strings.

Links:

- [[http://www.boost.org/doc/libs/1_65_1/libs/uuid/uuid.html][Uuid docs]]

*** Add support for object cloning                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to be able to clone object state so
that I don't have to do this manually.
#+end_quote

We should have a clone method which copy constructs all non-pointer
types, and then creates new objects for pointer types.

Ideally users should be able to mark specific object as "cloneable"
rather than generate clone methods for all objects in a model since it
only makes sense for objects which have pointers. We need some
meta-data knob to control the generation of the clone method.

*** Add support for boost pointer container                           :story:

It should be fairly straightforward to add support for the boost
pointer containers: list, vector, array, set, map, unordered set and
unordered map. In general we can rely on the existing families for the
STL containers. The only snag is that the element on the pointer
container is a pointer in some cases - but no all, as the container
will automatically dereference sometimes. This means we may have some
weird impedance mismatches.

We already have serialisation support for all containers.

Links:

- [[http://www.boost.org/doc/libs/1_64_0/libs/ptr_container/doc/headers.html][Pointer Container Library - Library Headers]]

*** Use pointer container                                             :story:

We should look for cases where we can simplify things using pointer
containers. Two use cases spring to mind:

- at present we are using shared pointers on all registrars. This
  makes no sense as the pointer ownership is clear (the registrar owns
  the pointer). We should use a boost pointer container and pass
  references around, via reference wrapper where required.
- model has a container of shared pointer of element. We don't really
  need the shared pointer abilities; once allocated these are pretty
  const.

*** Add support to foreign keys in ORM                                :story:

At present we are not generating foreign keys for ODB.

*** Add column name support to ORM                                    :story:

At present we need to fall back to ODB pragmas in order to rename a
column. We should have =yarn.orm.column_name=.

*** Allow users to override string prefixes in test data              :story:

At present we have a hard-coded string prefix in test data:
=a_string_". This has been is fine up to now, but we have bumped into
a problem when using it with ORM: some fields in the database are too
small to fit the prefix (e.g. =VARCHAR[5]=). The quick solution for
this is to make the prefix customisable when we instantiate the
generator.

Actually this is not quite that straightforward: in order to allow
users to configure the string prefix, we'd have to extend all helpers
to have a "prefix" argument of type string because we do not know
which helpers are the string helpers. An alternative is to have a test
data configuration, with the following configurable points:

- string prefix
- path prefix
- numeric start
- date start

The configuration is an optional parameter supplied to the
generator. If empty we use the default configuration which could
potentially be read from meta-data, although we do not have a use case
for this.

However, we have a slight problem: if a model M0 has types from
another model M1, we will end up with two configurations (one per
model). When we call a M0 generator which calls an M1 generator, we
need to somehow send the configuration across as well. Since they are
different types (even though identical in layout) we need to copy the
configuration across. This could be achieved with a template
method. Alternatively we could make all helper methods a template
method that takes in a configuration:

#+begin_src c++
template<typename Configuration>
create_XYZ(unsigned int position, const Configuratio& c) {
...
}
#+end_src

Actually this won't work: we still have the problem of calling
external generators.

A simpler but less typed solution is to use =std::tuple=:

: std::tuple<std::string, std::string, int, int> configuration

The other interesting point is that this is perhaps an ORM
problem. After all, we could have a =VARCHAR[2]= string, and
configuring the prefix won't help. What we really need is to figure
out how many digits one can put in the string, given the available
size. Users can supply the sizes as part of the ORM configuration. We
can then do a simple heuristic:

- does the prefix fit? if not, drop it.
- what is the max value for the counter that will fit the string size?
  Use it as a modulus.

*** Add a top-level "Visual Studio" knob                              :story:

We have a number of features that only make sense when on Windows and
building for Visual Studio. We should have a top-level knob that
enables or disables all of these features in one go:

- =quilt.cpp.visual_studio.enabled=

However, we don't really seem to have a way to "link" features such
that when a feature is enabled all of its sub-features are enabled. We
have some hacks for this for the relationship between facets and
formatters but this is not general. We need a general way to declare a
dependency between two "things" and to state a few rules for B depends
on A:

- if A is explicitly enabled, it does not matter if B is enabled or
  disabled.
- if A is not explicitly enabled, it is enabled if B is enabled and
  vice-versa; it defaults to B.
- if B is not explicitly enabled, it uses its default value.

It should be possible to declare arbitrary graphs with these
dependencies.

In this way we'd see features as a graph, with platform-independent
and platform-specific nodes:

- platform independent: types, test_data, io, serialisation, visual
  studio, etc.
- platform specific: c++ types. c++ test data. boost serialisation,
  c++ visual studio, etc.

Dependencies between features can be static or dynamic:

- static means that the state of the instances of the meta-model are
  not relevant to determining the outcome.
- dynamic means the opposite.

For example, forward declarations has a dynamic dependency on types
because depending on the state of the type we may need to force it to
come out. For example, if there is a pointer.

It would be nice if we could move all of these machinery into yarn or
quilt. It doesn't make a lot of sense to place it in either, to be
fair, since its not a platform-independent meta-model concept
(e.g. yarn) and whilst it is a platform-specific concept, it is not
kernel specific. Perhaps it should leave on its own model.

There are several aspects:

- the total list of formatters and facets
- the relationships between them
- functions for the dynamic dependencies that take in an element
- the computation of the enablement.

Actually this should be implemented in terms of profiles.

*** Add support for Visual Studio C++ projects                        :story:

Visual studio project needs the files to be listed by hand. We can
either generate the project or the user has to manually add the
files. This is a problem every time they change. Requirements:

- we need to be able to support multiple VS versions as well (user
  configurable)
- user may want to import property sheets
- need guids (as per C# projects)
- need additional library/include directories
- need to add pre-compiled headers support with /FI.
- add a solution for good measure, using the C# code.
- add filter files for headers and source files.

As per ODB, users may also want to build with different versions of
VS. We should allow generating more than one solution and postfix them
with the VS version.

We should also generate filters for the project:

- header files
- source files
- ODB header files
- ODB source files

The inclusion of ODB files must be done using regular expressions
because we do not want to have to do two passes for knit; so we don't
really know what files are available. However, if the ODB files have a
=cxx= extension, we can just =CLInclude= =*cxx=.

Links:

- [[https://msdn.microsoft.com/en-us/library/2208a1f2.aspx][Project Files]]

*** Add support for exports on windows                                :story:

We should add export macros for shared objects/DLLs for windows. We
should create a file =exports.hpp= probably at top-level with all the
exports.

#+begin_example
#pragma once

#ifdef MODEL_DECL
    #undef MODEL_DECL
#endif

#ifdef MODEL_EXPORTS
    #define MODEL_DECL __declspec(dllexport)
#else
    #define MODEL_DECL __declspec(dllimport)
#endif
#+end_example

It is used as follows:

: class MODEL_DECL Tags xxx

We should probably also add GCC support.

- [[https://gcc.gnu.org/wiki/Visibility][GCC Visibility]]

Links:

- [[https://stackoverflow.com/questions/16982144/cmake-and-generateexportheader][cmake and GenerateExportHeader]]: maybe we should just use the CMake
  support.

*** Add export macros support for Linux                               :story:

#+begin_quote
*Story*: As a dogen user, I want to export types selectively so that I
can control what external users can depend on.
#+end_quote

We've already looked into adding exports for Windows. There is also a
GCC equivalent explained [[https://gcc.gnu.org/wiki/Visibility][here]].

We should have some dynamic extensions to control the outputting of
these.

*** Add =targetver.h= support                                         :story:

On windows we should be generating the targetver header.

Links:

- [[https://github.com/Microsoft/Windows-classic-samples/blob/master/Samples/RadialController/cpp/targetver.h][targetver.h]]

*** Add support for DLL Main on windows                               :story:

At present we are manually generating DLL Main by hand and then
excluding it on regexes. This is not ideal and will be more of a
problem when we generate project files. Ideally we should code
generate it. Requirements:

- user must be able to disable it;
- user must be able to handcraft it in case they want different
  contents;

Links:

- [[https://msdn.microsoft.com/en-us/library/aa370448(v%3Dvs.85).aspx][DLL Main]]

*** Add support for pre-compiled headers on windows                   :story:

Most VS users have pre-compiled headers. We need to generate
=stdafx.h= etc. For now we can have it minimally populated until we
understand better the requirements.

Actually we could probably do a very simple computation in quilt to
figure out the most frequently used headers and add those to
=stdafx=. We just need to go through the entire model in the inclusion
expander to perform this calculation.

In addition we need to make sure =stdafx= is added as the first
include.

We should have a quilt setting for pre-compilation. We should also
check that visual studio support is enabled in order to generate
=stdafx=.

*** Add case conversion support                                       :story:

When we map a LAM model into C#, it will have whatever case we used
originally. This is not ideal as in C++ we'd like to use underscores
instead. It would be nice if there was an "identifier converter" that
went through the model and updated all identifiers from underscores to
camel case. This includes classes, attributes, enumerators, etc. The
LAM model would remain with underscores.

For this to work correctly we'd need some kind of "casing" enumeration
associated with the model, and then another one associated with each
language. This means that if the model is already in camel case, we
would just generate camel case for both C++ and C#.

*** Add support for generic container types to C#                     :story:

We should add all major container types and tests for them.

: IEnumerable<T>
: ICollection<T>
: IList<T>
: IDictionary<K, V>
: List<T>
: ConcurrentQueue<T>, ConcurrentStack<T>, LinkedList<T>
: Dictionary<TKey, TValue>
: SortedList<TKey, TValue>
: ConcurrentDictionary<TKey, TValue>
: KeyedCollection<TKey, TItem>

Notes:

- we need a way to determine if we are using a helper, the assistant
  or a sequence generator directly.

*** Add support for nullable built-ins and primitives                 :story:

One useful feature in C# is the ability to add nullable types:

: Nullable<int>
: ?

This is particularly useful for built-in types, although its also
applicable to value types. For primitives this is slightly more
straightforward and we can make it a property of the meta-type (since
the whole point is that users define new primitives for each domain
type). For built-ins its slightly more tricky because its a property
of the attribute. We'd have to extend:

- the name tree to add a "is nullable" to each name tree
- the parser to read nullable and do the right thing
- LAM, to suport some kind of =lam::nullable= which in C++ translates
  to =boost::optional= and C# =Nullable=. Interestingly enough we can
  create a "Nullable type" in the global namespace.

*** Add auxiliary function properties to C#                           :story:

We need to associate a function with an attribute and a
formatter. This could be the helper or the assistant (or nothing).

Actually this is not quite so straightforward. In =io= (c#) we have:

: assistant.Add("ByteProperty", value.ByteProperty, true/*withSeparator*/);

This is a bit of a problem because we now need to different
invocations, one for helper another for the assistant, which differ on
the function prototype. For the helper we need something like:

: Add(assistant, "ByteProperty", value.ByteProperty, true/*withSeparator*/);

So a string is no longer sufficient. Maybe we could have a struct with
auxiliary function properties:

- auxiliary function types = enum with { assistant, helper }
- auxiliary function name = string

So we can have a map of attribute id to map of formatter id to
auxiliary function properties.

Actually we should also create "attribute properties" as a top-level
container so that in the future we can latch on other attribute level
properties.

*** Add internal object dumper resolution in C#                       :story:

We should try to resolve an object to a local dumper, if one exists;
for all model types and primitives. Add a registrar for local dumpers.

: using System;
: using System.Collections.Generic;
:
: namespace Dogen.TestModels.CSharpModel
: {
:     static public class DynamicDumperRegistrar
:     {
:         public interface IDynamicDumper
:         {
:             void Dump(AssistantDumper assistant, object value);
:         }
:
:         static private IDictionary<Type, IDynamicDumper> _dumpers = new Dictionary<Type, IDynamicDumper>();
:
:         static void RegisterDumper(Type type, IDynamicDumper dumper)
:         {
:         }
:     }
: }

*** Add support for boxed types                                       :story:

At present we support built-in types such as =int= but not
=System.Integer=. In theory we should be able to add these types with:

:        "quilt.csharp.assistant.requires_assistance": true,
:        "quilt.csharp.assistant.method_postfix": "ShortByte"

And they should behave just like built-ins.

*** Add handcrafted class to C# test model                            :story:

We should make sure handcrafted code works in C#. All the components
including the profile is now present in the test model, but we do not
yet have a test type for it. We also need to check how enablement is
working to make sure it works for C#. This should all be done once we
finish with the archetypes model clean up.

*Previous Understanding*

Actually in order to get handcrafted types to work we need support for
enablement. This is a somewhat tricky feature so we should leave it
for after all the main ones are done.

In addition, we should also wait for the model level stereotypes. We
should have a system model that enables/disables formatters, sets the
overwrite flags, etc.

*** Add support for native arrays                                     :story:

At present the legacy yarn parser does not support array notation:
=string[]=. We need to look into how arrays would work for C++ and
implement it in a compatible way. This has been implemented in the new
parser but we haven't yet moved it into production.

Actually we should not add support for native arrays directly, but via
a masd type. Perhaps: =masd::native_array=? Similar with pointers,
etc. However, we need to understand how this relates to LAM.

Links:

- [[https://www.dotnetperls.com/array][array]]

*** Add fluency support for C#                                        :story:

We need to add fluent support for C#.

C# properties are not compatible with the fluent pattern. Instead, one
needs to create builders, across the inheritance tree.

Links:

- [[http://stackoverflow.com/questions/13761666/how-to-use-fluent-style-syntactic-sugar-with-c-sharp-property-declaration][How to use Fluent style syntactic sugar with c# property declaration]]

*** Add visitor support to C#                                         :story:

Implement the visitor formatters for C#.

*** Generate AssemblyInfo in C#                                       :story:

We need to inject a type for this in assets. For now we can leave it
mainly blank but in the future we need to have meta-data in yarn for
all of its properties:

: [assembly: AssemblyTitle ("TestDogen")]
: [assembly: AssemblyDescription ("")]
: [assembly: AssemblyConfiguration ("")]
: [assembly: AssemblyCompany ("")]
: [assembly: AssemblyProduct ("")]
: [assembly: AssemblyCopyright ("marco")]
: [assembly: AssemblyTrademark ("")]
: [assembly: AssemblyCulture ("")]
: [assembly: AssemblyVersion ("1.0.*")]

These appear to just be properties at the model level. We need to also
make it a bit more general and try to figure out what it maps to in
C++. For example, the version file could be generated from this file
- or perhaps these should be distinct concepts? At present we are
using CMake templates to generate =version.hpp=.

*** Consider adding a clone method for C#                             :story:

It would be nice to have a way to clone a object graph. We probably
have an equivalent story for this for C++ in the backlog.

*** Consider making the output directory configurable in C#           :story:

At present we are outputting binaries into the =bin= directory,
locally on the project directory. However, it would make more sense to
output to =build/output= like C++ does. For this to work, we need to
be able to supply an output directory as meta-data.

*** Add code generation support for importing nuget libraries         :story:

A proxy model may require obtaining a nuget package. Users should be
able to define a proxy model as requiring a nuget package and then
Dogen should generate =packages.config= and add all such models to it.

: +  <package id="NUnit" version="2.6.4" targetFramework="net45" />

This can be done by defining meta-data at the model level that allows
it to specify the required packages; then, when importing models, we
need to copy across that meta-data and create the set of all dependent
packages and use that to create the =package.config= file.

Meta-data:

- =quilt.csharp.package_name=
- =quilt.csharp.package_version=
- =quilt.csharp.package_target_framework=

Each proxy model can define only one set of these. When merging, we
need to read the meta-data of each referenced model and create a set
for all of them.

The only slight problem is for hand-crafted types. We may need to add
more entries due to hand-crafting, so that a user model may end up
with lots of these. It needs to be a collection.

Interestingly, a similar approach could be done for C++ with say conan.

*** Add feature to disable regions                                    :story:

We need a way to stop outputting regions if the user does not want
them.

*** Add parameters for using imported assemblies                      :story:

Assemblies imported via proxy models need to have the ability to
supply two parameters:

- assembly name: this is not always the same as the proxy model name;
- root namespace: similarly this may differ from the proxy model name.

These should be supplied as meta data and used when constructing
elements.

*** Add msbuild target for C# test model                              :story:

Once we are generating solutions, we should detect msbuild (or xbuild)
and build the solution. This should be a CMake target that runs on
Travis.

*** Add visibility to coding elements                                 :story:

We need to be able to mark types as:

- public
- internal

This can then be used by C++ (public headers) as well for visibility
in c# (internal).

*** Add partial element support to coding                             :story:

We need to be able to mark logical elements as "partial". It is then
up to programming languages to map this to a language feature. At
present only [[https://msdn.microsoft.com/en-us/library/wa80x488.aspx][C# would do so]].

It would be nice to have a more meaningful name at the logical model
level. However, seems like this is a fairly general programming
concept now: [[https://en.wikipedia.org/wiki/Class_(computer_programming)#Partial][wikipedia]].

*** Add final support in C#                                           :story:

We now have final in the meta-model. We need to check the status of C#
support.

Links:

- [[https://msdn.microsoft.com/en-us/library/88c54tsw.aspx][sealed (C# Reference)]]

*** Add aspects for C# serialisation support                          :story:

We need to add serialisation support:

- C# serialisation
- Data Contract serialisation
- Json serialisation

In C# these are done via attributes so we do not need additional
facets. We will need a lot of configuration knobs though:

- ability to switch a serialisation method on at model level or
  element level.
- support for serialisation specific arguments such as parameters for
  Json.Net.

Links:

- [[https://msdn.microsoft.com/en-us/library/ms731923(v%3Dvs.110).aspx][Types Supported by the Data Contract Serializer]]
- [[https://msdn.microsoft.com/en-us/library/ms731073(v%3Dvs.110).aspx][Serialization and Deserialization]]
- [[https://msdn.microsoft.com/en-us/library/ms733127(v%3Dvs.110).aspx][Using Data Contracts]]
- [[https://msdn.microsoft.com/en-us/library/ms731923(v%3Dvs.110).aspx][Types Supported by the Data Contract Serializer]]

*** Add support for cross-language LAM serialisation                  :story:

Now we have the basic support for LAM in place, it would be nice to be
able to serialise across languages. This could be done as follows:

- XML: create a "LAM" XML schema, and a set of formatters that read
  and write from it. This is kind of like reverse mapping the types
  back to LAM types when writing the XML.
- JSON: similar approach to XML, minus the schema.
- POF: use the coherence libraries to dump the models into POF.

*** Consider adding stereotype of noncopyable                         :story:

This is a common pattern is C++. However, its not yet clear how this
would work with regular domain types.

*** Add support for thrift and protocol buffers                        :epic:

#+begin_quote
*Story*: As a dogen user, I want to expose dogen models to other
languages so that I can make use of them on these languages.
#+end_quote

Amongst other things, these technologies provide cross-language
support, allowing one to create c++ services and consume them from say
ruby, python, etc. At their heart they are simplified versions of
CORBA/DCOM, with IDL equivalents, IDL compilers, specification for
wire formats, etc. As they all share a number of commonalities, we
shall refer to these technologies in general as Distributed Services
Technologies (DST). We could integrate DST's with Dogen in two
ways. First approach A:

- generate the IDL for a model; we have enough information to produce
  something that is very close to it's Dogen representation,
  translated to the type system of the IDL; e.g. map =std::string=,
  =std::vector=, etc to their types. This IDL is then compiled by the
  DST's IDL to C++ compiler. Note: we could use LAM for this, but the
  problem is if one starts with a C++ model, one would have to convert
  it into LAM just to be able to do the mappings. A solution for this
  problem would be to "reverse map" LAM from C++ and get to the
  generic type this way.
- possibly generate the transformation code that takes a C++ object
  generated by Dogen and converts it into the C++ object generated by
  the DST's C++ compiler and vice-versa. We probably have enough
  information to generate these transformers automatically, after some
  analysis of the code generated by the DST's C++ compiler.

In order for this to work we need to have the ability to understand
function signatures for services so that we can generate the correct
service IDL for the DST. In fact, we should be able to mark certain
services as DST-only so that we do not generate a Dogen representation
for them. The DST service then internally uses the transformer to take
the DST's domain types and convert them into Dogen domain types, and
then uses the Dogen object model to implement the guts of the
service. When shipping data out, the reverse process takes place.

Approach A works really well when a service has a very narrow
interface, and performs most of it's work internally without exposing
it via the interface. Once the service requires the input (and/or
output) of a large number of domain types, we hit a cost limitation;
we may end up defining as many types in Dogen as there are in the IDL,
thus resulting in a large amount of transformations between the two
object models.

In these cases one may be tempted to ignore Dogen and implement the
service directly in terms of the DST's object model. This is not very
convenient as the type system is not as expressive as regular C++ -
there are a number of conventions that must be adopted, and
limitations imposed too due to the expressiveness of the IDL. We'd
also loose all the services provided by Dogen, which was the main
reason why we created it in the first place.

Approach B is more difficult. We could look into the wire format of
each DST and implement it as serialisation mechanism. For this to
work, the DST must:

- provide some kind of raw interface that allows one to plug in types
  serialisation manually. Ideally we wouldn't have to do this for
  services, just for domain types, but it depends on the low-level
  facilities available. A cursory look at both thrift and protocol
  buffers does not reveal easy access to such an interface.
- provide either a low-level wire format library (e.g. =std::string=
  to =string=, etc) or a well specified wire format that we could
  easily implement from scratch.

This approach is the cleaner technically, but its a lot of work, and
very hard to get right. We would have to have a lot of round-trip
tests. In addition, DST's such as thrift provide a wealth of wire
formats, so if there is no easy-access low-level wire format library,
it would be very difficult to get this right.

Links:

- [[https://github.com/protobuf-c/protobuf-c][protobuf-c]]: protobuf stand alone library in C.
- [[https://github.com/mapbox/protozero][protozero]]: Minimalist protocol buffer decoder and encoder in C++
- [[https://github.com/protocolbuffers/upb][GH upb]]: a small protobuf implementation in C.
- [[https://richardstartin.github.io/posts/dont-use-protobuf-for-telemetry][Don't use Protobuf for Telemetry]]
- [[https://codeburst.io/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f][JSON vs Protocol Buffers vs FlatBuffers]]
- [[https://github.com/nanopb/nanopb][nanopb GH]]: Nanopb is a small code-size Protocol Buffers
  implementation in ansi C.
- [[https://github.com/StandardCyborg/protobag][protobag GH]]: Protobag: A bag of Serialized Protobuf Messages
- [[https://github.com/eclipse-iceoryx/iceoryx][iceoryx GH]]: "iceoryx is an inter-process-communication (IPC)
  middleware for various operating systems (currently we support
  Linux, MacOS and QNX). It has its origins in the automotive
  industry, where large amounts of data have to be transferred between
  different processes when it comes to driver assistance or automated
  driving systems." [[https://www.reddit.com/r/cpp/comments/ms2i0f/announcing_eclipse_iceoryx_100/][Reddit discussion]]. [[https://www.eclipse.org/community/eclipse_newsletter/2019/december/4.php][Basic overview]].

*** Add support for flatbuffers                                       :story:

Flatbuffers is a technology similar to protocol buffers but with a
simpler implementation. The generated code is not brilliant. We can
add support at several levels:

- add stereotypes and other meta-model constructs to allow users to
  express flatbuffer concepts: interop::table? ideally something which
  is not hard-coded to flatbuffers but can be extended to other IDL
  constructs such as Corba, etc. We will need a mapping layer for both
  types and meta-model concepts from a set of generic yarn terms to
  the concrete terms of the implementation technology. This needs to
  be hooked in to the mapping layer somehow.
- on a first stage we can just generate the IDL and then use their
  compiler to generate code. This is already useful because we can
  have a single model that covers the entire system.
- a second stage would be to generate the flatbuffers code
  directly. This is easier to achieve that with other technologies
  such as protobuf because the flatbuffers code is simpler. We could
  tidy-up a lot of their generated code: use string views, array
  views, do not inline helper methods, create better builders, add
  comments, use enum classes and make enums more readable, etc. We
  should generate this code into types, rather than into a flatbuffers
  facet directory. This directory is reserved for the code generated
  using their compiler (as we did for ODB). Once we generate the code
  in types we can also do other things such as generate test data, io
  etc for these types.
- inner classes support is required if we want to allow the user to
  generate more than one type in a file. However, since flatbuffers
  supports includes this is not a mandatory requirement.

*** Add support for BSON serialisation                                :story:

It would be useful to support Mongo DB's BSON. There is a C++ stand
alone library for this:

https://github.com/jbenet/bson-cpp

For examples on how to use the C++ API see the tutorial:

https://github.com/mongodb/mongo-cxx-driver/wiki/Tutorial

*** Add support for Decimal numbers in C++ and C#                     :story:

For C++:

- try using ICU DecNumber library.
- check compiler support (MSVC may have decimals; if so, use that
  instead)
- There is a cross-platform implementation of =std::decimal= available
  [[https://sourceforge.net/p/stddecimal/code/HEAD/tree/trunk/][here]].
- we should probably consider just supporting [[https://www.boost.org/doc/libs/1_72_0/libs/math/doc/html/math_toolkit/high_precision/use_multiprecision.html][boost multiprecision]]
  instead.

For C#:

- use System.Decimal.

LAM:

- update decimal mappings. We had:

:    {
:        "lam_id" : "lam.decimal",
:        "names_by_technical_space" : [
:            {
:                "technical_space" : "cpp",
:                "default_name" : {
:                    "simple": "decimal",
:                    "model_modules": "std"
:                }
:            },
:            {
:                "technical_space" : "csharp",
:                "default_name" : {
:                    "simple": "Decimal",
:                    "model_modules": "System"
:                }
:            }
:        ]
:    },

*** Add depth detection to io in C++                                  :story:

In C# we added support for detecting the depth of the graph and
exiting after we've gone too deep. This is an effective way of
handling cycles in the graph until we have better solutions. We need
to adopt something similar for C++.

*** Use clang format in physical model                                :story:

We need to indent the output coming out of the physical model. At
present our stitch templates are super-complex purely because we are
trying to get the indentation right. In most cases we don't. We need
to:

- remove indent filter and any other indentation "helpers"
- update all templates to output everything as simply as possible, in
  one long line if need be. We may still need to use sequence helper
  but hopefully for very trivial cases.
- plug in clang format at the end of the knit pipeline, using either a
  default set of options or a user supplied set of options (via a
  command line parameter).

Notes:

- integrating with clang-format in an efficient way is a bit
  tricky. We don't want to have to write each file and then reformat
  it just to find out if there have been any changes. The alternative
  is to apply clang format to the files in memory inside of
  extraction. This is more efficient but will require a dependency on
  [[https://clang.llvm.org/docs/LibFormat.html][libFormat]]. We can use [[https://github.com/llvm-mirror/clang/blob/master/tools/clang-format/ClangFormat.cpp][clang format's code]] to see how to use
  libformat.
- [[https://github.com/TheLartians/Format.cmake][Format.cmake]]: clang-format for CMake
- [[https://github.com/cheshirekow/cmake_format][cmake_format]]: "The cmake-format project provides Quality Assurance
  (QA) tools for cmake".
- [[https://github.com/BlankSpruce/gersemi][gersemi]]: "A formatter to make your CMake code the real treasure."
- when we integrate clang format, we will have to change how we
  process overwrite. At present we are just not writing if overwrite
  is on; in the future, we need to physically load the contents from
  the filesystem and override the generated contents, so that we can
  clang-format the existing files. We then write the file down if the
  formatting changes its content. If we don't do this we still need to
  run clang-format manually again for the handcrafted code.
- clang-format [[http://releases.llvm.org/9.0.0/tools/clang/docs/ReleaseNotes.html][now supports C#]].
- when we support the generation of clang format styles, we will have
  to write the style file first before we use it for formatting. See
  the story on this.

Merged Stories:

We should generate un-indented c++ code and then rely on clang-format
to do the indentation. We can allow users to supply their own
configurations and supply those to clang. This can be done via the
meta-data, or if there is a well defined file for clang, we could use
it instead.

Note that using clang to manage indentation will make things a lot
slower. Note also that clang supports Java and may in future support
C#. See [[http://clang.llvm.org/docs/LibFormat.html][LibFormat]].

Another option is to create fallback modes. The preferred indenter for
a given language (say c++) may not exist for another language (say
c#); for these we use a dogen created indenter that is very basic. It
may support some of the configuration parameters supplied for the
clang indenter. The key thing is that we take away indenting from the
formaters - they become flat - and then we always apply the indenter;
either a clang based one or a simplified one. Either way, the code
should live in formatters and make use of the language-specific
folders as required.

*Indent stitch output using clang format*

*Rationale*: we should just merge the final output, not each tool's.

We need to indent the output coming out of stitch as it is not
suitable for reading as-is.

Links:

- [[https://bbannier.github.io/blog/2015/05/02/Writing-a-basic-clang-static-analysis-check.html][Writing a basic clang static analysis check]]
- [[http://zed0.co.uk/clang-format-configurator/][clang-format configurator]]
- [[http://clangformat.com/][online clang format]] - older site
- [[https://github.com/llvm-mirror/clang/blob/master/tools/clang-format/ClangFormat.cpp][ClangFormat.cpp]]: clang format tool. We can use this as an example of
  how to use LibFormat.

*** Add an injector for visual studio models                          :story:

It should be "fairly straightforward" to add a frontend for visual
studio. A sample project has been added to test data:

: test_data/visual_studio_modeling

We should also extend tailor to output these projects so we can test
it with existing models.

Note that the XMI support approach may make this unnecessary. Where
possible, we should just use XMI instead of having lots of different
exogenous models.

*** Add an injector for ArgoUML                                       :story:

Seems like a popular UML tool. Not sure if it supports XMI.

Links:

- http://argouml.tigris.org/
- https://github.com/cscorley/argouml-mirror

*** Add support for "capitalisation theming"                          :story:

It would be nice if facets, classes etc which are at present in lower
case could be camel cased if the user chooses.

At present we need to override all facet directories, include
directories, etc.

In fact, we could copy the clang format approach and create the notion
of a "style". The style would then be applied to "targets", e.g.:

- classes, interfaces
- methods
- variables, member variables, constants
- enumerations

Each of these can then have a case style. We could create a meta-model
element to capture all of the aspects of the theme. A technical space
can have a default theme associated with it; we need to find out what
are considered the "standard" cases.

Notes:

- apparently there is =lowerCamelCase= and =UpperCamelCase=.
- according to wikipedia, 'The combination of "upper camel case" and
  "snake case" is known as "Darwin case"'
- we need to support both the "apply a theme" (in which case all model
  elements must be in snake case, else there is an error) and the "use
  the names as-is", in which case we will not touch the supplied
  elements.
- note that we do not care about the indent style. Or better, we do,
  but this is just an argument supplied to clang format. We will
  handle this separately. The clang format configuration should just
  be a parameter in the theme. We should allow for the out-of-the box
  styles: LLVM, Google, Chromium, Mozilla, WebKit, Microsoft.
- we need to merge the capitalisation theming with decoration. We
  don't want to have several dimensions across which we have separate
  themes. We need to ensure related concepts are modeled only once.
- we need to take into account artefact concepts such as file and
  directory names. See story on directory themes.

Links:

- [[https://medium.com/better-programming/string-case-styles-camel-pascal-snake-and-kebab-case-981407998841][Case Styles: Camel, Pascal, Snake, and Kebab Case]]
- [[https://en.wikipedia.org/wiki/Naming_convention_(programming)][Naming convention (programming)]]
- [[https://en.wikipedia.org/wiki/Camel_case][Camel Case]]
- [[https://en.wikipedia.org/wiki/Indentation_style][Indentation style]]
- [[https://llvm.org/docs/Proposals/VariableNames.html][LLVM: Variable Names Plan]]

Merged stories:

*Add camel case option*

#+begin_quote
*Story*: As a dogen user, I want my models to use camel case so that I
can integrate dogen code with my code base.
#+end_quote

It would be nice to have a command line option that switches names
from underscores into camel case. The default convention would be that
diagrams are always with underscores and then you can convert them at
generation time. There should be a regex for this conversion.

*** Add support for clang format configurations                       :story:

It would be nice if we could generate a clang format configuration. We
could have a meta-model element that represents a style and then use
it to generate the style file. We can generate multiple languages on a
given style file:

: ---
: # We'll use defaults from the LLVM style, but with 4 columns indentation.
: BasedOnStyle: LLVM
: IndentWidth: 4
: ---
: Language: Cpp
: # Force pointers to the type for C++.
: DerivePointerAlignment: false
: PointerAlignment: Left
: ---
: Language: JavaScript
: # Use 100 columns for JS.
: ColumnLimit: 100
: ---
: Language: Proto
: # Don't format .proto files.
: DisableFormat: true
: ---
: Language: CSharp
: # Use 100 columns for C#.
: ColumnLimit: 100
: ...

Links:

- [[https://zed0.co.uk/clang-format-configurator/][Clang format configurator]]
- [[https://clang.llvm.org/docs/ClangFormatStyleOptions.html][Clang-Format Style Options]]
- [[https://gist.github.com/andrewseidl/8066c18e97c40086c183][Clang-format style comparison]]
- [[https://leimao.github.io/blog/Clang-Format-Quick-Tutorial/][Format C/C++ Code Using Clang-Format]]

*** Add support for object caches                                      :epic:

It would be good to have meta-model knowledge of "cacheability". This
is done by marking objects with a stereotype of =Cacheable=. It then
could translate to:

- adding a serialisation like interface with gets, puts, etc. We need
  to bind this to a specific cache such as memcache, coherence, etc.
- create a type to string which converts a key made up of built-ins or
  built-in into a underscore delimited string, used as a key in the
  cache.
- we should also consider external libraries like [[https://github.com/cripplet/cachepp][cachepp]].
- we could consider code-generating code to link the far cache with
  the near cache but it seems this is the job of a caching library.

Links:

- [[https://pocoproject.org/docs/package-Foundation.Cache.html][POCO: Cache namespace]]
- [[https://pocoproject.org/slides/140-Cache.pdf][POCO: Cache namespace docs]]
- [[https://github.com/cachelot/cachelot][cachelot]]: LRU cache
- [[https://github.com/facebook/CacheLib][CacheLib GH]]: "Pluggable caching engine to build and scale high
  performance cache services. See www.cachelib.org for documentation
  and more information."
- [[https://engineering.fb.com/2021/09/02/open-source/cachelib/][CacheLib, Facebook’s open source caching engine for web-scale
  services]]
- [[https://www.reddit.com/r/cpp/comments/pgn6wi/cachelib_facebooks_open_source_caching_engine_for/][Reddit discussion on CacheLib]]

*** Generate Redis get/set code                                       :story:

In theory, there is nothing stopping us from having a Redis facet that
takes in as an input the serialisation method. For now we just need to
support boost serialisation. The interface could be configurable so
that users can choose the archive type. Types could be marked as
=cacheable= and then suitable parameters supplied such as the
serialisation mechanism.

As with hashing, we do not want to generate code for all objects; only
for those the user marks as cacheable.

The interface should support two main methods:

- get
- set

Both receive an instance of Redis. We could implement it in C to avoid
additional dependencies.

However, it should also be possible to use say =memcached= as the
cache rather than redis. We need to create a layer of indirection
between the generic caching (meta-model concept) and the actual
caching (platform, implementation layer). In fact we can leave this
for later and for now only worry about redis. =cacheable= will then be
built on top of the existing facets.

What we do need though is the ability to configure the meta-model to
link the serialisation type to the cache type. Say for example we
support BSON and JSON and boost serialisation. We may want the cache
facet to support one or more or all of these. We can think of these as
aspects that are enabled by the user at the model level (but possibly
overridable at the element level).

Other KVP stores:

- [[https://github.com/microsoft/FASTER/tree/master/cc][FASTER GH]]: see also the [[https://microsoft.github.io/FASTER/docs/fasterkv-cpp/][home page]].
- [[https://github.com/mzimbres/aedis][aedis GH]]: "Aedis is a Redis client library built on top of Asio that provides
  simple and efficient communication with a Redis server."

*** Add support for libnop                                            :story:

Interesting serialisation format.

Links:

- https://github.com/google/libnop

*** Add boost variant visitors                                        :story:

It would be nice if we could automatically generate boost variant
visitors with lambda support. See:

- [[http://stackoverflow.com/questions/7867555/best-way-to-do-variant-visitation-with-lambdas][Best way to do variant visitation with lambdas]]
- [[https://www.meetingcpp.com/blog/items/boostvariant-and-a-general-generic-visitor-class.html][boost::variant and a general, generic visitor class]]
- [[https://www.boost.org/doc/libs/1_72_0/doc/html/variant.html][Boost variant]]
- [[https://theboostcpplibraries.com/boost.variant][Boost variant - boost book]]

*** Add support for disabling unique filenames                        :story:

At present dogen uses formatter and facet prefixes for each file it
generates, ensuring the files are unique across all projects in a
model. This was done originally due to some issue with code coverage,
whereby the same filename was causing the gcov tool to get confused.

Originally we had a feature that allowed switching off the unique
filenames, but this bit-rotted and was eventually removed. It may make
sense to add it again, but it is really a shorthand for setting facet
and formatter prefix to empty string. We should at least have a model
that tests that this works.

*** Create a map between UML/MOF terminology and yarn                  :epic:

It would be helpful to know what a yarn type means in terms of
UML/MOF, and perhaps even explain why we have chosen certain names
instead of the UML ones. We should also cover the modeling of
relationships and the relation between yarn concepts and UML/MOF
classes. This will form a chapter in the manual.

The UML specification is available [[http://www.omg.org/spec/UML/2.5/][here]] and MOF specification is
available [[http://www.omg.org/spec/MOF/2.5][here]].

We need a way to uniquely identify a property. This could be done by
appending the containing type's qualified name to the property name.

See also [[http://www.uml-diagrams.org/][The Unified Modeling Language]] for a more accessible
treatment.

See [[http://www-01.ibm.com/support/knowledgecenter/SS5JSH_9.1.2/com.ibm.xtools.transform.uml2.cpp.doc/topics/rucppprofile.html][Stereotypes of the UML-to-C++ transformation profile]] for ideas.

*** Investigate boost log config files                                :story:

Our log files are growing quite a bit. We don't really want to log any
less since the logging is very useful for troubleshooting. However, at
any one time we just need to look at one or a couple of
components. What we really need is something like log4j, where we can
change log levels for a component or all components in a hierarchy. We
need to investigate boost log solutions for this.

It seems we cannot change severity per component ("channel") with our
current setup. We need something akin to this:

- [[http://www.boost.org/doc/libs/1_57_0/libs/log/doc/html/log/detailed/expressions.html#log.detailed.expressions.predicates.channel_severity_filter][Severity threshold per channel filter]]

This could be implemented as follows:

Create a log config file (say =logging.ini=) that contains a list of
strings and valid severities:

: root = trace
: cpp = debug
: cpp.settings = info

and so on. When the log is being initialised, a sorted list with these
is loaded into memory. It is sorted by channel name. Note that =root=
is a special value and is always at the bottom of the list (or even
removed from the list altogether and handled specially). If root was
not defined in the config file, we set it to a default. Note also that
we convert the severity strings into enums, with adequate validation.

Once the list is setup, we then loop through all the channels that
have been defined. There is an assumption that all channels were
defined statically and thus have already been defined by the time we
initialise the log. This needs to be verified.

For each channel, we loop through all values from the file - other
than root - applying them as a regex against the channel name. Note
that we sorted them so the closest match should be last. For each
value that matches, we set the severity accordingly. If no matches are
found, we apply the root setting.

Some other tidbits:

- we can now remove the =verbose= option, or perhaps it should be used
  as a short-hand for the log configuration? if so we need a rule that
  determines which one to use when both are present.
- we could monitor the config file for changes, although for dogen
  this is overkill.
- if sorting proves too hard we could just say that the regexes are
  applied in the order provided by the user, with the exception of
  root.

*** Create a PDM for the utility model                                :story:

We need to create a PDM for the utility model that only includes
utilities. To call it PDM may not be strictly correct as it is not
doing any mapping. Perhaps it is just a simple, stand-alone
product. We need to remove any "helper" code for other models - it
should be placed on the corresponding PDM (e.g. =boot=, =std=, etc).

*Previous Understanding*

We need to create a library with support code that is used by the
models. At present it is needed for =io= and =test_data=. However, we
ran into [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_67.org#update-legacy-formatters-to-use-needle-for-io][a lot of difficulties]] when we tried to implement it for =io=
using templates.

For the previous attempt to create the needle library see this commit:

feb4750 * integration needle: remove project and includes

We have analysed this idea from many angles. The key trade-offs we
have are as follows:

- original idea was to create models with zero dependencies; users
  should be able to code generate and compile. This would make the
  barrier to entry very low.
- however, there are downsides to this approach:
  - utility is needed on pretty much every product (logging, IO,
    etc). This means we still need some kind of hack to get things to
    work. The present solution was to somehow code generate utility
    itself. Whilst this could work, it is non-trivial because we now
    have "moving references" to utility types. This means a) we cannot
    add logging to tests because of it b) in order to solve this
    problem, we will have to add a lot of complexity to dogen.
  - helpers: we need to generate helpers for types that are sourced
    from PDMs. This means that every time you add a new type to Dogen
    which is not directly supported, you need to extend the code
    generator. It is very hard to add new types this way. In addition,
    the logic for assistants (families, etc) is very complex and
    complicates the code base quite a lot. In addition, there is a lot
    of duplication because we generate code specific to each use of a
    container in a model; if you have a list of strings and a list of
    ints in two types you end up with two helper methods per type. In
    reality this could all be compressed to one helper method across
    the two types.
  - the problem is only going to get worse going forward as we add
    more functionality that needs this kind of support. For example,
    data fountains would require lots of helpers per type.

In summary, it seems that the engineering trade-offs are clearly
tipping towards having a shared model for this functionality. This
being the case we could look at it as a product: =masd::core=. We
could then add types for the PDMs in a sensible way: in types we'd
create namespaces for each model, e.g. =std=, =boost=, etc. We'd then
disable types facet for these types and set all other (required)
facets to override. For example:

- =masd/core/io/std/list_io.hpp=: std::list IO support.
- =masd/core/test_data/std/list_td.hpp=: std::list test data support.

And so forth. It would still be a peculiar product in that we'd only
have a single dogen model. In addition, it is a "strange" model in
that users are not expected to reference it; if you did, you'd end up
being able to create a type like =masd::core::std::list=, which is
just a mapping type. Perhaps we should have a way of identifying these
types as "mapping only types"? This is important because we may want
to reference other types.

The core model would also contain the data fountains. It seems like
=core= is a product, made up of models:

- PDM extensions: IO, test data, serialisation, hash. Name: ext,
  extensions, etc.
- fountain: all data fountains.
- utility: logging, file, string, XML, environment, testing.

This would also mean that we can now have "model types" such as PDM
extensions, which are not "referrable" - e.g. we can throw if the user
attempts to reference it. Or, better yet, why have two PDM models?
Applying this approach naively would mean we'd need the =std= model to
define the types, and then the core model to provide the mappings. But
why have two models? Why not define a model in the namespace of the
original model? For example, if we had a =std= model defined in core
(and violating the principles around product and model naming), we
could place all the =std= types in it, set types to disabled and other
facets to enabled as required (e.g. IO). Then it would generate say
=std/io/list_io.hpp=. We would still keep all of the dependency
machinery we have at present so that we can point to say =<list>=
instead of =std/types/list.hpp=. We may also need some kind of hackery
around enablement because we do not want to generate a types file but
we do want users to be able to reference the overriden type. This
approach is *very* regular:

- PDMs now become very similar to dogen models. Interestingly, this
  also means we do not need the =library= folder any longer. This is
  because dogen will not work without the code that comes with the
  PDMs so you will need to git clone it anyway. In effect, the PDMs
  become just like any other Dogen model, so you'll have to install
  them and then point to them in order to code generate.
- we need PDMs per technical space: C++, C#, etc. Each would have its
  own product, etc.
- PDMs can contain all the required linking information. However, we
  probably should have a PDM per linking library (e.g. one for each
  linkable boost library). Else it will be hard to know what libraries
  to link against depending on usage.
- users can add a new PDM with very little knownledge of Dogen: add a
  new model following the existing pattern (copy and paste).
- no changes are required to dogen itself: it would automatically pick
  up the new PDMs (just need to git update core).
- all PDM code could have tests in the core model to ensure it all
  works in isolation (test data, IO, etc). We may even be able to
  reuse the generated tests for these types.

This approach is very elegant, but its not very nice to start creating
types in the =std= namespace. Perhaps we should instead to have "name
mapping" or alisaing? A way to associate a name with a type. Then the
PDM would follow the usual dogen naming rules, but when you load up
the model for referencing, it would export the aliases. The crux
though is that we just want a single model for both exporting the PDM
types and implementing the glue code.

*** Concepts, immutability and fluency                                :story:

At present we allow the object to determine if any attributes obtained
from modeling a concept are immutable and/or fluent. This seemed
logical at the time, but its actually not a good idea: this means that
two objects modeling a concept may not actually implement the same
interface, thus meaning that they are not really modeling the
concept.

Instead if this cleverness, users should be allowed to mark the
concept itself as immutable/fluent and we should simply throw if there
is an incompatibility between concept and object.

*** Create an interface for the text reader                           :story:

In order to do performance testing of the dia model we should create
an interface for text reader and implement it as a mock. This will
avoid the overhead of reading stuff from the hard drive.

*** Adding types to package namespace                                 :story:

Whilst it is possible to document a namespace, it is not possible to
add any classes etc to that namespace. For example, it may make sense
to add some constants at the namespace level. This is not possible
with the current setup.

This could be easily implemented by adding a flag that determines if a
type is at the namespace level.

This is also related to merging multiple types in a single formatter.

*** Add support for inner classes                                     :story:

Inner classes could be expressed in the same way as the short-hand for
namespaces. For example, given a class =a=, an inner class =b= could
be declared as =a::b=. The system would have to recognise that =a= is
a class and then treat it accordingly in yarn. The formatters would
have to be taught to express inner classes when formatting the main
class. This probably requires merging two yarn entities into a single
cpp entity. Finally some dynamic extension support would be required
to determine if the inner class is public or private.

We just need a use case for where inner classes would be useful.

*** Consider adding support for inline hashing                         :epic:

At present we have an entire facet for hashing. However, it is
conceivable that some users may find it a bit of an overkill and would
rather have it added to the types facet instead. We already do
something similar for the =operator<<=. We need to consider a more
generic mechanism for allowing the inlining of certain features when
they are more core.

In fact, one wonders if we are not getting towards a multi-option
implementation for certain things:

- as a facet;
- as a formatter;
- as a toggleable aspect of an existing formatter.

Where the user gets to choose one of the three possible
implementations. In some cases we may not support all of these
options; for example, for operators we could support the second and
the third option and so on.

See also Support "cross-facet interference".

*** Consider making header guards configurable                        :story:

It may be nice to be able to switch parts of the header guards off (do
not use namespaces, etc) or even to have a completely different policy
to generate header guards. We do not have a use case for this yet, but
this story is a place-holder for it.

The end-game for this is to have a set of "variables" that can be used
to express the header guards:

: $(namespaces)_$(class_name)_$(extension)

And so forth. One use case is when we somehow generate the same header
guard for two different model types (for example when one has a prefix
that is the same as the namespace of another). In this case it would
be nice to be able to override the header guard.

In addition, we could also make the namespace separators for modules
configurable; if we didn't use underscores for modules we would not
have a clash. However, its not clear what the separator would then be
(double underscore?).

*** Consider adding facet specific types                               :epic:

#+begin_quote
*Story*: As a dogen user, I want to code-generate simple types for
facets other than =types= so that I don't have to create them manually.
#+end_quote

Types in dogen are somewhat "uni-dimensional"; that is, the main focus
of all work is types and the other facets are thought to either be
code generated in total (serialisation, hashing, etc) or manually
generated in total (test for mock factories). However, in some cases
it may make sense to add a type directly to a facet. For example, we
may want to add simple value objects to the mock factory. We don't
want to pollute =types= with these classes, but at the same time we'd
rather not have to manually generate them. It would be nice to be able
to associate a type with just a facet via dynamic extensions. Of
course, this does mean we would not be able to rely on all other
facets such as serialisation and even streaming or else things would
get a bit confusing. But it would still be useful.

Another possible (less clean) approach is this:

#+begin_quote
It would be great if we could use dynamic extensions to enable and
disable facets (there probably already is a story for this). But in
addition to this, it would also be great if one could override the
default name for an object in a facet; for instance: one could add an
object called =serialization_manager=, disable all facets bar
serialisation, disable the serialisation postfix of this file and
disable code generation. This way one could add manual code to any of
the facets, independently.

At present we support this, but only for types as it is hard-coded.
#+end_quote

A better understanding for this problem is to think that we have
meta-model elements which have a default behaviour of being projected
into all facets: =Projectable=. We define projectable elemets in the
modeling space, and then automatically projections happen into
facets. However, it should be possible to also define
"non-projectable" elements, which need a facet to house them. We can
either create a namespace with a stereotype of say =<<facet_name>>= or
set the meta-data in the element. This has the same effect as
disabling all facets except for the one we're interested in. However,
not-projectable elements lose access to IO, etc (by definition).

*** Control JSON output via traits                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to configure the output JSON so that
I get exactly what the external users are looking for.
#+end_quote

Once we add support for JSON we will face the same sort of problems
that Json.net has already solved: we may want to have keys that do not
match the property names (for instance we may want to use human
readable names in the json), we may want to translate enumerations to
numbers or to human readable descriptions, we may want to collapse a
class into some less verbose JSON, etc. Some of these are describable
via traits, very much like Json.Net uses C# attributes. We should look
into the available attributes and see if they make sense as dogen
traits to control JSON. Some of these may have wider application and
be used to control other serialisation formats.

*** Add support for user defined literals                             :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of literals so that I can
make my code more type safe.
#+end_quote

With user defined literals in C++11, defining one's own numeric types
became more convenient. We should look into adding support for this in
dogen.

See [[http://www.codeproject.com/Articles/447922/Application-of-Cplusplus11-User-Defined-Literals-t][Application of C++11 User-Defined Literals to Handling Scientific
Quantities, Number Representation and String Manipulation]]

One interesting way of doing this is as per [[http://researcher.ibm.com/researcher/files/zurich-jku/mdse-08.pdf][MDSD]] book (p111): to
create meta-entities for the quantities. This needs a bit more
analysis but the gist of it is that we could then map quantities to
the most appropriate platform specific technology - in Java/C# create
a type or use a int, in c++ use literals, etc.

*** Modeling of visitors in =cpp= can be improved                     :story:

In the =cpp= model we are assuming that if the original parent was
visitable, then the visitor was named after it:

:             } else if (c.is_original_parent_visitable() && !c.is_parent()) {
: #>
: public:
:    virtual void accept(const <#= c.original_parent_name() #>_visitor& v) const override {

The right thing to do here is to have a =visitor_info= attached to the
=class_info= that is generated during transformation and deals with
all such rules. The template should just loop through the visitor
infos. In addition, the visitor infos should tell the template if they
are abstract or implemented. We may be able to reuse the existing
=visitor_info= class for this.

*** Move =invalid= value to a different value                         :story:

Value zero should always be assigned to the most used enumeration
value because it is very efficient to compare against zero. We should
use a different value for invalid such as =0xFF...= etc.

*** Allow renaming =invalid= enumerator                               :story:

At present we issue the =invalid= enumerator to signal no
choice. However, in certain cases there are domain specific options
that are better suited (such as =none=, etc). Users should be able to
change the name of the enumerator.

*** Allow non-throwing casts for enums                                :story:

In some cases we want to do a lexical cast of an enum but not actually
throw if the enum is invalid; instead, we just want the enum mapped to
invalid (if that is enabled). This should just be a case of adding a
boolean to the cast templates.

It seems boost now supports this via =try_lexical_convert=. However,
it seems its non-trivial to convert our use of lexical cast into this
new approach. The problem is that we did not simply provide an
=operator<<= for each enum, we overloaded =lexical_cast=. We did this
because we are already using =operator<<= for the purposes of =io=,
and that requires a JSON based output which is not suitable for
casting. Its not clear what the right approach is here.

Notes:

- we could make use of manipulators; perhaps a =masd::json=
  manipulator could be used to coerce =operator<<= to format into
  JSON, otherwise we'd get a default string representation.

- [[https://www.boost.org/doc/libs/1_71_0/boost/lexical_cast/try_lexical_convert.hpp][try_lexical_convert.hpp]]
- [[https://www.boost.org/doc/libs/1_58_0/doc/html/boost_lexical_cast/synopsis.html#boost_lexical_cast.synopsis.try_lexical_convert][try_lexical_convert docs]]
- [[https://stackoverflow.com/questions/2249711/how-to-use-the-boost-lexical-cast-library-for-just-for-checking-input][How to use the boost lexical_cast library for just for checking
  input]]
- [[http://boost.2283326.n4.nabble.com/lexical-cast-A-non-throwing-lexical-cast-Was-5-Observations-My-experience-with-the-boost-libraries-td2671153.html][A non-throwing lexical cast?]]
- [[https://stackoverflow.com/questions/3157098/whats-the-right-approach-to-return-error-codes-in-c][Whats the right approach to return error codes in C++]]

*** Consider adding a better enum representation                      :story:

Useful functionality:

- from string, no throw. Only available if enumeration has invalid.
- from string, throw. Requires stdexcept, throws std::invalid_argument.
- to string, no throw.
- to string, throw.
- parameterisation of strings: qualified or non-qualified.
- container. Using container conventions, =value_type= is the
  enumeration, =underlying_type= is the built-in type.

When creating the class, it would be nice to maintain the strong
semantics of C++ 11 enums whilst at the same time having a nice
namespace for the enumeration.

We probably need to have multiple enum providers - or better:
styles. This is c++ specific
(e.g. =masd.enumeration.cpp.style=). Styles:

- plain:
  - C++ 98: plain enum, no enum class.
  - C++ 11 and above: plain enum, no enum class.
- strong:
  - C++ 98: inside class
  - C++ 11 and above: plain enum with enum class.
- encapsulated: (improved, featureful, ?)
  - Enum as a class with iteration, string support, etc. This is the
    "masd" provider/style. With conversion methods.
- external library: [[https://github.com/therocode/meta_enum][meta-enum]], [[https://github.com/aantron/better-enums][better-enums]], [[https://github.com/nadult/fwk_enum][fwk_enum]], etc. The style
  is the name of the library.

We should also take into consideration the "boolean enum" scenario,
whereby we want a wrapper around a boolean. See [[https://codereview.stackexchange.com/questions/11300/boolean-enums-improved-clarity-or-just-overkill][Boolean enums:
improved clarity or just overkill?]] Ideally the metamodel should make
this clear rather than having to set meta-data type to bool and then
making sure there are only two values. In effect, we have three
different types of enumerations:

- regular enumerations;
- boolean enumerations;
- flags.

The metamodel must reflect this.

**** Scenarios:

- create from value:

: const my_enum a(my_enum::my_value);

  Type is named after the enum, values are accessible by scope, e.g.:
  =my_enum::=.

- initialise from string:

: const auto a(my_enum::from_string("my_value"));
: const auto a(my_enum::from_string("my_enum::my_value"));

- initialise from string with exceptions:

: const auto a(my_enum::from_string_throw("my_value"));
: const auto a(my_enum::from_string_throw("my_enum::my_value"));

  Problems: users cannot disable qualified/non-qualified conversions.

- dump to string, non-qualified:

: std::cout << a.to_string(); // my_value

- dump to string, qualified:

: std::cout << a.to_string_qualified(); // my_enum::my_value

- pretty print:

: std::cout << a; // my_enum::my_value

  Problems: we need to distinguish between dumping to string in the
  sense of pretty printing from a string conversion.

- iterate:

: for (const auto e : my_enum::values())
:    std::cout << e.to_string();

- similarly named enums cannot be confused.

: a::colour c = b::colour::red; // does not compile.
: a::colour c = c::colour::red; // compiles.

**** Notes

- a good compromise from the ideal scenario is to create a
  =my_enum_traits= class. Sadly, it cannot be =my_enum::traits=. The
  API then becomes:

: my_enum_traits::from_name("X"); // accepts both simple and qualified
: my_enum_traits::from_simple_name("X");
: my_enum_traits::from_qualified_name("X:X");
: my_enum_traits::to_simple_name(e);
: my_enum_traits::to_qualified_name(e);

: my_enum_traits::from_name_throw("X"); // accepts both simple and qualified
: my_enum_traits::from_simple_name_throw("X"); // rather than string
: my_enum_traits::from_qualified_name_throw("X:X");
: my_enum_traits::from_underlying_type(123); // validates cast
: my_enum_traits::from_underlying_type_throw(123); // validates cast
: my_enum_traits::to_simple_name_throw(e);
: my_enum_traits::to_qualified_name_throw(e);

: my_enum_traits::values(); // static array of enum.
: my_enum_traits::names(); // static array of names.

: my_enum_traits::to_index(e); // position of enum in names, values.
: my_enum_traits::to_index_throw(e); // position of enum in names, values.

- the non-throwable functions are only defined if invalid is
  available.
- use =noexcept= on the non-throwable functions.
- use =string_view= on C++ 17, =string= elsewhere.
- use =array= on c++ 11 and above, else =vector=.
- enum should be defined as a class on c++ 98. In this case we
  probably do not need a separate traits class.
- define a boost lexical cast from string and to string in terms of
  the above functions. This can be enabled by the user via meta-data.
- crazy idea: we could also create a map of enum to T where T is a
  =std::function=. Something like a "dispatchable" class. Users can
  set functions and then call =dispatch(x)= which will execute the
  method at =to_index(x)=. No use case for it yet though.
- stroustrup proposes using operators with enums. However, its not
  clear there are good operators for all of the use cases above.

Links:

- [[https://www.ibm.com/developerworks/rational/library/scoped-enums/index.html][C++0x, scoped enums]]
- [[https://stackoverflow.com/questions/36937912/specializing-boostlexical-cast-for-enums-in-boostproperty-maps][specializing boost::lexical_cast for enums in boost::property_maps]]
- [[https://stackoverflow.com/questions/28828957/enum-to-string-in-modern-c11-c14-c17-and-future-c20][enum to string in modern C++11 / C++14 / C++17 and future C++20]]
- [[https://stackoverflow.com/questions/21295935/can-a-c-enum-class-have-methods][Can a C++ enum class have methods?]]

*** Bitmask enumeration                                               :story:

#+begin_quote
*Story*: As a dogen user, I want to define a bitmask enumeration in
dogen so that I don't have to create it manually.
#+end_quote

We should have a dynamic extension flag that generates enumerators
with values that are powers of two. These can then be used for flags,
as per the bitset story.

The metamodel element should reflect the enumeration type (flag or
not). The computation of the flags should take place as part of the
metamodel tranforms. If the user supplies values, validation must
ensure they are valid as flags (powers of 2). Values should be written
as hex values (unless users specify decimal). Actually we should
create a separate metamodel element for flags called =flags= or
=bitmasks= so that it is obvious at the metamodel element level that
these are distinct concepts.

Framework guidelines state that enumerators for flags should use
plural nouns or noun phrases and simple enums should use singular
nouns or noun phrases.

Notes:

- interesting methods for flags: is exactly one bit set, count on
  bits, are all bits on, are any bits on, turn all bits on/off. A flag
  builder would be nice as well: turn_on / turn_off(flag) which
  returns the updated flag:

: my_flags_builder b;
: const auto f(b.turn_on(my_flags::flag_a).
:   turn_on(my_flags::flag_b));

- it should be possible to have flags which represent commonly used
  flags, for example: read, write, read_write. In this case, the value
  of the flag must be a set of existing flags on the enumeration.
- validation should not allow setting flags value to zero. Issue a
  warning violating the framework design guidelines. However, there
  should be a flag to force this if users really need it.
- for flags, 0 must always mean "none", as in "all flags are
  cleared".

Links:

- [[https://dreamdota.com/explore-c-20-bitmask-design/][Explore C++20 Bit Flag with Designated Initializer and Concepts]]
- [[https://m-peko.github.io/craft-cpp/posts/different-ways-to-define-binary-flags/][Different Ways To Define Binary Flags]]

*** Support for file level comments via annotations                   :story:

#+begin_quote
*Story*: As a dogen user, I may want to add comments at the file level
so that I can provide documentation to the model users.
#+end_quote

We could easily have a tag for file level comments and transport that
all the way to the output. The only problem is that it would be a one
liner only so it may not be that useful.

Multi-line support could be simulated by concatenating multiple
entries - cumbersome but workable...

*** Add getter and setter prefixes                                    :story:

#+begin_quote
*Story*: As a dogen user, I want to change the default getter and
setter conventions so that I can integrate my code with
dogen-generated code.
#+end_quote

External users may have getter and setter prefix conventions such as
=set_prop= or =SetProp=. It would be nice if we could pass in a
getter/setting prefix and then dogen would append them when converting
the diagram, e.g. =--getter-prefix=set_=.

We should check what ODB has done for this and implement the same
pattern.

We should also look for some support in clang format, although it
seems very unlikely.

In fact this is related to the notion of "themes" we need in dogen. We
need to do some work around the possible "styles" that exist for
member variable naming, and then allow users to choose their
style. See [[*Add support for "capitalisation theming"][Add support for "capitalisation theming"]]

*** Add support for boost concept                                     :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate boost concepts so
that I don't have to manually create them.
#+end_quote

Now dogen supports concepts, the natural thing to do is to express
them in C++ code. This could easily be done using boost concept, or
the C++-14 concepts light.

See [[http://www.boost.org/doc/libs/1_53_0/libs/concept_check/creating_concepts.htm][Creating Concepts]].

It is important to be able to switch the concept generation off
too. We may want to create concepts for internal purposes but have no
need to actually express it in code. This could be suppressed via
meta-data.

*** Add documentation for concepts                                    :story:

It seems it is possible to document a concept in doxygen:

[[http://stackoverflow.com/questions/10087171/documenting-a-c-concept-using-doxygen][Documenting a C++ concept using doxygen?]]

*** Consider adding support for keys                                   :epic:

We had originally added some half-baked support for key
generation. The basic idea is that users could mark certain properties
in a class as being part of a key and dogen would automatically create
a key. The key could be versioned or unversioned. However, we never
had a use case for it and the feature was not implemented in a
convincing way. Having said that, it seems like a useful feature for
things such as caches etc so we may want to resurrect it as some point
in the future.

This was removed in 4ed3fbd.

Validation rules:

- ensure that we can only define identity once across concepts and
  parents
- the name of all keys in objects, etc must be part of the current
  model.
- the qnames of all types as keys are consistent with the values.
- entity must have at least one key attribute.
- non entity must not have key attributes (value, service)
- keyed must be entity.
- aggregate root must be entity.

Merged stories:

*Definition of Identity Attribute*

:    {
:        "name" : {
:            "simple" : "identity_attribute",
:            "qualified" : "yarn.dia.identity_attribute"
:        },
:        "ownership_hierarchy" : {
:            "model_name" : "yarn.dia"
:        },
:        "value_type" : "boolean",
:        "definition_type" : "instance",
:        "scope" : "property"
:    }

This was removed in commit =5e80256=:

yarn.dia: remove support for identity attribute

*Create a =key_extractor= service*

We need a way to automatically extract a key for a =keyed_entity=.
The right solution is to create a service to represent this
concept.

Injector creates objects for these just like it does with keys; the
C++ transformer intercepts them and generates the correct view models.

*Use explicit casting for versioned to unversioned conversions*

At present we have to_versioned; in reality this would be dealt much
better using explicit casts:

: explicit operator std::string() { return "explicit"; }

Actually the real solution for this is to make the versioned key
contain the unversioned key; then dogen will generate all the
required code.

At this point in time we do not have enough use cases to make the
correct design decisions in this area. We need to wait until we start
using keys in anger in Creris and then design the API around the use
cases.

It is not possible to use global cast operators so we need to
introduce a dependency between versioned and unversioned keys in order
for this to work.

*Consider not creating unversioned keys for single property*

If a key is made up of a single property, its a bit nonsensical to
create an unversioned key. We should only generate the versioned
key. However, it does make life easier. Wait for real world use cases
to decide.

*Add new equivalence operator to domain types that ignores version*

#+begin_quote
*Story*: As a dogen user, I would like to know if two objects are
equal ignoring the version properties so that I can model my domain
more accurately.
#+end_quote

We should have an operator that compares the state of two objects
ignoring the version (for versioned objects).

*** Add versioning support                                            :story:

#+begin_quote
*Story*: As a dogen user, I want to make changes to diagrams in a
backwards compatible way so that I can upgrade the users of my code
incrementally.
#+end_quote

We had some basic support for versioning but it was half-baked. It was
removed in 4ed3fbd.

Validation rules:

- is versioned objects must have a property called version.

*New understanding*:

- Add versioning support by adding versions at the object level and at
  the property level. Properties with 0 version will have no special
  handling. Properties with non-zero version (V) will have the
  following code added in serialisation:

: if (version > V)
:    // read or write property

- If a number of consecutive properties all share the same version,
  dogen will group them under the same version if. There will be no
  other special grouping or otherwise changing of order of properties.
- The object version will be max(version) of all properties for that
  class, excluding inherited properties.
- The object version will be stamped using boost serialisation class
  version macro, unless the object version is zero.
- Dogen will make no validation or otherwise dictate the management of
  version numbers; its up to the users to ensure they make sensible
  backwards compatibility decisions such as adding only new properties
  and always adding to the end.
- The model version is a human level concept and has no direct
  relation to class versioning. It will be implemented as an
  implementation specific parameter in the Dia model and as a string
  in the yarn model class. See "improve code generation marker story".
- Model version will be used for the following:
  - stamped on doxygen documentation for the model namespace;
  - stamped on DLLs, etc.
  - used by humans to convey the "type" of changes made to the
    diagram/model (e.g. a minor version bump is a small change, etc).

Previous understanding:

Versioning support is now available in yarn, so we need to apply it to
yarn itself. That is, we need a way of having two versions of an yarn
model coexist, and allow Dogen to diff those two versions to make code
generation decisions so that we can add basic backwards compatibility
support.

Before we can do this, we need a way of stamping a model version into
models. This can easily be done via implementation specific
parameters. See "improve code generation marker story".

We then need to create some kind of strategy for version number
management:

- minor bumps are backwards compatible; e.g. only adding new fields.
- major bumps are not backwards compatible: e.g. deleting fields,
  classes, etc.

However, at present we only support a single version number. Perhaps
we should just declare which versions are backwards compatible and
which ones are not.

Once all of these are in place we should add versioning support to
dogen:

- add a new command line argument: =--previous-version= or something
  of the kind.
- the model supplied by this argument must have the same name as the
  model supplied by =--target=.
- change all yarn types to be versioned.
- dogen will load up both models, and stamp the versions in each
  type. Merger will then be responsible for stamping the versions on
  each property, taking previous and new as input.
- for every field which is in new model but not in previous, add boost
  serialisation code to handle that.
- add unit tests with v1, v2 models.
- in order for dia diagrams with multiple versions to coexist in the
  same directory we will probably need to add the version to the
  diagram name, e.g. =sml_1.dia= or =sml_v1.dia=. We probably need
  some parsing code that looks for the last few characters of the file
  name and if it obeys a simple convention such as =_v= followed by a
  number, it ignores these for the model name and uses it for the
  version.

With this in place, when rebasing we can now do a proper comparison
between expected and actual.

Potential future feature: to put the files of different versions in
separate folders. This would allow the creation of "conversion" apps
which take types for one version and transform them into the next
version.

*** Add support for boost parameter                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of boost parameter so
that I can generate classes with named parameters without having to
manually create this code.
#+end_quote

It would be nice to have boost parameter support. [[http://www.boost.org/doc/libs/1_53_0/libs/parameter/doc/html/index.html#named-function-parameters][Documentation here]].

Ideally one would mark a type with a stereotype such as =named
parameter= and this would result in a full constructor with named
parameters. However since it seems one has to add a lot of boiler
plate code, perhaps its better to have a create function on a separate
header which internally calls the appropriate setters.

*** Add composite stereotype                                          :story:

#+begin_quote
*Story*: As a dogen user, I want to code generate composite objects so
that I don't have to do it manually.
#+end_quote

It would be nice if one could just mark a object as =composite= and dogen
automatically created the composite structure. As we only support
boost shared pointer that's what we'd use. We have a few use cases for
this (node, nested qname, etc).

This would be part of the injection framework.

Links:

- [[https://en.wikipedia.org/wiki/Composite_pattern][Composite pattern]]

*** Add support for bitsets                                           :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of bitsets in dogen so
that I don't have to manually generate code for types that use it.
#+end_quote

We are using a lot of boolean variables in yarn. In reality, these all
could be implemented with =std::bitset=, plus an enumeration. One
possible implementation is:

- add =std::bitset= to std model.
- create a new stereotype of bitset.
- classes with stereotype bitset are like enumerations, e.g. users are
  expected to add a list of names to the class.
- dogen will then implement the properties of type bitset as a
  =std::bitset= of the appropriate size, and also generate an
  enumeration which can be used for indexing the bitset. This may need
  to be a C++-03 enumeration, due to type safety in C++-11
  enumerations.
- we should also implement default bitsets with values corresponding
  to the flags.

Example usage:

#+begin_src c++
const unsigned int my_bitset_size(10);
std::bitset<my_bitset_size> bs;

bs[first_flag_index] = 1;
bs = first_flag_value;
#+end_src

Links:

- [[http://www.java2s.com/Tutorial/Cpp/0360__bitset/Usebitsetwithenumtogether.htm][Use bitset with enum together]]
- [[http://stackoverflow.com/questions/9857239/c11-and-17-5-2-1-3-bitmask-types][C++11 and {17.5.2.1.3} Bitmask Types]]
- [[https://www.justsoftwaresolutions.co.uk/cplusplus/using-enum-classes-as-bitfields.html][Using Enum Classes as Bitfields]]

*** Add string table support                                          :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate "enumerations" of
strings so that I don't have to create them manually.
#+end_quote

We need a way of creating "tables" of strings such as for example for
listing all the valid values for dia field names, etc. We could
implement this by creating a new stereotype where the name is the
string name and the default value is the string value. All strings
would be static public members of a class.

We should also add a validate method which checks to see if a string
is a valid value according to the string table. We could have a "case
insensitive" validate too.

Actually, we don't even need a new stereotype for this; we could just
use =enumeration= and detect when the underlying type is
string. However, as we can have many types of strings, this needs to
be configurable at the meta-model level:

- char*
- char[]
- string view
- std::string
- QString and other toolkit specific strings
- etc.

Presumably the families we have already set up could be useful for
this:

: "quilt.cpp.helper.family": "String",

We just need to promote these from backend helper families to
something at the meta-model level (type families?). However, the user
should still be able to configure how the "enumeration" comes out
(e.g. string, string view etc), but the defaults should be best we can
given the user options (version of C++ standard, etc).

The other interesting aspect is whether there should be an enumeration
connected to the string table (perhaps optionally) such that users
could =switch= and so forth. These could all be options whenever you
choose a type of the string family (create enumeration, create
converter for container of string, etc).

Validation rules:

- string table cannot have duplicate entries.

*** Enumeration string conversion could be configurable               :story:

#+begin_quote
*Story*: As a dogen user, I want to configure the input strings that
get converted into enumerations so that I can adapt it to my
requirements.
#+end_quote

It should be possible to pass in one or more string values as model
specific parameters that tell Dogen what valid values a user defined
enumerator can have. We can then generate a "from string" a method
that does the appropriate conversions.

These values should be passed in as dynamic extensions. At present
enumerators do not have dynamic extensions support so we need to add
it too. We could add a property to the meta-model: alias. Users can
associate one or more alises with an enumerator. These are added to
the element via meta-data. The from string method goes through the
list of aliases and uses them for the conversion.

Note that the inverse use case is also valid: one may want to map an
enumerator to one or more strings (e.g. "to string"). As with "from
string", a set of dynamic extensions could be use to supply the
mapping. Ideally, one enumerator should map to a single string but it
is conceivable there are other use cases (context sensitive
mappings). Note that it is not necessarily the case that the mapping
is bidirectional - i.e. "to string" is not required to be the inverse
function of "from string" and vice-versa. For the simple case where we
just want one mapping we could have something like "string value".

Notes:

- Use =string_view= on c++ 17.

*** Enumeration string dumps could be configurable                    :story:

#+begin_quote
*Story*: As a dogen user, I want to output user defined strings from
enumerations so that I can adapt it to my requirements.
#+end_quote

It should be possible to pass in a string value as a dynamic extension
that tells dogen what string to use for debug dumping. At present
enumerators do not have dynamic extensions support so we need to add
it too (e.g. add the concept to them).

*** Add is comparable to coding                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to define types as comparable so that
I can use them in ordered containers.
#+end_quote

A object can have a stereotype of comparable. If so, then at least one
property must be marked as comparable. Properties are marked as
comparable if they have an implementation specific parameter called
=comparison_order=. =comparison_order= is a sequence starting at 0 and
incrementing by 1; it determines the order in which properties are
compared between two objects of the same type.

In order for a property to qualify as a comparison candidate its type
must be:

- built-in;
- =std::string=;
- a object marked as comparable.

Some facts about comparable objects:

- they generate =operator<= as a global operator in the type
  header file.
- they can be keys in =std::map= and =std::set=.

Relation to keys:

- If all properties that are part of a key are also comparable then
  the key will be comparable.
- comparable versioned keys always compare the version after all other
  comparable properties.

If an object itself is marked as comparable, then it is equivalent to mark
all properties as comparable using their relative position as the
comparison order.

Validation:

- objects marked as is comparable must follow the [[*Add is comparable to coding][comparison rules]].

Notes:

- maybe we need to have multiple strategies for =operator<=. The
  simplest case is the user marks the properties in the class and we
  call =operator<= on those in the defined order. The second case is
  where the user wants to create a key from those properties, as an
  external class. Another case is where the user wants to hand-craft
  the operator using merging support (any of the several possible
  merging strategies).

*Merged with ordered containers story:*

In order to provide support for ordered containers such as maps and
sets we need to define =operator<=. However, it makes no sense to code
generate this operator as its unlikely we'll get it right. We could
assume the user wants to always sort by key, but that seems like a bad
assumption. The alternatives are:

- to expect a user-defined =entity_name_less_than.hpp= in domain. we'd
  automatically ignore any files matching this patter so the user can
  create them and not lose it. The problem with this approach is that
  we may have different sort criteria. This is a good YAGNI start.
- to provide the =Compare= parameter in the template and then expect a
  user-defined =entity_name_Compare.hpp=. The same ignore
  applies. This would allow users to provide any number of comparison
  operations.

Either approach requires support for ignoring files and folders based
on regex.

Another way of generating comparators is explained here:

[[http://playfulprogramming.blogspot.se/2016/01/a-flexible-lexicographical-comparator.html][A flexible lexicographical comparator for C++ structs]]

*** Add support for type framing                                      :story:

#+begin_quote
*Story*: As a dogen user, I want to send model types over the wire so
that I can build networked applications.
#+end_quote

In places such as a cache or a socket, it may be useful to create a
basic "frame" around serialised types. The minimum requirements for a
frame would be a model ID, a type ID, a "format" (i.e. xml, text, etc)
and potentially a size, depending on the medium. The remainder of the
frame would be the payload - i.e. the serialised object.

In order for this to work we probably need the concept of a "model
group"; the type frame would be done for a group of models.

This can be done with or without boost fusion. See this presentation
for details on a boost fusion approach:

- [[https://www.youtube.com/watch?v%3DwbZdZKpUVeg][CppCon 2014: Thomas Rodgers "Implementing Wire Protocols with Boost Fusion"]]

*** Add pimpl support                                                 :story:

#+begin_quote
*Story*: As a dogen user, I want to code generate PIMPL objects so
that I don't have to do it manually.
#+end_quote

It may be useful to mark classes as pimpl and generate a private
implementation. On the public header we could forward declare all
types.

*** Manual typedef generation                                         :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of manually generated
typedefs in automated code so that I don't have to manually generate
all code that makes use of these typedefs when when its trivial code.
#+end_quote

- We should be able to create a stereotype of =typedef group=. This is
  a object type with lots of attributes. The code generator will take
  the name and type of each attribute and generate a file with the
  name of the group and all the typedefs inside.
- We should be able to create a forward declarations like header that
  defines typedefs for =shared_ptr= etc at the users choosing. This
  could be implemented as a tag. We could create a =memory_fwd= header
  to avoid cluttering the main =fwd= file for the type. We will need
  another type of relationship to model this, as well as another type
  of file in tags; the file would then have several Boolean flags one
  can tick such as =std_shared_ptr=, =boost_shared_ptr= and so on.
- it should also be possible to add some dynamic extensions to an
  attribute and get it to generate a typedef, e.g. cpp.typedef = "xyz"
  would result in the creation of typedef xyz using the type of the
  attribute; getters, setters and property would then be declared with
  the typedef.
- actually we should have a "type alias" at the modeling level. This
  is just a simple mapping of a type to a type definition. This makes
  using types such as =dec:decimal<8>= etc possible without changing
  the parsing code.

*** Automatic typedef generation                                      :story:

#+begin_quote
*Story*: As a dogen user, I want dogen to generate typedefs so that I
don't have to create them manually.
#+end_quote

We should generate typedefs for all smart pointers, containers, etc -
basically anything that has template arguments. This would make
generated code much more readable and could also be used by client
code. In theory all we need is:

1. determine if the property has type arguments;
2. if so, construct the typedef name by adding =_type= to the property
   name, e.g. =attribute_value= becomes =attribute_value_type=, etc;
3. create a typedef section at the top of the class declaring all
   typedefs;
4. add a property to the property view model containing the typedef
   name and use it instead of the fully qualified type name.
5. we should also generate a typedef for the key if the class is an
   entity. See Typedef keys for each type.

We could also always generate a typedef for smart pointers in the
class that uses the smart pointer, with a simple convention such as
=attribute_value_ptr_type= or =shared_attribute_value_type=.

*** Add support for iterable enumerations                             :story:

#+begin_quote
*Story*: As a dogen user, I want enumerations to be iterable so that I
don't have to manually create code to do this.
#+end_quote

We should create an additional aspect for each enumerations which
creates a =std::array= with the enumerators (excluding invalid). This
would allow plugging the enumerations into for loops, boost ranges,
etc. The CPP should contain a static array; The HPP contains a method
which returns it, e.g. =my_enumeration_to_array.hpp=:

: std::array<my_enumeration, 5> my_enumeration_to_array();

We could make this slightly more generic by adding the notion of
enumeration groups. Out of the box we have:

- all: includes invalid;
- valid: excludes invalid

Users could then add implementation specific properties to create
other groups if needed.

Links:

- [[https://github.com/nadult/fwk_enum%0A][fwk_enum]]: Improved C++14 enum class with iteration and conversion to
  strings

*** Add support for enumeration subsets                               :story:

Sometimes we have a large enumeration which is made up of several
smaller subsets. For example in yarn we have well-known stereotypes;
these can be grouped into subsets such as element types, etc. It would
be nice to be able to declare these subsets in the meta-model and:

- code-generate automatically methods that extract all enumerations in
  the subset, not in the subset;
- create a bool function that given an enumerator tells us if it is in
  a subset or not.

*** Add support for load testing                                      :story:

Once we have proper support for test data sets (e.g. generate
realistic data) and once we have proper support for networking
(e.g. HTTP, etc), we should consider adding "load testing"
features. That is, we could generate requests (based on suitably
annotated types). We could offer some kind of base functionality for
load testing (requests per second, size, etc).

Links:

- [[https://github.com/sepehrdaddev/Xerxes][Xerxes GH]]: DOS tool
- [[https://ntietz.com/tech-blog/load-testing-is-hard-but-why/][Load testing is hard, and the tools are... not great. But why?]]: blog
  post on load testing. [[https://news.ycombinator.com/item?id=25641234][HN discussion]].
- [[https://github.com/tsenart/vegeta][vegeta GH]]: "Vegeta is a versatile HTTP load testing tool built out
  of a need to drill HTTP services with a constant request rate. It
  can be used both as a command line utility and a library."
- [[https://en.wikipedia.org/wiki/Apache_JMeter][Apache JMeter wikipedia]]: "Apache JMeter is an Apache project that
  can be used as a load testing tool for analyzing and measuring the
  performance of a variety of services, with a focus on web
  applications."
- [[https://en.wikipedia.org/wiki/Gatling_(software)][Gatling wikipedia]]: "Gatling is an open-source load- and
  performance-testing framework based on Scala, Akka and Netty."
- https://k6.io/: Open source load testing tool and SaaS
for engineering teams.

*** Add support for user supplied test data sets                      :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of test data without
having to manually add code for it.
#+end_quote

*New understanding*:

We need to create a test data sets model. it should have an
enumeration for all of the available test data sets, and an
enumeration for the valid file formats. we should be able to pass in a
pair of file formats (input, actual/expected) and out should come a
triplet of directories. This would make maintenance really easy as
we'd only need to add new strings to a string table. The service would
also handle things like the actual and expected directories, etc.

It should fix the following issues:

- [[*Adding%20new%20engine%20spec%20tests%20is%20hard][Adding new engine spec tests is hard]]
- [[*Naming%20of%20saved%20yarn/Dia%20files%20is%20incorrect][Naming of saved yarn/Dia files is incorrect]]

Actually we should also look into using the DTL library:

- [[https://github.com/cubicdaiya/dtl][diff template library written by C++]]

*Old understanding*:

The correct solution for test data and test data sets is as follows:

- the code generated by dogen in the test data directory is one of
  many possible ways of instantiating a model with test data.
- there are two types of instantiations: code and data. code is like
  dogen =test_data=; data is XML, text or binary - or any other
  supported boost archive; it also includes other external formats
  such as dia diagrams.
- a model should have a default enum with all the available test data
  sets: =test_data::sets=. If left to its default state it has only one
  entry (say =dogen=). The use is free to declare an enumeration on a
  diagram with the name test_data_sets and add other values to it.
- there must be a set of folders under test_data which match the
  enumerators of =test_data::sets=. Under each folder there must be an
  entry point such as =ENUMERATOR_generator=. Dogen will automatically
  ignore these folders via regular expressions.
- a factory will be created by dogen which will automatically include
  all such =ENUMERATOR_generator=. It will use static methods on the
  generator to determine what sort of capabilities the generator has
  (file, code, which formats supported, etc.) and throw if the user
  attempts to misuse it.
- all models must have a repository. Perhaps we need a stereotype of
  =repository= to identify it. This is what the factory will create.
- users will instantiate the factory and call =make=:

: my_model::test_data::factory f1;
: auto r = f1.make(my_model::test_data::sets::dogen);
:
: my_model::test_data::factory f2(expected_dir, actual_dir);
: auto r = f2.make(my_model::test_data_sets::some_set,
:   my_model::test_data::file_formats::boost_xml, file_locations::expected);

- if the user requires parsing a non-boost serialisation file then it
  should be make clear on the enum: =std_model, std_model_dia=. The
  second enumerator will read dia files. It will not support any file
  formats. The file must exist on either the expected or actual
  directory as per =file_locations= parameter.

Another topic which is also part of test data is the generation of
data for specific tests. At present we have lots of ad-hoc functions
scattered around different places. They should all live under test
data and be part of a test data set. The test data set should probably
be the spec name.

*** Add support for =std::function=                                   :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of =std::function= so
that I don't have to manually generate code for types that use it.
#+end_quote

At present its not possible to declare an attribute of type
=std::function= anywhere in a diagram. It won't really be possible to
do so for entities and values because boost serialisation will always
be a problem. If this was really a requirement, we could look into
serialising functions:

- [[https://groups.google.com/forum/?fromgroups%3D#!topic/boost-list/sHWRPlpPsf4][how to serialize boost::function]]

However we don't seem to need this quite just yet. What we do need is
a way of having attributes in services and that is slightly easier:

- the parser needs to be able to understand the function template
  syntax (e.g. =void(int)=). It seems this could be hacked easily
  enough into the parser.
- Nested qualified names need to be able to remember that in the case
  of a function, the first argument is a return type (they also need
  to know they represent a function). MC: is this actually necessary?
  all we need is to be able to reconstruct this syntax at format time.
- we need a =void= type in the built-ins model. This is a bit more
  complicated since this type can't have values, only pointers, and we
  don't really support raw pointers at the moment. Adding the type
  blindly would open up all sorts of compilation errors.

This should be sufficient for services. At present we have a hack that
allows functions without any template arguments, e.g. =std::function=,
in services.

One possible solution is to support the "portable syntax" as per
=boost::function=, .e.g. =std::function<a, b, c>=. This would make it
similar to what we already do for variants. The only other problem is
that we would not be able to support const, by ref etc. Yet another
solution is to have an operation syntax in UML but somehow convert the
operation into a type. For example, we could have a =masd::function=
stereotype, which is applied to an object that can only have a single
operation. Then you can refer to that as an attribute. The
implementation is dependent on the programming language. This could be
a LAM type. This solves the parsing issues, and gives us strong
metamodel level support for these types.

*** Add support for references and pointers to types                  :story:

#+begin_quote
*Story*: As a dogen user, I want to make use of references and
pointers so that I don't have to manually generate code for types that
use it.
#+end_quote

At present its not possible to create a type that has a reference to
another type. This should be a case of updating the parser to cope
with references and adding reference to property or nested type
name. This would be a good time to inspect our support for raw
pointers, it probably suffers from exactly the same problem and
requires the same solution.

In addition we should also bear in mind moving. Ideally one should be
able to declare moveable attributes and the end result should be a
setter that takes the type by =&&=. The question then is should we
also move on the getter? Sometimes it may not be a copyable type
(e.g. asio's =socket=).

It seems we can't also cope with =const= or pointers. To be fair we
only need const for shared pointer for now. On all cases we need to
make the parser more clever:

: boost::shared_ptr<const my_type>
: std::string&

We should try to create tests for all the cases we consider important
and mark them as ignore until we can find a spirit expert to help out.

Actually the right approach is to rely on UMLs facilities for these
such as =inout= etc. We should not have programming language level
constructs for these. In addition, for raw pointers we could create a
type, e.g. =masd::pointer= which is used to signify this intention.

*** Add support for immutable attributes                              :story:

Once we have support for defaults on attributes, we can consider
adding immutable attributes. These are attributes in mutable classes
which have been marked as immutable.

Validation:

- class must not be immutable (probably a mistake)
- attribute must have a default value.

*** Add support for static attributes                                 :story:

At present we cannot add attributes that are static to objects. This
should be fairly straightforward:

- add a "is static" property to attribute
- populate it by reading static information from front-end such as
  dia's field for "class scope".
- update templates to mark property as static.

Open questions:

- what do we do for serialisation? Maybe it only makes sense to have
  const static properties.

*** Immutable static attributes and inheritance                       :story:

In some cases it may make sense to attach an "attribute" at class
level to all derived classes, and provide a virtual function in the
base class to retrieve it. For example, we could have a "meta-id"
attached to the base class:

: virtual std::string base::meta_id() const = 0;

The "meta-id" is populated to reflect the derived class, like so:

: std::string derived::meta_id() override {
:     static const std::string r("derived");
:     return r;
: }

This pattern could be supported directly by the code-generator by
having a "meta_attribute" flag in the base class. The derived class
would then also define this attribute (possibly repeating the
"meta_attribute" flag for good measure) and supplying the default
value.

*** Shared pointers as keys in associative containers                 :story:

This is not supported; it would require generating the
hashing/comparison infrastructure for shared pointers. Further, as it
has been pointed out, keys should be immutable; having pointers as
keys opens the doors to all sorts of problems. We need to throw an
error at model building time if an user tries to do this.

*** Add support for =smartref=                                        :story:

This seems like an interesting library. Not exactly clear how to use
it, but to support it should be easy: add generated code that
registers user defined types:

: class Person
: {
: public:
:    auto first_name() {...}
:    auto last_name() {...}
: };
: REFLECTABLE(first_name);
: REFLECTABLE(last_name);

However, to properly use it we probably need support for protected
regions so that we can overload calls manually.

Links:

- [[https://github.com/erikvalkering/smartref][GitHub]]
- [[https://medium.com/@eejv/on-the-origin-of-smartref-proxy-pattern-f6fe53f41286][On the Origin of smartref: Proxy Pattern]]

*** Add support for MetaStuff                                         :story:

Interesting serialisation library:

- https://github.com/eliasdaler/MetaStuff.

Links:

- [[https://eliasdaler.github.io/meta-stuff/][How my little C++ meta-serialization library works and how I wrote
  it]]

*** Add support for Franca IDL                                        :story:

Franca is an IDL language for which there is an XText definition and a
C++ (spirit) implementation [[https://github.com/martinhaefner/franca][here]]. It may be useful as a gateway for
other IDLs.

*** Add support for D-Bus                                             :story:

[[https://en.wikipedia.org/wiki/D-Bus][D-Bus]] is an RPC mechanism used on Linux.

Links:

- [[https://github.com/dbus-asio/dbus-asio][dbus-asio]]: DBus C++ implementation based on boost asio. [[https://fosdem.org/2019/schedule/event/dbus_asio/][FOSDEM talk]].
- [[https://gitlab.freedesktop.org/dbus/dbus][libdbus]]: original D-Bus C library.

*** Add support for Asio uring                                        :story:

#+begin_quote
Asio Uring adapts the io_uring functionality added in the Linux kernel
5.1 to the model of Boost.Asio with the framework laid for future
adaptation to the model of the Networking TS. This enables native
asynchronous file I/O within the context of Boost.Asio.
#+end_quote

Links:

- [[https://github.com/RobertLeahy/AsioUring][AsioUring GH]]

*** Add support for Microsoft Bond                                     :epic:

More of a placeholder - will require further investigation. Seems like
Microsoft has their own Thrift/ProtoBuf-like implementation called
[[https://github.com/Microsoft/bond/][Bond]]. It supports [[https://microsoft.github.io/bond/manual/bond_cpp.html][C++]]. Once could conceive Dogen support for it.

*** Add support for Aeron                                             :story:

Yet another RPC like framework.

Links:

- https://github.com/real-logic/aeron

*** Consider adding support for Boost.Operators                       :story:

It may or may not make sense to add support for [[http://www.boost.org/doc/libs/1_58_0/libs/utility/operators.htm][Boost.Operators]]. There
is a bit of boilerplate which seems easy to code-generate; however,
the point of adding operators is surely that one needs some
hand-crafted functionality. At any rate, this story keeps track of the
thinking on this topic. Articles:

- [[http://arne-mertz.de/2015/01/operator-overloading-the-basics/][Operator Overloading: The Basics]]
- [[http://arne-mertz.de/2015/01/operator-overloading-common-practice/][Operator Overloading: Common Practice]]
- [[http://arne-mertz.de/2015/02/operator-overloading-introduction-to-boost-operators-part-1/][Operator Overloading – Introduction to Boost.Operators, Part 1]]
- [[http://arne-mertz.de/2015/02/operator-overloading-introduction-to-boost-operators-part-2/][Operator Overloading – Introduction to Boost.Operators, Part 2]]
- [[http://arne-mertz.de/2015/03/operator-overloading-introduction-to-boost-operators-part-3/][Operator Overloading – Introduction to Boost.Operators, Part 3]]

*** Support for COM/CORBA                                              :epic:

#+begin_quote
*Story*: As a dogen user, I want to make use of COM/CORBA so that I
can create code to interface with legacy systems.
#+end_quote

We should investigate how hard it is to add support for these IDLs.

*** Support for pretty printing libraries                             :story:

[[https://github.com/p-ranav/pprint][pprint]] is a library for pretty printing of C++ objects. It could be
used as an alternative for our current JSON-based approach to IO. It
appears there are [[https://news.ycombinator.com/item?id=19780725][a number of similar libraries]].

Links:

- [[https://github.com/p-ranav/pprint][pprint]]: github repo.
- [[https://github.com/tfc/pprintpp][pprintpp]]: Typesafe Python Style Printf Formatting for C++.
- [[https://github.com/calebzulawski/hippo][hippo]]: Hierarchical Information Pretty-Printer Objects. More macro
  based.
- [[https://github.com/tcbrindle/pretty_print.hpp][pretty_print.hpp]]: another single-header C++14 library for printing
  STL-compatible container types to IO streams.
- [[https://lwn.net/Articles/815265/][Improving pretty-printing in Python]]: some ideas on the problem but
  for python.

** Miscellaneous

Stuff that is not directly related to the problem domain but its
needed by Dogen. Tooling, infrastructure, blog posts, etc. However,
they must still be in keeping with the vision, so not too left field.

*** Consider adding support for


*** Consider cross-platform building                                  :story:

At present we use windows and appveyor for windows builds. This is a
bit painful because we don't have a windows machine. It would be nicer
if we could install clang/llvm cross platform tools, including a
debugger, which could run on wine and build windows packages cross
platform. If we could do the same thing for OSX, then we would only
have to support Linux. This would also mean we could setup vcpkg
builds in docker containers with the cross-platform environment.

Links:

- [[https://gitlab.gnome.org/GNOME/msitools][GH msitools]]: set of programs to inspect and build Windows Installer
  files.
- [[https://github.com/tpoechtrager/osxcross][GH osxcross]]: Mac OS X cross toolchain for Linux, FreeBSD, OpenBSD
  and Android (Termux)
- [[https://askubuntu.com/questions/1117461/how-do-i-create-a-dmg-file-on-linux-ubuntu-for-macos][SO How do I create a DMG file on linux Ubuntu for MacOS]]
- [[https://stackoverflow.com/questions/286419/how-to-build-a-dmg-mac-os-x-file-on-a-non-mac-platform][SO How to build a dmg Mac OS X file (on a non-Mac platform)?]]
- [[http://www.elstensoftware.com/blog/2013/04/17/scripting-dmg-build-osx-linux/][Scripting DMG builds on OS X from Linux]]
- [[https://www.reddit.com/r/cpp/comments/p00vrk/github_actions_to_setup_gccclangmingww64/][reddit: GitHub actions to setup GCC/Clang/MinGW-w64]]

*** Log file per transform                                            :story:

At present we have a good view of the graph of the chain when tracing
is enabled. We then use the GUIDs to find the details of the
transformation in the log. It would be even easier if somehow it was
possible to have sinks in boost log that are added for each transform
and removed at the end of it. This way we could generate "mini log
files" that contain all of the activity that occurred in that
transform.

Boost log seems to be setup for it, we'd just have to manipulate the
logger from within the tracer. One slight problem is that we'd only
want to log from "leaf transforms". This probably means that we need
to make sure that only transform chains can call other transforms.

In theory all we'd have to do is to get the tracer to do the hooking
and unhooking for each transform. It should already know about chains
vs transforms.

*** Consider adding pre-commit hooks                                  :story:

Its not clear how these work with github but they seem interesting.

Links:

- [[https://github.com/pocc/pre-commit-hooks][pre-commit-hooks]]

*** Add fuzzing tests                                                 :story:

We need to setup some basic fuzzing tests for model inputs.

Links:

- [[https://nullprogram.com/blog/2019/01/25/][The Day I Fell in Love with Fuzzing]]

*** Updates to debian package                                         :story:

There are several problems with the debian package:

- shared folder is =dogen= not =masd.dogen=
- no hello world sample; need json and dia versions
- package name is =dogen-applications=, should be masd...

*** Add stitch section in manual                                      :story:

We need to document stitch:

- formal definition of the language and its limitations;
- command line usage of the tool.
- describe the t4 grammar, our similarities and differences. See the
  stories around using t4 terminology in sprint 64 and 68 (most
  important bits copied below).
- describe available directives.
- note on how we don't support class feature blocks and how we use the
  class feature block start marker to mean standard control block
  start marker.

Relevant comments from previous stories:

We found [[https://msdn.microsoft.com/en-us/library/bb126478.aspx][a page]] documenting the elements of T4. These are:

- *Directives*: Text template directives provide general instructions to
  the text templating engine about how to generate the transformation
  code and the output file.
- *Text blocks*: A text block inserts text directly into the output
  file. There is no special formatting for text blocks.
- *Control blocks*: Control blocks are sections of program code that
  are used to transform the templates. Two types:
  - *Standard control blocks*: A standard control block is a section
    of program code that generates part of the output file.
  - *Expression control blocks*: An expression control block evaluates
    an expression and converts it to a string.

Additional definitions we made up because we could not find anything
suitable in documentation:

- *Block*: one of: text block, control block or any of its descendants.
- *Statement*: either a directive or a control block.
- *Inline statement*: statement that starts and ends in one line.
- *Marker*: one of <#, <#@, <#=, #>. Mark-up that delimits statements.
- *Start Marker*: one of <#, <#@, <#=. Can also be specialised to
  "start X marker", e.g. start control block marker is <#, and so on.
- *End Marker*: #>. Can also be specialised to "end X marker",
  e.g. end directive marker is #>.

*** Diagrams used in manual should be in docs                         :story:

Users should be able to follow the examples in the manual by using a
set of diagrams supplied in the dogen package. However, to ensure
these samples are actually working we need to test them as part of
sanity. This means we need the same diagrams packaged twice.

*** Tidy-up dogen windows package                                     :story:

There are a few inconsistencies with the package:

- dogen components have a strange structure:
  "Dogen/runtime/dogen".
- we should probably have a top-level umbrella for MASD, under which
  dogen installs.
- package name is windows amd64. We should use the vcpkg triplets for
  simplicity (e.g. x64-windows).

*** Add project documentation                                         :story:

We should be able to create a simple set of docs following on from the
[[https://ned14.github.io/outcome/][outcome project]]. They seem to be using Hugo.

Links:

- https://github.com/foonathan/standardese
- https://github.com/ned14/outcome/tree/develop/doc/src

*** Create a =ci= folder in build                                     :story:

We should use the same approach as nupic for organising the scripts: a
top-level =ci= folder with folders per CI system. We should also
follow their naming convention for the build scripts which seem to
follow the CI events.

Links:

- https://github.com/numenta/nupic.core/tree/master/ci

*** Fix issues with bintray windows uploads                           :story:

At present we are doing a lot of hacks for windows:

- hardcoding the path to the package
- not uploading on just tags
- uploading to the top-level folder instead of the version.

Ideally we want to reuse the Travis BinTray descriptor but AppVeyor
does not support this directly.

*** Add support for ODB schema version                                :story:

ODB has good support for schema migration. We need to make sure Dogen
uses it.

Links:

- [[https://www.codesynthesis.com/products/odb/doc/manual.xhtml][C++ Object Persistence with ODB]]: section "13.1 Object Model Version
  and Changelog" and "13.2 Schema Migration"

*** Add proper DEB support to bintray                                 :story:

At present we are uploading our Debian package to a "generic" area in
bintray. This has served us well so far but has one shortcoming: we
need to manually update packages every time there is a new release,
requiring change of paths, manual downloads, =apt-get remove= etc. It
would be much easier to upload the package as a DEB and then use
apt-get to install it. We just need to look into the bintray
descriptors to figure out how to upload it to the correct repository.

*** Add scripts to sanity check packages                               :epic:

Create a set of scripts that validate packages:

- install packages on all supported platforms.
- sanity check that package installed correctly, e.g. check for a few
  key files.
- run sanity tests, e.g. create a dogen model and validate the results
- run uninstaller and sanity check that files are gone
  - this should actually be a build agent so we can see that deployment
    is green. We should create a deployment CMake script that does this.

We had some initial support for this in commit ba54859.

*** Consider adding profiling support via build type                  :story:

At present we used a dogen specific variable for profiling:
=WITH_PROFILING=. However, we could extend the cmake build type
mechanism with a =Profiling= build type.

See:

- [[https://github.com/Milkyway-at-home/milkywayathome_client/blob/master/cmake_modules/Profiling.cmake][milkywayathome_client's profiling for CMake]]

*** Add support for coverity analysis                                 :story:

We've created a project and added the badge but we have not yet setup
travis for this.

- [[https://scan.coverity.com/projects/domaindrivenconsulting-dogen/builds/new?tab%3Dtravis_ci][Automate Analysis with Travis CI]]
- [[https://github.com/d-led/picojson_serializer/blob/master/.travis.yml][example .travis.yml]]

We should upload a build whenever we tag a release.

Badge:

: [![Coverity](https://img.shields.io/coverity/scan/16865.svg)](https://scan.coverity.com/projects/masd-project-dogen)

Patch:

#+begin_example
+env:
+  global:
+    # COVERITY_SCAN_TOKEN
+    secure: "VwgaUWxbt+sGiYNlBWVOcAQOD6BNKYi2+T4Sco0FBddAClDRjOMHBEA4C2DUsK8dbKcihT+YPlz2CnAw/nIn3mn5cVy3Tf0X8ber06nRXMfKTOU84raX8bKNUCaU9tkiqdTeNLwchQutOk5Bgvbt5nRDUwEEOaCexh44vAIymzE="
+
 matrix:
   include:
     #
@@ -62,6 +67,32 @@ addons:
     sources:
       - ubuntu-toolchain-r-test

+addons:
+  coverity_scan:
+
+    # GitHub project metadata
+    # ** specific to your project **
+    project:
+      name: my_github/my_project
+      version: 1.0
+      description: My Project
+
+    # Where email notification of build analysis results will be sent
+    notification_email: marco.craveiro@gmail.com
+
+    # Commands to prepare for build_command
+    # ** likely specific to your build **
+    build_command_prepend: ./configure
+
+    # The command that will be added as an argument to "cov-build" to compile your project for analysis,
+    # ** likely specific to your build **
+    build_command: make
+
+    # Pattern to match selecting branches that will run analysis. We recommend leaving this set to 'coverity_scan'.
+    # Take care in resource usage, and consider the build frequency allowances per
+    #   https://scan.coverity.com/faq#frequency
+    branch_pattern: coverity_scan
+
 install:
   - export SCRIPTS_DIR=${TRAVIS_BUILD_DIR}/build/scripts
   - source ${SCRIPTS_DIR}/travis.install.${TRAVIS_OS_NAME}.sh
@@ -106,6 +137,25 @@ notifications:
     on_success: always
     on_failure: always

+
+
+env:
+  global:
+
+   - COVERITY_SCAN_BRANCH_PATTERN="master"
+   - COVERITY_SCAN_NOTIFICATION_EMAIL="marco.craveiro@gmail.com"
+   - COVERITY_SCAN_BUILD_COMMAND_PREPEND="mkdir -p coverity-build && cd coverity-build && cmake .. && cd .."
+   - COVERITY_SCAN_BUILD_COMMAND="make -C coverity-build"
+   - COVERITY_SCAN_PROJECT_NAME="$TRAVIS_REPO_SLUG"
+
+
+before_script:
+    - mkdir -p coverity-build && cd coverity-build && cmake .. && cd ..
+    - if [[ "x${CC}" = "xclang" ]] ; then curl -s 'https://scan.coverity.com/scripts/travisci_build_coverity_scan.sh' | bash || true ; fi
+
+
+
+
#+end_example

*** Link against boost dynamic libraries                              :story:

At present we are statically linking against Boost just to make our
life easier. This was done mainly to fix the problems we had with
Travis red builds but had the side effect of fixing our Debian
packages. However, as soon as boost 1.60 is available on Travis we
probably should go back to dynamic linking. Removed code:

:    # elseif (UNIX)
:    #     if (library_path MATCHES "boost")
:    #         set(version "${Boost_MAJOR_VERSION}")
:    #         set(version "${version}.${Boost_MINOR_VERSION}")
:    #         set(version "${version}.${Boost_SUBMINOR_VERSION}")
:
:    #         get_filename_component(library_name ${library_path} NAME_WE)
:    #         string(REGEX REPLACE "_" "-" library_name "${library_name}")
:
:    #         # FIXME: boost 1.55 is not the default in unstable yet.
:    #         # set(library_name "${library_name}-dev")
:    #         set(library_name "${library_name}${Boost_MAJOR_VERSION}.")
:    #         set(library_name "${library_name}${Boost_MINOR_VERSION}-dev")
:    #         if ("${boost_deps}" STREQUAL "")
:    #             set(boost_deps "${library_name} (>= ${version})")
:    #         else()
:    #             set(boost_deps "${boost_deps}, ${library_name} (>= ${version})")
:    #         endif()
:    #     endif()

See also main CMakeLists.txt around =6b837334c3247bee2f6be8099cd5288a284fa36c=.

Or is the best practice that the user should choose how to link
against boost in the same way it chooses to generate static or dynamic
libraries internally for Dogen? This seems like a better approach; we
could just add parameters to our shell scripts that build statically
for the build machine.

*** Enable doxygen warnings for all undocumented code                 :story:

At present doxygen only warns about undocumented parameters when a
function already has documented parameters. We should consider
enabling warnings for all undocumented code. We also need to figure
out how to mark code as ignored (for example serialisation helpers,
etc won't require documentation).

*** Self-contained build files                                        :story:

#+begin_quote
*Story*: As a dogen user, I would like to build models without having
to add any code so that I use dogen without needing to learn lots
about build systems.
#+end_quote

It would be nice to be able to generate a complete application from a
given model, or a library. At present there is an expectation that the
user will slot in the generated CMake files into a larger, more
comprehensive CMake build. All we need is:

- some kind of binary type: e.g. executable or library. we should have
  this anyway. meta data at the model level could be used to convey
  this.
- if executable, we should automatically ignore a main.cpp in the
  source directory.
- generate a stand-alone CMake template.

The idea is that with this the user could immediately generate a
binary without any further configuration required.

*** Remove calls to unix utilities directly                           :story:

We have a couple of cases where we are calling UNIX utilities when
there are equivalent CMake calls. For example
=test_data/CMakeLists.txt=:

:        set(target_name "rebase_${model_name}")
:        if(EXISTS "${git_expected}/")
:            add_custom_target(${target_name}
:                COMMAND rm -rf ${expected}/*
:                COMMAND cp ${actual}/* ${expected}
:                COMMAND cp ${actual}/* ${git_expected}/
:                WORKING_DIRECTORY ${stage_bin_dir})

These can be replaced with:

:        file(REMOVE_RECURSE ${stage_test_data_dir}/${dir}/expected)
:        file(COPY ${curdir}/${dir} DESTINATION ${stage_dir}/test_data)
:        file(MAKE_DIRECTORY ${stage_test_data_dir}/${dir}/actual)

etc.

*** Detect the presence of the diff command                           :story:

The diff targets is dependent on the presence of the diff command, but
we are not checking to see if it exists:

:        set(target_name "diff_${model_name}")
:        add_custom_target(${target_name}
:            COMMAND diff -r -u ${expected} ${actual}
:            WORKING_DIRECTORY ${stage_bin_dir})
:        add_dependencies(diff_dataset ${target_name})

We need to detect it and only add this targets if diff has been
found. We probably should look for a =FindDiff.cmake= script.

*** Installable package names should follow a well-known convention   :story:

We need to make sure our package names are consistent with the
platform conventions.

- [[http://pastebin.com/TR17TUy9][Example of platform IFs]]
- [[http://libdivsufsort.googlecode.com/svn-history/r6/trunk/CMakeModules/ProjectCPack.cmake][Example CPack]]
- [[http://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.htmlhttp://cmake.3232098.n2.nabble.com/Automatically-add-a-revision-number-to-the-CPack-installer-name-td7356239.html][Automatically add a revision number to the CPack installer name]]
- [[http://www.cmake.org/Wiki/CMake:CPackConfiguration][CPack Configuration]]

There are some known limitations in package naming:

- [[http://public.kitware.com/Bug/view.php?id%3D12997][0012997: Provide a more flexible way to name package produced by CPack]]

*** Allow user to choose whether to build shared or static libraries  :story:

We removed the hard-coding, but the code does not compile due to
problems linking Boost Log:

- [[https://github.com/Microsoft/vcpkg/issues/6148][Errors building shared library due to Boost Log and PIC]]

*Previous understanding*

#+begin_quote
*Story*: As a dogen user, I want to generate models as shared objects
so that I don't have to statically link models all the time.
#+end_quote

With the increase in tests build speeds have started to suffer,
especially on low hardware. One potential way to mitigate this is to
avoid unnecessary linking. The problem we have at present is that
every time something changes in any model we have to relink all the
binaries that use that model as it is consumed as a static library. If
all the static libraries were converted to shared objects this would
no longer be necessary.

We probably need a dogen command line option to determine what to
build so that users are not forced to always build static / shared
libraries. We should make sure one of the tests is using a static
library to make sure this scenario doesn't get borked.

Actually we need to figure out how to pass this as a parameter into
CMake to let the user configure how the libraries are generated
(static or dynamic). Look for a best practices presentation on CMake
that talks about this.

Merged Stories:

*Build static and dynamic libraries*

At present we are always building static libraries from
Dogen. However, CMake supports both.

Links:

- [[http://stackoverflow.com/questions/2152077/is-it-possible-to-get-cmake-to-build-both-a-static-and-shared-version-of-the-sam][Is it possible to get CMake to build both a static and shared
  version of the same library?]]
- [[https://github.com/friendlyanon/cxx-static-shared-example][GH cxx-static-shared-example]]: "CMake Example: support building as
  either shared or static".
- [[https://alexreinking.com/blog/building-a-dual-shared-and-static-library-with-cmake.html][Building a Dual Shared and Static Library with CMake]]

*** Migrate from Boost Test to Catch                                  :story:

The [[https://github.com/philsquared/Catch][Catch]] testing framework appears to be very nice:

[[https://vimeo.com/131632252][Testdriven C++ with Catch - Phil Nash]]

We should look into moving from Boost test to Catch. We should also
take the opportunity to extend catch with a few [[https://github.com/philsquared/Catch/tree/master/include/reporters][reporters]]:

- boost log reporter such that all the logging from unit testing goes
  into the log file and the log file is setup automatically for each
  scenario.
- "emacs" reporter such that we get compilation error like
  messages. This may already be in Catch.
- CDash reporter so that we don't have to do all of the CMake magic in
  order to see the unit tests in CDash

Merged Stories:

We should check the Martin Fowler article on [[http://martinfowler.com/bliki/GivenWhenThen.html][Given-When-Then]] as a way
of specifying unit tests, to see if it would make our tests
clearer. We are already following some kind of Given and Then, but we
should consider making it explicit.

The best approach may be to move all unit tests to [[https://github.com/philsquared/Catch][Catch]], since it
natively supports GWT.

*** Add a GitHub page for Dogen                                       :story:

We should be able to create a simple project page for Dogen by using
the automatic project generator:

https://pages.github.com/

However, to do so we probably should convert the README into MarkDown
first. It is used to generate the project page's contents.

*** Add Doxygen docs to GitHub page                                   :story:

Once we setup the Dogen page, we should automatically upload the
Doxygen documentation there. This should be done from Travis:

- setup a Doxygen build in Travis that builds the docs into staging,
  then changes branch into =gh-pages=, then =rm -rf= the previous docs
  and copies across the new docs then commits and pushes.
- we need to make sure the docs are not very large.

Example from RapidJSON available [[https://github.com/miloyip/rapidjson/blob/master/travis-doxygen.sh][here]].

This can be done by adding a build to the matrix with a
=DOCUMENTATION= env variable. When this is on, the travis build builds
and deploys documentation. We should only do this when tagging.

Merged Stories:

*Add doxygen build in travis using GitHub pages*

It seems is pretty straightforward to add a doxygen build to travis:

- [[http://blog.gockelhut.com/2014/09/automatic-documentation-publishing-with.html][Automatic Documentation Publishing with GitHub and TravisCI]]

See also [[https://github.com/tgockel/nginxconfig/blob/master/config/publish-doxygen][this script]].

*** Add vagrant and docker support                                    :story:

It would be nice to provide vagrant and docker support to dogen in
terms of development. The idea is that you can =git clone= the repo
and then =vagrant up= it and you would be ready to start coding. This
would make drive-by patches much easier.

*** Use clang format to format the code base                          :story:

It seems clang-format is being used by quite a lot of people to save
time with the formatting of the code. More info:

http://clang.llvm.org/docs/ClangFormat.html

Emacs support:

https://llvm.org/svn/llvm-project/cfe/trunk/tools/clang-format/clang-format.el

Links:

- [[https://github.com/STEllAR-GROUP/hpx/blob/master/.clang-format][HPX clang format]]
- [[https://engineering.mongodb.com/post/succeeding-with-clangformat-part-1-pitfalls-and-planning][Succeeding With ClangFormat, Part 1: Pitfalls And Planning]]
- [[https://github.com/basiliscos/cpp-rotor/blob/master/.clang-format][example: clang format in rotor]]
- [[https://github.com/jbapple-cloudera/clang-format-infer][clang-format-infer GH]]
- [[https://zed0.co.uk/clang-format-configurator/][clang-format-configurator]]
- http://clangformat.com/
- [[https://github.com/johnmcfarlane/unformat][Unformat]]: Python3 utility to generate a .clang-format file from
  example code-base.
- [[https://www.reddit.com/r/cpp/comments/pnli5r/cc_precommit_hooks_for_static_analyzers_and/][C/C++ pre-commit hooks for static analyzers and linters]]

*** Consider adding an emblem for clang's analysis checks             :story:

NeoVim appears to have emblems for waffle as well as clang's analysis
checks:

https://github.com/neovim/neovim

*** Consider adding nuget support                                     :story:

Seems like AppVeyor supports deployment into nugget. Example:

http://www.nuget.org/packages/RxCpp/

It also comes with a couple of useful emblems:

https://github.com/Reactive-Extensions/RxCpp

We should push both the C++ libraries as well as the dogen binary.

Merged stories:

*Add nuget support to bintray*

It would be nice to add a nuget "repo" (if that is the right term) to
bintray, so that users can install Dogen from nuget. It should support
MSIs - but if it doesn't we may need to manually package Dogen as a
=nupkg=. This is not too difficult: we need the descriptor and some
basic shell scripts.

*** Consider hosting documentation in read the docs                   :story:

We should consider hosting the documentation here:

- https://readthedocs.org/

This probably means we need to move to Mark Down.

*** Consider adding support for clang-tidy                            :story:

As [[http://clang.llvm.org/extra/clang-tidy/][per docs]]:

#+begin_quote
clang-tidy is a clang-based C++ “linter” tool. Its purpose is to
provide an extensible framework for diagnosing and fixing typical
programming errors, like style violations, interface misuse, or bugs
that can be deduced via static analysis. clang-tidy is modular and
provides a convenient interface for writing new checks.
#+end_quote

As with clang-format, we should create a meta-model element to
generate this file.

See also:

- [[https://github.com/polysquare/clang-tidy-target-cmake][clang-tidy-target-cmake]]
- [[https://www.kdab.com/clang-tidy-part-1-modernize-source-code-using-c11c14/?utm_source%3DMaster%2520List%252006-16&utm_campaign%3Dd11fea20e3-EMAIL_CAMPAIGN_2017_03_23&utm_medium%3Demail&utm_term%3D0_bdde4cdc11-d11fea20e3-101553725&goal%3D0_bdde4cdc11-d11fea20e3-101553725][Clang-Tidy, part 1: Modernize your source code using C++11/C++14]]
- [[https://github.com/STEllAR-GROUP/hpx/blob/master/.clang-tidy][hpx clang tidy file]]
- [[https://github.com/lballabio/QuantLib/pull/1369][Split .clang-tidy into one for fixes and one for checks #1369]]

*** Add support for the =scan-build= static analyser                  :story:

=scan-build= is a command line utility that enables a user to run the
static analyzer over their codebase as part of performing a regular
build.

Links:

- [[%20http://clang-analyzer.llvm.org/scan-build.html][scan-build]] project page

*** Add support for iwyu                                              :story:

There is a clang based tool that checks if which includes are actually
used by the translation unit. We should have a build for this that
breaks whenever one includes something which is not required.

- [[https://include-what-you-use.org/][include-what-you-use]]
- [[https://github.com/include-what-you-use/include-what-you-use][iwyu project page]]
- [[http://mpd.jinr.ru/svn/mpdroot/trunk/cmake/modules/FindIWYU.cmake][FindIWYU.cmake]]
- [[https://github.com/christophgysin/addp/blob/master/cmake/iwyu.cmake][iwyu.cmake]]
- [[https://github.com/clangd/clangd/issues/79][IWYU: Unused headers/missing includes #79]]

build.linux:

: cmake_defines="${cmake_defines} -DCMAKE_CXX_INCLUDE_WHAT_YOU_USE=/path/to/include-what-you-use/build/include-what-you-use"

Patches:

: diff --git a/CMakeLists.txt b/CMakeLists.txt
: index 7489d14..863162f 100644
: --- a/CMakeLists.txt
: +++ b/CMakeLists.txt
: @@ -161,7 +161,7 @@ target_link_libraries(include-what-you-use
:    LLVMObject # BitReader, Core, Support
:    LLVMBitReader # Core, Support
:    LLVMCore # BinaryFormat, Support
: - LLVMBinaryFormat # Support
: - # LLVMBinaryFormat # Support
:   LLVMSupport # Demangle
:   LLVMDemangle
:   )
: diff --git a/iwyu_include_picker.cc b/iwyu_include_picker.cc
: index 840057f..232a5c6 100644
: --- a/iwyu_include_picker.cc
: +++ b/iwyu_include_picker.cc
: @@ -832,7 +832,7 @@ void MakeNodeTransitive(IncludePicker::IncludeMap* filename_map,
:      // in /internal/.  We could CHECK-fail in such a case, but it's
:      // probably better to just keep going.
:      if (StartsWith(key, "\"third_party/") ||
: -        key.find("internal/") != string::npos) {
: +        key.find("boost/") != string::npos) {
:        VERRS(4) << "Ignoring a cyclical mapping involving " << key << "\n";
:        return;
:      }

*** Add support for clang sanitizers                                  :story:

This seems like another interest dynamic analysis tool:

- [[https://code.google.com/p/address-sanitizer/wiki/AddressSanitizer#Introduction][Address Sanitizer]]
- [[https://clang.llvm.org/docs/ThreadSanitizer.html][Thread Sanitizer]]
- [[https://clang.llvm.org/docs/MemorySanitizer.html][MemorySanitizer]]
- [[https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html][UndefinedBehaviorSanitizer]]

Links:

- [[https://www.reddit.com/r/cpp/comments/jzky8o/what_linters_do_you_recommend/][What linters do you recommend?]]

*** Add support for CPPCheck                                          :story:

Seems like CPPCheck has a different take on dynamic analysis when
compared to Valgrind. We should look into how hard it is to integrate
it with CTest, Travis, etc.

Links:

- [[http://cmake.3232098.n2.nabble.com/Static-code-analysis-with-CDash-td6079787.html][CMake and CPPCheck]]

*** Create a demo of installing dogen and generating hello world      :story:

#+begin_quote
*Story*: As a dogen user, I want basic instructions on using the
program so that I don't have to figure it all out by myself.
#+end_quote

We need to start creating a series of quick videos demoing dogen. The
script for the first video of the series is as follows:

- download packages from GitHub and install them.
- obtain the hello world model from git.
- generate the hello world model.
- create a hello world main with make files and compile it.
- give a quick overview of the available facilities.
- update hello world to latest diagram version, with right colour
  theme.

*** Mine MongoDB Smasher for test data ideas                          :story:

There is a project to generate large data sets for MongoDB:

- [[https://github.com/duckie/mongo_smasher][mongo_smasher]]

We should have a look at their feature set and see if any of the
features make sense for dogen. It would be great to have a generalised
"service smasher" based on a service model.

*** Write blog post on reflexive programming                          :story:

Basic idea is that domain expertise tightens the agile circle,
allowing for much faster progress. Check Soros definition of
reflexive.

*** Read A Concept Design for the STL                                 :story:

It seems this paper has a lot of useful information for meta-modeling:

- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3351.pdf][A Concept Design for the STL]]

** Visionary

These stories are far in the future or just plain crazy: visionary
work and random ideas. They are require fundamental rethinking of the
core, or are not obviously related to dogen's current vision.

*** Investigate =lombok= for ideas                                    :story:

#+begin_quote
Project Lombok is a java library that automatically plugs into your editor and
build tools, spicing up your java. Never write another getter or equals method
again, with one annotation your class has a fully featured builder, Automate
your logging variables, and much more.
#+end_quote

Links:

- https://projectlombok.org/features/all

*** Consider adding support for                                       :story:

#+begin_quote
crunchy_bytes is a C++ 17 library that serializes structured data.

You bring the data definitions, and crunchy_bytes will generate C++ classes that
can serialize and deserialize themselves into a compact binary representation
that can be easily be transferred between processes.

crunchy_bytes is ideal for IPC, or long term data storage
#+end_quote

Links:

- [[https://github.com/crabmandable/crunchy-bytes][crunchy-bytes GH]]

*** Consider adding support for asio-grpc                             :story:

#+begin_quote
An Executor, Networking TS and std::execution interface to grpc::CompletionQueue
for writing asynchronous gRPC clients and servers using C++20 coroutines,
Boost.Coroutines, Asio's stackless coroutines, callbacks, sender/receiver and
more.
#+end_quote

Links:

- [[https://github.com/Tradias/asio-grpc][asio-grpc GH]]:

*** Consider adding support for osmanip                               :story:

#+begin_quote
Library with useful output stream tools like: color and style manipulators,
progress bars and terminal graphics
#+end_quote

Links:

- [[https://github.com/JustWhit3/osmanip][osmanip GH]]

*** Consider making hexagonal architecture a pattern                  :story:

We were already following the ideas of hexagonal architecture, even though we
did not know it was an established pattern. We need to read up on it to try to
figure out if there are elements of this architecture which can be encoded into
the LPS.

Links:

- [[https://alistair.cockburn.us/hexagonal-architecture/][Hexagonal architecture]]

*** Consider adding support for typed factories                       :story:

A common pattern in code is the need to create a factory where we associate an
enum or a string with a specific implementation of an interface. For example:

: IMyFactory
: TriangleFactory -> Enum Shapes.Triangle
: SquareFactory -> Enum Shapes.Square

And then we create an instance of the factory that does little more than
dispatching the enum to the appropriate instance. In Windsor Castle this pattern
is known as Typed Factory Facility. In Dogen we can do this all at compile time,
making the entire exercise more useful because:

- you can see exactly what the mappings are; there is no black magic.
- you can debug it since its normal code.

We can reuse all of the castle analysis and simply make these meta-parameters.

Links:

- [[https://github.com/castleproject/Windsor/blob/master/docs/typed-factory-facility-interface-based.md][Typed Factory Facility - interface-based factories]]

*** Consider adding support for email libraries                       :story:

A common use case is the need to send emails from an application. This story
keeps track of c++ libraries that provide support for sending and receiving
emails.

Links:

- [[https://github.com/karastojko/mailio][mailio GH]]: "mailio is a cross platform C++ library for MIME format and SMTP,
  POP3 and IMAP protocols. It is based on the standard C++ 17 and Boost library"
- [[https://github.com/MailCore/mailcore2][mailcore2 GH]]: "MailCore 2 provides a simple and asynchronous Objective-C API
  to work with the e-mail protocols IMAP, POP and SMTP. The API has been
  redesigned from the ground up."
- [[https://github.com/kisli/vmime][vmime GH]]: "VMime is a powerful C++ class library for working with RFC-822 and
  MIME messages and Internet messaging services like IMAP, POP or SMTP."

*** Consider adding support for =idle=                                :story:

#+begin_quote
Idle is an asynchronous, hot-reloadable, and highly reactive dynamic component
framework similar to OSGI that is:

- Modular: Your program logic is encapsulated into services that provide
  interfaces for usage
- Dynamic: Services can come and go whenever they want, Idle will keep your
  application stable
- Progressive: Your code is recompiled automatically asynchronously into your
  application Therefore Idle can improve your workflow significantly through
decreased code iteration time and cleaner code-bases based on the SOLID
principles, especially interface segregation. Idle is a C++ engine that is
capable of hot-reloading large multi-module codebases on the fly through a
scheduled dependency graph. What a game engine is to graphics, that is Idle for
C++, without targeting a specific domain. Idle can be used for almost any C++
application type e.g. servers, GUIs, or graphic engine.s
#+end_quote

Links:

- [[https://github.com/Naios/idle][idle GH]]

*** Add support for search at product level                           :story:

It would be nice if we could annotate certain types as searchable and
all code to index and search them would be automatically generated,
with choices of backends.

Another interesting use, which may not be Dogen related, is to index
all documents in a product and have an emacs mode to interface with
the search engine.

Links:

-
- [[https://github.com/synhershko/clucene][clucene GH]]: "It is a high-performance, full-featured text search
  engine written in C++."
- [[https://github.com/typesense/typesense][typesense GH]]: "Typesense is a fast, typo-tolerant search engine for
  building delightful search experiences."
- [[https://github.com/casouri/xeft][xeft GH]]: "A dynamic module that exposes a very basic indexing
  feature to Emacs Lisp, that lets you index and search a text files
  very fast."

Merged stories:

*Consider adding support for typesense*

#+begin_quote
Typesense is a fast, typo-tolerant search engine for building
delightful search experiences.
#+end_quote

It would be nice if dogen could generate code to integrate with
typesense - whatever that means precisely, upload documents into the
search engine? Serialise to a compatible format? At any rate, ideally
one just wants to annotate types in a product as searchable and then
the infrastructure needed is generated.

Links:

- [[https://github.com/typesense/typesense][GH typesense]]
- [[https://developer.mozilla.org/en-US/docs/Web/OpenSearch][OpenSearch]]
- [[https://github.com/phaistos-networks/Trinity][GH Trinity]]: "Trinity is a modern C++ information-retrieval library
  for building queries, indexing documents and other content, running
  queries and scoring documents matching them."

*** Consider adding support for =pg_timetable=                        :story:

#+begin_quote
pg_timetable is an advanced standalone job scheduler for PostgreSQL,
offering many advantages over traditional schedulers such as cron and
others. It is completely database driven and provides a couple of
advanced concepts. It allows you to schedule PostgreSQL commands,
system programs and built-in operations [...]
#+end_quote

Links:

- [[https://github.com/cybertec-postgresql/pg_timetable][GH]]
- [[https://www.cybertec-postgresql.com/en/pg_timetable-advanced-postgresql-cron-like-scheduler-released/][pg_timetable: Advanced PostgreSQL cron-like scheduler released!]]
- [[https://www.cybertec-postgresql.com/en/pg_timetable-advanced-postgresql-job-scheduling/][pg_timetable: advanced postgresql scheduling]]

*** Context provider                                                  :story:

This story is not really directly connected with Dogen, but as we have
no better place, we'll keep it in the backlog for now. It would be
really nice to have a "companion" tool which would be following the
user and supplying context. By this we mean that as the user is moving
around the code base, the context providing tool would be:

- drawing a class diagram with classes at a "close" distance from
  where we are. That is, given a function, it would scan the types
  immediately surrounding the function, and then the class and so on,
  and add these to a class diagram. The class diagram should also
  contain the documentation for the classes involved (UML notes?) as
  well as methods.
- drawing an interaction diagram for the function (who is calling it
  and who does it call).
- listing all references of near types (who is using them, and who are
  they using).

Ideally this is being computed on the fly and for each computation
SVGs are produced (at least for the diagrams). The SVGs cannot be
clicked on, but we could have a text based buffer with additional
information (a-la org-ref). In effect, we are probably fusing org-ref
with UML here.

Notes:

- All the information should be obtained by querying an LSP server, so
  that the same approach would work for any language that supports
  LSP.
- the protocol to communicate with the server should be very similar
  to LSP. Ideally emacs would send commands (the same commands it
  sends to LSP server, e.g. position etc) and the server would reply
  with artefacts such as SVGs etc.
- actually instead of generating SVGs, it may make more sense to
  generate text representations of files. These could be in a file
  format that emacs can understand so that we could have links for
  every element in an image. It would make navigation, searching etc
  much easier.

*** Add support for internationalisation                              :story:

Links:

- [[https://github.com/zauguin/i18n-cpp][i18n-cpp GH]]: I18N for C++20 based on libintl/gettext

*** Consider using a simpler graph library                            :story:

Our use cases for graphs are very simple. It seems Boost.Graph may be
overkill and it does not seem to be well maintained. This story keeps
track of newer graph libraries in C++.

Links:

- [[https://gitlab.com/MartenBE/varia][varia GH]]: "Algorithms and more in modern C++. These algorithms are
  all being unit tested."
- [[https://github.com/nspo/graphs-cpp][graphs-cpp GH]]: "Code for weighted and unweighted graph and digraph
  problems"
- [[http://lemon.cs.elte.hu/pub/doc/latest-svn/index.html][LEMON]]: "LEMON stands for Library of Efficient Models and
  Optimization in Networks. It is a C++ template library aimed at
  combinatorial optimization tasks which often involve in working with
  graphs."
- [[https://github.com/haasdo95/graphlite][graphlite GH]]: Graphlite is a lightweight generic graph library that
  supports: a) node properties b) edge properties c)
  directed/undirected graphs d) multi-edges & self-loops e)
  user-specified data structures; for the adjacency list and neighbor
  containers

*** Consider adding support for Vega                                  :story:

#+begin_quote
Vega is a visualization grammar, a declarative format for creating,
saving, and sharing interactive visualization designs. With Vega you
can describe data visualizations in a JSON format, and generate
interactive views using either HTML5 Canvas or SVG.
#+end_quote

Links:

- [[https://vega.github.io/vega/][Vega home]]
- [[https://github.com/vega/vega][vega GH]]

*** Mine "repo visualisation" for ideas                               :story:

#+begin_quote
How can we “fingerprint” a codebase to see its structure at a glance?
Let’s explore ways to automatically visualize a GitHub repo, and how
that could be useful.
#+end_quote

Notes:

- it would be nice if we could map this to an emacs mode that linked
  back to the product.
- ideally you'd want an SVG representation with embedded
  links. Actually we did a test, created SVG with embedded links and
  these are not visible from within Emacs.
- we should also diff this against source trail, and org-roam UI.

Links:

- [[https://octo.github.com/projects/repo-visualization][Visualizing a codebase]]
- [[https://github.com/org-roam/org-roam-ui][org-roam-ui GH]]
- [[https://www.reddit.com/r/emacs/comments/ozwvcs/svgmode_clickable_links_in_documents/][reddit: svg-mode: Clickable links in documents]]
- [[https://github.com/sabof/svg-thing][svg-thing GH]]
- [[https://github.com/jave/inkmacs][inkmacs GH]]
- [[https://www.emacswiki.org/emacs/EmacsSvg][EmacsSvg]]

*** Consider adding support for provisioning                          :story:

This is a general story to keep track of ideas in the provisioning
space. We need to generate code for product provisioning, possibly
across multiple frameworks:

- chef
- puppet
- ansible
- etc.

We should also consider having direct systemd support, generating unit
files for each product.

Links:

- [[https://github.com/spanezz/transilience][transilience GH]]: "Python provisioning library. Ansible-like
  modules. Declarative actions. Generate actions with Python. No
  templatized YAML. Mitogen-based connections."

*** Consider adding support for dbcritic                              :story:

Where a project has a database, it would be nice to automatically have
a =dbcritic= target.

#+begin_quote
dbcritic finds problems in a database schema.
#+end_quote

Links:

- [[https://github.com/channable/dbcritic][dbcritic GH]]
- [[https://www.channable.com/tech/dbcritic-constructively-criticizing-your-postgres-schema][dbcritic: Constructively criticizing your Postgres schema (using
  Idris)]]

*** Consider adding support for CppSerDes                             :story:

#+begin_quote
CppSerdes is a serialization/deserialization library designed with
embedded systems in mind
#+end_quote

Links:

- [[https://github.com/DarrenLevine/cppserdes][cppserdes GH]]
- [[https://www.reddit.com/r/cpp/comments/odbaev/cppserdes_a_minimalistic_memcpylike_library_for/][reddit discussion]]

*** Consider adding support for Djinni                                :story:

#+begin_quote
Djinni is a tool for generating cross-language type declarations and
interface bindings. It's designed to connect C++ with either Java or
Objective-C.
#+end_quote

Links:

- [[https://github.com/cross-language-cpp/djinni-generator][djinni-generator GH]]

*** Update Dogen to c++ 20                                            :story:

Once its properly supported in clang and GCC, we should move to
C++ 20.

Links:

- [[https://oleksandrkvl.github.io/2021/04/02/cpp-20-overview.html][All C++20 core language features with examples]]

*** Consider adding support for =mnemonicode=                         :story:

#+begin_quote
These routines implement a method for encoding binary data into a
sequence of words which can be spoken over the phone, for example, and
converted back to data on the other side.
#+end_quote

Link:

- [[https://github.com/singpolyma/mnemonicode][mnemonicode GH]]
- [[http://web.archive.org/web/20101031205747/http://www.tothink.com/mnemonic/][Article: Mnemonic encoder]]
- [[https://github.com/Debdut/uuid-readable][uuid-readable GH]]
- [[https://www.reddit.com/r/PostgreSQL/comments/meo45i/how_to_shorten_a_uuid/][reddit: How to shorten a uuid?]]
- [[https://datatracker.ietf.org/doc/html/draft-peabody-dispatch-new-uuid-format][RFC: New UUID Formats]]. [[https://news.ycombinator.com/item?id=28088213][HN discussion]].

*** Consider adding support for immutable containers                  :story:

#+begin_quote
immer is a library of persistent and immutable data structures written
in C++. These enable whole new kinds of architectures for interactive
and concurrent programs of striking simplicity, correctness, and
performance.
#+end_quote

Notes:

- we should consider making all containers in dogen immutable. This
  would mean that each transform would not mutate the containers in
  place, but update their versions. With this and with some kind of
  data flow framework we would be able to achieve some form of
  parallelism.

Links:

- [[https://github.com/arximboldi/immer][immer GH]]
- [[https://github.com/dougbinks/enkiTS][enkiTS GH]]: "A permissively licensed C and C++ Task Scheduler qfor
  creating parallel programs. Requires C++11 support." Useful once we
  have immutable containers.
- [[https://github.com/oneapi-src/oneTBB][TBB GH]]: probably makes more sense to use TBB for the data flow.
- [[https://github.com/arximboldi/lager][lager GH]]: dataflow based on immer.

*** Consider adding support for QTL                                   :story:

#+begin_quote
QTL is a C ++ library for accessing SQL databases and currently
supports MySQL, SQLite, PostgreSQL and ODBC. QTL is a lightweight
library that consists of only header files and does not require
separate compilation and installation. QTL is a thin encapsulation of
the database's native client interface. It can provide a friendly way
of using and has performance close to using the native
interface. Using QTL requires a compiler that supports C++11.
#+end_quote

Link:

- [[https://github.com/znone/qtl][QTL GH]]:

*** Consider adding support for =enumerate()=                         :story:

#+begin_quote
Python has a handy built-in function called enumerate(), which lets
you iterate over an object (e.g. a list) and have access to both the
index and the item in each iteration. You use it in a for loop, like
this:

for i, thing in enumerate(listOfThings):
    print("The %dth thing is %s" % (i, thing))
#+end_quote

Link:

- [[https://www.reedbeta.com/blog/python-like-enumerate-in-cpp17/][Python-Like enumerate() In C++17]]

*** Consider adding support for =PartitionAlloc=                      :story:

#+begin_quote
PartitionAlloc is a memory allocator optimized for security, low
allocation latency (when called appropriately), and good space
efficiency (when called appropriately). This document aims to help you
understand how PartitionAlloc works so that you can use it
effectively.
#+end_quote

Links:

- [[https://chromium.googlesource.com/chromium/src/+/master/base/allocator/partition_allocator/PartitionAlloc.md][PartitionAlloc Design]]
- [[http://struct.github.io/pdfium_partitionalloc.html][Porting PartitionAlloc To PDFium]]
- [[http://struct.github.io/partition_alloc.html][PartitionAlloc - A shallow dive and some rand]]
- [[https://github.com/struct/HardenedPartitionAlloc][GH HardenedPartitionAlloc]]: "This is a standalone library containing
  PartitionAlloc, the allocator used in Chrome's Blink engine. It
  needs a lot of work and testing before it should be used on
  production code. This code is changing rapidly and should be
  considered unstable and untested until this notice is removed."

*** Mine GSL for ideas                                                :story:

#+begin_quote
GSL/4.1 is a code construction tool. It will generate code in all
languages and for all purposes. If this sounds too good to be true,
welcome to 1996, when we invented these techniques. Magic is simply
technology that is twenty years ahead of its time. In addition to code
construction, GSL has been used to generate database schema
definitions, user interfaces, reports, system administration tools and
much more.
#+end_quote

Links:

- [[https://github.com/imatix/gsl][GSL GH]]

*** Investigate integrating HTM with dogen                            :story:

#+begin_quote
This is a Community Fork of the nupic.core C++ repository, with Python
bindings. This implements the theory as described in Numenta's BAMI.
#+end_quote

Links:

- [[https://github.com/htm-community/htm.core][htm.core GH]]

*** Add support for digests                                           :story:

#+begin_quote
Experimental C++11 header-only message digest library. Derived from
cppcrypto in an attempt to devise a more modern yet flexible and
universal C++ API for cryptographic hash functions.
#+end_quote

Links:

- [[https://github.com/kerukuro/digestpp][digestpp GH]]
- [[https://numenta.com/resources/biological-and-machine-intelligence/][Biological and Machine Intelligence (BAMI)]]

*** Consider adding support for scheduling                            :story:

Links:

- [[https://github.com/PerMalmberg/libcron][libcron GH]]: "A C++ scheduling library using cron formatting."

*** Consider adding support for =cpp-equiv=                           :story:

#+begin_quote
Creating equivalence relations with indexes as elements, merging
elements and testing for equivalence. Also supports adding and
removing elements from existing equivalence relations, comparing
fine-ness, creating all equivalence relations of a given size, and
other auxilary functions/methods.
#+end_quote

Links:

- [[https://github.com/Ahajha/cpp-equiv][cpp-equiv GH]]

*** Consider adding support for trees                                 :story:

We often require a tree datastructure. There must be c++ libraries
implementing these that we can use.

Links:

- [[https://github.com/gvinciguerra/PGM-index][PGM-index GH]]: "The Piecewise Geometric Model index (PGM-index) is a
  data structure that enables fast lookup, predecessor, range searches
  and updates in arrays of billions of items using orders of magnitude
  less space than traditional indexes while providing the same
  worst-case query time guarantees."
- [[https://github.com/Kronuz/cpp-btree][C++ B-tree GH]]: "Code in this repository is based on Google's B-tree
  implementation. C++ B-tree is a template library that implements
  ordered in-memory containers based on a B-tree data structure."

*** Consider adding support for =libvineyard=                         :story:

#+begin_quote
Vineyard is an in-memory immutable data manager that provides
out-of-box high-level abstraction and zero-copy in-memory sharing for
distributed data in big data tasks, such as numerical computing,
machine learning, and graph analytics.
#+end_quote

Links:

- [[https://github.com/alibaba/libvineyard][libvineyard GH]]
- [[https://github.com/apache/arrow/blob/master/cpp/apidoc/tutorials/plasma.md][Using the Plasma In-Memory Object Store from C++]]: "Apache Arrow
  offers the ability to share your data structures among multiple
  processes simultaneously through Plasma, an in-memory object store."

*** Consider adding support for =flashlight=                          :story:

#+begin_quote
Flashlight is a fast, flexible machine learning library written
entirely in C++ from the Facebook AI Research Speech team and the
creators of Torch and Deep Speech.
#+end_quote

Links:

- [[https://github.com/flashlight/flashlight][flashlight GH]]. [[https://www.reddit.com/r/cpp/comments/msb8kv/flashlight_a_c_standalone_library_for_machine/][Reddit discussion]].

*** org-block extras                                                  :story:

See if we can mine this org documentation for ideas that can be
applied to Dogen:

- [[https://alhassy.github.io/org-special-block-extras/#Judgements-Inference-rules-and-proof-trees][org-special-block-extras]]

*** Investigate =ganja.js=                                            :story:

It is not entirely clear what exactly this library does, but it is
related to code generation, so it may come under dogen's umbrella:

#+begin_quote
Technically, ganja.js is a code generator producing classes that
reificate algebraic literals and expressions by using reflection, a
built-in tokenizer and a simple AST translator to rewrite functions
containing algebraic constructs to their procedural counterparts.
#+end_quote

Links:

- [[https://github.com/enkimute/ganja.js][GH project]]
- [[https://news.ycombinator.com/item?id=25471046][HN discussion]]

*** Investigate adding support for unity builds                       :story:

Now that CMake has unity builds support, we should try building dogen
this way and see if its faster. However, we need to first remove all
of the static state we have in translation units at present:

- loggers
- strings with error messages, etc.

These should become class state. There is also the issue with private
classes defined in the cpp; we need to ensure these have unique names.

Links:

- [[https://onqtam.com/programming/2018-07-07-unity-builds/][A guide to unity builds]]
- [[https://www.qt.io/blog/2019/08/01/precompiled-headers-and-unity-jumbo-builds-in-upcoming-cmake][Precompiled Headers and Unity (Jumbo) Builds in upcoming CMake]]

*** Consider adding support for replxx                                :story:

#+begin_quote
A small, portable GNU readline replacement for Linux, Windows and
MacOS which is capable of handling UTF-8 characters. Unlike GNU
readline, which is GPL, this library uses a BSD license and can be
used in any kind of program.
#+end_quote

It would be nice if Dogen could generate code to integrate with
=replxx=.

Links:

- [[https://github.com/AmokHuginnsson/replxx][GH replxx]]

*** Consider adding support for rocksdb                               :story:

#+begin_quote
This code is a library that forms the core building block for a fast
key-value server, especially suited for storing data on flash
drives. It has a Log-Structured-Merge-Database (LSM) design with
flexible tradeoffs between Write-Amplification-Factor (WAF),
Read-Amplification-Factor (RAF) and Space-Amplification-Factor
(SAF). It has multi-threaded compactions, making it especially
suitable for storing multiple terabytes of data in a single database.
#+end_quote

Links:

- [[https://github.com/facebook/rocksdb][GH rocksdb]]

*** Consider adding support for snowflake                             :story:

#+begin_quote
In certain situations you need a low-latency, distributed,
uncoordinated, (roughly) time ordered, compact and highly available Id
generation system. This project was inspired by Twitter's Snowflake
project which has been retired.
#+end_quote

Links:

- [[https://github.com/Shenggan/SnowFlake][GH SnowFlake]]
- [[https://github.com/sniper00/snowflake-cpp][GH snowflake-cpp]]
- [[https://developer.twitter.com/en/docs/twitter-ids][twitter ids]]
- [[https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake.html][Announcing Snowflake]]
- [[https://medium.com/ingeniouslysimple/why-did-we-shift-away-from-database-generated-ids-7e0e54a49bb3][Why Did We Shift Away From Database-Generated Ids?]]
- [[https://github.com/RobThree/IdGen][GH IdGen]] (C#)
- [[https://github.com/KMakowsky/CppShortID][CppShortID]]
- [[https://kvz.io/create-short-ids-with-php-like-youtube-or-tinyurl.html][Create Youtube-Like IDs With PHP/Python/Javascript/Java/SQL]]
- [[https://github.com/turbo/pg-shortkey][pg-shortkey]]
- [[https://www.2ndquadrant.com/en/blog/sequential-uuid-generators/][Sequential UUID Generators]]
- [[https://hashids.org/][hashids]]: generate short unique ids from integers
- [[https://github.com/schoentoon/hashidsxx][GH hashidsxx]]

*** Consider moving to =PEGTL=                                        :story:

Seems like a modern replacement for Boost.Spirit.

#+begin_quote
The Parsing Expression Grammar Template Library (PEGTL) is a
zero-dependency C++ header-only parser combinator library for creating
parsers according to a Parsing Expression Grammar (PEG).
#+end_quote

Links:

- [[https://github.com/taocpp/PEGTL][GH repo]]
- [[https://github.com/foonathan/lexy][lexy GH]]: "lexy is a parser combinator library for C++17 and
  onwards. It allows you to write a parser by specifying it in a
  convenient C++ DSL, which gives you all the flexibility and control
  of a handwritten parser without all the manual work."
- [[https://www3.nd.edu/~dthain/compilerbook/][Introduction to Compilers and Language Design]]: generic book on the
  subject.

*** Consider integrating with =NN-512=                                :story:

#+begin_quote
NN-512 is a compiler that generates C99 code for neural net inference:

- It takes as input a simple text description of a convolutional
  neural net inference graph
- It produces as output a stand-alone C99 implementation of that graph
- The generated C99 code uses AVX-512 vector instructions to perform
  inference
#+end_quote

Links:

- [[https://nn-512.com/][NN-512 site]]
- [[https://www.reddit.com/r/cpp/comments/k3hiud/generate_standalone_cc_code_for_neural_net/][reddit discussion]]
- [[https://www.ccoderun.ca/darkhelp/api/API.html][DarkHelp]]

*** Investigate =refureku=                                            :story:

#+begin_quote
Refureku is a powerful C++17 RTTI free runtime reflection library
based on Kodgen. It allows to retrieve information on namespaces,
structs/classes, fields, methods, non-member variables, non-member
functions, enums and enum values at runtime.
#+end_quote

Links:

- [[https://github.com/jsoysouvanh/Refureku][GH repo]]

*** Investigate =kodgen=                                              :story:

#+begin_quote
Kodgen is a C++17 library based on libclang. It provides tools to
parse C++ source files, extract data from it, and generate files from
the parsed data. The C++ entities parsed in the current implementation
are namespaces, structs/classes, non-member and member variables,
non-member and member functions, enums and enum values.
#+end_quote

One possible use for this would be to backout PDM models from existing
source code.

Links:

- [[https://github.com/jsoysouvanh/Kodgen][GH repo]]

*** Investigate cookietemple project templates                        :story:

#+begin_quote
A cookiecutter based project template creation tool supporting several
domains and languages with advanced linting, syncing and standardized
workflows to get your project kickstarted in no time.
#+end_quote

Links:

- [[https://github.com/cookiejar/cookietemple][GH project]]
- [[https://www.reddit.com/r/cpp/comments/jzp8pe/if_you_are_looking_for_an_easy_way_to_start_a_new/][reddit: If you are looking for an easy way to start a new project,
  checkout Cookietemple!]]

*** Consider adding support for =repr=                                :story:

#+begin_quote
repr is a header-only library. Add include/ to your
target_include_directories and include <repr/repr.h>.

repr(value) returns a printable string representation of value. To
achieve this, repr uses libfmt, magic_enum and boost::pfr, and
provides a nice convenience function.

repr() supports fundamental types, strings, containers, container
adapters, aggregate-initializable classes/structs and chrono types.
#+end_quote

Links:

- [[https://github.com/p-ranav/repr][GH repo]]
- [[https://www.reddit.com/r/cpp/comments/jzgzoz/repr_for_c_return_printable_string_representation/][reddit: repr for C++: Return printable string representation of a value]]

*** Product based operations                                          :story:

It would be interesting to provide a "product API" which is closer to
how we develop in practice. At present we are forced to think at the
level of git (feature branches, master, etc) and associated tools
(docker, kubernetes, etc=. In reality, we have a very small set of
simple operations we do with products. It would be nice if dogen could
provide a wrapper around these. In order to do so we need to force
structural patterns on how these are implemented (i.e. do not allow
for variation outside of a narrow set of well-defined use cases). This
story attempts to capture these use cases. The basic ideas are coming
from several existing tools such as =dotnet=, =cargo=, etc.

When we start working on a product, it would be nice to be able to do
something like:

: dogen product new

or

: dogen new product

It would require some additional parameters such as:

: --name abc
: --forge https://github.com/ORGANISATION

Note that a dogen product maps to a forge organisation (e.g. github,
etc). This may be different for other forges. It assumes some
top-level config with all of the authentication setup. Presumably
there could be an =--add-forge= command or just edit files manually.

The new product command would a top-level directory for the product
and create the organisation.

: dogen component new

This would create a new git repo with some standard components. There
could be a =--template= command to choose the template for the
component. The approach should be similar to =dotnet=. Note that these
templates are models inside of dogen. Also, when creating the repo, it
would perform the initial git commit, create the repository remotely
and push it. It would also do the worktree setup:

- =PRODUCT.COMPONENT.git.remote=: points to remote master and should always be
  kept clean for pushes and pulls.
- =PRODUCT.COMPONENT.git.local=: local checkout to do work. Further locals can be
  created, e.g. =local0=, etc.

Links:

- [[https://docs.microsoft.com/en-us/dotnet/core/tools/custom-templates][Custom templates for dotnet new]]

*** Integration of =vcpkg= and =build2=                               :story:

There are two types of dependencies that we need to worry about:

- those external to dogen; these we call "the platform".
- those "internal" to dogen; these are the user models.

As it turns out, there are two different ways of managing these two
types of dependencies. The best package managers for C++, such as
conan and vcpkg, have a very large number of packages
available. These are great for the first kind of dependencies because
they have large coverage of C++ libraries. In addition, vcpkg is
written in c++, making integration easier - though perhaps a "vcpkg
lib" would have to be created.

Build2, on the other hand, would be very suitable to manage the dogen
created artefacts because it is a "modern" package manager with an
understanding of versions etc. It has some limitations (does not
support binaries, etc), but these are not too taxing if its only for a
small set of models.

The trick would be for dogen to orchestrate these two package managers
in a way that is transparent to the user somehow; e.g. you can declare
a vcpkg dependency on your product and it would result in vcpkg doing
the right thing. You can also build a docker image with all the vcpkg
dependencies at a given version and then use that for the build
machines, deployments etc. We would require some conceptual framework
around vcpkg: snapshots, versioning, publishing, etc. Note also that
we should be able to rely on system packages if available. vcpkg
should be seen as the override, used only when we cannot find the
package at that version.

We may also want products to share their vcpkg setup so that we don't
have to build core libraries multiple times. Or perhaps we need the
concept of a product family with its associated vcpkg directory setup,
and then the products with their own setup. Libraries can be installed
at the family level or at the product level - or even at the component
level? we need to look for use cases for this.

For extra bonus points, it would be nice if we could create a DEB
package for each of these vcpkg exports, so that the src package of
the product and components could depend on the vcpkg, and similarly
the binary package could depend on the vcpkg binary package. One could
then install the platform and then install the product. You could
create dependency trees between the platform and the product. And
these should ideally be side-by-side installable, so that multiple
versions of the platform and the product can coexist. However, note
that we have platform packages at the different levels:

- family
- product
- component

These are placed in the path accordingly, so that the component takes
priority, then the product and finally the family. Note that the
packages themselves have versions, which are not at all related with
the individual libraries they contain. As an example, say we have:

- masd: family
- dogen: product
- dogen: component

We could create a =masd.platform_1.0.0.deb= package, which installs
into a well known hierarchy. The directory name should have the
family, product etc as well as the version:

: /usr/local/masd.platform/1.0.0

Links:

- [[https://github.com/microsoft/vcpkg][vcpkg GH]]
- [[https://github.com/microsoft/vcpkg/issues/9258][vcpkg #9258: Add ability to export .deb and .rpm]]
- [[https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard][Filesystem Hierarchy Standard (FHS)]]
- [[https://learn.hashicorp.com/waypoint][hashicorp's waypoint]]

*** Investigate build time library configuration post                 :story:

#+begin_quote
Why do libraries offer these build-time switches? What do they
accomplish?

In the majority of situations, I believe that offering compile-time
tweaks are an antipattern. I could write a detailed post on why, but
that’s not the point I want to focus on.

Instead, I want to focus on the cases where having these dials and
switches is hugely beneficial or essential.
#+end_quote

Links:

- [[https://vector-of-bool.github.io/2020/10/04/lib-configuration.html][A New Approach to Build-Time Library Configuration]]

*** Investigate CG-SQL                                                :story:

#+begin_quote
CG/SQL is a code generation system for the popular SQLite library that
allows developers to write stored procedures in a variant of
Transact-SQL (T-SQL) and compile them into C code that uses SQLite’s C
API to do the coded operations. CG/SQL enables engineers to create
highly complex stored procedures with very large queries, without the
manual code checking that existing methods require.
#+end_quote

Links:

- [[https://github.com/facebookincubator/cg-sQL][GH]]
- [[https://engineering.fb.com/open-source/cg-sql/][CG/SQL: Easy, accurate code generation for SQLite]]

*** AWS boilerplate generation                                        :story:

Consider generating AWS boilerplate.

#+begin_quote
The primary objective of this boilerplate is to give you a production
ready code that reduces the amount of time you would normally have to
spend on system infrastructure's configuration. It contains a number
of services that a typical web application has (frontend, backend api,
admin panel, workers) as well as their continuous deployment. Using
this boilerplate you can deploy multiple environments, each
representing a different stage in your pipeline.
#+end_quote

Links:

- [[https://github.com/apptension/aws-boilerplate][aws-boilerplate GH]]

*** MDE links and information                                         :story:

Research story to gather information about MDE.

Links:

- [[https://modelpractice.wordpress.com/about/][modelpractice]]

*** Org-mode integration with modeling                                :story:

Since org-mode will be very central to all our modeling efforts, we
need to learn more about it. This story is a general gathering of
links of org-mode and tools.

Links:

- [[https://kitchingroup.cheme.cmu.edu/blog/2017/01/03/Find-stuff-in-org-mode-anywhere/][Find stuff in org-mode anywhere]]
- [[https://www.mcls.io/blog/encouraging-a-culture-of-written-communication][Encouraging a Culture of Written Communication]]
- [[https://github.com/alphapapa/org-rifle][org-rifle]]: What does my rifle do? It searches rapidly through my Org
  files, quickly bringing me the information I need to defeat the
  enemy.
- [[https://writequit.org/articles/emacs-org-mode-generate-ids.html][Emacs Org-mode: Use good header ids!]]
- [[https://github.com/org-roam/org-roam][org-roam GH]]: Org-roam is a solution for effortless non-hierarchical
  note-taking with Org-mode. With Org-roam, notes flow naturally,
  making note-taking fun and easy. Org-roam should also work as a
  plug-and-play solution for anyone already using Org-mode for their
  personal wiki.
- [[https://github.com/bastibe/org-journal][org-journal GH]]: Functions to maintain a simple personal diary /
  journal using in Emacs.
- [[http://www.howardism.org/Technical/Emacs/capturing-intro.html][Org Capturing Introduction]]: The following essay attempts to be a
  gentle introduction to org-capture. I will assume you are familiar
  with both Org and Emacs, but don’t know anything about org-capture.
- [[https://github.com/weirdNox/org-noter][org-noter GH]]: Org-noter’s purpose is to let you create notes that
  are kept in sync when you scroll through the document, but that are
  external to it - the notes themselves live in an Org-mode file.
- [[https://github.com/samhedin/rustdoc-to-org][rustdoc-to-org:]] A Pandoc filter that converts rust documentation to
  .org-files, and a minor mode to go with!
- [[https://alphapapa.github.io/org-almanac/#Parsing][org-mode parsers]]
- [[https://github.com/milisims/tree-sitter-org][tree-sitter-org]]

*** Investigate telosys                                               :story:

It appears to an interesting code generator, though we cannot locate
any academic papers.

Links:

- [[http://www.telosys.org/index.html#intro][telosys homepage]]
- [[https://github.com/telosys-tools-bricks][GH telosys]]
- [[https://modeling-languages.com/telosys-tools-the-concept-of-lightweight-model-for-code-generation/][Telosys – a lightweight and pragmatic code-generator]]

*** Consider allowing users to create their own parts                 :story:

It would be nice if one could create our own parts. However the main
problem is how would you allocate modeling elements to a part. At
present this is done via the formatter; perhaps we could override this
in meta-data? This is a very complex task and we need clear use cases
for it. Alternatively we could state that a user defined part's
content is ignored entirely.

*** Check RedHat guidelines on release notes                          :story:

This post seems to contain a lot of interesting information about
release notes:

- [[http://community.redhat.com/blog/2015/12/guidelines-for-announcing-software-releases/][Guidelines For Announcing Software Releases]]

*** Add support for CBadge                                            :story:

It seems we can add badges for CDash.

Links:

- [[https://github.com/brennonbrimhall/CBadge][GH]]

*** Consider adding support for =libcudacxx=                          :story:

#+begin_quote
libcu++, the NVIDIA C++ Standard Library, is the C++ Standard Library
for your entire system. It provides a heterogeneous implementation of
the C++ Standard Library that can be used in and between CPU and GPU
code.
#+end_quote

Links:

- [[https://github.com/NVIDIA/libcudacxx][GH]]
- [[https://news.ycombinator.com/item?id=24526145][HN post]]
- [[https://docs.nvidia.com/cuda/thrust/index.html][thrust]]

*** Consider adding =using= statements to models                      :story:

It probably would be quite trivial, once we move over to org-mode, to
add a top-level section with =using= statements:

: using identification::entities::physical_meta_id

This would then mean that every time we see =physical_meta_id= we
replace it with the full name. Ideally these should be added at a
package level and have a recursive use. It should be transparent to
code generation. If we detect duplicates we should error.

*** Consider creating wrappers for containers                         :story:

A common pattern we see in Dogen are indices and repositories. These
tend to contain standard library containers with domain types. For
example, in =mapping_set= we have =by_agnostic_id= which has:

: std::unordered_map<
:     identification::entities::technical_space,
:     std::unordered_map<
:         identification::entities::logical_id,
:         identification::entities::logical_name>
: >

What would be really nice is if we could avoid exposing all of the
internals of the STL containers and provide only the operations that
are needed for the domain types in question. This is very experimental
at present, but the idea is that maybe there is a small subset of
operations we actually do in these types; if that's the case then we
could create a meta-type for them, e.g.:

- =repository=
- =index=

And then enable the operations via meta-data. In many ways this is a
bit like a primitive: the meta-type would have a =key= and a =value=,
and then define the set of operations. However, we need to do more
thinking. We should start by looking at how these containers are being
used and see if we can abstract the operations from there.

Notes:

- in every case where we have containers we have a simple pattern of
  iteration over the elements (and possibly sub-elements, for
  containers of containers), e.g.:

:    for (auto& as_pair : m.regions_by_logical_id()) {
:        auto& as(as_pair.second);
:        for (auto& a_pair : as.artefacts_by_archetype()) {
:            auto& a(*a_pair.second);
:            apply(a, m.outputting_properties().force_write());

  It would be really nice if we could generate a set of top-level
  functions in the container that would allow iteration at each of
  these levels via a lambda, const and non-const, e.g.:

: container.for_each([](const X& x) { ... }

  For extra bonus points, we should have an overload with dereferenced
  pointer.
- we should find appropriate exceptions in the standard library for
  all the operations in the container so that we do not have to rely
  on the presence of a generated exception.
- we also have a very common get by ID, which throws:

:        const auto i(global_enablement_properties.find(pmid));
:        if (i == global_enablement_properties.end()) {
:            BOOST_LOG_SEV(lg, error) << global_configuration_not_found << pmid;
:            BOOST_THROW_EXCEPTION(transform_exception(
:                    global_configuration_not_found + pmid.value()));
:        }
:        const auto& gep(i->second);

- we also have an insert by ID that checks for duplicates:

:    for (const auto& region : regions) {
:        const auto lid(region.logical_element()->name().id());
:        const auto pair(std::make_pair(lid, region.physical_region()));
:        const auto inserted(rbli.insert(pair).second);
:        if (inserted)
:            continue;
:
:        BOOST_LOG_SEV(lg, error) << duplicate_id << lid;
:        BOOST_THROW_EXCEPTION(transform_exception(duplicate_id + lid.value()));
:    }

  We can detect if we are an associative container and generate this
  operation. We could have a "strict" and "non-strict" insert.

*** Consider adding support for Matplot++                             :story:

#+begin_quote
 Matplot++ is a graphics library for data visualization that provides
 interactive plotting, means for exporting plots in high-quality
 formats for scientific publications, a compact syntax consistent with
 similar libraries, dozens of plot categories with specialized
 algorithms, multiple coding styles, and supports generic backends.
#+end_quote

Links:

- [[https://github.com/alandefreitas/matplotplusplus][matplotplusplus GH]]
- [[https://github.com/davecom/SVGChart][SVGChart GH]]: A C++ Library for Making SVG Charts (fork of PPlot)
- [[https://github.com/pabristow/svg_plot][svg_plot GH]]: Plot data in SVG format using C++ library.
- [[https://github.com/thclark/cpplot][cpplot GH]]: Interactive graphs and charts for C++ 11 upward, viewable
  in-browser using cpplot-viewer.
- [[https://www.researchgate.net/post/What_are_the_best_plot_and_charting_C_package_which_can_be_used_for_data_visualization][What are the best plot and charting C++ package which can be used
  for data visualization?]]

*** Consider adding support for Crypto3                               :story:

#+begin_quote
Modern cryptography suite built in C++ with concept-based architecture.
#+end_quote

Links:

- [[https://github.com/NilFoundation/crypto3][GitHub]]

*** Consider adding support for =univalue=                            :story:

#+begin_quote
An easy-to-use and competitively fast JSON parsing library for C++17,
forked from Bitcoin Cash Node's own UniValue library.

Supports parsing and serializing, as well as modeling a JSON
document. The central class is UniValue, a universal value class, with
JSON encoding and decoding methods. UniValue is an abstract data type
that may be a null, boolean, string, number, array container, or a
key/value dictionary container, nested to an arbitrary depth. This
class implements the JSON standard, RFC 8259.
#+end_quote

Links:

- [[https://github.com/cculianu/univalue][univalue GH]]

*** Consider adding support for libstud-json                          :story:

From project:

#+begin_quote
The goal of this library is to provide a pull-style parser (instead of
push/SAX or DOM) and push-style serializer with clean, modern
interfaces and conforming, well-tested (and well-fuzzed, including the
serializer) implementations. In particular, pull-style parsers are not
very common, and we couldn't find any C++ implementations that also
satisfy the above requirements.
#+end_quote

Links:

- [[https://github.com/libstud/libstud-json/][libstud-json]]: "JSON parser/serializer library for C++"

*** Consider adding support for automigrate                           :story:

Interesting tool for SQL migrations.

Links:

- [[https://github.com/abe-winter/automigrate][automig GH]]: "Automigrate is a command-line tool for SQL
  migrations. Unlike other migration tools, it uses git history to do
  diffs on create table statements instead of forcing you to write
  up/down diffs for every change."
- [[https://abe-winter.github.io/2020/08/03/yr-of-git.html][One year of automatic DB migrations from git]]
- [[https://www.sqlalchemy.org/][sqlalchemy]]: "SQLAlchemy is the Python SQL toolkit and Object
  Relational Mapper that gives application developers the full power
  and flexibility of SQL."
- [[https://alembic.sqlalchemy.org/en/latest/][Alembic’s documentation]]: "Alembic is a lightweight database
  migration tool for usage with the SQLAlchemy Database Toolkit for
  Python."

*** Consider adding support for eCAL                                  :story:

Yet another interprocess communication framework.


Links:

- [[https://github.com/continental/ecal][ecal GH]]: The enhanced communication abstraction layer (eCAL) is a
  middleware that enables scalable, high performance interprocess
  communication on a single computer node or between different nodes
  in a computer network.

*** AutoMapper in Dogen                                               :story:

AutoMapper is an interesting C# library:

#+begin_quote
AutoMapper is an object-object mapper. Object-object mapping works by
transforming an input object of one type into an output object of a
different type. What makes AutoMapper interesting is that it provides
some interesting conventions to take the dirty work out of figuring
out how to map type A to type B. As long as type B follows
AutoMapper’s established convention, almost zero configuration is
needed to map two types.
#+end_quote

Whilst we don't entirely think its a good idea to use auto mapping at
run time, perhaps it may make more sense in terms of meta-model
constructs. We could abstract the rules that auto mapper has and use
them to generate mapping code. We can also reuse all of the mapping
annotations as meta-data to hint the mapping. This would be useful in
the cases where we generate two representations of objects that are
very similar such as a HTTP data model vs a domain model, or even in
some cases a relational model.

Links:

- [[https://docs.automapper.org/en/stable/Getting-started.html][AutoMapper: Getting Started Guide]]

*** Consider adding support for aspects in logical model              :story:

At present we avoided having any kind of logging in the generated
code. This was because we did not want users to be coupled to choices
in Dogen such as our logging choices. Since we mainly generate data
objects this has not been a great problem. However, as we move up the
stack and start generating other kinds of elements such as services,
factories and so on, it would be really nice to be able to have
logging. One possible way of solving this is to add support for
aspects. There are a few use cases:

- logging
- security, permissions
- metrics, benchmarks

For each of these cases we have fairly well defined patterns, but
which need some configuration. The user may not want them at all, or
may want to use a specific library for its use case. Each aspect then
has some options related to which implementation to use (this can be
supplied as meta-data). The aspect exists as a stand alone meta-model
element, but must be somehow accessible to any other meta-model
element such that when we start to code generate we can check the
aspects content's. Note that we do not allow users to supply their own
implementation of the aspect; they can merely configure the
aspect. Also, note that both the joint point (e.g. the place where the
advice will be put) as well as the advice (e.g. the code that we want
to put in if the aspect is enabled) will not be configurable by the
user. All that can be done is to choose from a list of options.

As a bit of context, Wikipedia [[https://en.wikipedia.org/wiki/Aspect-oriented_programming][states]]:

#+begin_quote
Join point models

The advice-related component of an aspect-oriented language defines a
join point model (JPM). A JPM defines three things:

1. When the advice can run. These are called join points because they
   are points in a running program where additional behavior can be
   usefully joined. A join point needs to be addressable and
   understandable by an ordinary programmer to be useful. It should
   also be stable across inconsequential program changes in order for
   an aspect to be stable across such changes. Many AOP
   implementations support method executions and field references as
   join points.
2. A way to specify (or quantify) join points, called
   pointcuts. Pointcuts determine whether a given join point
   matches. Most useful pointcut languages use a syntax like the base
   language (for example, AspectJ uses Java signatures) and allow
   reuse through naming and combination.
3. A means of specifying code to run at a join point. AspectJ calls
   this advice, and can run it before, after, and around join
   points. Some implementations also support things like defining a
   method in an aspect on another class.

Join-point models can be compared based on the join points exposed,
how join points are specified, the operations permitted at the join
points, and the structural enhancements that can be expressed.
#+end_quote

Links:

- [[https://en.wikipedia.org/wiki/Aspect-oriented_programming][Aspect-oriented programming]]
- [[https://stackoverflow.com/questions/325558/aspect-oriented-programming-examples][SO: Aspect-oriented programming examples]]

*** Consider adding support for static site generators                :story:

It should be possible to add meta-model elements that model static
site generators. We should then be able to generate the site. The
content files are added as org-babel. We should also add the build
files scripts to generate the file.

In an ideal world we want to be able to mix and match the static
generated content with Wt.

Links:

- [[https://lwn.net/Articles/825507/][Hugo: a static-site generator]]
- [[https://blog.getpelican.com/][Pelican Static Site Generator, Powered by Python]]: Pelican is a
  static site generator that requires no database or server-side
  logic.
- [[https://github.com/theiceshelf/firn/][firn GH]]: "Firn generates a static site from org-mode files. It is a
  bit different from other static site generators, in that it intends
  to be a drop in solution for creating sites from already existing
  folders of org-files." [[https://firn.theiceshelf.com/tutorial][Example site]].

*** Consider adding support for cpprouter                             :story:

From the site:

#+begin_quote
A modern, header-only request router for C++

Routing requests

This library is designed to route requests to an associated
callback. Requests are matched using a pattern, which may contain
embedded regular expressions. The data matched with the regular
expression is then parsed and provided to the callback.
#+end_quote

Links:

- https://github.com/omartijn/cpprouter

*** Consider adding support for LogDevice                             :story:

It would be nice if we could dump types into a distributed log such as
log device. Similar to AMQP support, we probably could also add
elements to the logical model to model core concepts of the log, which
need to be analysed but presumably must be like a topic, etc. We
should also be able to read and write to the log, etc.

Links:

- [[https://github.com/facebookincubator/LogDevice][LogDevice]]: "LogDevice is a scalable and fault tolerant distributed
  log system. While a file-system stores and serves data organized as
  files, a log system stores and delivers data organized as logs. The
  log can be viewed as a record-oriented, append-only, and trimmable
  file."
- [[https://github.com/cdapio/tigon][tigon]]: "Tigon is an open-source, real-time, low-latency,
  high-throughput stream processing framework."

*** Improve C# primitives                                             :story:

First we need to check if these are already supported. Then we need to
see if there are any things we're missing from projects already
implementing such types.

Primitives are in the current C# ref impl model.

Links:

- [[https://github.com/GeirGrusom/IdGenerator][IdGenerator GH]]: "This project adds a source generator to your
  project that creates identity types. It's used for types that
  identifies some resource. For example a user id or application id."
  See template at the bottom of [[https://github.com/GeirGrusom/IdGenerator/blob/master/src/IdGenerator/IdGenerator.cs][this]] file.

*** Consider allowing restrictions on primitives                      :story:

In XML it is possible to state the following:

#+begin_src xml
  <xsd:simpleType name="Scheme">
    <xsd:annotation>
      <xsd:documentation xml:lang="en">The base class for all types which define coding schemes that are allowed to be empty.</xsd:documentation>
    </xsd:annotation>
    <xsd:restriction base="NormalizedString">
      <xsd:maxLength value="255" />
    </xsd:restriction>
  </xsd:simpleType>
#+end_src

Then the type =Scheme= is now a restriction of a string:

#+begin_quote
The normalizedString data type is derived from the String data type.

The normalizedString data type also contains characters, but the XML
processor will remove line feeds, carriage returns, and tab
characters.
#+end_quote

It would be really nice if we could also have restrictions on our
primitive types, particularly with things such as normalised string.

*** Consider adding scripts or targets for core dumping               :story:

We will likely need the same kind of scripts to retrieve core dumps
from generated applications. We could either create CMake targets for
these or maybe shell scripts. At least for the most common scenarios.

Links:

- [[https://github.com/Microsoft/ProcDump-for-Linux][ProcDump GH]]: ProcDump is a Linux reimagining of the classic ProcDump
  tool from the Sysinternals suite of tools for Windows. ProcDump
  provides a convenient way for Linux developers to create core dumps
  of their application based on performance triggers.
- [[https://linux.byexamples.com/archives/371/gcore-obtain-core-dump-of-current-running-application/][gcore: obtain core dump of current running application]]

*** Consider adding support for =zq= and ZNG                          :story:

Interesting tool for log parsing, and associated format.

Links:

- [[https://github.com/brimsec/zq][GH project]]

*** Consider adding support for =objectbox=                           :story:

This is a very interesting project:

#+begin_quote
ObjectBox is a superfast database for objects. These are the C and C++
APIs to run ObjectBox as an embedded database in your C/C++
application. In this embedded mode, it runs within your application
process.
#+end_quote

Links:

- [[https://github.com/objectbox/objectbox-c][GH project]]

*** Consider adding support for =libmish=                             :story:

#+begin_quote
Libmish adds a command prompt to your program, and telnet access. With
your own commands.
#+end_quote

Links:

- [[https://github.com/buserror/libmish][libmish GH]]
- [[https://github.com/ggerganov/incppect][incppect GH]]: Inspect C++ memory in the browser

*** Consider adding support for gatbage collection via Oilpan         :story:

This looks like an interesting project: Oilpan.

#+begin_quote
Oilpan is a garbage collector written in C++ for managing C++ memory
#+end_quote

Links:
- [[https://chromium.googlesource.com/v8/v8.git/+/HEAD/include/cppgc/][source code]]
- [[https://v8.dev/blog/high-performance-cpp-gc][High-performance garbage collection for C++]]

*** Add support for QR codes                                          :story:

It should be pretty straightforward to add support for QR
codes. Wikipedia definition:

#+begin_quote
QR code (abbreviated from Quick Response code) is the trademark for a
type of matrix barcode (or two-dimensional barcode) first designed in
1994 for the automotive industry in Japan. A barcode is a
machine-readable optical label that contains information about the
item to which it is attached. In practice, QR codes often contain data
for a locator, identifier, or tracker that points to a website or
application. A QR code uses four standardized encoding modes (numeric,
alphanumeric, byte/binary, and kanji) to store data efficiently;
extensions may also be used.
#+end_quote

Links:

- [[https://github.com/nayuki/QR-Code-generator/tree/master/cpp][QR-Code-generator GH]]: This project aims to be the best, clearest QR
  Code generator library in multiple languages. The primary goals are
  flexible options and absolute correctness. Secondary goals are
  compact implementation size and good documentation comments. See
  also [[https://github.com/nayuki/QR-Code-generator][top-level page]].
- [[https://en.wikipedia.org/wiki/QR_code][QR code Wikipedia]]
- [[https://github.com/RaymiiOrg/cpp-qr-to-png][cpp-qr-to-png GH]]: A bridge between two great libraries,
  QR-Code-Generator and Tiny-PNG-Out. Article [[https://raymii.org/s/software/Cpp_generate_qr_code_and_write_it_to_png_scaled.html][here]].

*** Investigate SQLancer                                              :story:

This looks like an interesting project:

#+begin_quote
SQLancer (Synthesized Query Lancer) is a tool to automatically test
Database Management Systems (DBMS) in order to find logic bugs in
their implementation. We refer to logic bugs as those bugs that cause
the DBMS to fetch an incorrect result set (e.g., by omitting a
record).
#+end_quote

Links:

- [[https://github.com/sqlancer/sqlancer][GH]]

*** Investigate how RBAC can be integrated with Dogen                 :story:

At some point we will need permissions in Dogen. RBAC is a possibility.

Links:

- http://cmw.web.cern.ch/CMW/rbacAPI/index.html
- http://www.authenticationworld.com/Single-Sign-On-Authentication/SSOandLDAP.html
- https://payhip.com/b/41Tw
- https://git.shibboleth.net/view/?p=cpp-opensaml.git;a=summary
- [[https://github.com/casbin/casbin-cpp][casbin-cpp GH]]: An authorization library that supports access control
  models like ACL, RBAC, ABAC in C/C++.
- [[https://github.com/sirikata/liboauthcpp][liboauthcpp GH]]: A pure C++ OAuth library.
- [[https://github.com/InfiniteRasa/Authentication-Server][Authentication-Server]]
- [[https://stackoverflow.com/questions/11580944/client-to-server-authentication-in-c-using-sockets][Client to Server Authentication in C++ using sockets]]
- [[https://www.omg.org/news/meetings/workshops/presentations/docsec_present/1997/1-1.pdf][Introduction to CORBA Security]]
- [[https://fusionauth.io/learn/expert-advice/oauth/modern-guide-to-oauth/][The Modern Guide to OAuth]]

*** Consider adding OpenHMI support                                   :story:

This is a Qt only framework but seems very interesting:

#+begin_quote
"Open HMI Tester" or OHT is an application framework for the
development of GUI testing tools. It uses real-time GUI introspection
to capture and simulate real user interaction, which enhances
robustness and tolerance to changes during testing stage.
#+end_quote

Links:

- [[https://github.com/pedromateo/openhmitester][GH project]]

*** Consider adding AMQP support                                      :story:

It seems fairly straightforward to extend the logical model to support
AMQP. We could add the following entities:

- cluster: package. Has a name.
- node: package. belongs to a cluster, has a name.
- virtual host: package. belongs to a node, has a name.
- exchange: class. has a name, belongs to a host.
- queue: class. has a name, belongs to a host.
- binding: relationship. associates exchanges with exchanges or
  queues. Has a routing key and a set of KVPs (called arguments).

With this we can do the basic modeling of simple setups for AMQP. We
could also generate code that generates this infrastructure (or
destroys it).

Notes:

- we could associate objects in the object model to topics and to
  routing keys. This could be done via a short-hand that says "for
  this object create an associated topic" whatever that means
  precisely.
- we could generate all the code for reading and writing the object.
- we should add these logical model elements to AMQP namespace and
  wait until other MQs show up. Then we can abstract them into a MQ
  namespace, potentially.
- initialisation code should only be triggered on demand; this is
  because if we have multiple services we may have race conditions. We
  need to ensure one of them has the lock to create the
  infrastructure.

Links:

- [[https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol][Advanced Message Queuing Protocol]]
- [[https://www.rabbitmq.com/tutorials/amqp-concepts.html][RabbitMQ: AMQP 0-9-1 Model Explained]]
- [[https://qpid.apache.org/releases/qpid-cpp-1.39.0/index.html][Qpid C++ 1.39.0]]: Qpid C++ offers a connection-oriented messaging API
  and a message broker written in C++ that stores, routes, and
  forwards messages using AMQP.
- [[https://github.com/apache/qpid-cpp][qpid GH]]. [[https://github.com/apache/qpid-cpp/tree/master/examples/messaging][Examples]].
- [[https://github.com/bloomberg/corokafka][corokafka GH]]: Coroutine-based Kafka messaging library! CoroKafka is
  a scalable and simple to use C++ library built on top of cppkafka
  and quantum, supporting any number of parallel producers and
  consumers.
- [[https://github.com/mfontanini/cppkafka][cppkafka GH]]: "cppkafka allows C++ applications to consume and
  produce messages using the Apache Kafka protocol. The library is
  built on top of librdkafka, and provides a high level API that uses
  modern C++ features to make it easier to write code while keeping
  the wrapper's performance overhead to a minimum.

*** Consider supporting nano-id                                       :story:

At present we are relying on UUIDs for several things such as tracing
file names. These will be used even more once we get into
services. Apparently there is a slightly more compact way of doing
this: nano-id. We could not locate a port to C++.

Links:

- [[https://github.com/codeyu/nanoid-net][GH nanoid-net]]: nano-id for C#
- [[https://medium.com/javascript-in-plain-english/you-might-not-need-uuid-v4-for-generating-random-identifiers-89e8a28a7d77][You Might Not Need UUID V4 for Generating Random Identifiers]]

*** Consider developing a org-mode log format                         :story:

We seem to be applying org-mode to everything, with some
success. Perhaps it could also be applied to log files. Whilst the
structure of the logs is normally flat, they are encoding a space with
topology. That is to say, if one imagines a single-threaded program
(multi-threaded is much more complex), it would be great if one could
see the logged data as a graph that is isomorphic to the product and
component structure. Note though that we are not proposing a
"physical" representation of the graph, or else we will end up with a
debugger output or a profile (e.g. function call tree). Instead, the
idea is that with the domain knowledge one has, you can choose what
containment relationships are relevant and these may not at all
coincide with the function call tree. Instead, what you are looking
for is "readability" of the graph: can a user follow it through and
figure out what high-level operations the system is performing?

Most of the data in a log file can be encoded in org-mode:

- timestamps can be encoded as org-mode timestamps.
- log level can be a tag.
- object and attribute names can be titles in a header.
- content can be encoded depending on the types; see the story on
  tracing via org-mode for ideas.

In addition, for performance reasons we could encode the data into a
binary format, which can be cracked open into org-mode as required. We
could also create a "org-logger" designed specifically to log data in
this hierarchy, with appropriate functions (create scope, etc). We
could add appropriate scope guards to perform entry and exit of scopes
as we do with tracing. We could also use macros as boost log does for
performance reasons.

Its important to note that this would be very complicated in a
multi-threaded world. We could take the easy approach and make each
thread a root topic, but then its not clear how one would follow the
context from where the thread was created.

Links:

- [[https://kitchingroup.cheme.cmu.edu/blog/2017/01/03/Find-stuff-in-org-mode-anywhere/][Find stuff in org-mode anywhere]]: org-mode in a SQLite database.

*** Consider adding support for SignalR                               :story:

SignalR:

#+begin_quote
SignalR is a free and open-source software library for Microsoft
ASP.NET that allows server code to send asynchronous notifications to
client-side web applications. The library includes server-side and
client-side JavaScript components.
#+end_quote

Once we have support for Casablance, we can also consider adding
support for SignalR.

Links:

- [[https://github.com/aspnet/SignalR-Client-Cpp][SignalR-Client-Cpp]]: C++ Client for ASP.NET Core SignalR
- [[https://blog.3d-logic.com/2015/05/20/signalr-native-client/][SignalR c++ client]]: blog post on c++ client implementation.
- [[https://github.com/SystemTera/signalr-cpp][signalr-cpp]]: SignalR C++ Implemenation (server)

*** Add support for T2T transforms                                    :story:

We have done a lot of analysis work on text-to-text (T2T) transforms,
and even implemented the key classes but it was not suitable for
stitch and wale so it was removed. It should be usable for cartridges
though such as ODB and XSD tool.

Commits:

: b9e7fe0263 text: remove text to text transforms
: c4e1e092d6 text: remove t2t transform data
: 9fcf0ec8fc integration text.cpp, physical: remove unused attribute for text to text transform
: ceee85f654 text.cpp: remove used wale key for text template transform
: 5bd2ea45b4 text.cpp: remove used functionality in wale for text template transform

*** Consider making =stitch_template_builder= a text template builder :story:

At present we are manually crafting a stream in
=stitch_template_builder= that resembles a stitch template. Perhaps we
should be building the domain objects of a stitch template instead,
and then have a formatter than renders the template as a string. This
would be the "opposite" of the current formatter which instantiates
the template.

*** Consider implementing transforms as dataflow graphs               :story:

Transforms are very similar data structures to the elements of a data
flow pipeline: it would probably not require huge amounts of mapping
to create a data flow pipeline that represents the transforms and
transform chains in Dogen. We also now have a very good idea of what
the interfaces for transforms should be in each model. We should
probably wait until we have sorted out the story around error messages
but once that is done its worth investigating if transforms can be
implemented as a graph in a data flow library. Performance may benefit
a little - there are a few things we can do in parallel such as
injection - but its not the key objective here.

[[https://github.com/taskflow/taskflow][Taskflow]] in particular looks very promising:

#+begin_src c++
tf::Executor executor;
tf::Taskflow taskflow;

auto [A, B, C, D] = taskflow.emplace(  // create four tasks
        [] () { std::cout << "TaskA\n"; },
        [] () { std::cout << "TaskB\n"; },
        [] () { std::cout << "TaskC\n"; },
        [] () { std::cout << "TaskD\n"; }
        );

A.precede(B, C);  // A runs before B and C
D.succeed(B, C);  // D runs after  B and C

executor.run(taskflow).wait();
#+end_src

We could easily declare dependencies between transforms this way. This is
specially useful for chains.

Links:

- [[http://flowbasedprogramming.com/docs/html/index.html][DSPatch]]: The Refreshingly Simple C++ Dataflow Framework
- [[https://github.com/RaftLib/RaftLib][RaftLib GitHub]]: The RaftLib C++ library, streaming/dataflow
  concurrency via C++ iostream-like operators. See also [[http://raftlib.io][home page]].
- [[https://github.com/taskflow/taskflow][Taskflow GH]]: Modern C++ Parallel Task Programming. [[https://taskflow.github.io/taskflow/index.html][API docs]].

*** Consider adding suppot for SeqAn                                  :story:

This is a very interesting library, though its not clear how we'd use
it.

Links:

- [[https://github.com/seqan/seqan3][seqan3 GitHub]]: The modern C++ library for sequence
  analysis. Contains version 3 of the library and API docs

*** Use static registration with initialisers                         :story:

Since the start, we avoided using static registration for
initialisation due to the static initialisation order fiasco. Its much
better to manually determine the order of initialisation and do it
under programatic control rather than depend on the linker. However,
the downside is that we now have lots of code that needs to be called,
and every so often we forget to join all the dots. Perhaps we need
something in between complete "manual registration" and static
registration. Instead of supplying the registrars from the top-level,
we could instead:

- use static registration for a top-level initialiser. This is a very
  simple interface that has only one method: initialise. It uses
  regular static registration, but it merely adds itself to a
  list. Nothing else happens during static initialisation.
- when program starts, we call =initialise()= on all initialisers.
- within a given component, the top-level initialiser calls other
  initialisers. Internally, it obtains references to static registrars
  as required (e.g. features, etc). All of this happens during normal
  program execution, so we can log.
- DLLs can register initialisers on load. However, we are expected to
  load them prior to calling initialisation.
- all registrars should have a "validate" method. We should check that
  they are not empty. This method should be called prior to use. We
  should also have a "initialised" flag that stops
  double-initialisation. It should be set as the last step of
  initialisation.

Actually we changed our minds on this story. We are keeping it in the
backlog because it may be of use for something else, but for this
particular case what we really want is to implement "static DI". There
shall be a story for this.

Links:

- [[https://dxuuu.xyz/cpp-static-registration.html][C++ patterns: static registration]]

*** Consider adding a =--dump-stats= option                           :story:

It would be nice if one could know some stats for a given run:

- total models loaded.
- total elements processed.
- total files generated.
- total files written to the filesystem.
- total files that have not changed.
- total files ignored via regexes.
- time spent on each part of the pipeline. Percentages of the overall
  cost.

Users could supply a command line option such as =--dump-stats= and
then Dogen would output all of the stats to the console.

Data should be dumped in a way that would facilitate the creation of
timeseries and plots. For example, we should be able to do a dump for
each release of dogen and then create a timeseries. We could have a
=--reporting-style= of =csv= for instance.

Merged stories:

*Dump model stats and metrics*

It would be nice to be able to get some information about a model. We
already have a dumpspecs command but that is more for the system
itself. What we also need is to dump stats on the model such as number
of objects, whether the generated code is up-to-date, etc. We could
also link this story with [[*Add support for dependency graphs and complexity data][measurements of complexity]].

*** Feature toggles and code generation                               :story:

This article by Fowler has some interesting ideas:

- [[http://martinfowler.com/articles/feature-toggles.html][Feature Toggles]]

It is not clear how it applies to code generation, if at all. This
story is to bookmark the article so we can mine it later for ideas.

Note that we would not use feature toggles directly in dogen, because
we are already using feature models for this. But we could conceivably
generate code for applications to use feature toggles. Actually that's
not quite true; variability model has a role to play within
code-generation. However, we could still make use of a feature model
for other purposes:

- when we have a web api, we may want to enable or disable
  functionality in that service, possibly dynamically (for A/B
  testing).
- when we have a UI, we will have a similar requirement.

These two approaches serve different needs. In addition, feature
toggles are not specific to Dogen and can be used by any product
generated by Dogen.

*** Add support for design by contract                                 :epic:

If dogen had a way to parse [[https://en.wikipedia.org/wiki/Object_Constraint_Language][OCL]], we could allow users to specify OCL
constraints on operations. These could then be used to code-generate
design by contract code. For C++ we could use [[https://www.boost.org/doc/libs/develop/libs/contract/doc/html/index.html][Boost.Contract]]. This
appears to be a very difficult thing to do. There may also be C++ 20
support for contracts.

Links:

- [[https://www.reddit.com/r/cpp/comments/cmk7ek/what_happened_to_c20_contracts/][What Happened to C++20 Contracts?]] Contracts didn't make it to
  C++ 20.
- [[https://www.modernescpp.com/index.php/c-core-guidelines-a-detour-to-contracts][C++ Core Guidelines: A Short Detour to Contracts in C++20]]

*** Add the the EMF purchase order examples to manual                 :story:

We should make use of the examples for eCore to demo Dogen. We could
make a similar chapter flow in the manual, starting with a simpler
model and then evolving it to the most complicated one. It would
require a little bit of an adaptation since Dogen does not support all
of the features that EMF has, but it seems a better way to demo Dogen
rather than just talk about the code structure. We could add this
after the section on authoring diagrams. Or maybe we should try to
build all of the missing features in Dogen as part of this
exercise. This could form the basis of the Dogen manual.

*** Investigate CLAFER                                                :story:

As per their site:

#+begin_quote
Clafer is a general-purpose lightweight structural and behavioral
modeling language developed by GSD Lab, University of Waterloo, and
MODELS group at IT University of Copenhagen. Clafer can be used for
modeling of static hierarchical structures and for modeling the change
of the structures over time (behavior). Clafer allows for naturally
expressing variability in both the structure and behavior. The main
goal of Clafer is to make modeling more accessible to a wider range of
users and domains.
#+end_quote

Links:

- [[https://github.com/gsdlab/clafer][GitHub repo]]

*** Consider adding a GIR/GObject injector                            :story:

GIR seems to be yet another format that could be suitable for a dogen
injector.

Links:

- [[https://gitlab.com/mnauw/cppgir][cppgir]]
- [[https://www.reddit.com/r/cpp/comments/bbp25s/gobjectintrospection_c_binding_wrapper_generator/][GObject-Introspection C++ binding wrapper generator]]

*** Message replay                                                    :story:

This is a very speculative story. Once we are able to generate
services with messages, etc., it would make sense to have a "replay"
mode. This basically recalls a session with requests and responses so
that users can replay a session. The messages should be stored on a
database (or perhaps the filesystem? maybe it should be a meta-data
variable). We should also have an interface with an history of
requests and responses and allow users to re-submit a request,
switching on logging for that particular request. Logging is also
written to a database table, allowing one to correlate the request
processing with the logging. Even better would be if any request made
to downstream systems would also enable those flags so that we could
follow through with the logging.

Links:

- [[https://firefox-replay.com/][Firefox WebReplay]]: main site. Should give us some ideas for the UI.
- [[https://developer.mozilla.org/en-US/docs/Mozilla/Projects/WebReplay][Firefox WebReplay]]: documentation.

*** Consider adding support for binary logging                        :story:

Dogen logs are very large when we log at high-level such as =debug=
and =trace=. It also takes a bit longer to run. This means its not
very practical to enable logging all the time, only when we are
troubleshooting. However, for "dogen as a service" we probably will
end up using logging a lot more often so it would be nice to have a
more efficient way of logging. One possibility is to use binary logs.

Links:

- [[https://github.com/Morgan-Stanley/binlog][GH binlog]]: A high performance C++ log library, producing structured
  binary logs http://binlog.org
- [[https://github.com/odygrd/quill][quill]]: Asynchronous Low Latency Logging Library
- [[https://github.com/KjellKod/g3log][g3log]]: G3log is an asynchronous, "crash safe", logger that is easy
  to use with default logging sinks or you can add your own.
- [[https://github.com/PlatformLab/NanoLog][NanoLog]]: Nanolog is an extremely performant nanosecond scale logging
  system for C++ that exposes a simple printf-like API.
- [[https://www.reddit.com/r/cpp/comments/n67jhg/fmtlog_fastest_c_logging_library_using_fmtlib/][reddit: fmtlog: fastest C++ logging library using fmtlib syntax]]: not
  binary logging but faster logging nonetheless.

*** Consider using command line indicators                            :story:

Dogen is, by design, pretty quick to execute. However, we may want to
consider supporting a command line progress bar. This would be useful
when we use relational dumping, for example.

Links:

- [[https://github.com/p-ranav/indicators][indicators]]
- [[https://github.com/Llawliet0872/progress_bar_cpp][progress_bar_cpp GH]]: "This is a simple library for progress bars in
  c++."

*** Consider adding support for tabulate                              :story:

This library allows dumping tables to the command line. It would be
nice if we could have a facet that allows IO as a table for certain
types.

Links:

- [[https://github.com/p-ranav/tabulate][tabulate]]

*** Consider visualisation like source trail                          :story:

Once we move to org files, we will no longer be able to visualise the
files easily. We had considered using graphviz. An interesting
alternative is to see what lessons can be learned from source
trail. We don;t necessarily need a UI exactly like source trail, but
instead something that would work within emacs and display a
"graphical" (e.g. text) view of the model as we are editing. We also
need some kind of integration like org-noter, whereby as we move
around the org document, the visualisation changes. The data being
visualised should be like source trail. It should also be possible to
link back to the physical model (e.g. for a given model element,
display the files it generates). In effect we need more of a "dogen
mode" which does a number of things at the same time:

- LSP on org-mode.
- specialised treemacs.
- COGRE displaying UML diagram.
- preview of file which updates when model is regenerated.

Similarly, if we had a "dogen studio", the UI could contain a
visualisation for this.

We should also see C++ Depend.

Notes:

- this could be implemented as an emacs mode that calls dogen behind
  the scenes to generate the lisp representing the portion of the
  diagram we're interested in. In fact, better than COGRE would be to
  just do a partial PlantUML diagram with classes "near" the current
  class; as you are moving in the org mode file, the dogen LSP server
  is returning the svg for the "nearby" classes. This could be
  implemented in a generic way so that any LSP server could use it.

Links:

- [[https://lsif.dev/][LSIF]]: "The Language Server Index Format (LSIF, pronounced “else if”)
  is a standard format for language servers or other programming tools
  to emit their knowledge about a code workspace. This persisted
  information can later be used to answer LSP requests for the same
  workspace without running a language server."
- [[https://github.com/sourcegraph/sourcegraph][sourcegraph GH]]
- [[https://www.youtube.com/watch?v=Cfu6f0uyzc8][SourceTrail demo]]
- [[https://github.com/weirdNox/org-noter][org-noter github]]
- [[https://www.youtube.com/watch?v=Un0ZRXTzufo][org-noter demo]]
- [[http://cedet.sourceforge.net/uml.shtml][CEDET COGRE]]: Emacs mode for UML diagrams.
- [[https://www.cppdepend.com/cppdependv2020][C++ Depend]]

*** Create some basic naming guidelines                               :story:

As per Framework Design Guidelines, we need some basic guidelines for
naming in Dogen. We don't need to go overboard, we just need something
to get us started and evolve it as we go along.

For extra bonus points, it would be nice to integrate these with
Dogen, such that one would get warnings or errors when processing a
diagram.

Links:

- [[https://isocpp.org/wiki/faq/coding-standards][C++ Coding Standards]]
- [[http://wiki.c2.com/?CapitalizationRules][Capitalization Rules]]
- [[https://en.wikipedia.org/wiki/Snake_case][Snake Case]]
- [[http://cs.smu.ca/~porter/csc/ref/stl/naming_conventions.html][Naming Conventions for these STL Reference Pages]]
- [[https://style-guides.readthedocs.io/en/latest/cpp.html][C++ coding style guide]]
- [[https://stxxl.org/tags/1.4.1/coding_style.html][Coding Style Guidelines]]
- [[https://www.fluentcpp.com/2018/04/24/following-conventions-stl/][Make Your Containers Follow the Conventions of the STL]]

*** Add Levenshtein distance to enums when converting from string     :story:

When we fail to convert a string to an enum, it would be nice if the
converter could, optionally, calculate the levenshtein distance of the
string to valid enums and provide suggestions. We could add these to
the exception error.

*** Add GraphViz/Dot export support                                   :story:

At present we assume the physical model is the only destination for
models. However, once we start moving to text based injectors, it may
be useful to generate UML diagrams. This could be achieved via dot.

Notes:

- this could be added to the convert activity.
- we could then have options such as include attributes, include
  comments, etc.
- PlantUML makes use of dot to generate nice-looking diagrams. We
  should try to get some examples of the dot code they generate and
  see if we can copy it. [[https://real-world-plantuml.com/][Image examples here]]. It [[https://forum.plantuml.net/2034/add-dot-as-output-format][may not be possible]]
  though.
- graphviz can be consumed as a library. This would mean we wouldn't
  have any external dependencies, and we could generate PDFs
  directly.
- not yet supported on [[https://github.com/Microsoft/vcpkg/issues/737][vcpkg]].

Links:

- [[http://www.ffnn.nl/pages/articles/media/uml-diagrams-using-graphviz-dot.php][UML Diagrams Using Graphviz Dot]]
- [[https://lornajane.net/posts/2011/uml-diagrams-with-graphviz][UML Diagrams with Graphviz]]
- [[https://www.graphviz.org/pdf/libguide.pdf][Using Graphviz as a Library]]
- [[https://fsteeg.wordpress.com/2006/11/17/uml-class-diagrams-with-graphviz/][UML class diagrams with Graphviz]]
- [[https://www.graphviz.org/pdf/dotguide.pdf][Drawing graphs with dot]]
- [[https://www.drdobbs.com/cpp/graphviz-and-c/184402049][GraphViz and C++]]

*** Syntax checking support for stitch templates                      :story:

It would be really nice to have syntax checking for stitch
templates. This may be simpler than expected. We have two types of
errors:

- stitch errors: these are invalid stitch templates. We already detect
  these. We just need to output a GCC-style error message.
- c++ errors: these occur after we've weaved, by compiling the c++
  code. These are slightly trickier: we need to map the c++ lines to
  the stitch lines; this can be done during the generation of the
  stitch template, where as we output the code, we also keep track of
  the lines. For example, in a loop, one stitch template line will map
  to many c++ lines. We also need access to the compilation
  database. We then shell out using Boost.Process, run the compiler
  against the c++ code, retrieve the GCC/Clang errors, and map the
  line and columns from the compiler back into the stitch line and
  columns. We then output the errors, but using the stitch line and
  column numbers (presumably the column remains unchanged, or perhaps
  we should just remove it).

Ideally we want to preserve the original compiler output as much as
possible, only interfering with the mapping. We should also make sure
we only map lines in the template file, ignoring everything else.

We will also have to reactivate the weaving mode, so this will require
a bit of thinking:

- we only want to weave a single template; however, in the new world,
  that is actually equivalent to weaving the entire model because we
  need to go through all of the transforms.
- this also means we may end up with lots of errors from other
  templates. We need to avoid this.

This should be implemented via LSP delegation (look for story on
this).

*** Consider adding TableGen                                          :story:

Its not entirely clear how one would use this but since its code
generation, we've added a story here. From the docs:

#+begin_example
TableGen’s purpose is to help a human develop and maintain records of
domain-specific information. Because there may be a large number of
these records, it is specifically designed to allow writing flexible
descriptions and for common features of these records to be factored
out. This reduces the amount of duplication in the description,
reduces the chance of error, and makes it easier to structure domain
specific information.

The core part of TableGen parses a file, instantiates the
declarations, and hands the result off to a domain-specific backend
for processing.

The current major users of TableGen are The LLVM Target-Independent
Code Generator and the Clang diagnostics and attributes.
#+end_example

Links:

- https://llvm.org/docs/TableGen/

*** Consider adding support for TOML                                  :story:

[[https://github.com/toml-lang/toml][TOML]] (Tom's Obvious, Minimal Language) is a configuration file
format. Its very similar to INI files.

Links:

- [[https://github.com/marzer/tomlplusplus/][toml++]]: header-only toml parser and serializer for C++17, C++20 and
  whatever comes after.
- [[https://github.com/ToruNiina/toml11][toml11]]: TOML for Modern C++
- [[https://github.com/skystrife/cpptoml][cpptoml]]: cpptoml is a header-only library for parsing TOML
- [[https://www.reddit.com/r/cpp/comments/f70io2/toml_a_toml_parser_and_serializer_for_c17/][reddit discussion]]

*** Consider integration with GNU plot library                        :story:

For certain models which produce time-series data, it may make sense
to integrate with GNU plot to generate graphs. In fact, in general, it
would be nice if one could decide to serialise a type into a GNU Plot
graph. We need to find specific examples.

Links:

- [[https://github.com/vincent-picaud/GnuPlotScripting][GnuPlotScripting]]: A simple C++17 lib that helps you to quickly plot
  your data with GnuPlot

*** Consider adding JSON schemas and XSDs as an injection source      :story:

Similar to XSD, perhaps JSON schemas could also be used to inject
models into Dogen. This could be useful in the context of the Open API
specification.

Links:

- [[https://tools.ietf.org/html/draft-wright-json-schema-00][JSON Schema: A Media Type for Describing JSON Documents]]
- [[https://cswr.github.io/JsonSchema/spec/introduction/#json][JSON schema introduction]]
- [[https://www.json.org/json-en.html][JSON spec]]
- [[http://json-schema.org/][JSON Schema]], [[https://modeling-languages.com/jsonschema-uml-tool-generate-diagrams-json/][JSONSchema To UML]]
- [[https://jsonnet.org/][jsonnet]]
- [[https://json-schema.org/learn/miscellaneous-examples.html][JSON schema examples]]
- [[https://github.com/aspnet/Home/wiki/Project.json-file][Project.Json]]
- [[https://github.com/json-schema-org/JSON-Schema-Test-Suite][JSON-Schema-Test-Suite]]
- [[https://github.com/pboettch/json-schema-validator][json-schema-validator]]: JSON schema validator for JSON for Modern C++

*** Consider generating SSL certificates and keys                     :story:

When creating an HTTPS service, one needs to have certificates,
etc. It would be nice if dogen could create self-signed certificates,
or added support for Lets encrypt. This could be done via integration
with OpenSSL library.

*** Add support for terraform                                         :story:

Terraform is a Infrastructure as Code (IaC) language. We should be
able to model most of its concepts into the assets meta-model
(modules, variables, =main.tf=, etc). In an ideal world, the user
should be able to add a model with terraform elements and generate the
IaC for it.

For extra bonus points we should integrate the CI (travis, etc) with
the IaC. We should also have a policy of naming for environments
etc. This could be part of a theme.

Links:

- [[https://en.wikipedia.org/wiki/Terraform_(software)][Terraform Wikipedia]]
- [[https://www.terraform.io/docs/configuration/syntax.html][Terraform: Configuration Syntax]]
- [[https://www.terraform.io/docs/configuration/index.html][Configuration Language]]
- [[https://github.com/syohex/emacs-terraform-mode][terraform emacs mode]]

*** Consider using ditaa                                              :story:

Ditaa is a diagramming tool that takes ascii art and creates
diagrams. It could be useful as an injector or as an output technical
space.

Links:

- http://ditaa.sourceforge.net/
- http://plantuml.com/ditaa: integration with Plant UML.

*** Consider adding a wt frontend                                     :story:

Once we have the relational model hooked in, the next step is to make
use of the available information in the database. We will start with
simple PL/SQL scripts etc but this is a cumbersome way of
understanding the data. It would be really nice to have a wt interface
to visualise all the information in the database such as the transform
graph.

Ideas:

- we could create a baobab like graph, if it is available in wt, to
  show the cost of each transform.
- we could also display the associated log lines with each transform
  and have a "filtering function" that uses postgres' text search.
- we could also add the transform graph. The ideal scenario would be
  to have an interactive graph with weighted sizes for each transform
  and chain, and allow users to click on a chain to see its
  components.
- we can start making comparisons of run duration per target model,
  per operation. We can even make sure we are comparing like with like
  because we can see what the initial options were - though it is a bit
  harder to tell if the model configuration is comparable. This could
  be achieved by diffing the variability models...
- we need a screen that captures the "element lifecycle" across all
  stages of the pipeline, and can be drilled down to individual
  transforms. Even better would be to be able to see the "before" and
  "after" state of the element. In an ideal world, the JSON in the
  database would be formatted so that we can run a quick diff (DTL
  would do) on the before and after. This way we can quickly validate
  that a given model element was processed correctly all the way until
  generation.
- it would be nice to have a graph that shows how a given injection
  element was converted to a number of assets elements and these into
  a number of artefacts. Perhaps a tree is better to display this?
  Maybe there are several "tree views": the assets view, the injection
  view, the "extraction view" and the tree represents the model for
  that view?
- we should have a log viewer that enables us to see the log
  entries. This could be just a flat log viewer, or a transform graph
  based log viewer. In this case it would have the transform tree on
  the left and as the user selects nodes the log lines would narrow to
  the current transform or transform chain. The log view should have
  filters and be searchable. It should make errors and warnings very
  easy to spot and allow filtering on those as well (e.g. log level
  filter). For log lines with JSON, it would be nice to be able to
  detect the JSON by looking at object markers (e.g. ={}=, =[]=, etc)
  and extract the JSON out of the log line and format it. An
  interactive JSON widget would be even better.
- it would be nice to have a "validation checks view", where we could
  see all of the checks that were performed and their result. These
  are the checks that one day will be converted to errors and warnings
  to output to end users by the command line.
- use injection elements as a filter: it may be nice to have a list of
  all injection elements and then only display those which are
  selected. The idea would be that the user would have a "complete
  view" on the right with all transforms, etc - but only the assets
  and extraction elements that are associated with selected injection
  elements are displayed. This would make troubleshooting much easier.

Merged stories:

*Charts in site*

When we have a dogen site with code-generation functionality, we could
allow users to produce transform probe data. This could be stored in
redis. We could then have a json diff in the site. We could also have
a visualisation like baobab with a tree on one side and a graph
breaking down the time. And even more ideal would be if we could see
all log lines from the start and end transform.

Links:

- [[https://github.com/nlohmann/json][JSON]]: C++ JSON library that supports the json diff spec.
- [[https://wiki.gnome.org/Apps/Baobab][baobab screenshot]]

*** Consider using typedefs for variants                              :story:

When we create types that have variants, the generated code is really
awkward. It would be nice if users could create typedefs that are used
internally in the class using the variant (and possibly
everywhere?). We need support for typedefs as a meta-model element
first.

*** Investigate if Seamless template can be used instead of stitch    :story:

According to their project:

#+begin_quote
Seamless (CMLS) is a brand-spanking new experimental web framework. It
includes:

- A C++20 template engine that allows you to write C++ and markup in
  the same file, just like you would with Laravel's blade engine for
  PHP, or ASP.NET Razor. The file extension for these files is .cmls.
  ...
#+end_quote

Links:

- [[https://github.com/haikarainen/cmls][cmls]]

*** Consider adding support for OData                                 :story:

OData is defined as follows:

#+begin_quote
OData (Open Data Protocol) is an ISO/IEC approved, OASIS
standard that defines a set of best practices for building and
consuming RESTful APIs. OData helps you focus on your business logic
while building RESTful APIs without having to worry about the various
approaches to define request and response headers, status codes, HTTP
methods, URL conventions, media types, payload formats, query options,
etc. OData also provides guidance for tracking changes, defining
functions/actions for reusable procedures, and sending
asynchronous/batch requests.
#+end_quote

Dogen could generate the schema, the entities in the expected formats
and the end-points. It could also be used to generate the client code
as per examples.

Note that there are a lot of similarities between GraphQL and OData so
we should look at these two stories at the same time to make sure we
capture the commonalities.

Links:

- [[https://www.odata.org/][OData]]
- [[https://www.odata.org/documentation/][OData documentation]]
- [[https://stackoverflow.com/questions/32858371/what-is-swagger-and-does-it-relate-to-odata][What is Swagger and does it relate to OData?]]
- [[https://github.com/OData/odatacpp-server][odatacpp-server]]: github project for an OData server.
- [[https://github.com/OData/odatacpp-client][odatacpp-client]]: github project for an OData client.
- [[https://www.odata.org/getting-started/basic-tutorial/][OData: Basic Tutorial]]
- [[https://jeffhandley.com/2018-09-13/graphql-is-not-odata][GraphQL is not OData]]
- [[https://github.com/microsoft/cppgraphqlgen][cppgraphqlgen]]
- [[https://taocpp.github.io/][A collection of high-quality C++ libraries]]
- [[https://github.com/taocpp/PEGTL][PEGTL]]: library used to generate a parser for GraphQL.

*** Consider adding support for BERT                                  :story:

It seems this is yet another RPC framework:

#+begin_quote
BERT and BERT-RPC are an attempt to specify a flexible binary
serialization and RPC protocol that are compatible with the
philosophies of dynamic languages such as Ruby, Python, PERL,
JavaScript, Erlang, Lua, etc.
#+end_quote

Links:

- [[http://bert-rpc.org/][specification]]

*** LSP daemon for MOP                                                :story:

One of the problems we have when creating models is that there is no
assistance from the editor. At present, because we are using either
JSON or Dia, this is more or less expected - though conceivably, one
can imagine some python extensions into Dia to make the modelers' life
easier. In the future, when we finally settle on a MOP frontend such
as text UML, what would be ideal is a full blown LSP mode (we already
have [[*Model Oriented Programming][a story for MOP]]).

LSP use cases:

- completion:
  - when adding attributes, we need completion based on the models
    referenced.
  - when setting stereotypes, we need completion based on the profiles
    and object templates that have been defined.
  - when typing meta-data keys, we need completion based on the
    feature model, and taking into account the binding types of each
    feature. In addition, if the feature is linked to an enumeration,
    we also need a way to see what the valid values are.
  - when adding a reference, a list of all of the referenceable
    models, given the current include paths.
- find usages:
  - given a model element, find all of its uses in the models.
  - given a model element, find all of its uses in the generated and
    handcrafted code, with the ability to choose (both, handcrafted,
    generated).
- find definition:
  - given a model element usage, take me to its definition.
  - given a model reference, take me to its definition.
- hierarchies (treemacs support):
  - list all of the model elements in a model as a tree, with their
    attributes.
  - follow current position in the tree.
- syntax checker:
  - for models, highlight lines where there are errors and warnings.
  - needs the ability to parse invalid models.
  - diagnostics and code actions (clang FixIts). This is useful for
    levenshtein-like errors in spelling.
  - for stitch templates, highlight lines where there are errors and
    warnings at the stitch level; map errors from C++ code back into
    the stitch location by calling out to clang somehow. There is
    already a story for this.
- symbol rename:
  - given a symbol, rename all of its uses across all referenced
    models. For extra bonus points, also rename them in the generated
    language.
- documentation:
  - bring up documentation for symbol at point.

Notes:

- we probably will need a separate git repo for the language server,
  as we are running out of resources on our travis builds. We will
  need a lot of extra code to implement the language server because
  the data structures are completely different from how we are
  querying the data at present - though we can probably reuse a large
  number of the transforms.
- we should look at existing c++ language servers to see if we can
  reuse or steal some of their ideas. In particular, the protocol
  implementation part should be common to all servers (messages,
  etc). We should be able to create a simple library with model types
  that implement the messages and the request/response interactions
  without binding it to the dogen specific logic.

Links:

- [[https://microsoft.github.io/language-server-protocol/][Language Server Protocol]]
- [[http://llvm.org/viewvc/llvm-project/clang-tools-extra/trunk/clangd/][clangd]]
- [[https://github.com/cquery-project/cquery][cquery]]
- [[https://github.com/MaskRay/ccls][ccls]]

*** Add design by contract to code-base                               :story:

We should really declare pre/post conditions, etc in Dogen. This can
either be done using C++ 20 when we upgrade or an already existing
library such as [[https://www.boost.org/doc/libs/1_70_0/libs/contract/doc/html/index.html][Boost.Contract]]. The only slight problem is that
libraries have very macro based syntax whereas C++ 20 has proper
attribute support, making the code look much nicer.

Links:

- [[https://speakerdeck.com/rollbear/accu2019-programming-with-contracts-in-c-plus-plus-20][Programming with contracts in C++ 20]]

*** Improving boost test support with org-mode                        :story:

This story is somewhat random at present but there is a kernel of
usefulness to it. At present we are using [[https://github.com/sgoericke/boost-test][boost-test mode]] in emacs to
run boost tests and visualise a test report. This mode has its
shortcomings - its more of a report viewer. In an ideal world, we
would like to have an org-mode file with all test suites in a given
product, and then with the output of each test suite inside a
tree. Then, we need some kind of way to execute nodes of the
tree. Org-mode has [[https://orgmode.org/worg/org-contrib/babel/intro.html][babel support]] for this. Thing is dogen may help
code-generate this org-mode file:

- dogen is alreay aware if boost test is being used or not. Dogen also
  knows of all projects in a product.
- at present we are using a hacked CMake macro to detect tests and add
  them to CTest. A better way would be to have dogen code generate the
  code to do this. This code could also be used to generate the
  org-mode file. We could even make use of top-level arguments such as
  =log_level=, etc. These are set at the model level.
- another way of looking at the problem is to create a separate tool
  that takes the boost.test XML and uses it to create the org-mode
  file. The advantage is that we can then take into account things
  like timings, etc. It is also dogen independent. We could create an
  XSD for the XML and then use XSD tool to generate C++ code for
  it. The tool would then be responsible for converting the result to
  an org file. Dogen could be responsible for creating a "project
  file" telling the tool about the location of the tests (though its
  not clear how it would know about this).
- we could use labels for generated tests, or add them to a separate
  tree.
- you can execute all blocks using =org-babel-execute-buffer=.
- if we could have a "test location" variable somewhere in the
  org-file, we could create a single org-mode file and then execute it
  against the different configurations.
- we could look into how boost-test parses the XML; maybe the output
  of boost test could be parsed and used to compute things in the
  org-mode file such as pass/fail, timings, etc. We could even build a
  org-mode table with the timings at each level: product, test suite,
  etc.
- for extra bonus points we could insert file links that take you to
  the original c++ file for the test, test suite etc.

Actually a much easier way to do this does not involve dogen at
all. We could simply create a transform from the XML output of test
unit frameworks into org-mode. You could call the "dumper" against the
output of the tool and it would produce an org-mode file which is able
to execute the tests and get the output of the tests. It should also
be able to mark tests as pass or fail. Dogen could then also comply
with this format, if required, but this is more or less
orthogonal. This could be its own product, where the domain of the
product is a uniform format for unit testing data:

- injector models: boost test, nunit, etc.
- core model: models unit test frameworks in generat.
- extraction: org-mode.

Then, from emacs you could call this utility the first time to build
the org-mode files per unit test framework. The interesting thing
about this is that you could use the org-mode document as
documentation for the unit tests. These could then generate HTML/Latex
files without the test results. The key thing is that check-ins to git
should be done without the output of the unit tests. We should also
have variables pointing to the different output locations (it should
integrated with the build system).

Links:

- [[http://fgiasson.com/blog/index.php/2016/05/30/creating-and-running-unit-tests-directly-in-source-files-with-org-mode/][Creating and Running Unit Tests Directly in Source Files with Org-mode]]
- [[https://github.com/structureddynamics/org-mode-clj-tests-utils][Clojure test utilities for Org-mode]]
- [[https://irreal.org/blog/?p=5297][Running Unit Test with Org Mode]]

*** Consider splitting transforms into rules                          :story:

According to MDE, transforms are split out into several parts:

- the filtering of the targets of the transform.
- the application of the transform.

For example, we seem to loop through the model and then find elements
of interest to mutate. They seem to suggest we should first
query/filter the model using a rule and then apply the mutation to the
result of the query/filter. This would perhaps improve the code. The
query result could be a typed container (pointer container?) with the
elements that match. That means we can then start to converge towards
a rule engine. However, we'd have to go through all the transforms and
see if they would all benefit from this split.

We don't really have a need for a flexible rules application engine
that schedules rules and so forth, but we could take on the aspects
that make the code cleaner.

In [[https://gsd.uwaterloo.ca/sites/default/files/ibm06.pdf][Feature-Based Survey of Model Transformation Approaches]], Czarnecki
and Helsen state:

#+begin_quote
Transformation rules. In this paper, transformation rules are
understood as a broad term describing the smallest units of
transformation. Rewrite rules with a left-hand side (LHS) and a
right-hand side (RHS) are obvious examples of transformation rules;
however, we also consider a function or a procedure implementing some
transformation step as a transformation rule. In fact, the boundary
between rules and functions is not so clear-cut;
#+end_quote

Notes:

- our source-target relationship is chosen based on the needs of the
  transform. Sometimes we use in-place, in other cases (such as
  merging and translation) we use distinct source and targets. We
  should probably add comments to each transform describing these
  properties.
-

*** Create a new data-driven architecture for dogen                    :epic:

We finally found a good model for the data in dogen, and its based on
data driven design. Basically, we need to let go of OOP constructs
that aggregate data that is logically related from a human perspective
and start to think of data usage patterns. From this perspective,
there is no such thing as a modeling element - just a scattering of
properties that are used by different transforms all the way into text
transformation. Even text transforms don't really operate on
elements - they operate on specific element properties. If we segment
the text transforms this way, we end up with a large number of small
text transforms, each with very specific tasks, generating chunks of a
file. Finally, at the end, we need to concatenate these chunks to form
the end product. With this approach we solve the impedance mismatch
between the modeling model and the generational model - all we now
have are additional containers with properties. This also means that
its much easier to have different models further down the pipeline
because they are just adding more properties. We can even use
inheritance for this, e.g. generational model inherits from modeling
model and adds properties x, y, z; generational cpp model inherits
from generational model and so forth. As an example, instead of
generating the decoration properties for every artefact, we now split
all elements into two groups: those that have "standard" decoration
properties and those that have overriden decoration properties. We
then generate these two groups. For those sharing the common
decoration properties, we insert those in their vector X times. This
is still not the most efficient (we really only need one copy) but its
orders of magnitude (at least one) faster than the current
approach. Similarly with namespaces: we only need to do the namespaces
once for all types in a facet.

However, we are very far from this at present.

Merged stories:

*Evaluate all of our data structures based on usage*

This presentation is very interesting:

- [[https://www.youtube.com/watch?v%3DrX0ItVEVjHc][CppCon 2014: Mike Acton "Data-Oriented Design and C++"]]

The presenter makes a lot of points that are directly applicable to
Dogen. The main one is that we need to look at all the data structures
to see how they are used, and to try to extract deeply nested if's
that in many cases can easily be extracted from the bottom and moved
to the top. There are many other excellent points, we probably need to
watch the presentation again and write each of them down.

The key point though is that the re-engineering exercise should only
be done after we finished all of the current refactorings - we must
make sure the code does all that it is intended to do first and then
tackle the Acton's suggestions. This is to ensure that we have
captured all the main use cases. Data analysis can be done after this.

Links:

- [[https://www.youtube.com/watch?v=yy8jQgmhbAU][CppCon 2018: Stoyan Nikolov “OOP Is Dead, Long Live Data-oriented
  Design”]]
- [[https://en.wikipedia.org/wiki/Data-oriented_design][Data-oriented design]]

*** Support for locations other than filesystems                      :story:

It would be nice to be able to point to an injection model anywhere,
such as for example in github and generate it. For this we just need
to detect the protocol in the path and if supported, read in the file;
the frontend implementations should take in a stream rather than a
file path. For cases such as LibXML, we may need to read up all of the
data first and then create a stringstream for it.

We should not implement this story unless we need it such as for
example when we have a site/service. At that point it would be nice to
allow users to just point to their models in github. This could be
done when we add support for =-I= to Dogen.

*** HTTP API support                                                  :story:

It should be possible to define a few classes in a diagram, annotate
them as HTTP and have dogen generate all of the restful
infrastructure. We should take a similar approach to ServiceStack.

Notes:

- the user should supply the routes as meta-data against each object,
  as well as the HTTP verbs supported (PUT, GET, etc).
- we should also allow the user to pair request and responses
  DTOs. This is also done via meta-data.
- the user should also define a top-level service class, with the
  comments that are to be displayed on the web-page that we
  automatically generate. The attributes of the service are the DTOs
  it supports. However, since the service contains the implementation,
  we can't code generate it really. We could either treat it as a
  concept or create an interface for it that the user has to
  implement. We could use the "lambda approach": the user just has to
  initialise the generated code by supplying lambdas that do the
  processing; we automatically generate all of the HTTP
  machinery. However, we then have to worry about state.
- we should support both XML and JSON via the same API as service
  stack:

: http://localhost:/students?format=json

- it should support multiple backends, chosen via meta-data. Example:
  casablanca/RestSDK, Boost.Beast.
- things like hostname, port etc must be handled outside the generated
  code.
- insane idea: we can use stitch templates to generate HTML just like
  with razor. Example razor template:

: @using StudentReports.Models
: @inherits ViewPage<ienumerable<student>>
:
: @{
:     ViewBag.Title = "Student List";
: }
:
: <div class=""row"">
:     <div class=""panel" panel-default"="">
:
:                 @{
:                     foreach (var item in Model)
:                     {
:
:                     }
:                 }

- if we use stitch this means we can define classes (or even profiles)
  and associate them with stitch and wale templates just like we do
  with formatters. We then generate code to bind these to the API. The
  objective is to make an interactive view of the API, not to replace
  Wt. We could also use mustache templates for simple cases. There is
  also static sites generation (see hugo, etc).
- actually, we should definitely not use stitch for this. We want to
  use a specialised library that is well maintained with lots of use
  cases outside of dogen. Example: [[https://github.com/pantor/inja][inja]]
- we should generate the swagger interactive interface as they do for
  the [[https://petstore.swagger.io/][petstore]].
- we should also allow users to add documentation for the API,
  including introduction, examples etc. This should be done in
  Markdown format and follow the same layout as for [[https://www.consul.io/api/index.html][Hashicorp]].
- our logical model elements should probably follow the latest ASP MVC
  API. It seems simple enough. Controllers are the top-level handler
  of a URL. They are associated with a number of API versions. This
  should be handled by Dogen versioning (see story on this for the
  POCO types). Each method of the controller is associated with an
  HTTP verb (GET, etc) and a route. It also may be associated with
  permissions.

Links:

- [[http://www.dotnetcurry.com/aspnet/1056/introducing-service-stack-tutorial][Introducing ServiceStack]]
- See also the story on Swagger/OpenAPI
- [[https://docs.microsoft.com/en-gb/teamblog/announcing-rest-improvements][Announcing Updates to the REST API Experience]]: microsoft has created
  a centralised place to try all REST APIs. We should be able to code
  generate something similar. However, we need a way to create a
  single site with all APIs - this is not so easy because we'd have to
  load up shared objects from different models. It would be nicer to
  have some kind of tree with all the products on the left and then
  all of the internal APIs for each product (perhaps grouped by
  top-level functionality?). Then, when you click on a request you get
  the documentation for the end point as well as an interactive "try
  it" environment. This should work even without a login. The MSFT
  documentation includes a description of all of the returned types as
  well as the input parameters and the return codes. These contain
  documentation from the types and their attributes. The graph is
  expanded to aggregate types so that there is a link to the aggregate
  type. Primitive types have their documentation inline. We have two
  options for the API browser: either have a product that is
  responsible for it and each product has a serialised description of
  their APIs, or have a =webapi.browser.PRODUCT= for each product that
  is code-generated.
- [[https://docs.microsoft.com/en-gb/rest/api/?WT.mc_id=docsmsft-blog&view=Azure][REST API browser]]
- [[https://docs.microsoft.com/en-gb/rest/api/appservice/webapps/list][Web Apps - List]]
- [[https://github.com/cpp-netlib/cpp-netlib][cpp-netlib GitHub]]
- [[https://github.com/boostorg/beast][Boost.Beast GitHub]]
- [[https://github.com/CrowCpp/crow][crow GH]]
- [[https://github.com/microsoft/cpprestsdk][C++ Rest SDK]]
- [[https://github.com/omartijn/cpprouter][cpprouter GH]]: A modern, header-only request router for C++. Can be
  used with Beast.
- [[https://github.com/an-tao/drogon][drogon GH]]: C++14/17-based HTTP application framework. Drogon can be
  used to easily build various types of web application server
  programs using C++.
- [[https://gitlab.com/eidheim/Simple-Web-Server/][Simple-Web-Server GL]]: A very simple, fast, multithreaded, platform
  independent HTTP and HTTPS server and client library implemented
  using C++11 and Asio (both Boost.Asio and standalone Asio can be
  used).
- [[https://www.techempower.com/benchmarks/#section=data-r19&hw=ph&test=composite][Benchmarks Composite scores]]: benchmarks for several web frameworks.
- [[https://github.com/matt-42/lithium][lithium GH]]: Lithium's goal is to ease the development of C++ high
  performance HTTP APIs.
- [[https://github.com/yhirose/cpp-httplib][cpp-httplib GH]]: A C++11 single-file header-only cross platform
  HTTP/HTTPS library.
- [[https://github.com/blockspacer/CXTPL][CXTPL GH]]: "Template engine with full C++ power (transpiles template
  into valid C++ code, supports Cling, etc.)."

*** Consider generating consul service discovery code                 :story:

Once we support services, we should also generate service discovery
code. We could use an implementation such as consul for this. There
are several C++ libraries that support consul, see other story on
consul.

Links:

- [[https://www.consul.io/discovery.html][Consul - Service discovery made easy]]

*** Grepping non-generated code                                       :story:

In order to determine if a type is no longer used, it would be really
useful to grep non-generated code. This could be an option in dogen
itself: given a project or set of projects, it determines the list of
non-generated files and then runs grep on those.

Notes:

- this could be implemented as an activity, e.g. =grep=. It would take
  as input either a model, a project or a directory with models; it
  would load up all models, figure out the list of handcrafted files
  and then run grep on those.
- we probably need specific transform chains for these, to avoid doing
  lots of unnecessary work. However, unpicking transforms won't be
  trivial.
- think of code as a graph, where handcrafted files are leaves and
  generated files are non-leafs. In fact, it probably makes more sense
  to generate a graph using dot which highlights all of the entities
  which are not reached by non-generated entities. This is a bit
  tricky because we may make use of an entity in regular C++ code, so
  we will need something like LSP to produce a list of "types used" by
  a given type.

Links:

- [[https://www.boost.org/doc/libs/1_61_0/libs/regex/example/grep/grep.cpp][Grep in c++ using boost regex]]
- [[https://unix.stackexchange.com/questions/494595/how-can-i-search-a-specific-list-of-files-with-ripgrep][How can I search a specific list of files with ripgrep?]]

*** Add support for data anonymisation                                :story:

If users could mark certain fields as "sensitive", we could use test
data generation for them. The process would be as follows:

- mark certain fields in a model as sensitive. Names, addresses, etc.
- provide a anonymiser facet. For each field, it creates a map using
  the appropriate test data source for the field. The map is important
  so that each original value corresponds to one and only one mapped
  value, thus preserving the properties of the original data. Users
  supply a graph of objects to anonymise.
- anonymised data can now be serialised and supplied as test
  data. Dogen itself can use this framework. See "Model anonymizer and
  slicer".

*** Consider using indices rather than associative containers          :epic:

Once we generate the final model the model becomes constant; this
means we can easily assign an [[https://en.wikipedia.org/wiki/Ordinal_number][ordinal number]] to each model
element. These could be arranged so that we always start with
generatable types first; this way we always generate dense
containers - there are some cases where we need both generatable types
and non-generatable types; in other cases we just need generatable
types; we never need just non-generatable types. We also need to know
the position of the first non-generatable type (or alternatively, the
size of the generatable types set).

Once we have this, we can start creating vectors with a fixed size
(either total number of elements or just size of generatable
types). We can also make it so that each name has an id which is the
ordinal (another model post-processing activity). Actually we should
call it "type index" or some other name because its a transient
id. This means both properties and settings require no lookups at all
since all positions are known beforehand (except in cases where the
key of the associative container must be the =yarn::name= because we
use it for processing).

In theory, a similar approach can be done for formatters too. We know
upfront what the ordinal number is for each formatter because they are
all registered before we start processing. If formatters obtained
their ordinal number at registration, wherever we are using a map of
formatter name to a resource, we could use a fixed-size
vector. However, formatters may be sparse in many cases (if not all
cases?). For example, we do not have formatter properties for all
formatters for every =yarn::name= because many (most) formatters don't
make sense for every yarn type. Thus this is less applicable, at least
for formatter properties. We need to look carefully at all use cases
and see if there is any place where this approach is applicable. It is
probably going to be more useful for formatters than elements.

Tasks:

- in resolver, assign element indices and update property names with
  them.
- change final model to have a vector of size maximum index (a
  property of the intermediate model).
- in the final model generation, for each type, look at its index and
  populate the slot accordingly.
- update quilt to use the indices where possible.

*** Model Oriented Programming                                        :story:

There is a type of programming called MOP - Model Oriented
Programming. Umple seems to be the main language implementing
MOP. Some of the ideas of MOP are quite interesting: to develop a
modeling language that can be used just like regular code. This would
be ideal for Dogen because we are using UML diagrams with lots of
extensions to be ale to convey the full Dogen DSL. Instead, we could
create a MOP lanugage that is the dogen DSL:

: import "my_profile";
: import "model_a";
: import "model_b";
:
: traits {
:     internal trait Definable {
:         facet cpp.types {
:             option default_constructor = 1;
:         }
:     }
:
:     internal trait Serialisable {
:         comments "Model type can be serialised."
:         facet cpp.serialisation.boost {
:             option with_xml = 1;
:         }
:     }
:
:     internal trait SomeTrait {
:         inherit Definable, Serialisable;
;     }
: }
:
: model my_model {
:     name a;
:     external_modules b.c;
:     model_modules d.e;
:     input_language cpp;
:     output_language cpp; # or just language cpp;
:     type generatable; # or proxy, defaults to generatable.
:     target executable; # library
:     using profile p1;
:
:     module a_module {
:         using profile p2;
:         enumeration my_enum {
:             comments "My enumeration";
:             type int;
:             enumerators {
:                 a = 1,
:                 b = 2
:             }
:         }
:
:         public object my_object {
:             immutable; # defaults to mutable
:             comment "This is an object";
:             using profile p2; # override global profile
:             generation handcrafted;
:             concepts c1, c2;
:             parents o2;
:             using traits types, serialisation, hashing; # using abstract facets, relies on mapping
:             using traits types.cpp; # using concrete facets.
:             public property p1 {
:                 comments "my property";
:                 type int;
:             }
:             public method a(in std::list<std::string> l) returns std::string;
:         }
:     }
: }

The language could have profiles as first class citizens and support
includes. This would make sharing profiles across models much easier,
and also defining a product (just include all models into the
product).

Notes:

- its not obvious that there are a lot of advantages over plain
  JSON. At present: 1) language could have support for comments, 2)
  inclusion in JSON is not obvious 3) the syntax can be made
  specifically to fit our DSL (for example, we could force that files
  must start with =model=, or =product= or =profile= and not require
  ={}= for these elements). An alternative to JSON which could be more
  suitable is [[https://github.com/toml-lang/toml#user-content-example][TOML]].
- creating a parser for the language will not be trivial.
- on the plus side, we can then place the model definition with the
  component itself. There is a nice symmetry with source code.
- MOP then becomes the DSL for the methodology which is nice from a
  MDE perspective.
- its not clear there are advantages over org-mode. The latter would
  be much more flexible for code-merging for example. Or perhaps we
  could still call it MOP - but the AST is org-mode.

Links:

- [[https://cruise.eecs.uottawa.ca/umple/][UMPLE Home]]
- [[https://github.com/umple/umple][GitHub repo]]
- [[https://cruise.eecs.uottawa.ca/umple/GettingStarted.html][UMPLE manual]]

*** Interactive interface to models                                    :epic:

There are a number of interactive interfaces available such as
Jupiter. It may be interesting to think of ways to generate the
interactive interface automatically from a model. Its not quite clear
what that interface would do: are we updating the model or the
entities it generates? We need to have a much clearer use case before
we start to look into this.

Links:

- [[http://glench.github.io/fuzzyset.js/ui/][FuzzySet]]: A Human-Readable Interactive Representation of a Code
  Library
- [[https://github.com/QuantStack/xeus-cling][xeus-cling]]: C++ interface for Jupyter notebooks.

*** Consider extending stitch with ideas from Xpand/Xtend              :epic:

We created our own templating language to make it easy. Stitch works,
but its not as nice as Xpand:

- we are mixing C++ and template logic in templates. It would be nice
  if templates just had template logic.
- Xpand makes the "template methods" more explicit, making variability
  regions etc more natural.
- it is already a standard language so we can rely on existing
  documentation / examples, to an extent.
- seems like Xpand has been replaced by Xtend: [[http://blog.efftinge.de/2013/06/five-good-reasons-to-port-your-code.html][Five good reasons to
  port your code generator to Xtend]] (presentation included). Xtend has
  a lot of interesting features including commands for spacing like
  =before= and =after=, =separator= etc. These could be copied and
  mapped. We do not need grey space support because we believe pretty
  printing is done by a cartridge that knows the language.

Our implementation of Xpand would be a bit special though:

- the metamodel is hardcoded to our code generation metamodel.
- we would transform Xpand code into C++ code, that would then be
  compiled in.
- file names would be automatically generated.
- there is no assistant, so we need some way to access the things
  which are not directly available in the metamodel.
- we would have to write a boost::spirit parser for Xpand, which would
  keep the template blocks (potentially it could even reuse the stitch
  model / abstract syntax).
- we would probably have to bastardise Xpand. We can't allow multiple
  files on a template, filenames are defined by the framework, we need
  support for boilerplate, etc.
- we still need support for includes, computed externally. So the
  formatter would be made up of the existing formatting code plus the
  template. On the plus side we could agument Xpand with the
  variability support required so that all functions could have a very
  simple predicate based on features being on or off. We could also
  have support for inclusion built in, via some commands specifically
  designed for it. This would mean we could associate inclusion with
  "template methods" so that if a template method is off, the
  inclusion will not be added.
- so really we could make the template language specifically for our
  code generation metamodel, with understanding of facets, features,
  etc so that making a template would be very simple. In addition, we
  should also have a fixed interface so that the C++ code could define
  both the implementation and the interface in the C++, as well as
  register itself. Then we could get rid of wale. The objective is for
  the generated code to match the code that stitch is generating it at
  present.

Changes to the stitch approach:

- there is no wale; wale machinery is hard-coded in stitch2. These are
  expressed as "template types", either a formatter or a helper.
- there are no inclusion dependencies. They are automatically done for
  you.
- inclusion building is done via DSL extensions. We need to add
  commands to the DSL to allow it to support all of the use cases we
  identified with stitch.
- boilerplate etc are commands in the language. We need a name for
  these.
- helper commands are commands in the language. For example, instead
  of:

: if (a.supports_defaulted_functions()) {

  you need to create a predicate for the variability region:

: supports_defaulted_functions

  these are exposed as part of the API.

- variability regions must support =ELSEIF= predicates.
- variability regions may have a comment/description as well as a
  name.
- variability regions can have a predicate based on a feature
  configuration or on any other structural query.

Requirements for the template language:

- create templates that are very close to the generated code so that
  you can take an existing file and templatise it, or you can look at
  an existing template and predict what the generated code will look
  like.
- produce simple C++ code that any regular c++ developer can debug.
- The idea is that the user can take an existing text file and add
  some additional metadata and it becomes a template; very low barrier
  to entry. In addition, generator should also create a CMakeFile to
  create a DLL. All boilerplate code is generated by stitch: DLL entry
  points, plugin registration (Boost.DLL), etc. The user simply has to
  compile.

*** Feature Models                                                    :story:

Feature Models are a useful way to describe features in a system. If
there was a textual way to describe features, we could link them to
Dogen models. Its not clear at all how this would work.

Links:

- [[http://www.boost.org/community/feature_model_diagrams.html][Feature Model Diagrams in text and HTML]]
- [[https://github.com/EmilianoSanchez/Feature-Model-Optimization][Feature-Model-Optimization]]

*** Make more use of stack instead of heap                            :story:

We should do some analysis as to how the following classes are being
used:

- name
- location
- achetype location

They are all making a lot of use of the heap. In reality they follow a
very simple usage pattern: we construct them, update them slightly on
resolution and then they remain constant for the rest of program
execution.

If we can spot some kind of constraints in terms of sizes, we could
possibly eliminate the use of lists and strings and instead use
=std::array=. For strings, these can be exposed as
=std::string_view=. It seems we have a few code-generation patterns:

- array of enumerations: we align the enumeration to fit the array
  indices and provide some kind of getter that casts the enum into an
  int and queries the array. This is the case with the qualified name
  map.
- stack based strings. This is just a case of receiving a string view
  in the constructor, then ensuring we have enough space on the array
  to copy across the string, then copying the string across. We could
  easily have a setter that receives string views and a getter that
  returns the string view based on the array. When a user sets a
  property to string view, we automatically generate all the
  boilerplate to read and write to the array. The user has to supply
  the size of the array (we should offer a sensible default). We need
  to look for the correct exception to throw if the string is not big
  enough. This approach seems to be useful enough that we should have
  some kind of stereotype for it like "stack string" or some such.
- the other interesting thing is that we probably have a very limited
  number of locations in a model: these will all manifest themselves
  as modules. If we somehow could have a handle to the module we
  wouldn't have to carry the location with every name.
- similarly with name trees, we don't really need to carry all the
  name information - really a name tree is more of a tree of
  handles. If these handles were indices into an array then we'd have
  a very efficient way to perform lookups.

In general, it seems the biggest problem we have is we started of with
an OOP model of the world which is very far from how the data is
actually used. Once we have finished with the current clean-ups we
need to do some analysis on how the data is being used and try to work
out data structures that are more suitable for this. We should also
measure the cache hits / misses before and after the changes, as well
as the memory sizes.

Links:

- [[https://www.boost.org/doc/libs/1_72_0/libs/beast/doc/html/beast/ref/boost__beast__static_string.html][boost static_string]]: fixed size string with no dynamic
  allocation. We could probably convert these to string views as well.

*** Add support for field renaming in annotations                     :story:

At present if a field changes its name between dogen releases,
diagrams stop working. For example, we renamed:

: yarn.dia.external_modules

to

: yarn.external_modules

With this change, dogen stopped working. We need some way to
"remember" the previous name before a rename so that previous versions
still work.

*** Transform tracing "coordinates"                                   :story:

Allow users to supply coordinates to transforms in some form so that
we only dump a subset of the transform data. This could be the ID of
the transform.

*** Add support for UML profiles and XMI                               :epic:

In order to communicate with external tools, we should try to support
XMI as an injector. For this we could use the XSD for XMI as the basis
for the injector. We also need to create the MASD UML profile with
full support for all of the logical model elements.

Previous understanding:

We now understand most of the picture of how UML is related to
Dogen. eCore is an implementation of UML's meta-model - it diverges
from UML slightly, but for our purposes we can think of it as
equivalent to UML's meta-model. At the meta-model level, it is
possible to extend UML: this is done via profiles and
stereotypes. Profiles are basically a collection of types which make
use of UML stereotypes to extend the UML meta-model. So it is possible
to inherit from say =Class= and create a new meta-class that extends
=Class= in some way (in eCore terms, =eClass=).

For instance, we could create a meta-class called =visitor=. In fact,
all meta-types in =masd= can be redone as extensions to the UML
meta-model (e.g. MASD profile). It just so happens that we did not
understand the UML meta-model properly so we created a completely
separate meta-model and then we mapped the Dia representation of the
UML meta-model into yarn. But this muddles things because we are using
Dia's diagram meta-model, which contains an incomplete representation
of UML's meta-model (since it exists purely for drawing diagrams). A
better way to go about this is:

- create a c++ implementation of the ecore model. Use exactly the same
  terminology - e.g. =eclass= etc.
- create a dia to XMI exporter. This would have obtained the essence
  of the UML diagram required for code-generation, removing all of the
  "diagramatic" aspects of the model. [[https://www.ibm.com/developerworks/library/x-wxxm24/][XMI example]]:

#+begin_src xml
<?xml version="1.0"?>
<XMI xmi.version="1.2" xmlns:UML="org.omg/UML/1.4">
 <XMI.header>
  <XMI.documentation>
   <XMI.exporter>ananas.org stylesheet</XMI.exporter>
  </XMI.documentation>
  <XMI.metamodel xmi.name="UML" xmi.version="1.4"/>
 </XMI.header>
 <XMI.content>
  <UML:Model xmi.id="M.1" name="address" visibility="public"
              isSpecification="false" isRoot="false"
              isLeaf="false" isAbstract="false">
   <UML:Namespace.ownedElement>
    <UML:Class xmi.id="C.1" name="address" visibility="public"
               isSpecification="false" namespace="M.1" isRoot="true"
               isLeaf="true" isAbstract="false" isActive="false">
     <UML:Classifier.feature>
      <UML:Attribute xmi.id="A.1" name="name" visibility="private"
                     isSpecification="false" ownerScope="instance"/>
      <UML:Attribute xmi.id="A.2" name="street" visibility="private"
                     isSpecification="false" ownerScope="instance"/>
      <UML:Attribute xmi.id="A.3" name="zip" visibility="private"
                     isSpecification="false" ownerScope="instance"/>
      <UML:Attribute xmi.id="A.4" name="region" visibility="private"
                     isSpecification="false" ownerScope="instance"/>
      <UML:Attribute xmi.id="A.5" name="city" visibility="private"
                     isSpecification="false" ownerScope="instance"/>
      <UML:Attribute xmi.id="A.6" name="country" visibility="private"
                     isSpecification="false" ownerScope="instance"/>
     </UML:Classifier.feature>
    </UML:Class>
   </UML:Namespace.ownedElement>
  </UML:Model>
 </XMI.content>
</XMI>
#+end_src

- note that the dia to XMI exporter would have to make use of some
  "extensions" to dia such as the =yarn.dia.comment=. But the key
  thing is we would end up with a UML meta-model description of the
  diagram (XMI is roughly speaking an XML representation of the UML
  meta-model).
- create a UML profile for =yarn= describing all of the meta-model
  elements in terms of the UML meta-model. This could be the exact
  same diagram we have for =yarn= right now, except all the meta-model
  entities would be marked as =stereotypes= (i.e. with a UML
  stereotype of =stereotype=) and would have to inherit from =ecore=
  classes such as =eclass=.
- create a =ecore= XMI reader.
- create a =yarn.ecore= transformer. The transformer outputs a =yarn=
  model, just like the current =yarn= frontends would. The remaining
  of the processing is done as it is for the current =yarn= models.
- in the main =yarn= workflows: whenever it encounters a class with a
  stereotype of =stereotype= it knows it must inherit from the
  appropriate meta-model. It effectively expands these
  meta-stereotypes into inheritance relationships from the meta-model.
- create a UML profile for =quilt.cpp= (and =quilt.csharp=). Again,
  this could be the exact same =quilt.cpp= model we have, except all
  of the meta-model entities are described as meta-stereotypes, and
  inherit from the =yarn= meta-model entities. This means that instead
  of extending the =yarn= types via composition, we would have to use
  inheritance. This is somewhat non-trivial because we need to extend
  the base class =element= since all =quilt.cpp= elements have common
  properties.
- note: it is important to note that we're not interested in eCore
  models per se. To generate an eCore model requires having the eCore
  model at run-time as well. This is a completely separate kernel from
  =quilt=. The kind of models we'd be generating would be very
  different from the =quilt= models (generated types would inherit
  from =eObject=, they would have reflection support and so forth -
  all features that we are not interested in at present). However,
  eCore as a plain meta-model is useful because it is directly related
  to XMI and to UML profiles (since its just UML meta-model in
  disguise).
- proxy models now become just regular XMI models (either JSON or XML)
  with suitably annotated models indicating its a proxy model.
- the total set of expanders are our implementation of meta-model to
  meta-model transformation. Some of these expanders are replaceable
  by generated code: all that are simply reading meta-data. Others
  must remain. This task will be tricky because we have mixed the
  meta-data expansion with additional logic in the expanders. We now
  need to decouple the two.
- we will lose scoping on the tagged values. We can still have some
  kind of hierarchical structure, perhaps =MODEL.CLASS.ATTRIBUTE= but
  this is now limited to the meta-model structure because we must be
  able to mechanically map from a tag value to a class and
  attribute. We should also read the UML spec on valid tagged values
  in case they forbid some characters such as dots.
- up to now we have extended the dia format to perform yarn-like
  operations; for example, we read stereotypes in the =yarn.dia=
  frontend and convert them into =yarn= types. In an XMI world this
  will no longer happen. We must pass-through the dia stereotypes into
  a XMI class. All changes to dia must be seen as XMI extensions.

Advantages of this approach:

- we'd be relying on standards: any tool that can output XMI can be
  used as a front-end, and all the extensions used for Dogen are part
  of XMI/UML. Nothing is proprietary.
- since all extensions to meta-model elements are done via profiles,
  we are able to figure out where the meta-data is (all tagged values
  in the profile are annotations). It is still not totally clear how
  one would map the =annotation= types (e.g. =text=, =number= etc) to
  programming language types, but the gist is we should be able to
  generate the factories that read meta-data and instantiate
  objects. Not all mappings are trivial of course.
- we can create arbitrary extensions to the meta-model just by using
  the meta-stereotype. Users can create their own on their diagrams,
  and because of the stitch integration, they can already add their
  own templates. This makes the system arbitrarily extensible. Of
  course, we still need to plug in the new meta-model items to the
  pipeline (how would they get populated?).

High-level tasks to get us there:

- create a =ecore= =yarn= model.
- add a =ecore.xmi= model that reads XMI and instantiates
  =ecore=. Actually we should probably create an =xmi= model that
  reads XML and JSON and then use that model as input for the
  =ecore.xmi= transformer.
- change the =yarn= model to inherit from the appropriate =ecore=
  entities. This will be a bit problematic if there is use of
  reflection (e.g. eObject etc).
- transform =quilt.cpp= and =quilt.csharp= into =yarn=
  extensions. Instead of composition we need to use inheritance
  somehow. Composition is possible of course, but basically we need to
  be able to express it as a UML profile.
- add a =yarn.ecore= model that converts =ecore= into =yarn=.
- update =yarn= to understands meta-stereotypes. When it sees a
  stereotype of =stereotype= it looks for a tagged value for
  inheritance and instantiates the inheritance relationship of
  meta-model items. In practice we can support this right now in yarn,
  there is no dependency on anything else for this. Users just need to
  add the meta-model to their list of referenced models, add a
  stereotype of =stereotype= and the appropriate generalisation
  relationship. The =stereotype= is just for cosmetic purposes, =yarn=
  workflows would already inherit correctly.
- create a =dia.xmi= model that outputs XMI documents, possibly in
  [[https://github.com/dsevilla/xmi-to-json][JSON as well]]. Delete the current =yarn.dia= frontend and rewrite it
  in terms of =dia.xmi= and =yarn.xmi=.

Note:

- it is interesting to notice that in practice we don't actually need
  to link =ecore= to =yarn= (inheritance, etc), provided that we
  support interoperability between the two: stereotypes, tagged
  values, etc. As long as the mapping results in the correct
  instantiation of the =yarn= model, this is semantically equivalent
  to making =yarn= a proper specialisation of the UML
  meta-model. Problems start when users try to make use of UML
  meta-model types for their own templates though.
- it seems there is no publicly available XMI XSD for UML. Actually it
  seems there is no proper UML support in XMI and each vendor has
  created incompatible extensions. See:
  - https://bugs.kde.org/show_bug.cgi?id=56184
  - http://plantuml.com/xmi
  - https://github.com/plantuml/plantuml
  The only solution for this problem is to create a "libxmi" that has
  many frontends and supports a single object model that covers all of
  UML as per OMG UML XMI. Then manually create XMI XSD schemas for
  each variant of UML, use the XSD tool to create C++ representations
  of these and manually create mappings between these dialects and the
  standard UML object model. One should try to make the mappings
  bidirectional so that the library could be used to create a
  converter between XMI formats. However, this is a very large task
  and its not clear its worth it. Actually we probably want a "libuml"
  product which contains the XMI adapters. It should also contain
  other adapters such as Dot/GraphViz, Dia, etc. It should also define
  its own libuml XMI with XSD. It should also include OCL parsing. It
  could share code with umbrello.

Links:

- [[https://github.com/catedrasaes-umu/emf4cpp][EMF4CPP]] (see also https://github.com/mdoerfel/emf4cpp/tree/v2)
- [[https://github.com/MDE4CPP][MDE4CPP]]
- [[https://msdn.microsoft.com/en-us/library/dd465146.aspx][Standard stereotypes for UML models]]
- [[https://www.omg.org/spec/XMI/20131001/XMI.xsd][XMI XSD v2.5.1]]
- [[http://www.omg.org/spec/xmi/2.5.1/][XMI specification v2.5.1]]
- [[https://wiki.eclipse.org/images/f/f0/OMCW_chapter04_IntroductionToXMI.SINTEF.pdf][slides: Introduction to XML Metadata Interchange]]

*Previous Understanding*

It would be nice to be able to read an eCore model and generate code
from it. This would be particularly useful if we could generate java
code. We would require a mapping layer of eCore types into LAM.

- [[http://eclipsesource.com/blogs/tutorials/emf-tutorial/][What every Eclipse developer should know about EMF]]

*** Reducing the overhead of other facets in types                    :story:

Note: This story is a bit far-fetched at the moment, but it is a place
to collect ideas on this space.

There is a tricky problem with io and inheritance: when using a facet,
a user should only pay the cost of that facet and nothing else;
however, we could not find any efficient ways of type dispatching
across models for io. This meant that we ended up adding a
=to_stream= method to types that are part of an inheritance
relationship. The downside of this approach is that even if one does
not use io, one ends up paying the cost of carrying this method
around.

No good alternatives have been found:

- its not possible to use visitors because we now allow cross-model
  inheritance; thus we do not know what visitor to use.
- one could register types against a base streamer for an inheritance
  tree; the downside of this approach is efficiency. We'd have to do a
  map look-up to find the correct streamer. Its possible but not
  entirely trivial to use a vector as we only know the size of the
  inheritance tree at run-time and so we'd have to assign positions in
  the vector as types register. This means we'd have to have some kind
  of static member variable on each type to remember their index, and
  this would be populated as a result of registration. This also means
  we'd still be impacting types with the static index. This is akin to
  a vtable but with a twist. Whereas the vtable is associated with an
  object, we'd have a vtable per inheritance tree; the index for each
  object is in each class (but it must be populated at run time). The
  size of the vtable must also be determined once all types have
  registered (or we can continue to grow it during the registration
  phase; a one-off cost).

Actually this seems to be a common problem; we did the same for
visitors. It would be nice to only pay visitor costs when one intends
to use it. The current implementation menas we are carrying a vtable
just because of this (and of =to_stream=). In an ideal scenario,
visitor would itself carry the vtable.

Links:

- [[http://www.learncpp.com/cpp-tutorial/125-the-virtual-table/][12.5 — The virtual table]]

*** Model anonymiser and slicer                                       :story:

A user may create a model which triggers a problem in dogen. For
example it may cause it to abort code generation, or generate invalid
code, etc. The user may not have the desire to spend time creating a
small test model that reproduces the bug and they may not be able to
send us the full model (or models, if there are references). One way
in which we could make this use case simpler is to generate an
anonymized version of the model.

The anonymizer takes an intermediate model and maps every element to a
generated name such as =object_0=, =object_1= and so forth. We should
only do this for non-system types (non-proxy models).

For extra bonus points, the anonymizer should use a hash of the
element id such that an element always maps to the same anonymized
element. Actually if we generalise this slightly: we could create a
class that given a map of element ids (old to new), produces a new
model by replacing those elements. This means if we give it first map
A it will convert model a into a'. If we then give it A^-1, it will
reverse the transformation converting a' back into a. Making it
generic may have other uses such as renaming.

In addition, the user may not want to send us an entire model. It is
likely the problem is contained with one or a few elements. We could
have an additional tool that takes a model and a set of element ids
and generates a second model with just that element and all of its
dependencies. For extra bonus points we should be able to give it a
property id and only that property is preserved. It could also
collapse all references of external models (non-proxy) into the same
model.

Users could run the slicer first and then the anonymizer.

Links:

- [[https://github.com/TonicAI/masquerade][masquerade:]] real time data annonymiser. See this [[https://www.tonic.ai/post/masquerade-a-postgres-proxy/][blog post]].

*** Consider adding support for the =fmt= library                     :story:

It would be interesting to benchmark dogen using iostreams vs dogen
using the =fmt= library. It would also be nice to have a IO based on
=fmt=.

We should also consider replacing iostreams in M2T with =fmt=. We need
to find a way to benchmark both approaches.

Links:

- [[https://github.com/fmtlib/fmt][GH fmt]]
- [[https://github.com/fmtlib/fmt/issues/122][#122: Can't write into a pre-existing buffer?]] It seems we can
  pre-allocate the string for the buffer.

Merged stories:

*Add support for the fmt library*

We could add a facet for fmt that acts like the iostreams
facet.

Links:

- https://accu.org/index.php/journals/2509
- https://github.com/fmtlib/fmt

*** Consider adding support for the =fast_io= library                 :story:

Yet another library for formatting:

#+begin_quote
fast_io is a new C++20 library for extremely fast input/output and
aims to replace iostream and cstdio. It is header-only (module only in
the future) for easy inclusion in your project. It requires a capable
C++20 compiler supporting concepts.
#+end_quote

Links:

- [[https://github.com/expnkx/fast_io][GH fast_io]]

*** Determine types that are no longer used                           :story:

One of the problems of code generation is that we may end up producing
a lot code generated types, and its hard to figure out what is being
used. We could either rely on a clang tool for this, and treat
generated code exactly the same way as non generated code. Or we could
try to provide helpers for this.

If we do provide helpers, we could supply a list of all of the "root
types" for the generated types (e.g. root nodes of a graph of
types). Then, for all models that reference a model, we could
determine if a root type is used in non-generated code. Because we
have a list of all files that are not code-generated, we could simply
grep that list. The objective of this tool is:

- load all models in a product.
- for each model, extract all the root types.
- then, for each model, get a list of all non-generated files. Search
  for each root type on the non-generated files. The search could be a
  clang search or if not available a more basic grep. Then, if the
  type is found, exclude the type from the list.
- then, go through all models that have a reference to that model and
  do the same thing. At the end, produce a list of types that are not
  referenced any where.

For extra bonus points, it would be even more useful if we could mark
types as internal and external so that we could exclude external
types. However, here there are two notions of external: Intra-product
external (e.g. other models use it, but not the outside world) and
inter-product external (e.g. other products use it). Its not clear how
this would work.

Interestingly, we could use this to produce a weighted list of usages
(heat map?). This would allow us to understand how used a type is, so
that if we are trying to remove it we can figure out how used the type
is and where.

See also the story about language attributes: [[*Consider adding C#/C++ attributes][Consider adding C#/C++
attributes]].

Links:

- [[https://github.com/llvm-mirror/clang-tools-extra][clang tools extra]]: including clangd, a LSP-based code indexer.

*** Overuse of =std::set=, =std::map=, etc                            :story:

According to this paper by Matt Austern we should see if we really
need sets/etc or if there are other alternatives:

[[http://lafstern.org/matt/col1.pdf][Why you shouldn't use set (and what you should use instead)]]

In cases where we know upfront the number of elements we require and
we already have them sorted, we may not need a set.

*** Add support for multiple kernels                                   :epic:

#+begin_quote
This story is a very vague story that keeps track of ideas on making
dogen useful for code generators of other kinds.
#+end_quote

One of the stories in the backlog covers other targets of code
generation:

[[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-thrift-and-protocol-buffers][Add support for thrift and protocol buffers]]

Originally we thought about adding support for these within a model;
that is to say, one would have additional serialisation "kinds"
available with a given dogen model. However, there is another way to
look at this; one could make other kinds of code generators using the
dogen infrastructure.

That is, contrary to it's name, dogen isn't necessarily just for
"domain model generation". Nothing stops one from building a protocol
buffers or thrift compiler using dogen infrastructure that outputs
*exactly* the same code as the original tools. All that would be
required to do so is:

- create a frontend that reads in their specification;
- to ensure yarn is expressive enough to cover all of the aspects of
  the code that needs to be generated;
- to create the formatters.

However we can't just add this formatters to =quilt=. Quilt is a
single-purpose backend (as all backends should be): to generate domain
objects for different languages. All kernels in =quilt= are aligned
with this vision.

But we could introduce additional backends. As as example of an
additional backend, we have eCore. We already have a story for
supporting eCore as a frontend, but there the idea is simply that we
read the XML, convert it into yarn (as we do with any other frontend)
and then generate "native types". But an alternative dimension to this
is to provide a eCore backend which generates a completely different
kind of types, which look like a cross-platform version of eCore:

- provide inheritance from an =EObject= supertype
- provide cross-language reflection

And so forth. This is very different from =quilt=. this =ecore=
backend would have a dependency on the =yarn.ecore= frontend and we'd
have to probably extend =yarn= to cope with all the nitty-gritty
detail of eCore.

Other examples:

- a =protobuf= tool that generates code identical to the =protobuf=
  compiler and reads its input from the IDL;
- a database mapping tool like =odb=; one can conceive a clang
  front-end that reads in source code and generates an yarn model
  ([[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-c-itself-as-a-front-end][already in backlog]]); this model then can be used to generate C++
  code that is identical to the code produced by ODB (again assuming
  that yarn is extended to be expressive enough to represent all the
  constructs required by ODB).
- an =xsd= tool.

In this world we'd somehow start creating a bit of binding between the
frontend and the backend because each model most likely will only support
a backend: either protobuf, or xsd, etc.

This approach is compelling proposition because it means people who
want to create code generators don't have to worry about a lot of the
boilerplate code. However, the biggest problem is that we'd be
imposing a large and complex "framework" on them with all the evilness
that that entails.

In the light of this story, a better name for dogen would be =codegen=
(or =cogen= to make it a bit more unique in google). The tag line is
then The Generic Code Generator. Unfortunately there are already a few
projects with the name =cogen= so we may need to find a better
name. Alternatively we can maintain the name dogen, but take away its
meaning (i.e. no longer "The Domain Generator").

*** Consider adding support for OCL                                   :story:

UML supports adding constraints via [[http://www.omg.org/spec/OCL/2.4/][OCL]], the Object Constraint
Language. We could generate code from it.

*** Code generation and system configuration                          :story:

This is a very sketchy story. We probably should look into any
relationships between system configuration and code generation. A
starting point on this are these papers:

- [[http://www.infosun.fim.uni-passau.de/publications/docs/PTD%2B14.pdf][Coevolution of Variability Models and Related Software Artifacts]]
- [[http://homepages.dcc.ufmg.br/~mtov/pub/2015_modularity.pdf][Feature Scattering in the Large: A Longitudinal Study of Linux
  Kernel Device Drivers]]

*** Supporting user defined generic types                             :story:

At present we have done a bit of a hack to support templates. However,
all that is required to allow users to create their own template types
is:

- parse dia information for type arguments;
- change object to have type arguments;
- change merger to allow variables of the type of the type argument;
- change view model to propagate type arguments;
- change formatter to create template class if type arguments are
  present.

However this would then mean that IO and serialisation would fail
since they are implemented on the cpp. As there is no need for
template types, this seems like an ok limitation.

*** Visitor adaptor for usage in ranges                               :story:

It would be great if we automatically generated an adaptor to visitors
which could be plugged into a range. Internally the adaptor would
perform the accept on its =operator()=. We could also have an adaptor
for a =std::pair= which would be templatised on the first member of
the pair. Or should one just use a keys or values range iterator.

*** Visitor with =std::function= for each =visit= method              :story:

#+begin_quote
*Story*: As a dogen user, I want an extensible visitor so that I don't
have to manually generate one.
#+end_quote

It would be nice if the code generator created a visitor which has as
its properties a set of =std::function= which match the signature of
the visit functions; then the visit functions would just check that
the functions have been assigned and call them. If not, throw.

*** Create a visitor interface with multiple implementations          :story:

#+begin_quote
*Story*: As a dogen user, I need multiple visitor interfaces so that I
can decide which one is best for my requirements.
#+end_quote

We decided to use a base class for visitor; it would have been better
to create an interface, with multiple implementations:

- "negative" visitor: any unimplemented methods throw;
- default visitor: all methods do nothing;
- [[*Visitor%20with%20%3Dstd::function%3D%20for%20each%20%3Dvisit%3D%20method][std::function visitor]]: usr supplies lambdas to visitor.

Users can then inherit from these visitors where appropriate
(e.g. negative and default visitors). Each of these would have their
own formatter.

Actually we don't necessarily need visitor interfaces, but we need
some knobs to control the kind of visitor we generate. We should also
have a way to control the number of visitable methods generated (const
visitor, mutable visitor etc).

*** Add targets to output manual in downloadable formats              :story:

#+begin_quote
*Story*: As a dogen user, I would like to read the manual offline and
have the ability to print them.
#+end_quote

We should build HTML and PDF representations of the manual. When we
start using worktrees we could also have a simple script that copies
across these into the website. This can be done manually for now at
the end of each script (running the script).

The manual needs to be moved over to org-mode format. However, this
story still applies. We just need to figure out how to use emacs in
batch process to generate all of the targets.

*** Investigate the possibility of creating a mock facet              :story:

#+begin_quote
*Story*: As a dogen user, I want to code-generate mocks for interfaces
so that I don't have to generate this code manually.
#+end_quote

This is straight out of left-field (pardon the pun), but may actually
be a good idea. One annoying thing with mocking frameworks such as
[[http://turtle.sourceforge.net/index.html][turtle]] is the amount of macros. However, =dogen= already has all the
required information needed to create an expectation based mock - the
meta-model. We could mimic the turtle API with a mock facet that is
made up of real C++ objects. When a class is marked as an interface,
we could automatically generate its mock in a mock facet and allow
users to supply lambdas for the expectations.

This will require proper operations support.

Actually this may not be so easy because mock frameworks have so much
functionality. An easy way is simply to have a =mock= facet that
instantiates the mocks using the macros. It would be easy to code
generate this code.

Links:

- [[https://github.com/rollbear/trompeloeil][trompeloeil]]: Header only C++14 mocking framework
- [[http://turtle.sourceforge.net/index.html][Turtle]]: A C++ mock object library for Boost
- [[https://enterprisecraftsmanship.com/posts/when-to-mock/][Article: When to mock]]
- [[https://github.com/Moq/moq4][GH Moq]]
- [[https://github.com/moq/Moq.AutoMocker][GH Moq.AutoMocker]]
- [[https://github.com/Guardsquare/mocxx][GH mocxx]], article: [[https://tech.guardsquare.com/posts/mocxx-the-mocking-tool/][The C++ Mocking Tool]], [[https://www.reddit.com/r/cpp/comments/k0atyi/mocxx_a_versatile_c_function_mocking_framework/][reddit]].
- [[https://github.com/Smertig/rcmp][GH rcmp]]
- [[http://blog.zhangliaoyuan.com/blog/2015/05/15/thy-ways-to-break-the-dependency-during-unit-testing-in-c_c++-programming/][The Ways to Break the Dependency During Unit Testing in C/C++ Programming]]
- [[https://github.com/ThrowTheSwitch/CMock][GH CMock]]: "CMock is a mock and stub generator and runtime for unit
  testing C. It's been designed to work smoothly with Unity Test,
  another of the embedded-software testing tools developed by
  ThrowTheSwitch.org. CMock automagically parses your C headers and
  creates useful and usable mock interfaces for unit testing."
- [[https://interrupt.memfault.com/blog/unit-test-mocking][Embedded C/C++ Unit Testing with Mocks]]

*** Investigate ribosome for ideas                                    :story:

Ribosome seems like a templating tool with a simple syntax. May have
some ideas useful for stitch.

#+begin_quote
In 50 words

 1. You write standard JavaScript, Ruby or Python scripts.
 2. However, lines starting with a dot (.) go straight to the output.
 3. To expand JavaScript/Ruby/Python expressions within dotted lines
    use @{expr} construct.
#+end_quote

Links:

- http://sustrik.github.io/ribosome/: website
- [[https://github.com/sustrik/ribosome][GitHub project]]

*** Add support for languages available in Dia2Code                    :epic:

Dia already has a default code generator: [[http://dia2code.sourceforge.net/][Dia2Code]]. At present it
supports the following outputs (as per [[http://dia2code.sourceforge.net/features.html][features]] page):

- Ada, C, C++, C#, CORBA IDL, Java, PHP, PHP5, Python, Ruby,
  shapefile, and SQL.

We could probably add support for these by creating a test Dia
diagram, running it through Dia2Code an then making sure we get a
binary identical output. This would be a good way to bootstrap
multi-language support.

We couldn't find simple test diagrams in Dia2Code so perhaps we could
also contribute these to the project. It does have a set of [[http://dia2code.sourceforge.net/examples.html][examples]],
which could be used as a starting point for the tests. They are a bit
complex though.

They also appear to have support for functions etc. We should see what
they've done as this would also be a good way to evolve Dogen.

*** Generate model from database tables                               :story:

Insane idea: one could point knit to a database from a supported
vendor and generate the C++ code needed to represent those
tables. This could be filtered by the user. Idea came from here:

- [[https://msdn.microsoft.com/en-us/data/gg558520.aspx][T4 Templates and the Entity Framework]]

All we need is a "database frontend" responsible for connecting to the
database and performing the mapping between the database types and the
language specific types. The model would then have ODB support to read
and write these tables.

We should have a specific tool to do this. Ideally the tool itself
should use ODB or at least the ODB database specific libraries to
access the DML. We should also investigate existing ODB support for
this.

If we think about this in more generic terms, what we need is
something like a LSP for SQL, built using ODB libraries (or some other
libraries that support multiple databases). The entire database schema
is loaded into memory into a database agnostic representation. That
could then be useful for things such as answering LSP queries and also
converting this information into a generatable dogen model. There are
a lot of similarities between these two problems.

Links:

- https://github.com/emacs-lsp/lsp-mssql
- https://github.com/microsoft/sqltoolsservice
- [[https://github.com/kristiandupont/kanel][kanel]]: Generate Typescript types from Postgres

*** Generate model from CSV files                                     :story:

Following on from the idea of using database tables, another
interesting use case are CSV files. It is common that one has a number
of CSV files that one needs to process - for example, import to a
database to perform further comparisons. It would be great if one
could just point knitter at a set of CSV files with headers and have
it:

- create a class for each file type.
- given the data in the file, find the most appropriate type for the
  field. Note when there are more than one file of the same type, and
  instead of generating multiple classes, just use the data for type
  inference.
- generate ODB support if requested by the user. If more than one file
  have the same field with the same type, create foreign keys.

With [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-csv-serialisation][support for CSV serialisation]], this means one could build a CSV
importer and processor very quickly. With support for diffs, this
would mean one could create a CSV file differ easily.

We should probably add this to the same tool that supports JSON/XML
imports.

Notes:

- we could make a CSV injector for this. It could have a few
  additional parameters such as first line contains field names,
  delimiter, etc (check Excel/Calc import dialogs). It would then
  guess the field types using some simple heuristics (regex for
  numeric, regex for float and so forth until last option is string).
- actually maybe a two step process. First we'd have the "model
  generator", which parses the CSV file and creates a JSON model. Then
  we can use the regular JSON model. We could, of course, just plug it
  in so that we guess the JSON model and then generate it, but if
  users want to fiddle with the model it would be nice to have the
  option.

*** Generate model from Excel files                                   :story:

Similar to CSV files, it may be nice to be able to point Dogen to a
spreadsheet and generate a model from it that is able to at least read
data from the spreadsheet. With this we can then easily convert the
data from this format to any other supported formats.

Links:

- [[https://github.com/troldal/OpenXLSX][OpenXLSX GitHub]]: "OpenXLSX is a C++ library for reading, writing,
  creating and modifying Microsoft Excel® files, with the .xlsx
  format."
- [[https://sourceforge.net/projects/libxls/][libxls GitHub]]: "The libxls library is a C library for reading
  files in the legacy Excel file format, .xls. It cannot be used for
  writing or modifying Excel files."
- [[https://sourceforge.net/projects/xlslib/][xlslib GitHub]]: "The xlslib library is a C/C++ library for creating
  files in the legacy Excel file format, .xls. It cannot be used for
  reading or modifying Excel files."
- [[https://libxlsxwriter.github.io][libxlsxwriter GitHub]]: "The libxlsxwriter library is a C library
  for creating .xlsx files. It cannot be used for reading or modifying
  Excel files."
- [[http://www.libxl.com][LibXL GitHub]]: "The LibXL library can read, write, create and
  modify Excel files, in both the .xls and .xlsx formats. It is the
  most feature complete library available and has interfaces for C,
  C++, C# and Delphi. It is only available for purchase, however.
- [[https://github.com/dbzhang800/QtXlsxWriter][QtXlsx GitHub]]: Of the open source libraries, the QtXlsx library
  is the most feature complete. It is, however, based on the Qt
  framework. While I'm a big fan of Qt for application programming
  purposes, I don't believe it is the best option for lower-level
  libraries.
- [[https://github.com/tfussell/xlnt][XLNT GitHub]]: Recently, I found the XLNT library on GitHub. It was
  not available when I began developing OpenXLSX.

*** Add serialisation support to and from spreadsheets                :story:

This is probably mainly useful for reading, though it may also be
useful for writing if we want to say generate graphs. It would be nice
if we could dump a list of objects into a spreadsheet.

See also [[*Generate model from Excel files][Generate model from Excel files]].

Links:

- [[https://stackoverflow.com/questions/2876720/c-library-to-load-excel-xls-files][C++ library to load Excel (.xls) files]]
- [[https://www.codeproject.com/Articles/42504/ExcelFormat-Library][ExcelFormat Library]]
- [[https://github.com/tfussell/xlnt][xlnt]]: Cross-platform user-friendly xlsx library for C++14
- [[https://sourceforge.net/projects/xlsxio/][xlsxio]]

*** Generate model from JSON or XML instance files                    :story:

It would be great if one could point dogen at a given JSON or XML file
and have it infer the model. For XML, if the schema is available, one
should just use the XSD tool of course. This approach is just for
quick and dirty modeling where one needs to build a tool to process a
set of existing documents.

We would have to have the ability to use more than one "source
document" and to "merge" properties across these. For example, if 3
JSON documents are supplied, we should be able to create a type that
is the superset of these.

To be clear, this story is not about using a JSON or XML schema to
generate a yarn model, but to use instances of a given schema in JSON
or XML to infer a possible yarn model that could read that instance
document. This is useful for example to consume a web service from
C++.

We should have a separate tool to do this work.

Links:

- [[http://blog.quicktype.io/debut/][quicktype's big debut]]
- https://quicktype.io/
- [[https://github.com/quicktype/quicktype][quicktype GH]]
- [[https://github.com/Artawower/quicktype.el][quicktype.el]]

*** Investigate GDB visualisers for generated models                  :epic:

#+begin_quote
*Story*: As a dogen user, I would like pretty-printing for my types in
GDB so that I debug more easily programs using dogen models.
#+end_quote

It would be great if the code generator created GDB visualisers for
the types in a generated models such that one could inspect values of
STL containers with types of that model. It should hook into the
pretty-printing facet.

- [[http://sourceware.org/gdb/onlinedocs/gdb/Pretty-Printing.html][Pretty printing]]
- [[https://github.com/ruediger/Boost-Pretty-Printer][Boost pretty printer]]
- [[https://groups.google.com/group/boost-list/browse_thread/thread/ff232ac626bf41cf/18fbf516ceb091da?pli%3D1][Example for multi-index]]

*** Add memory measurements support                                   :story:

Firefox has an interesting approach to estimating memory usage:

- [[http://njn.valgrind.org/measuring.html][Measuring data structures: Firefox (C++) vs. Servo (Rust)]]

The gist of it is to add code like this:

: struct CookieDomainTuple
: {
:   nsCookieKey key;
:   nsRefPtr<nsCookie> cookie;
:
:   size_t SizeOfExcludingThis(mozilla::MallocSizeOf aMallocSizeOf) const;
: };

Where:

: size_t
: CookieDomainTuple::SizeOfExcludingThis(MallocSizeOf aMallocSizeOf) const
: {
:   size_t amount = 0;
:   amount += key.SizeOfExcludingThis(aMallocSizeOf);
:   amount += cookie->SizeOfIncludingThis(aMallocSizeOf);
:   return amount;
: }

We could add something similar to Dogen. This should be an optional
aspect.

Once we got the size information, the next thing should be a way to
dump a size report and make it visualisable in a tool such as baobab
(or in a manner similar to baobab) so that users can, at any time, get
a break down of where the objects are. Of course this would imply we
need a top-level object that contains all objects or some kind of way
of registering sizes. This could be done on construction and
destruction perhaps.

*** Add cling support                                                 :story:

One thing that would be really cool is if one could use generated code
from [[https://github.com/vgvassilev/cling][cling]]. For each project we generate, we could also generate a
"setup cling script" as described [[https://github.com/vgvassilev/cling/issues/67#issuecomment-107004157][here]].

The script simply loads up the SO/static library and all of the
includes of the SO. It also loads up all of the scripts of its
dependencies.

We need to generate SOs for models first.

*** Consider adding support for spelling on comments                  :story:

It would be nice if we could spell-check comments on models. This
could come up as warnings once we have warning/error support.

Links:

- [[https://github.com/nuspell/nuspell][nuspell]]: Free and open source C++ spell checking library
  https://nuspell.github.io

*** Consider offering identifier suggestions for misspelling           :story:

We just bumped into an error where a type was not spelled correctly:

: Error: Object has property with undefined type: <cpp><expansion><inlcusion_directives_repository>

We meant to say =inclusion= rather than =inlcusion=. We should
investigate if the clang "misspell" matching is available to use from
the outside word; it would be useful in cases such as this. We should
be able to just push all of the available qnames in yarn and then,
given a name, see if it matches.

See Spell Checker section of [[http://blog.llvm.org/2010/04/amazing-feats-of-clang-error-recovery.html][this]] post.

Seems like the name of the algorithm used is [[http://en.wikipedia.org/wiki/Levenshtein_distance][Levenshtein
distance]]. Still haven't found clang's API for this but there are
several implementations available.

- https://github.com/cdmh/algorithms
- https://github.com/Martinsos/edlib

We could compute the Levenshtein distance as part of resolution, in
cases where we fail to resolve - e.g. second pass after all has
failed.

We should also do this for command line options.

*** Add support for type "labels" or "tags"                           :story:

It may be useful to "tag" a number of types with a "label", and to
allow users to access these at run time. This only makes sense in the
context of reflection. This story needs a bit more fleshing out. We
don't yet have a use case for this.

This would allow for example to list all objects with a given tag.

*** Add support for plugins                                           :story:

An interesting idea is to generate a model that contains formatters,
create a dynamic library and then have some kind of loading mechanism
in Dogen. The interesting thing is that with static factories, dogen
could make use of this without any code changes at all (e.g. loading
the library into the process is sufficient to trigger registration,
and then its up to the dynamic extensions to decide whether to use the
formatters or not). So a user could create a model with formatters,
add its own text templates, compile and link it and then add it to
Dogen and then make use of the new formatters. The usual constraints
apply such as ABI (ensure one is using the same compiler as used to
compile Dogen, flags, etc).

This may also make sense for the front-end, but less so. At present we
already have support for registering both frontends and backends so
this should work out of the box. We just need a way to load DLLs. For
this we can use  [[http://apolukhin.github.io/Boost.DLL/index.html][Boost.DLL]]. We can add an additional command line
argument so that the user can supply the plugins; before doing
anything we must load all DLLs.

An interesting logical conclusion for this would be that - if stitch
was able to generate all of the infrastructure required to create a
new formatter - we should have a tool that creates all of the
infrastructure to build the stitch template into a shared object. This
would mean generating CMakeLists with right targets, conan file to
pull in dogen dependencies, etc and possibly even having a target to
call dogen loading the SO. In terms of conan - since we need to have
the dogen package installed, we probably should just require users to
install the dev package too. Or perhaps dogen as a tool (e.g. the
EXEs) could also be provided via conan? If we go for packages, we
should generate an "install script" to install all the required
dependencies, including compiler and so on.

We can either extend dogen or create a new utility for this.

Notes:

- [[https://github.com/GNOME/libpeas][libpeas GitHub]]: gnome extensibility library. We should mine it for
  ideas.
- [[https://begriffs.com/posts/2021-07-04-shared-libraries.html?hn=2][Dynamic linking best practices]]
- [[https://akkadia.org/drepper/dsohowto.pdf][How to write shared libraries]]

*** Improving test data generation via "reflection"                   :story:

A really random but perhaps implementable idea: to create a
description of test data as JSON objects. For example, one could
supply a "yarn instantiation description language" (YIDL?) for a user
model, such as:

: {
:    "__type__" : {
:        "model_name": "my_model",
:        "external_module_path": []
:        "module_path": [],
:        "simple_name": "my_type"
:    },
:    "property_0" : true
: }

Where =__type__= is the "meta-type" for the object we want to
instantiate, allowing us to locate it in the yarn model, and the
remaining properties are as per user yarn model. Once the type is
located, one could then iterate through the properties in yarn and
locate their instance values in the JSON.

With this JSON and an yarn instance of =my_model= we could generate
code that looks like so:

: my_model m;
: m.property_0(true);

This would allow users to provide JSON descriptions of the test data
factories. If we took this approach we should consider renaming test
data to something a bit better (sample data?).

In this light, the current test data is akin to a "generic
instantiation language" in that all strings are instantiated with the
same values (or algorithm of value generation), all ints etc.

This is almost like reflection, in that if one had a strongly typed
model for the instantiation description language, it would look like a
reflected yarn model. The problem, of course, is that we do not code
generate yarn; we code generate in language specific models such as
=cpp=, etc. We would also have to have some kind of reflection support
for these models, and a transformation layer between the yarn
reflection and the implementation model. This is what the formatter
would then use to output.

Actually, this is more like compile-time reflection. We do not need to
know at run time what the model looks like, we just need to code
generate instantiations of the objects at compile time. So in theory,
loading yarn into memory is sufficient for this. In addition, we could
simplify "YIDL" by using yarn qualified names. This can be done for
both types and properties:

: {
:    "type_id" : "<my_model><my_type>",
:    "<my_model><my_type><property_0>" : true
: }

This is sufficient to resolve this to a specific yarn type and its
property. The syntax would get a bit more complicated in the presence
of composition, arrays etc but its more or less usable. One could even
conceivably extend it to collections by supporting some additional
properties (collection size for example) and allowing users to assume
the existence of counters (special values for strings perhaps such as
=%count%=?).

*Notes*

- see also [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#make-test-data-generator-more-configurable][Make test data generator more configurable]]
- this idea stems from the need to generate [[http://www.neuron.yale.edu/neuron/static/docs/refman/hoc.html][Hoc]] files from a neuron
  model described yarn. For that particular use case we would have a
  language model (=hoc=) that uses the instantiation description
  language to generate hoc files. Actually this is really not an
  appropriate solution for this use case; it would be far too limited
  to deal with this problem in general. We need to find a use case for
  this other than just generalising test data.
- we need a better name than YIDL ("yarn instantiation description
  language"). Perhaps something that reflects the nature of this being
  a test data templating engine.
- this is really complicated. The fountains approach gives us many of
  the benefits and it is much simpler.

*** Consider using bounded integers                                   :story:

This library seems to improve static error checking with integers,
etc:

- https://bitbucket.org/davidstone/bounded_integer

However, off the top of my head, there are no cases for bounded
integers in dogen yet.

*** Investigate ModelQ for ideas                                      :story:

We need to mine this project for potential ideas - see how their
approach compares to ours, see if we can learn any lessons from it:

*ModelQ*: ModelQ is a code generator for creating Golang codes/models
to access RDBMS database/tables (only MySQL supported for now).

ModelQ: https://github.com/mijia/modelq

*** Use =observer_ptr= in factory methods                             :story:

We should not return =shared_ptr= etc from factory methods; this
forces the client to use a specific type of pointer. Instead we should
return raw pointers and let the client handle RAII. A slightly better
option may be to use the new observer pointer.

We need to add this type to dogen.

Links:

- [[http://en.cppreference.com/w/cpp/experimental/observer_ptr][observer_ptr]]

*** Consider adding XML schema as a front-end                          :epic:

Tools like =Xsd= and =ejc= use the XML schema to provide the input to
the code generator. There is a simple mapping between the XML schema
and the language. [[http://en.wikipedia.org/wiki/Java_Architecture_for_XML_Binding][JAXB]] is a good example of this. As we already have a
dependency on libxml, we can load XML schemas without any additional
dependencies.

Note though that the idea is not to generate another xsd tool since we
already have a good [[http://www.codesynthesis.com/products/xsd/][C++ xsd tool]]. The point is to figure out if a XML
schema based front-end to Dogen is more convenient than Dia XML or
JSON. This may be appealing for a certain class of users: those using
the XSD tool or JAXB to generate POJOs or their C# equivalent.

It may also be worth looking at the [[https://jaxb.java.net/2.2.4/docs/xjc.html][xjc]] command line tool interface to
pick up ideas for Dogen.

We should also support the annotations used by JAXB such as
inheritance, etc.

*** XML serialisation for interoperability                             :epic:

A very good point raised by Boris on his [[https://www.youtube.com/watch?v%3DAuamDUrG5ZU][XML talk]] is that XML is
useful as an interchange format, mainly for interoperability and
future-proofing. It would be nice if we could use dogen to generate a
"controlled" XML, using a well defined schema (also
code-generated). This would be a stable format that could allow
third-parties to hook into the serialisation - as opposed to boost
serialisation's XML.

*** Add support for CSV serialisation                                 :story:

It would be nice to be able to serialise some simpler types into CSV
files, and to be able to read them from CSV files. We should use a
third-party library to perform the serialisation. We should check that
the types only have simple types. We should also provide some dynamic
extensions for options such as "use quoted strings", etc.

Links:

- [[https://github.com/vincentlaucsb/csv-parser][csv-parser]]: very promising CSV parsing library, with read and write
  support.
- [[https://github.com/shaovoon/minicsv][minicsv]]: Minimalistic CSV Streams.
- [[https://github.com/shaovoon/csv_stream][csv_stream]]: C++14 CSV Stream based on C API.
- [[https://github.com/p-ranav/csv][CSV]]: CSV for Modern C++ (C++ 17). Deprecated.
- [[https://github.com/p-ranav/csv2][csv2 GH]]: new version of csv project.
- [[https://github.com/shaovoon/minicsv_benchmark][benchmark of CSV libaries]]
- [[https://github.com/ashtum/lazycsv][lazycsv GH]]: lazycsv is a c++17, posix-compliant, single-header
  library for reading and parsing csv files.
- [[https://github.com/JamesBoer/Tbl][Tbl GH]]: Tbl is a lightweight CSV (comma separated value) and
  tab-delimited table reader.
- [[https://github.com/antonmks/nvParse][nvParse GH]]: Parsing CSV files with GPU
- [[https://bravenewmethod.com/2016/09/17/quick-and-robust-c-csv-reader-with-boost/][Quick and robust C++ CSV reader with boost]]
- [[https://stackoverflow.com/questions/1120140/how-can-i-read-and-parse-csv-files-in-c][How can I read and parse CSV files in C++?]]

*** Consider generation of validators                                  :epic:

It would be nice if we could constraint the domain of a type via some
kind of rules; for example, provide a regular expression or an EBNF
definition with a string that tells dogen how to validate it. We could
then construct simple validators. As usual these expressions can be
supplied via dynamic extensions.

This is in effect an attempt to add OCL (object constraint language)
support. This would be extremely difficult, but we capture the story
nonetheless.

*** Providing view model hints                                         :epic:

Once we start supporting view models, it would be nice to be able to
take an inheritance tree of objects and collapse it into a list view,
handling all of the use cases (reorder columns, remove columns, add
columns, updating rows, etc). All of this code can be inferred from
the type hierarchy.

*** Consider adding SWIG and Boost.Python support                      :epic:

#+begin_quote
*Story*: As a dogen user, I want to expose models to other languages
so that I can make use of them from there.
#+end_quote

We could generate the code required to expose the C++ types into ruby
or python by creating a formatter for it. Boost.Python would be more
straightforward as it is plain C++ code; SWIG would require generating
an interface file (IDL-like) and as such is closer to [[*Add%20support%20for%20thrift%20and%20protocol%20buffers][this]] story.

When generating code for SWIG we must also take into account that SWIG
will generate the type definitions for all other languages, and as
such we cross-language support will not be compatible with it. For
example, if a user wants C# interoperability, it will not be possible
to also generate C# code from Dogen as the types will clash. In
addition, it should be possible to add Dogen level overrides for SWIG,
e.g.:

: %typemap(csattributes) AClass          "[ThreadSafe]"
: %csattributes AClass::AClass(double d) "[ThreadSafe(false)]"
: %csattributes AClass::AMethod()        "[ThreadSafe(true)]"
:
: %inline %{
: class AClass {
: public:
:  AClass(double a) {}
:  void AMethod() {}
: };
: %}

The typemap attributes must be supplied as meta-data in the class and
attributes. They must also be assigned to a language, e.g.:

: swig.attribute.csharp=[ThreadSafe]"

and in the attribute:

: swig.attribute.csharp=[ThreadSafe(false)]"

Links:

- [[https://www.softwariness.com/articles/api-design-for-swig/][C++ API Design for SWIG]]
- [[https://github.com/pybind/pybind11][pybind11]]: Seamless operability between C++11 and Python
- [[https://blog.esciencecenter.nl/irregular-data-in-pandas-using-c-88ce311cb9ef][50 times faster data loading for Pandas: no problem]]: talks about
  integration of c++ with python (cookie cutterk).

*** Add yuml markup language support                                  :story:

#+begin_quote
*Story*: As a dogen user, I want to generate diagrams using yuml so
that I don't have to install Dia.
#+end_quote

It should be fairly straightforward to add a yuml front end that reads
a file using their markup language and generates an yarn model from it.

*** Automatic generation of C interfaces for C++ code                  :epic:

#+begin_quote
*Story*: As a dogen user, I want to make use of c++ models from C so
that I can create "bridge" APIs.
#+end_quote

Once we have proper C support, it should be doable to have a C++ facet
that automatically exposes a C interface.

*** Improvements to Dogen's dia model                                 :story:

Assorted notes on cleaning-up the dia model:

- create a base class such as =value= and make all values inherit from
  it instead of using boost variant.
- according to DTD, a composite can be made up of either composites or
  attributes. We incorrectly modeled it as having just one inner
  composite.
- perhaps this is better thought of slightly differently: an attribute
  has child nodes. The child nodes can either be leaf nodes, in which
  case they are values, or non-leaf nodes in which case they are
  composite nodes. Composite nodes themselves can have child nodes. If
  they are leaf nodes they are values; if they are non-leaf nodes they
  are either attributes or composites.
- note that we do not need to use shared pointers in composite: we
  could simply have an attribute by value. However, we still need to
  handle the case where the children are either composite or
  attributes. So if we somehow could get composite and attribute to
  have a common base class, we could have a container of that base
  class in composite. For this we would need a shared pointer.
- consider adding the postfix =node= to class names and make it a real
  tree, as per dia's implementation.
- covert all vectors to lists since we do not know their sizes on
  construction.
- one thing to bear in mind is that if we fix the tree structure, we
  will break the XML parsing code in hydrator, which took quite a
  while to get right (and has hacks such as "inner composite").
- its not obvious why we need to treat =dia::string= in a different
  way from all other attribute values (except for =dia::font=).
- alternative: create an XML schema from the DTD and use it with the
  XSD tool to create the model.

*** Add support for XSD tool                                          :story:

Similar to what we've done with ODB, it would be nice to be able to
integrate the XSD tool with dogen:

- have a meta-model element for XSD schemas. We should also have a
  part called =xsd= to store the XSD schemas. In this part we should
  be able to have packages (e.g. =xsd::confirmation=). The meta-model
  element must support all of the configuration options used by XSD
  tool:

: xsdcxx cxx-tree --generate-polymorphic --generate-serialization --cxx-suffix .cpp --hxx-suffix .hpp
: --namespace-map http://www.fpml.org/FpML-5/confirmation=fpml511
: --namespace-map http://www.w3.org/XML/1998/namespace=xml_ns
: --namespace-map http://www.w3.org/2000/09/xmldsig#=xmldsig_nsdone
: --polymorphic-type Product --polymorphic-type DirectionalLeg
: --polymorphic-type PricingStructure --polymorphic-type
: Rate --polymorphic-type PricingStructureValuation $b

  These should be in meta-data so that we can inherit them as
  profiles.
- we should handle decorations via prologue/epilogue.
- useful options: =--std version=, =--guard-prefix=, etc. See man page
  for details.
- generate CMake targets that execute the XSD tool and place the
  output in appropriate locations.
- we should also support a =data= part with XML data files as
  examples. Ideally these should then be used by generated tests to do
  round-trip tests.
- generated code should conform as much as possible to the dogen
  structure (=include=, =src= folders, etc).

Links:

- [[https://manpages.debian.org/jessie/xsdcxx/xsdcxx.1][xsdcxx man page]]
- [[https://www.codesynthesis.com/products/xsd/][XSD: XML Data Binding for C++]]
- [[https://www.codesynthesis.com/pipermail/xsd-users/2013-July/003990.html][[xsd-users] schema to c++ namespace mapping problem]]

*** Harmful workload generator                                        :story:

Noisia is a "Harmful workload generator for PostgreSQL." We could try
to extract some of these ideas and place them into generated code.

Links:

- https://github.com/lesovsky/noisia

*** Add Coherence POF serialisation support                            :epic:

If coherence has open source C++ libraries, we should add support for
serialisation to and from POF.

Links:

- [[http://docs.oracle.com/cd/E24290_01/coh.371/e22839/cpp_api.htm][Understanding the Coherence for C++ API]]

*** Generate Visual Studio solutions                                   :epic:

#+begin_quote
*Story*: As a dogen user, I want to use visual studio solutions
directly so that I don't have to rely on CMake.
#+end_quote

At present we rely on CMake as the C++ meta-build system. There is
nothing stopping us from supporting more native build systems such as
Visual Studio. Consider adding direct support for Visual Studio.

*** Consider adding YQL support                                        :epic:

YQL offers a REST based API with lots of interesting information; an
example of the information provided is available [[https://github.com/yql/yql-tables/blob/master/yahoo/finance/yahoo.finance.quant.xml][here]]. There should be
somewhere a matching XML schema for each of these queries, at least
for the end points that return XML. It would be great if one could
take one of those schemas and generate an yarn representation for them.

More generally, it would be great if dogen was able to create a domain
model off of an XML schema. However, we already have the Code
Synthesis [[http://www.codesynthesis.com/products/xsd/][XSD tool]] for that, so maybe this is just scope creep.

Links:

- https://yuilibrary.com/yui/docs/yql/
- [[https://en.wikipedia.org/wiki/Yahoo!_Query_Language][Wikipedia: Yahoo! Query Language]]

*** Generate state diagrams                                            :epic:

There is nothing stopping us from reading the UML State Chart objects
in Dia and generating an FSM off of it, using boost's state
machines. The state machine could be drawn in the same diagram as the
classes; users could create a state machine associated with a class
somehow. The states would refer to the methods which the user would
manually implement. The system would spit out a state machine for the
target class.

These seem to be vaguely related to workflows. Some interesting posts
in dia mailinglist:

- https://mail.gnome.org/archives/dia-list/2015-June/msg00013.html
- https://mail.gnome.org/archives/dia-list/2015-June/msg00014.html

Now that we moved over to PlantUML it makes more sense to add parsing
support for Plant UML.

See also:

- [[http://blog.qt.io/blog/2017/01/23/qt-scxml-state-chart-support-qt-creator/][Qt SCXML and State Chart Support in Qt Creator]]
- [[https://en.wikipedia.org/wiki/SCXML][SCXML (wikipedia)]]
- [[https://www.w3.org/TR/scxml/][State Chart XML (SCXML): State Machine Notation for Control Abstraction]]
- [[https://github.com/tklab-tud/uscxml][uscxml]]
- There are two possible implementations in boost: [[https://www.boost.org/doc/libs/1_67_0/libs/msm/doc/HTML/index.html][MSM]] and [[https://www.boost.org/doc/libs/1_67_0/libs/statechart/doc/index.html][StateChart]].
- See ragel as well: http://www.colm.net/open-source/ragel/
- [[https://github.com/josteink/wsd-mode][Web Sequence Diagrams emacs mode]]: not exactly directly related.
- [[https://github.com/Skoparov/csm][csm GH]]: "Compact State Machine: An event based header-only
  lightweight state machine that extensively employs compile time to
  avoid virtual calls and simplify the declaration of transition
  tables/action rules."

*** Add reflection support by using model and type enums               :epic:

#+begin_quote
*Story*: As a dogen user, I want to use reflection on generated models
so that I can do run-time introspection.
#+end_quote

It may be useful to create enumerations for models, types and
properties within objects. This would in the future form the basis of
reflection. One could use implementation specific properties to set
the model ID and objects IDs.

A more advanced version would allow you to invoke methods via
reflection. However, since some languages support this natively, and
since it may even be part of C++ in the future, we probably should not
spend a huge amount of effort on this.

Use cases:

- diff support

Links:

- [[https://github.com/billyquith/ponder][Ponder]] a c++ reflection library.
- [[https://github.com/cginternals/cppexpose][cppexpose]]: library for type introspection, reflection, and scripting
  interface.
- [[https://github.com/Manu343726/siplasplas][siplasplas]]
- [[https://github.com/rttrorg/rttr][RTTR]]
- [[https://github.com/hugoam/mud][mud]]

Merged stories:

*Consider adding support for RTTR*

Instead of creating our own reflection, we could just add the code
necessary to register types and use a third party library such as
RTTR. It would be most useful if the models contained operations.

Links:

- [[https://github.com/rttrorg/rttr][GitHub Page]]

*** Add model diff support                                             :epic:

One very useful feature we need to implement is the ability to compare
objects (even composite objects such as an entire model) and to obtain
a list of differences. This has several use cases:

- when something breaks we want to know the before and after (or the
  actual and expected) but we also want to know why they are
  different, in particular for complex object graphs. This is the case
  with unit tests and with changes to code when we have A/B paths.
- in general, users of a system want to explain differences. For
  example, if we had a report that was generating one set of numbers S
  but is now generating another set of numbers S' we need to know
  what contributes to this change.
- for efficiency purposes, we may want to only send the differences
  between two very large objects over the wire rather than the
  entirety of the new version.

In theory nothing stops us from code generating all of the required
classes needed to diff objects in a model. There are two possible
approaches:

*Simple approach*:

Just create a new facet call diff and make these classes generate a
simple textual representation of differences, inspired in
=diff=. Where the object is an entity provide its ID. In general just
provide some "path" to the difference, e.g. model/object/member
variable/etc.

This is mainly useful for the first use case and it is simpler to
implement.

*Complex Approach*:

Models can have a =diff= option. When switched on, knit generates a
set of =diff= classes. These are system types like keys and live in a
sub-folder of =types=. They have full serialisation, hashing etc
support like any other model type. The generated classes are:

- =differ=: for each model type a differ gets generated. this is a
  top-level class that diffs two objects of the same type.
- =changeset=: for each model type a changeset gets generated. it has
  a variant called =changeset_types=, made up of all the types of all
  properties in the model. if a model property has a model type then
  it uses the changeset for that type rather than the type itself; for
  all other cases, including containers, it uses the type itself.

In addition, we need set of enumerations in =reflection=. To start off
with, all it contains is a list of classes in the model and a list of
fields in each class.

The =changeset= then has a container of =changeset_types= against a
reflection class and field.

Diff support is injected into the model just like keys. It also
requires that basic reflection support gets injected too.

With this approach we can satisfy all use cases.

It seems this is known in the literature as a "difference model". Once
we have a difference model, we can apply it to the modeling metamodel
itself. This is useful when we change the metamodel and want to tell
users what chances to make to their templates. Some people advocate
automating the changes to the templates (paper below) but this seems
like quite a hard thing to implement. However, it is conceivable that
we could end up with a fixed set of commands to apply (e.g. rename
class, rename method, etc). these could be thought of as clang
transformations. of course, stitch template are not visible to clang
transformations so they would have to be updated by some other
means. on top of that, its not clear that you can apply the clang
transforms when the code does not compile, so the transforms would
have to be applied prior to running dogen; the actual metamodel code
would be first updated by the transform and then regenerated.

in order for the stitch templates refactoring to work, we'd need some
kind of way of relating the sources back to the templates, much like
protected regions; then we would have to generate a graph
representation in memory of the generated code that resembles the
fragments of the template, and update the template from the generated
code.

this whole problem can be seen as a variation of the "rename problem",
which we captured elsewhere: if a model element is renamed, how do we
propagate that change to all of the dependent code? one possibility is
to split it into several steps:

- change the model. users should only do one change at a time to make
  the process less brittle.
- run dogen with git awareness (see rename story) so that we can see
  both the old and the new model.
- compute the diffs between the two and come up with a set of
  diffs.
- convert the diffs into clang refactor commands. apply those commands
  to the *entire* code base.
- perform reverse mapping to the stitch templates, and update them.
- user then inspects the diffs to make sure it looks above board and
  signs them off. code generator is rebuilt and all models are
  regenerated.

its not clear if this kind of work (diffs, etc) is not more suitable
to domodl, with an ecore based metamodel.

links:

- [[http://www.academia.edu/download/41278180/dealing_with_the_coupled_evolution_of_me20160116-11059-1d1j84g.pdf][dealing with the coupled evolution of metamodels and model-to-text
  transforms]].
- [[https://dl.acm.org/citation.cfm?id=2523603][bridging state-based differencing and co-evolution]]
- [[https://ieeexplore.ieee.org/abstract/document/4634773/][automating co-evolution in model-driven engineering]]
- [[https://s3.amazonaws.com/academia.edu.documents/30769313/paper9.pdf?awsaccesskeyid=akiaiwowyygz2y53ul3a&expires=1526902206&signature=huqoussmbwz%252fylfquvumqmz66nc%253d&response-content-disposition=inline%253b%2520filename%253da_metamodel_independent_approach_to_diff.pdf][a metamodel independent approach to difference representation]]
- [[https://www.eclipse.org/emf/compare/][emf compare]]

*** Add sql support to dogen                                           :epic:

it would be nice to generate all of the tables required to store a
model as well as the stored procs to read and write instances of the
model.

notes:

- use an attribute with the type to determine if we want only the id
  of the foreign key in c++ code or if we want a whole type.
- file names are: "qualified name-entity", e.g. model_type.
- drop/create statements: schema, table, load, save, erase, test data
  generators, test.
- ability to drop/create all: table, procs, etc.
- cmake deployment support: targets such as deploy_database,
  undeploy_database
- testing/ci: target that deploys all the sql to a clean database,
  runs all sql tests and un-deploys all the sql.
- database connection settings: use pgpass.
- must cope with the test/development database separation. at present
  there is a massive hack required to populate both databases
  (changing makefile manually and then reverting the change).
- there should be a way of passing in the database name as an
  environment variable into the cmake
- note that the objective is not to make something like odb. the sql
  is generated specifically for dogen types, not general purpose.

Links:

- [[https://github.com/yandex/ozo][OZO GH]]: "OZO is a C++17 library for asyncronous communication with PostgreSQL
  DBMS. The library leverages the power of template metaprogramming, providing
  convenient mapping from C++ types to SQL along with rich query building
  possibilities."
-

*** Consider adding support for =sandbox-api=                         :story:

From their site:

#+begin_quote
What is Sandboxed API?

The Sandboxed API project (SAPI) aims to make sandboxing of C/C++
libraries less burdensome: after initial setup of security policies
and generation of library interfaces, an almost-identical stub API is
generated, transparently forwarding calls using a custom RPC layer to
the real library running inside a sandboxed environment.

Additionally, each SAPI library utilizes a tightly defined security
policy, in contrast to the typical sandboxed project, where security
policies must cover the total syscall/resource footprint of all
utilized libraries.
#+end_quote

It is not entirely clear how this works or how it would be integrated
with Dogen, but it would be nice to have a top-level flag that would
generate the glue code for sandbox-api.

Links:

- [[https://github.com/google/sandboxed-api][GitHub repo]]: Sandboxed API automatically generates sandboxes for
  C/C++ libraries

*** Consider adding support for draw.io                               :story:

As pointed out by Klemens Morgenstern, this is a web-based diagramming
tool:

- https://draw.io

It can export into XML, but the XML seems to be at a very low-level:

:    <mxCell id="5d2195bd80daf111-8" value="&amp;laquo;interface&amp;raquo;&lt;br&gt;&lt;b&gt;Name&lt;/b&gt;" style="html=1;rounded=0;shadow=0;comic=0;labelBackgroundColor=none;strokeColor=#000000;strokeWidth=1;fillColor=#ffffff;fontFamily=Verdana;fontSize=10;fontColor=#000000;align=center;" parent="1" vertex="1">
:      <mxGeometry x="490" y="1004" width="110" height="50" as="geometry"/>
:    </mxCell>

In order to use this format, the injector would have to have a lot of
cleverness to assemble the objects into UML elements.

Links:

- https://about.draw.io/features/import-export/

*** Consider adding "sidecar" service functionality                   :story:

A new trend emerging for HTTP based services is to have "sidecars"
that run next to the application providing common
functionality. Examples are envoy, kong. The idea of sidecars is that
most of the functionality is kept separate from the application
itself, making application code simpler. Upgrading the sidecar can now
be done transparently without impacting the application (in theory at
least). However, another way to look at this is to consider this
functionality to be facets of the product which can be code generated
on demand.

The easiest thing is to add the config file to our meta-model so that
we can generate it.

Links:

- [[https://medium.com/@far3ns/kong-the-microservice-api-gateway-526c4ca0cfa6][KONG - The Microservice API Gateway]]
- [[https://docs.konghq.com/hub/kong-inc/oauth2/][kong oauth2 plugin]]
- [[https://docs.konghq.com/hub/kong-inc/jwt/][kong jwt plugin]]
- [[https://github.com/envoyproxy/envoy][envoy GitHub]]
- [[https://www.envoyproxy.io/docs/envoy/latest/intro/life_of_a_request][envoy: Life of a Request]]

*** Consider generating code for RAII wrapper                         :story:

It would be nice to be able to automatically generate a RAII wrapper
for a type. Once we have operations this may be doable. However, it is
not clear how we would know what methods to call on construction and
destruction. We could perhaps have some meta-data that annotates
operations as "start RAII" and "end RAII". Then we'd generate a class
with a constructor that matches the signature of these operations, and
constructs an instance of the owning class.

Actually this is very similar to make shared. We need to see what
approach they have taken on the constructor of this function. Example
[[https://gcc.gnu.org/onlinedocs/gcc-4.6.0/libstdc++/api/a01033_source.html][for GCC]]:

#+begin_src c++
/**
 *  @brief  Create an object that is owned by a shared_ptr.
 *  @param  __args  Arguments for the @a _Tp object's constructor.
 *  @return A shared_ptr that owns the newly created object.
 *  @throw  std::bad_alloc, or an exception thrown from the
 *          constructor of @a _Tp.
 */
template<typename _Tp, typename... _Args>
inline shared_ptr<_Tp>
make_shared(_Args&&... __args)
{
    typedef typename std::remove_const<_Tp>::type _Tp_nc;
    return allocate_shared<_Tp>(std::allocator<_Tp_nc>(),
        std::forward<_Args>(__args)...);
}
#+end_src

*** Add jailer like functionality to ORM support                      :story:

Jailer is a Java tool that allows exporting subsets of a relational
database ("Database Subsetting Tool"). This is useful for example for
creating test databases. We could support jailer directly from
Dogen. However, we probably need to have built-in SQL support
first. This story also intersects other stories such as data
anonymisation.

Links:

- [[http://jailer.sourceforge.net/exporting-data.htm][jailer tutorial]]
- [[http://jailer.sourceforge.net/][jailer]]
- [[https://github.com/Wisser/Jailer][Jailer GH]]: "Jailer is a tool for database subsetting and relational
  data browsing."
- [[https://martinfowler.com/articles/evodb.html][Evolutionary Database Design]]: Fowler article talking about how
  jailer is used with Agile.
- [[https://github.com/TonicAI/condenser][condenser]]: Condenser is a config-driven database subsetting
  tool. Seems similar to jailer.

*** Add support for portable services                                 :story:

It seems we just need to generate a small number of files in order to
support portable services. We could add these to our product model.

Links:

- [[http://0pointer.net/blog/walkthrough-for-portable-services-in-go.html][Portable Services Walkthrough (Go Edition)]]
- [[http://0pointer.net/blog/walkthrough-for-portable-services.html][Portable Services with systemd v239]]

*** Add support for the sobjectizer actor framework                   :story:

#+begin_quote
SObjectizer is one of a few cross-platform and OpenSource "actor
frameworks" for C++. But SObjectizer supports not only Actor Model,
but also Publish-Subscribe Model and CSP-like channels. The goal of
SObjectizer is significant simplification of development of concurrent
and multithreaded applications in C++.
#+end_quote

Links:

- [[https://github.com/Stiffstream/sobjectizer][GH]]

*** Add support for the rotor actor framework                         :story:

#+begin_quote
rotor is a non-intrusive event loop friendly C++ actor micro
framework, similar to its elder brothers like caf and sobjectizer. The
new release came out under the flag of pluginization, which affects
the entire lifetime of an actor.
#+end_quote

Links:

- [[https://github.com/basiliscos/cpp-rotor][GH]]
- [[https://habr.com/ru/company/crazypanda/blog/522588/][What's new in rotor v0.09]]

*** Add support for CAF serialisation                                 :story:

The C++ Actor Framework is a C++ library implementing the [[https://en.wikipedia.org/wiki/Actor_model][Actor
Model]]. It requires serialisation support. This could be implemented as
a facet.

We may also be able to add simple actors, but this will probably
require some experience with the framework.

Links:

- [[https://actor-framework.readthedocs.io/en/latest/TypeInspection.html][Type Inspection (Serialization and String Conversion)]]

*** Add sonar cloud checks to project                                 :story:

Sonar cloud seems to provide a set of security etc checks for open
source projects. A project was created but we still need CI integration.

Links

- [[https://github.com/arvidn/libtorrent][libtorrent]]: example project using sonar cloud with badges. Their
  [[https://sonarcloud.io/dashboard?id=libtorrent][project page]].
- [[https://sonarcloud.io/dashboard?id=MASD-Project_dogen][MASD-Project_dogen]]
- [[https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner][Analyzing with SonarQube Scanner]]

*** Consider adding weaving support as a command                      :story:

With the refactorings, we removed top-level weaving support in sprint
16 (around [[https://github.com/MASD-Project/dogen/commits/master?after=28ea047b0ee64e00f8d0f9df817194e83422419b+139][Apr 27 2019]]). Weaving made sense originally because stitch
was responsible for processing a stitch template and had all the
information required to convert a sttich template into a compilable
C++ file. This was because stitch had its own annotations and access
to the profiling system. However, with the integration of variability
into the meta-model, sadly it no longer made sense to have a
self-contained template because the variability elements had to come
from a model. In order to have a self-contained template we need to:

- process an associated model (or set of models, via references);
- build all of the variability structures;
- supply them into the stitch template workflow.

This means that its almost as hard as code-generating the entire model
in the first place. It may still make sense to create some transform
chains to execute this use case, but we do not have any use cases for
it at present and it is non-trivial to implement this.

A second, related, approach is to allow the mere "template
instantiation". In this case, you can generate stitch templates but
these have no knowledge of the model. You could use stitch outside of
dogen for whatever use case you may have (e.g. as a stand-alone T4
like language) but you have no access to any of the model machinery
(at least to start off with). Similarly, with wale (and inja, in the
future), you could expand templates etc but without connection to a
model. Again, we don't really have concrete use cases for these as of
yet so we haven't decided on implementation details, but this story
will be used to capture ideas in this space.

*** Consider creating textual UML injectors                           :story:

We could create a frontend that reads TextUML files:

- https://github.com/abstratt/textuml
- http://abstratt.github.io/textuml/readme

Example (copied from the [[http://abstratt.github.io/textuml/docs/tutorial.html][tutorial]]):

#+begin_example
package payment;

class PaymentMethod
end;

class Cheque specializes PaymentMethod
end;
#+end_example

Now that we are thinking about moving to XMI, this may make slightly
less sense, but nevertheless, its worth capturing the idea. Actually
there are many different languages that implement textual UML:

- [[https://blog.abstratt.com/2007/11/01/uml-101-the-templates-package/][UML 101 with TextUML: the Templates package]]
- [[http://abstratt.github.io/textuml/docs/structure.html][Modeling structure]]: note that they do not use standard UML - there
  are a number of extensions.
- list of textual UML tools: [[https://modeling-languages.com/text-uml-tools-complete-list/][Text to UML tools – Fastest way to create
  your models]]
- [[https://modeling-languages.com/umple-language-model-oriented-programming/][UMPLE]]
- OMG created [[https://www.omg.org/spec/HUTN/About-HUTN/][HUTN]] (human-usable textual notation specification).
- [[https://www.eclipse.org/epsilon/doc/articles/hutn-basic/][Using the Human-Usable Textual Notation (HUTN) in Epsilon]]
- [[https://www.cooperate-project.de/images/publications/MODELSWARD2016.pdf][Survey on Textual Notations for the Unified Modeling Language]]:
  academic paper.

*** Consider implementing a "mock server" for HTTP                    :story:

This project looks very interesting: [[https://www.mock-server.com/][MockServer]].

From the docs:

#+begin_quote
For any system you integrate with via HTTP or HTTPS MockServer can be
used as:

- a mock configured to return specific responses for different
  requests
- a proxy recording and optionally modifying requests and responses
- both a proxy for some requests and a mock for other requests at the
  same time

When MockServer receives a request it matches the request against
active expectations that have been configured, if no matches are found
it proxies the request if appropriate otherwise a 404 is returned.

For each request received the following steps happen:

- find matching expectation and perform action
- if no matching expectation proxy request
- if not a proxy request return 404

An expectation defines the action that is taken, for example, a
response could be returned.
#+end_quote

It seems we could generate a "mock server" facet for models with HTTP
support.

*** Consider adding metrics support                                   :story:

All services should have support for metrics. Dogen should implement
this directly in the meta-model and then code-generate it using a PDM.

Links:

- [[https://github.com/jupp0r/prometheus-cpp][prometheus-cpp]]: This library aims to enable Metrics-Driven
  Development for C++ services. It implements the Prometheus Data
  Model, a powerful abstraction on which to collect and expose
  metrics. We offer the possibility for metrics to be collected by
  Prometheus, but other push/pull collections can be added as plugins.
- [[https://github.com/ultradns/cppmetrics][cppmetrics]]: C++ port of the codahale/dropwizard metrics library. See
  also [[https://github.com/konstan/cppmetrics][this fork]].
- [[https://github.com/orca-zhang/influxdb-cpp][GH influxdb-cpp]]: influxdb is used as a timeseries DB for metrics.

*** Consider adding support for RAFT protocol                         :story:

It may be useful to add RAFT as a meta-model concept. From wikipedia:

#+begin_quote
Raft is a consensus algorithm designed as an alternative to the Paxos
family of algorithms. It was meant to be more understandable than
Paxos by means of separation of logic, but it is also formally proven
safe and offers some additional features.
#+end_quote

Links:

- [[https://en.wikipedia.org/wiki/Raft_(computer_science)][Wikipedia: Raft]]
- [[https://github.com/eBay/NuRaft][NuRaft GitHub]]: Raft implementation derived from the cornerstone
  project, which is a very lightweight C++ implementation with minimum
  dependencies, originally written by Andy Chen.
- [[https://github.com/datatechnology/cornerstone][cornerstone GitHub]]: A very lightweight but complete Raft Consensus
  C++ implementation, the original implementation was published by
  Andy Chen.
- [[https://github.com/typesense/braft][brcp GH]]: "An industrial-grade C++ implementation of RAFT consensus
  algorithm and replicated state machine based on brpc. braft is
  designed and implemented for scenarios demanding for high workload
  and low overhead of latency, with the consideration for
  easy-to-understand concepts so that engineers inside Baidu can build
  their own distributed systems individually and correctly."

*** Consider adding support for static site generators                :story:

Its not entirely clear how this would work, but it may be interesting
to describe a site at the meta-model level and then use an existing
generator to output the code as a cartridge. This could also be an
interesting way to document an API a-la OpenAPI.

The second link here is org-mode. If we had a library that could
convert org-mode documents into static HTML, then this would be a
natural thing to do. For good measure, if models were themselves
org-mode documents we could then generate the documentation for a
model in this way. We could also augment it via images (UML diagrams,
etc). For extra bonus points, we could integrate the documentation
generator for the source code; for example, it would be nice to have
links from a given model element to all the files it generates and all
of the associated documentation. We could use standardese for this as
a library.

Links:

- [[https://github.com/nifty-site-manager/nsm][Nift]]: Nift is a cross-platform open source framework for managing
  and generating websites. See also [[https://www.nift.cc/index.html][homepage]].
- [[https://github.com/standardese/standardese][standardese GitHub]]: Standardese aims to be a nextgen Doxygen. It
  consists of two parts: a library and a command line tool.

*** Consider adding support for Boost.Pool                            :story:

The basic idea is to integrate Boost.Pool with the generated code such
that we could generate objects that are instantiated inside a pool or
an arena. This will be helpful when we finally move to a data driven
approach.

Links:

- [[https://theboostcpplibraries.com/boost.pool][Chapter 4. Boost.Pool]]

*** Unmaintained emblem                                               :story:

If we ever reach a point where Dogen is complete according to the
vision, we should add this emblem:

http://unmaintained.tech/
