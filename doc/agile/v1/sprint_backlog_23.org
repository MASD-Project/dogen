#+title: Sprint Backlog 23
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Mission Statement

- Start work on the generation refactor.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2020-03-24 Tue 08:45]
| <75>                                                   |         |       |      |       |
| Headline                                               | Time    |       |      |     % |
|--------------------------------------------------------+---------+-------+------+-------|
| *Total time*                                           | *40:29* |       |      | 100.0 |
|--------------------------------------------------------+---------+-------+------+-------|
| Stories                                                | 40:29   |       |      | 100.0 |
| Active                                                 |         | 40:29 |      | 100.0 |
| Edit release notes for previous sprint                 |         |       | 7:27 |  18.4 |
| Create a demo and presentation for previous sprint     |         |       | 0:28 |   1.2 |
| Sprint and product backlog grooming                    |         |       | 2:17 |   5.6 |
| Rename archetypes model to physical model              |         |       | 0:28 |   1.2 |
| Rename assets model to logical model                   |         |       | 0:29 |   1.2 |
| Consider renaming "meta-model" namespace               |         |       | 0:37 |   1.5 |
| Rename engine to orchestration                         |         |       | 1:04 |   2.6 |
| Merge extraction model with physical model             |         |       | 4:35 |  11.3 |
| Remove manual feature in profile merge transform       |         |       | 0:08 |   0.3 |
| Add variability domains to the dumpspecs activity      |         |       | 0:12 |   0.5 |
| Add org-mode option to dumpspacs                       |         |       | 0:53 |   2.2 |
| Analysis on designing and implement physical types     |         |       | 6:52 |  17.0 |
| Remove =element= from the modeling location            |         |       | 0:02 |   0.1 |
| Fix clang-cl LNK4217 warnings                          |         |       | 1:04 |   2.6 |
| Analysis on creating a masd kernel model               |         |       | 0:53 |   2.2 |
| Rename generation models to =m2t=                      |         |       | 1:04 |   2.6 |
| Move all generation features away from generation      |         |       | 0:17 |   0.7 |
| Rename generation transforms                           |         |       | 4:49 |  11.9 |
| Remove kernel from models                              |         |       | 0:36 |   1.5 |
| Rename the default MASD kernel                         |         |       | 0:44 |   1.8 |
| Consider removing archetype location by family         |         |       | 0:40 |   1.6 |
| Add part to physical location                          |         |       | 1:16 |   3.1 |
| Make physical location entries simple names            |         |       | 0:11 |   0.5 |
| Add names to logical model                             |         |       | 0:41 |   1.7 |
| Implement archetype locations from physical meta-model |         |       | 2:42 |   6.7 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2020-03-19 Thu 19:47]
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 08:01]--[2020-03-20 Fri 08:51] =>  0:50
    CLOCK: [2020-03-19 Thu 21:01]--[2020-03-19 Thu 21:58] =>  0:57
    CLOCK: [2020-03-19 Thu 20:00]--[2020-03-19 Thu 20:03] =>  0:03
    CLOCK: [2020-03-19 Thu 19:48]--[2020-03-19 Thu 19:59] =>  0:11
    CLOCK: [2020-03-19 Thu 19:02]--[2020-03-19 Thu 19:47] =>  0:45
    CLOCK: [2020-03-18 Wed 20:05]--[2020-03-18 Wed 23:59] =>  3:54
    CLOCK: [2020-03-18 Wed 19:01]--[2020-03-18 Wed 19:33] =>  0:32
    CLOCK: [2020-03-16 Mon 08:51]--[2020-03-16 Mon 09:06] =>  0:15
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.22, "Cine Teatro Namibe"

#+BEGIN_SRC markdown
![Cine Teatro Namibe](https://i.pinimg.com/originals/8c/09/18/8c091838ed68d58681fd1beb6e619945.jpg)
_Cine Teatro Namibe, MoÃ§amedes, Angola. (C) 2015 [MESSYNESSY](https://www.messynessychic.com/2015/06/17/documenting-africas-old-cinemas)._

# Introduction

Welcome to yet another busy Dogen sprint! Originally, we had intended to focus on the fabled "generation refactor" but, alas, it was not to be (yet again). Our preparatory analysis revealed some fundamental deficiencies on the variability implementation and, before you knew it, we were stuck wading in the guts of the variability subsystem for the entirety of the sprint. On the plus side, the end product was a much better designed subsystem, free of unwanted dependencies, and a newly found clarity in the conceptual model with regards to both logical and physical dimensions. On the down side, the refactor produced a lot of churn with regards to stereotypes and feature names, resulting on a fair bit of breakage to user diagrams. In other words, it was quite the eventful sprint. Let's see how we fared in more detail.

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing them in more detail. There have been a number of breaking changes, which have been highlighted with the symbol :warning:.

[![Sprint 1.0.22 Demo](https://img.youtube.com/vi/RysjvA2eZ4o/0.jpg)](https://youtu.be/RysjvA2eZ4o)
_Video 1: Sprint 22 Demo._

## Split templatised from non-templatised variability meta-model elements

A pet peeve of ours, pretty much since profiles were introduced to the meta-model [many moons ago](https://github.com/MASD-Project/dogen/releases/tag/v1.0.16), was the name chosen for the stereotype: ```masd::variability::profile_template```. The postfix ```_template``` was a glaring leak from the implementation; a result of trying to be "too clever by half" in generalising all profiles to be "profile templates", when, in reality, there were only 2 or 3 cases of _actual_ profile template instantiation across the code base. As it was, with this story we _finally_ tackled this annoyance. However, before we proceed, a word is probably needed on what is meant by "templates" and "instantiation" in this context. The explanation will also prove helpful in understanding much of the remaining work carried out in the release.

### Setting the Scene: Quick Primer on Variability Templates

As with many other modeling approaches, MASD divides the modeling of software products into two distinct dimensions: the logical dimension and the physical dimension. The logical dimension is pretty much what you are used to when creating UML class diagrams: the structural world of classes and their relationships (though, of course, in MASD there is a twist to it, but we need to leave _that_ for another time). The physical dimension is, predictably, the world of files and directories. So far, so similar to UML and the like. What MASD does differently, however, is to impose a _well-defined shape_ into the entities that live in the physical dimension, as well as a process by which these instances are derived. That shape is governed by the physical model's _meta-model_, which has existed since the early days of Dogen, albeit in an implicit manner. It is composed of vocabulary such as kernel (_e.g._, "masd"), backend  (_e.g._, C++, C#), facet  (_e.g._, "types", "hash", "serialisation" and so forth) and archetype  (_e.g._, "class header", "class implementation", _etc._).

![Feature bundles](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/dogen_coding_features.png)
_Figure 1_: Examples of Dogen feature bundles prior to the refactor.

The shape of the physical dimension is a function of the implementation; that is, as we add formatters (model-to-text transforms)  to generate new kinds of output, these inject archetypes and facets and so on, augmenting the physical dimension. It became clear early on that adding features needed by all formatters manually was too painful. For example, we need to know if a kernel, backend, facet or archetype is enabled or disabled by the users. Thus a feature called ```enabled``` must exist for every element of the physical meta-model. We started by doing this manually, but it soon became obvious that what we were after was a generic way of saying that a feature with a given name ```n``` applies to every registered ```x``` - with ```x``` being an element of a set ```X```, composed of kernels, backends, facets or archetypes. And so it was that variability templates were born. These were subsequently modeled within the logical model as both "feature bundles" (_i.e._, providing _feature definitions_, as per _Figure 1_) and "profile templates" (_i.e._, groups of configurations created by users for reuse purposes, performing _feature selection_; see _Figure 2_). In both cases we had the notion of an "instance template":

```
#DOGEN masd.variability.template_kind=instance
```

This was a "pseudo" or "identity" template, which does not really get instantiated but is instead copied across. We also had "real templates", associated with one of the "levels" in physical space (_e.g._, all, backend, facet, archetype):

```
#DOGEN masd.variability.template_kind=archetype
```

An additional modeling error was that, whilst profile templates only allowed a template kind at the profile level (that is, all attributes in the profile are of the same ```template_kind```), we did not take the same approach for feature bundles, opening the gates for all sorts of weird and wonderful permutations: one attribute could be a template of kind ```instance``` whereas another could be a template of kind ```archetype```. In practice, we were disciplined enough to avoid any such crazy stunts but, as old saying goes, "a good domain model should make invalid states unrepresentable".

![Dogen's profiles Model](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/profiles_model.png)
_Figure 2_: Dogen's Profiles model before the refactor.

One final word on the dependency between the variability model and the physical model. Though its clear that there is a _connection_ between the two models - at the end of the day, templates can only be initialised when we know the lay of the physical land - it is not necessarily the case that the coupling needs to be made in terms of "direct dependencies" (_i.e._ using a type from the physical model), because it comes at a cost: the graph of dependencies is made more complex because variability is used by many models, and these are then coupled to the physical model by way of this small connection. In truth, these models were joined more due to expediency than thought, for, as we mentioned, most features do not actually need template instantiation. Therefore, our core objective was to _decouple_ the physical model from the variability model.

### The tidy-up

One of the side-effects of the decoupling was to make us focus on creating a clear separation between the templatised and non-templatised elements of the logical model modeling variability. This was mainly to avoid increasing the end users cognitive load for no good reason ("why is this a 'template'? what's an 'instance template'?", _etc._). As a result, the stereotypes are now as follows:

> :warning: **Breaking change**: the names and meaning of these stereotypes have changed. User diagrams must be updated.

|Stereotype|Description|
|--------------|----------------|
|```masd::variability::profile_template```| Meta-model element defining a profile template. The template is instantiated over a _domain_, as we shall explain in the next section.|
|```masd::variability::profile```|Meta-model element defining a non-templatised profile. This is equivalent to the deprecated template kind of ```instance```.|
|```masd::variability::feature_template_bundle```|Meta-model element defining a feature bundle template. As with profile templates, the template is instantiated over a domain. Note that all features belong to the same domain and all are templates, cleaning up the previous modeling mistake.|
|```masd::variability::feature_bundle```|Meta-model element defining a non-templatised feature bundle. This is equivalent to the deprecated template kind of ```instance```.|
|```masd::variability::initializer```|Replaces the previous ```masd::variability::feature_template_initializer```, providing initialisation for both feature templates and features.|

_Table 1_: Stereotypes related to feature bundles and profiles.

While we were at it, we took the opportunity to update the colour theme, making the distinction between these elements more obvious:

![Dogen's profiles Model](https://github.com/MASD-Project/dogen/raw/master/doc/blog/images/dogen_variability_palette.png)
_Figure 3_: Colour theme for all variability meta-model elements.

In addition to the stereotype changes, we also modified the approach to template instantiation, as explained on the next story.

## Introduce "Domains" for Template Instantiation

The concept of _domains_ was introduced as a way to achieve the before mentioned decoupling of the variability model from the physical model. Domains are simple sets of strings that can be used as the basis for template instantiation. When users declare templates (_e.g._, profile templates or feature bundle templates), they must now also provide the domain under which instantiation will take place:

```#DOGEN masd.variability.instantiation_domain_name=masd```

This is, of course, a breaking change:

> :warning: **Breaking change**: ```masd.variability.template_kind``` is no longer supported and must be replaced with ```masd.variability.instantiation_domain_name```. This feature can only be used at the top level with ```masd::variability::profile_template``` and ```masd::variability::feature_template_bundle```.

The following domains are available (with ```${X}``` representing a "pseudo-code" variable):

|Domain name|Sample ```X``` Value|Description|
|------------------|----------------|---------------|
|```${kernel}```|```masd```|The only supported kernel at present. All backends, facets, and archetypes are part of it.|
|```${kernel}.backend```|```masd.backend```|All backends in the MASD kernel. At present, C++ and C#.|
|```${kernel}.facet```|```masd.facet```| All facets in the MASD kernel, across all backends.|
|```${kernel}.archetype```|```masd.archetype```|All archetypes in the MASD kernel, across all backends and facets.|
|```${backend}.facet```|```masd.generation.cpp.facet```|All facets in the C++ backend of the MASD kernel.|
|```${backend}.archetype```|```masd.generation.cpp.archetype```|All archetypes in the C++ backend of the MASD kernel.|
|```${facet}.archetype```|```masd.generation.cpp.types.archetype```|All archetypes in the ```types``` facet, in the C++ backend of the MASD kernel.|

_Table 2_: List of domains available out of the box.

Mind you, not all of these domains are being used at present, but, for completeness sake, we created a simple combinatorial function over the existing physical type to generate all sensible permutations. With this very simple approach we get all of the functionality we had previously, without any direct dependencies between the variability and physical models.

## Remove name duplication from feature bundles and profiles

As you can clearly see from both _Figure 1_ and _Figure 2_, defining a profile or a feature bundle often resulted in a great deal of duplication of feature name prefixes, _e.g._, ```masd.generation.decoration``` in the case of the ```decoration``` profile:

```
masd.generation.decoration.enabled
masd.generation.decoration.licence_name
masd.generation.decoration.modeline_group_name
...
```

This release introduces a new feature that allows setting a prefix for all features in the bundle or profile:

```
#DOGEN masd.variability.key_prefix=masd.generation.decoration
```

Given a sensible profile or feature bundle name, the individual attributes should be meaningful enough to determine what they are about, with minimal repetition. For cases where mixing and matching is required, the old behaviour is still available.

## Mapped default values for feature templates

In the past we found certain weird cases of feature templates where we needed the feature to expand over a domain, but we required different defaults for certain elements of the domain. For example, take the ```postfix``` feature. Ideally, each facet should have the postfix set to a string that correlates with a facet name (say ```hash```) but sometimes to a smaller string (say ```lc``` for ```lexical_cast```) or sometimes to the empty string (say for ```types```). This setup was so complicated we just decided to create these features manually.

With this release we found a solution for the problem in the form of _mapped default values_. These are KVPs as follows:

```
#DOGEN masd.variability.default_value_override.cpp.tests="tests"
#DOGEN masd.variability.default_value_override.cpp.hash="hash"
#DOGEN masd.variability.default_value_override.cpp.lexical_cast="lc"
#DOGEN masd.variability.default_value_override.cpp.io="io"
...
```

With this new feature, we managed to model with one single feature template features that previously required tens of instances.

## Add command line option to dump all specs

Dogen relies heavily on dynamic registration for a lot of its functionality, be it for injectors, features, backends and so forth. To top it all off, we keep changing names of things in our quest for tidying up the conceptual model. As a result, we find ourselves often grepping the code base to figure out what is available - an option that is not exactly practical for end users. With this release we've added a new activity to the command line client: ```dumpspecs```. It works like so:

```
$ ./dogen.cli dumpspecs
Group: Injection
Purpose: Read external formats into Dogen.
    injection.dia: Decodes Dia diagrams. Extension: '.dia'
    injection.json: Decodes diagrams in JSON format. Extension: '.json'

Group: Conversion
Purpose: Output to an external format from a Dogen model.
    injection.dia: Encodes diagrams as JSON documents. Extension: '.json'

Group: Generators
Purpose: Available backends for code generation.
    masd.generation.cpp: Generates C++ code according to the MASD generative model.
    masd.generation.csharp: Generates C# code according to the MASD generative model.

Group: Features
Purpose: Available features for configuration.
    masd.decoration.modeline.editor: Editor to use in this modeline. Binding point: 'any'. Value type: 'masd::variability::text'.
    masd.decoration.modeline.location: Where to place the modeline. Binding point: 'any'. Value type: 'masd::variability::text'.
    masd.decoration.modeline.technical_space: Technical space targeted by the modeline. Binding point: 'any'. Value type: 'masd::variability::text'.
    masd.enumeration.add_invalid_enumerator: If true, adds an enumerator to represent an invalid choice. Binding point: 'element'. Default value: ''. Value type: 'masd::variability::boolean'.
    masd.enumeration.underlying_element: Name of the underlying element to use for the enumeration. Binding point: 'element'. Value type: 'masd::variability::text'.
...
```

Though the documentation may not be the best, we did go through all features and provided _some_ kind of description. Note also that for feature templates, all instances share the same comment.

## Renaming of Extraction Features

With the merging of the extraction model into the physical model (see internal stories below), we found ourselves having to rename a number of features. These names are not final, but at least they avoid referring to a model that no longer exists.

> :warning: **Breaking change**: Users that are making use of any of these features must update their diagrams as per Table 3.

|Old Feature Name| New Feature Name|
|------------------------|----------------------------|
|```masd.extraction.delete_extra_files```|```masd.physical.delete_extra_files```|
|```masd.extraction.output_technical_space```|```masd.physical.output_technical_space```|
|```masd.extraction.force_write```|```masd.physical.force_write```|
|```masd.extraction.delete_empty_directories```|```masd.physical.delete_empty_directories```|
|```masd.extraction.enable_backend_directories```|```masd.physical.enable_backend_directories```|

_Table 3_: List of renamed features.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the [sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_22.org).

## Significant Internal Stories

The sprint was mostly dominated by a large number of small refactors that changed the internals of Dogen dramatically - though in many cases, mainly with regards to naming and location of classes. We've aggregated all of these stories under two themes.

### The Variability Model Refactor

The majority of the work in refactoring the variability model had user facing consequences, and so is described in great detail above. The main internal consequence was a dramatic reduction on the number of features required, due to an increased use of feature templates now that we can default them correctly; but there were also other smaller tasks related to this work:

- dramatic simplification of the template instantiation code, which now merely loops through the list of elements in the domain when instantiating feature templates and profile templates.
- changes related to ensuring lists and key value pairs within variability are stable sorted. In the past we had used unordered maps in the processing of variability data, resulting on tests breaking across operative systems due to re-ordering. We ended up having to make a fairly difficult surgical intervention, which resulted in a fair amount of breakage.

> :warning: **Breaking change**: Order of header files may change with this release. Other values dependent of order of lists and KVPs may also change such as order of database systems in ORM, and so forth.

### The Physical Model Refactor

The second largest refactor this sprint was related to the physical model. This was comprised of a number of tasks:

- rename the ```assets``` model to ```logical```. In truth, assets has always been the model housing all of the meta-modeling elements for the logical model, so it makes sense to name it after its function.
- rename the ```archetypes``` model ```physical``` model, and merge it with the extraction model. It took us a long time to understand that the extraction model was really the physical model in disguise. Originally, we had only used it to write files into the filesystem, but now it has taken on additional responsibilities such as defining the types in the physical meta-model.
- move features related to physical aspects to physical model. This task was started but has not yet been completed.
- rename the namespace ```meta-model``` used in a number of models to ```entities```. The name was more or less meaningless the way it was being used. In addition, now that we need a meta-model for the physical model, it was becoming confusing. The "blander" name entities should avoid this confusion.
- deletion of unused types in the generation model, as well as the removal of the partially implemented support for RapidJSON in the C++ model.

## Resourcing

All and all, it was a very successful sprint from a resourcing perspective. At  51%, our utilisation rate was high but not quite the highest it's ever been (the previous sprint wins on that front at 56%). The high utilisation rate was a reflection of the fact that we worked full time for a big portion of the sprint. Sadly, this indicator is scheduled for a massive drop next sprint as we resume part-time work on Dogen proper, but hey-ho, we should celebrate the wins and this sprint was surely one on this front. Additionally, due to the undivided focus we managed to allocate over 82% of the commitment to stories directly related to the sprint's mission, including a couple of spikes (6.8% on unexpected tests breakage). We spent 17.5% on process, with a solid 10% on backlog grooming. Over half of the product backlog was reviewed this sprint, which we consider to be [a task of vital importance](https://mcraveiro.blogspot.com/2016/01/nerd-food-on-product-backlogs.html). In addition, the cost of the demo has gone down dramatically since we started doing "one take demos", and we achieved a new low this sprint of 0.5%. The quality may not be quite what it used to be, but given the [worse is better](https://en.wikipedia.org/wiki/Worse_is_better) approach we favour so much, we deem it to be "good enough". A final note on Emacs, which had some minor blips but was overall fairly well behaved, costing us around 1.3%.

![Story Pie Chart](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_22_pie_chart.jpg)
_Figure 4: Cost of stories for sprint 22._

## Roadmap

The road map continues to work rather like a Delphic oracle, and we keep trying to divine some kind of prediction that makes sense in terms of the current work. Thus far, it has failed to provide any such information but the visualisation of the gantt chart seems to be reassuring us that there is an end in sight - even though, like the proverbial carrot, it keeps moving forwards.

![Project Plan](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_22_project_plan.png)

![Resource Allocation Graph](https://github.com/MASD-Project/dogen/raw/master/doc/agile/v1/sprint_22_resource_allocation_graph.png)

# Next Sprint

We finally started the generation refactor this sprint, though, to be fair, we just about scratched the surface. Next sprint we will hopefully proceed in anger onto the generation breach and finally make a dent on it.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen/1.0.22) for OSX and Linux (all 64-bit):

- [dogen_1.0.22_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.22/dogen_1.0.22_amd64-applications.deb)
- [dogen-1.0.22-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.22/DOGEN-1.0.22-Darwin-x86_64.dmg)

**Note 1:**: Due to a bug on the build scripts, Windows binaries were not generated for this release. If you do not want to build Windows from source, you can grab the unstable binaries for the next sprint: [dogen-1.0.23-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/DOGEN-1.0.23-Windows-AMD64.msi).
**Note 2:** The OSX and Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this a trivial undertaking.

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.

Happy Modeling!
#+END_SRC markdown

- [[https://twitter.com/MarcoCraveiro/status/1240728672128172033][twitter]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6646494675207278592/][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** COMPLETED Create a demo and presentation for previous sprint      :story:
    CLOSED: [2020-03-19 Thu 19:47]
    :LOGBOOK:
    CLOCK: [2020-03-19 Thu 18:02]--[2020-03-19 Thu 18:30] =>  0:28
    :END:

Time spent creating the demo and presentation. Use the demo project:

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2020-03-23 Mon 08:25]--[2020-03-23 Mon 08:40] =>  0:15
    CLOCK: [2020-03-22 Sun 11:33]--[2020-03-22 Sun 11:45] =>  0:12
    CLOCK: [2020-03-21 Sat 09:25]--[2020-03-21 Sat 09:32] =>  0:07
    CLOCK: [2020-03-20 Fri 14:17]--[2020-03-20 Fri 14:30] =>  0:13
    CLOCK: [2020-03-20 Fri 11:23]--[2020-03-20 Fri 11:52] =>  0:29
    CLOCK: [2020-03-18 Wed 19:01]--[2020-03-18 Wed 19:44] =>  0:43
    CLOCK: [2020-03-16 Mon 09:07]--[2020-03-16 Mon 09:16] =>  0:09
    CLOCK: [2020-03-16 Mon 08:41]--[2020-03-16 Mon 08:50] =>  0:09
    :END:

Updates to sprint and product backlog.

*** COMPLETED Rename archetypes model to physical model               :story:
    CLOSED: [2020-03-16 Mon 10:15]
    :LOGBOOK:
    CLOCK: [2020-03-16 Mon 09:17]--[2020-03-16 Mon 09:45] =>  0:28
    :END:

According to the new understanding, the role of the archetypes model
is to model entities in the physical dimension of MASD. Rename the
model accordingly, and create the new entities namespace while we're
at it.

*** COMPLETED Rename assets model to logical model                    :story:
    CLOSED: [2020-03-16 Mon 10:15]
    :LOGBOOK:
    CLOCK: [2020-03-16 Mon 09:46]--[2020-03-16 Mon 10:15] =>  0:29
    :END:

- rename all references to archetypes to "physical", e.g.:
  =artefact_properties= should be renamed, etc.

*** COMPLETED Consider renaming "meta-model" namespace                :story:
    CLOSED: [2020-03-16 Mon 10:43]
    :LOGBOOK:
    CLOCK: [2020-03-16 Mon 10:43]--[2020-03-16 Mon 10:54] =>  0:11
    CLOCK: [2020-03-16 Mon 10:16]--[2020-03-16 Mon 10:42] =>  0:26
    :END:

Originally we created a number of namespaces in models called
"meta-model". It started with assets, where it really was the
meta-model, but we now have meta-models on pretty much all models
(injection, extraction, etc). Its no longer clear what value this
prefix adds. In addition its a technical word, so it seems to imply
there is some meaning to it, but since pretty much we have in dogen is
a meta-model of something, its not exactly useful. We need a term that
is more neutral.

Ideas:

- elements
- entities

Notes:

- look for ideas on other projects.

*** COMPLETED Rename engine to orchestration                          :story:
    CLOSED: [2020-03-16 Mon 11:59]
    :LOGBOOK:
    CLOCK: [2020-03-16 Mon 10:55]--[2020-03-16 Mon 11:59] =>  1:04
    :END:

Since this model is responsible for the top-level orchestration, its
probably a more meaningful name. Whilst we are at it, might as well do
this rename now.

While we were at it we also created namespaces in physical model.

*** COMPLETED Merge extraction model with physical model              :story:
    CLOSED: [2020-03-17 Tue 12:56]
    :LOGBOOK:
    CLOCK: [2020-03-17 Tue 16:29]--[2020-03-17 Tue 16:41] =>  0:12
    CLOCK: [2020-03-17 Tue 15:12]--[2020-03-17 Tue 15:24] =>  0:12
    CLOCK: [2020-03-17 Tue 12:57]--[2020-03-17 Tue 13:05] =>  0:08
    CLOCK: [2020-03-17 Tue 08:53]--[2020-03-17 Tue 12:56] =>  4:03
    :END:

It is becoming clear that the extraction model is just an instance of
the physical meta-model. We should just merge the two.

Notes:

- rename the kernel model to "meta-model".
- remove origin_element_id

*** COMPLETED Remove manual feature in profile merge transform        :story:
    CLOSED: [2020-03-20 Fri 09:00]
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 08:52]--[2020-03-20 Fri 09:00] =>  0:08
    :END:

We are still using features manually in the profile merge transform
even though we have generated code for it.

*** COMPLETED Add variability domains to the dumpspecs activity       :story:
    CLOSED: [2020-03-20 Fri 09:13]
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 09:01]--[2020-03-20 Fri 09:13] =>  0:12
    :END:

At present we have no way of knowing what the valid variability
domains are. We should dump them when we dump the specs.

*** COMPLETED Add org-mode option to dumpspacs                        :story:
    CLOSED: [2020-03-20 Fri 10:04]
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 10:05]--[2020-03-20 Fri 10:08] =>  0:03
    CLOCK: [2020-03-20 Fri 09:14]--[2020-03-20 Fri 10:04] =>  0:50
    :END:

It should be possible to output the specs in org mode format.

*** COMPLETED Analysis on designing and implement physical types      :story:
    CLOSED: [2020-03-20 Fri 11:22]
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 10:09]--[2020-03-20 Fri 11:22] =>  1:13
    CLOCK: [2020-03-17 Tue 08:40]--[2020-03-17 Tue 08:53] =>  0:13
    CLOCK: [2020-03-17 Tue 08:25]--[2020-03-17 Tue 08:39] =>  0:14
    CLOCK: [2020-03-16 Mon 20:41]--[2020-03-16 Mon 21:46] =>  1:05
    CLOCK: [2020-03-16 Mon 12:42]--[2020-03-16 Mon 16:49] =>  4:07
    :END:

- implement locator in terms of new types.
- get kernels to export the new information.
- using the information compute the paths. Create a new field so that
  we can diff new and old paths.
- once there are no differences, remove all locator related legacy
  code.

Notes:

- start by removing all types which are no longer needed. Then create
  new types in the physical model.
- replace references to archetypes location with physical location.
- create a model for the physical world, and replace the archetype
  location repository with it. Kernels return the components of the
  model.
- kernel model is meta-model.
- physical model and extraction model need to merge. We must supply
  the artefact for updates to the formatters.
- generation has a pair of logical element, artefact (e.g. formattable
  by another name).
- physical model properties must exist in the artefact.
- enablement and overwrites are physical model concerns.
- artefact / archetype properties are physical model concerns (mainly
  enablement, really).
- decoration should move to the logical model.
- create a top-level interface called "kernel". It should return the
  kernel meta-data of the physical model. Get the backends to register
  with the kernel, and the facets and formatters to register with the
  backends, so that we return a complete physical meta-model. Create a
  MASD kernel.

*** CANCELLED Remove =element= from the modeling location             :story:
    CLOSED: [2020-03-20 Fri 11:55]
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 11:53]--[2020-03-20 Fri 11:55] =>  0:02
    :END:

*Rationale*: this is in use by attributes at present.

We introduced this for inner classes, but its (probably) not being
used. If so, remove it and add a story for inner classes, if one does
not yet exist.

*** COMPLETED Fix clang-cl LNK4217 warnings                           :story:
    CLOSED: [2020-03-20 Fri 18:26]
    :LOGBOOK:
    CLOCK: [2020-03-22 Sun 11:10]--[2020-03-22 Sun 11:23] =>  0:13
    CLOCK: [2020-03-20 Fri 14:44]--[2020-03-20 Fri 15:35] =>  0:51
    :END:

We also have a number of warnings left to clean up, all related to
boost.log:

: masd.dogen.utility.lib(lifecycle_manager.cpp.obj) : warning LNK4217: locally defined symbol
: ?get_tss_data@detail@boost@@YAPEAXPEBX@Z (void * __cdecl boost::detail::get_tss_data(void const *))
: imported in function "public: struct boost::log::v2s_mt_nt6::sinks::basic_formatting_sink_frontend<char>::formatting_context * __cdecl boost::thread_specific_ptr<struct boost::log::v2s_mt_nt6::sinks::basic_formatting_sink_frontend<char>::formatting_context>::get(void)const " (?get@?$thread_specific_ptr@Uformatting_context@?$basic_formatting_sink_frontend@D@sinks@v2s_mt_nt6@log@boost@@@boost@@QEBAPEAUformatting_context@?$basic_formatting_sink_frontend@D@sinks@v2s_mt_nt6@log@2@XZ)

Since we can't get to the bottom of this, try to ignore the warnings
instead: /IGNORE:LNK4217

Notes:

- opened issue: [[https://github.com/Microsoft/vcpkg/issues/5336][Building with clang-cl on windows generates warnings
  from vcpkg-installed libraries]]
- it seems that the log files show a lot more warnings than those
  reported by cdash,
- Updated issue on CDash parsing problems for clang-cl: [[https://github.com/Kitware/CDash/issues/733][Parsing of
  errors and warnings from clang-cl]]
- sent email to clang mailinglist:
  [[http://lists.llvm.org/pipermail/cfe-dev/2019-February/061326.html][Clang-cl -
  errors and warning messages slightly different from MSVC]]. Clang
  [[http://lists.llvm.org/pipermail/cfe-dev/2019-February/061339.html][have patched]] the diffs now.
- we are now seeing all the warnings.
- [[https://stackoverflow.com/questions/50274547/windows-clang-hello-world-lnk4217/57788067#57788067][Windows clang Hello World lnk4217]]
- [[https://stackoverflow.com/questions/6979491/how-to-delete-warnings-lnk4217-and-lnk4049/6979586#6979586][How to delete warnings LNK4217 and LNK4049]]
- [[https://docs.microsoft.com/en-us/cpp/build/reference/ignore-ignore-specific-warnings?view=vs-2019][/IGNORE (Ignore Specific Warnings)]]

*** COMPLETED Analysis on creating a masd kernel model                :story:
    CLOSED: [2020-03-21 Sat 09:24]
    :LOGBOOK:
    CLOCK: [2020-03-21 Sat 08:31]--[2020-03-21 Sat 09:24] =>  0:53
    :END:

Idea:

- create a kernel interface and a backend interface in generation.
- add a registrar for kernels.
- create a new model called masd. Implement the kernel
  interface. Return the meta-model by calling all registered backends.
- implement the backend interface in the existing backends.

Notes:

- actually, we assumed the notion of a "kernel" without thinking too
  much about it. In reality there is not need for multiple
  kernels. This is because the logical model (and to an extent, the
  physical model) are designed to house MASD principles. Therefore
  they are only useful to output code that conforms to MASD
  principles. If a user was to want to define a new kernel - say for
  example for protobufs - then it would either:

  - be fitted into the MASD logical model, as we have done thus far
    with all facets; in which case it is part of the MASD kernel; or
  - require a new logical model, in which case it would be outside of
    Dogen, really.

  Therefore it doesn't make a lot of sense to have more than one
  kernel.
- in addition, terms such as kernel, backend, formatter, generation
  etc are not MDE terms, and we have been using them for historic
  reasons. In reality, the generation model is the entry point of the
  model-to-text (M2T) chain; the backend models are specialisations of
  the M2T chain for specific technical spaces; and formatters are M2T
  transforms.
- in light of this we could align Dogen to MDE with a small number of
  changes:
  - drop kernel from archetype location, meta-model, features,
    etc. Features become located at =masd.m2t=. Conceptually this is
    equivalent to a kernel, but its non-optional. We could call this
    the "prefix" and have it set in the meta-model. Or have a
    "traits-like" class in the physical model.
  - rename =generation= to =m2t=.
  - rename interfaces to =m2t_chain= (top-level),
    =m2t_technical_space_chain= (interface), =m2t_cpp_chain= (backend),
    =m2t_transform= (formatter) and so forth.

*** COMPLETED Rename generation models to =m2t=                       :story:
    CLOSED: [2020-03-21 Sat 14:49]
    :LOGBOOK:
    CLOCK: [2020-03-21 Sat 15:18]--[2020-03-21 Sat 15:28] =>  0:10
    CLOCK: [2020-03-21 Sat 14:50]--[2020-03-21 Sat 15:00] =>  0:10
    CLOCK: [2020-03-21 Sat 14:48]--[2020-03-21 Sat 14:49] =>  0:01
    CLOCK: [2020-03-21 Sat 14:30]--[2020-03-21 Sat 14:47] =>  0:17
    CLOCK: [2020-03-21 Sat 09:33]--[2020-03-21 Sat 09:59] =>  0:26
    :END:

These models are really just containers of M2T transforms, so name
them accordingly.

*** COMPLETED Move all generation features away from generation      :story:
    CLOSED: [2020-03-21 Sat 15:17]
    :LOGBOOK:
    CLOCK: [2020-03-21 Sat 15:00]--[2020-03-21 Sat 15:17] =>  0:17
    :END:

Rename the meta-data keys of the generation features from
=masd.generation= to =masd.m2t=.

*** COMPLETED Rename generation transforms                            :story:
    CLOSED: [2020-03-21 Sat 23:57]
    :LOGBOOK:
    CLOCK: [2020-03-22 Sun 17:28]--[2020-03-22 Sun 17:31] =>  0:03
    CLOCK: [2020-03-22 Sun 16:53]--[2020-03-22 Sun 17:21] =>  0:28
    CLOCK: [2020-03-21 Sat 20:42]--[2020-03-21 Sat 23:57] =>  3:15
    CLOCK: [2020-03-21 Sat 18:30]--[2020-03-21 Sat 19:26] =>  0:56
    CLOCK: [2020-03-21 Sat 15:29]--[2020-03-21 Sat 15:36] =>  0:07
    :END:

Renames:

- top-level: =m2t_chain=
- interface: =m2t_technical_space_chain=
- backend: =m2t_cpp_chain=
- formatter: =m2t_transform=
- namespaces

*** COMPLETED Remove kernel from models                               :story:
    CLOSED: [2020-03-22 Sun 09:33]
    :LOGBOOK:
    CLOCK: [2020-03-22 Sun 08:57]--[2020-03-22 Sun 09:33] =>  0:36
    :END:

We don't really need the notion of kernel in MASD, so remove
it. However, make sure we still preserve the notion of a top-level
container for backends - for now =masd.generation=.

*** COMPLETED Rename the default MASD kernel                          :story:
    CLOSED: [2020-03-22 Sun 11:24]
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 15:36]--[2020-03-20 Fri 16:20] =>  0:44
    :END:

Up to now we have conflated the generation model with the default MASD
kernel. The generation model is responsible for expanding the logical
model into the physical dimension and then using all available kernels
to populate the content of the artefacts. Given this we should really
start to separate generation from the MASD default kernel, which is
the current implementation of the model to text transforms. We need a
name for the kernel because we can't keep calling it "generation" as
its just confusing. The name needs to also be distinct from MASD since
we use it as the prefix all all features (e.g. =masd.masd= would not
be enlightening). We could just give it a distinctive name which is
not particularly meaningful: =genie= (from generation, little
generator). Then we'd have =masd.genie.enabled=, etc. It would also
allow users to create their own kernels with distinctive names,
e.g. =ddc.xyz.enabled=.

*** COMPLETED Consider removing archetype location by family          :story:
    CLOSED: [2020-03-22 Sun 16:52]
    :LOGBOOK:
    CLOCK: [2020-03-22 Sun 16:12]--[2020-03-22 Sun 16:52] =>  0:40
    :END:

Check to see if this container is in use and if not, remove it and all
associated infrastructure.

*** COMPLETED Add part to physical location                           :story:
    CLOSED: [2020-03-24 Tue 08:45]
    :LOGBOOK:
    CLOCK: [2020-03-24 Tue 07:45]--[2020-03-24 Tue 08:45] =>  1:00
    CLOCK: [2020-03-22 Sun 17:44]--[2020-03-22 Sun 17:52] =>  0:08
    CLOCK: [2020-03-22 Sun 11:24]--[2020-03-22 Sun 11:32] =>  0:08
    :END:

We need to express the idea that archetypes live in different parts of
a component. Add a part to the physical location, and update all model
to text transforms to populate it. Then, change the archetype name to
use the part as well as the facet on the name.

A slight issue is that the part name cannot be fully qualified. For
example, say:

: masd.cpp.include

is not a good part name, at least inside of the location. If we do
that, then when we add the facet, we get:

: masd.cpp.types.masd.cpp.include.

In truth, we have been using the fully qualified name incorrectly all
along. We should really have a location that only denotes each
"region":

- backend: =masd.cpp=
- facet: =types= (not =masd.cpp.types=)
- part: =include=
- archetype: =class_header=

And then the fully qualified name for the archetype.becomes:

: masd.cpp.types.include.class_header

This also means we are completely symmetric with the logical model. So
we really should have a notion of a name (simple, qualified) with a
location. The ID is the qualified name.

*** STARTED Make physical location entries simple names               :story:
    :LOGBOOK:
    CLOCK: [2020-03-22 Sun 17:32]--[2020-03-22 Sun 17:43] =>  0:11
    :END:

At present all names in a location are qualified, e.g. =types= facet
is given as:

: masd.cpp.types

We need these to be simple, e.g.: =types=. We should do this after we
move to names because we will need a way to obtain the qualified
name - e.g. what we currently call the archetype. First step should be
to populate the logical name with the correct qualified name, then
replace calls to archetype with calls to qualified name, then do this
change.

*** STARTED Add names to logical model                                :story:
    :LOGBOOK:
    CLOCK: [2020-03-22 Sun 17:22]--[2020-03-22 Sun 17:28] =>  0:06
    CLOCK: [2020-03-22 Sun 11:46]--[2020-03-22 Sun 12:21] =>  0:35
    :END:

We need to move towards the same approach as we have in the logical
model but for the physical model:

- have a name class with =simple= and =qualified= and a
  location. =qualified= is the location plus simple.
- Use =qualified= as the ID on any container (e.g. archetype location
  repository).
- containers with facets must have a concatenation of =backend= plus
  =facet=.
- create a name builder and/or name factory that make qualified names.

*** STARTED Implement archetype locations from physical meta-model    :story:
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 14:30]--[2020-03-20 Fri 14:44] =>  0:14
    CLOCK: [2020-03-20 Fri 13:01]--[2020-03-20 Fri 14:16] =>  1:15
    CLOCK: [2020-03-17 Tue 17:15]--[2020-03-17 Tue 17:24] =>  0:09
    CLOCK: [2020-03-17 Tue 15:25]--[2020-03-17 Tue 16:29] =>  1:04
    :END:

We need to use the new physical meta-model to obtain information about
the layout of physical space, replacing the archetype locations.

Tasks:

- make the existing backend interface return the layout of physical
  space.
- create a transform that populates all of the data structures needed
  by the current code base (archetype locations).
- replace the existing archetype locations with a physical meta-model.
- remove all the archetype locations data structures.

Merged stories:

*Clean-up archetype locations modeling*

We now have a large number of containers with different aspects of
archetype locations data. We need to look through all of the usages of
archetype locations and see if we can make the data structures a bit
more sensible. For example, we should use archetype location id's
where possible and only use the full type where required.

Notes:

- formatters could return id's?
- add an ID to archetype location; create a builder like name builder
  and populate ID as part of the build process.

*Implement the physical meta-model*

We need to replace the existing classes around archetype locations
with the new meta-model types.

Notes:

- formatters should add their data to a registrar that lives in the
  physical model rather than expose it via an interface.

*** Model SQL scripts as meta-model entities                          :story:

At present we are adding SQL scripts to the relational model under the
=sql= directory. These should be part of the model. We need meta-types
to represent these files. For now they just need to generate an empty
file - or perhaps just the SQL modeline and decoration. They should
also be marked as handcrafted. We also need to add a part for SQL.

*** Model lisp scripts as meta-model entities                         :story:

We are using lisp scripts in the dia and templating projects. These
need to be modeled and generated. Generation can have just decoration.

*** Consider allowing users to create their own parts                 :story:

It would be nice if one could create our own parts. However the main
problem is how would you allocate modeling elements to a part. At
present this is done via the formatter; perhaps we could override this
in meta-data? This is a very complex task and we need clear use cases
for it. Alternatively we could state that a user defined part's
content is ignored entirely.

*** Implement the generation model in terms of "formattables"         :story:

We need to find a way to expand the generation model into a pair of:

- element
- artefact

In effect, a formattable. Then we need to update the backends to stop
expanding across physical space and instead use the expansion created
by the generation model. We then need to update formattables to have
an artefact, and supply the artefact to all formatters.

*** Implement enablement in physical model                            :story:

We need to move the types in generation model related to enablement
into the physical model. We also need to move the types in the logical
model related to enablement into the physical model. We need to create
the enablement transform in the physical model. These are then called
from the generation model.

Notes:

- split enablement features by facet, backend, kernel etc.
- add code generation support for static configuration on templates.

Merged stories:

*Refactor enablement types*

These types all have historical names.

Tasks:

- =local_archetype_location_properties=: these are just enablement
  properties. We need to also add =backend_enabled=, at which point
  the type in the logical model is identical to the one in the
  generation model.
- =global_archetype_location_properties=: with the exception of
  =denormalised_archetype_properties=, these types are just used to
  read the meta-data for enablement. They could be private to a helper
  that generates =enablement_properties= and could be used for both
  global and local.
- the enablement transform (probably) has no dependencies and could be
  lifted into the physical model.

*** Implement locator in physical model                               :story:

Merged stories:

*Create a archetypes locator*

We need to move all functionality which is not kernel specific into
yarn for the locator. This will exist in the helpers namespace. We
then need to implement the C++ locator as a composite of yarn
locator.

*Other Notes*

At present we have multiple calls in locator, which are a bit
ad-hoc. We could potentially create a pattern. Say for C++, we have
the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine
  the placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or
  "natural" location inside of facet.
- archetype location: used to determine the facet and archetype
  postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location
(a given artefact has a fixed placement). So a naive approach to this
seems to imply one could create a data driven locator, that works for
all languages if supplied suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project
  sub-directories". There is a mapping between the artefact location
  and a project sub-directory.
- there is a mapping between the artefact location and the facet and
  artefact postfixes.
- extensions are a slight complication: a) we want to allow users to
  override header/implementation extensions, but to do it so for the
  entire project (except maybe for ODB files). However, what yarn's
  locator needs is a mapping of artefact location to  extension. It
  would be a tad cumbersome to have to specify extensions one artefact
  location at a time. So someone has to read a kernel level
  configuration parameter with the artefact extensions and expand it
  to the required mappings. Whilst dealing with this we also have the
  issue of elements which have extension in their names such as visual
  studio projects and solutions. The correct solution is to implement
  these using element extensions, and to remove the extension from the
  element name.
- each kernel can supply its configuration to yarn's locator via the
  kernel interface. This is fairly static so it can be supplied early
  on during initialisation.
- there is still something not quite right. We are performing a
  mapping between some logical space (the modeling space) and the
  physical space (paths in the filesystem). Some modeling elements
  such as the various CMakeLists.txt do not have enough information at
  the logical level to tell us about their location; at present the
  formatter itself gives us this hint ("include cmakelists" or "source
  cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the
  archetype location and the physical space. Although, if this is the
  only case of a modeling element not mapping uniquely, perhaps we
  should do exactly this.
- However, we still have inclusion paths to worry about. As we done
  with the source/include directories, we need to somehow create a
  concept of inclusion path which is not language specific; "relative
  path" and "requires relative path" perhaps? These could be a
  function of archetype location.

Merged stories:

*Generate file paths as a transform*

We need to understand how file paths are being generated at present;
they should be a transform inside generation.

*** Implement formatting styles in physical model                     :story:

We need to move the types related to formatting styles into physical
model, and transfors as well.

Merged stories:

*Move formatting styles into generation*

We need to support the formatting styles at the meta-model level.

*** Make physical model name a qualified name                         :story:

At present we are setting up the extraction model name from the simple
name of the model. It should really be the qualified name. Hopefully
this will only affect tracing and diffing.

*** Create a common formatter interface                               :story:

Once all language specific properties have been moved into their
rightful places, we should be able to define a formatter interface
that is suitable for both c++ and c# in generation. We should then
also be able to move all of the registration code into generation. We
then need to look at all containers of formatters etc to see what
should be done at generation level.

*** Implement dependencies in terms of new physical types             :story:

- add dependency types to physical model.
- add dependency types to logical model, as required.
- compute dependencies in generation. We need a way to express
  dependencies as a file dependency as well as a model
  dependency. This caters for both C++ and C#/Java.
- remove dependency code from C++ and C# model.

Notes:

- in light of the new physical model, we need a transform that calls
  the formatter to obtain dependencies. The right way to do this is to
  have another registrar (=dependencies_transform=?) and to have the
  formatters implement both interfaces. This means we can simply not
  implement the interface (and not register) when we have no
  dependencies - though of course given the existing wale
  infrastructure, we will then need yet another template for
  formatters which do not need d

Merged stories:

*Formatter dependencies and model processing*

At present we are manually adding the includes required by a formatter
as part of the "inclusion_dependencies" building. There are several
disadvantages to this approach:

- we are quite far down the pipeline. We've already passed all the
  model building checks, etc. Thus, there is no way of knowing what
  the formatter dependencies are. At present this is not a huge
  problem because we have so few formatters and their dependencies are
  mainly on the standard library and a few core boost models. However,
  as we add more formatters this will become a bigger problem. For
  example, we've added formatters now that require access to
  variability headers; in an ideal world, we should now need to have a
  reference to this model (for example, so that when we integrate
  package management we get the right dependencies, etc).
- we are hard-coding the header files. At present this is not a big
  problem. To be honest, we can't see when this would be a big
  problem, short of models changing their file names and/or
  locations. Nonetheless, it seems "unclean" to depend on the header
  file directly.
- the dependency is on c++ code rather than expressed via a model.

In an ideal world, we would have some kind of way of declaring a
formatter meta-model element, with a set of dependencies declared via
meta-data. These are on the model itself. They must be declared
against a specific archetype. We then would process these as part of
resolution. We would then map the header files as part of the existing
machinery for header files.

However one problem with this approach is that we are generating the
formatter code using stitch at present. For this to work we would need
to inject a fragment of code into the stitch template somehow with the
dependencies. Whilst this is not exactly ideal, the advantage is that
we could piggy-back on this mechanism to inject the postfix fields as
well, so that we don't need to define these manually in each
model. However, this needs some thinking because the complexity of
defining a formatter will increase yet again. When there are problems,
it will be hard to troubleshoot.

*Move dependencies into archetypes*

Actually the dependencies will be generated at the kernel level
because 99% of the code is kernel specific. However, we need to make
it an external transform. We need to figure out an interface that
supplies archetypes with the data needed to create the dependencies
container.

Tasks:

- create the locator in the C++ external transform
- create a dependencies transform that uses the existing include
  generation code.

*Previous understanding*

It seems all languages we support have some form of "dependencies":

- in c++ these are the includes
- in c# these are the usings
- in java these are the imports

So, it would make sense to move these into yarn. The process of
obtaining the dependencies must still be done in a kernel dependent
way because we need to build any language-specific structures that the
dependencies builder requires. However, we can create an interface for
the dependencies builder in yarn and implement it in each kernel. Each
kernel must also supply a factory for the builders.

*Tidy-up of inclusion terminology*

Random notes:

- imports and exports
- some types support both (headers)
- some support imports only (cpp)
- some support neither (cmakelists, etc).

*** Move decorations to their "final" resting place                   :story:

At present we are handling decorations in the generation model but
these are really logical concerns. The main reason why is because we
are not expanding the decoration across physical space, but instead we
expand them depending on the used technical spaces. However, since the
technical spaces are obtained from the formatters, there is an
argument to say that archetypes should have an associated technical
space. We need to decouple these concepts in order to figure out where
they belong.

*** Move technical space and generability transforms                  :story:

At present these transforms are in generation, but we don't think
that's the right place. We need some analysis to understand what they
do and why they are not in the logical model.

*** Use static registration with initialisers                         :story:

Since the start, we avoided using static registration for
initialisation due to the static initialisation order fiasco. Its much
better to manually determine the order of initialisation and do it
under programatic control rather than depend on the linker. However,
the downside is that we now have lots of code that needs to be called,
and every so often we forget to join all the dots. Perhaps we need
something in between complete "manual registration" and static
registration. Instead of supplying the registrars from the top-level,
we could instead:

- use static registration for a top-level initialiser. This is a very
  simple interface that has only one method: initialise. It uses
  regular static registration, but it merely adds itself to a
  list. Nothing else happens during static initialisation.
- when program starts, we call =initialise()= on all initialisers.
- within a given component, the top-level initialiser calls other
  initialisers. Internally, it obtains references to static registrars
  as required (e.g. features, etc). All of this happens during normal
  program execution, so we can log.
- DLLs can register initialisers on load. However, we are expected to
  load them prior to calling initialisation.
- all registrars should have a "validate" method. We should check that
  they are not empty. This method should be called prior to use. We
  should also have a "initialised" flag that stops
  double-initialisation. It should be set as the last step of
  initialisation.

Links:

- [[https://dxuuu.xyz/cpp-static-registration.html][C++ patterns: static registration]]

*** Consider bucketing elements by meta-type in generation model      :story:

At the moment we have a flat container of elements in the main
model. However, it seems like one of its use cases will be to bucket
the elements by meta-type before processing: formatters will want to
locate all formatters for a given meta-type and apply them all. At
present we are asking for the formatters for meta-name
repeatedly. This makes no sense, we should just ask for them once and
apply all formatters in one go.

For this we could simply group elements by meta-name in the model
itself and then use that container at formatting time. However, there
may be cases where looping through the whole model is more convenient
(during transforms) so this is not without its downsides.

Alternatively we could consider just bucketing in the formatters'
workflow itself.

This work will only be useful once we get rid of the formattables
model.

This can be done in the generation model, as part of the generation
clean up.

*** Dimension vs view vs perspective                                  :story:

We need to find the definition for how these terms are used within
UML and see which one is more appropriate for MASD.

*** Add support for product skeleton generation                       :story:

Now that dogen is evolving to a MDSD tool, it would be great to be
able to create a complete product skeleton from a tool. This would
entail:

- directory structure. We should document our standard product
  directory structure as part of this exercise. Initial document added
  to manual as "project_structure.org".
- licence: user can choose one.
- copyright: input by user, used in CMakeFiles, etc. added to the
  licence.
- CI support: travis, appveyor
- CMake support: top-level CMakefiles, CPack. versioning
  templates, valgrind, doxygen. For CTest we should also generate a
  "setup cron" and "setup windows scheduler" scripts. User can just
  run these from the build machine and it will start running CTest.
- vcpkg support: add "ports" code? user could point to vcpkg directory
  and a ports directory is created.
- agile with first sprint
- README with emblems.

Name for the tool: dart.

Tool should have different "template sets" so that we could have a
"standard dogen product" but users can come up with other project
structures.

Tool should add FindODB if user wants ODB support. Similar for EOS
when we support it again. We should probably have HTTP links to the
sources of these packages and download them on the fly.

Tool should also create git repo and do first commit (optional).

For extra bonus points, we should create a project in GitHub, Travis
and AppVeyor from dart.

We should also generate a RPM/Deb installation script for at least
boost, doxygen, build essentials, clang.

We should also consider a "refresh" or "force" statement, perhaps on a
file-by-file basis, which would allow one to regenerate all of these
files. This would be useful to pick-up changes in travis files, etc.

One problem with travis files is that each project has its own
dependencies. We should move these over to a shell script and call
these. The script is not generated or perhaps we just generate a
skeleton. This also highlights the issue that we have different kinds
of files:

- files that we generate and expect the user to modify;
- files that we generate but don't expect user modifications;
- files that the user generates.

We need a way to classify these.

Dart should use stitch templates to generate files.

We may need some options such as "generate boost test ctest
integration", etc.

Notes:

- [[https://github.com/elbeno/skeleton][Skeleton]]: project to generate c++ project skeletons.
- split all of the configuration of CMake dependencies from main CMake
  file. Possible name: ConfigureX? ConfigureODB, etc. See how find_X
  is implemented.
- detect all projects by looping through directories.
- fix CMake generation so that most projects are generated by Dogen.
- add option to Dogen to generate test skeleton.
- detect all input models and generate targets by looping through
  them.
- add CMake file to find knitter etc and include those files in
  package. We probably should install dogen now and have dogen rely on
  installed dogen first, with an option to switch to "built" dogen.
- generate git ignore files with common regexes. See [[https://github.com/github/gitignore][A collection of
  useful .gitignore templates]]. We could also model it as a meta-model
  object with associated options so that the user does not have to
  manually edit the file.
- generate top-level CMake, allowing user to enter dependencies and
  their versions (e.g. Boost 1.62 etc) and CMake version.
- inject dogen support automatically to CMake (on a feature switch).
- determine the list of projects by looking at the contents of the
  input models directory.
- user to enter copyright, github URL.
- we probably need to create a kernel for dart due to the
  peculiarities of the directory structure.

*Directory Themes*

It seems obvious no one in C++ will agree with a single way of
structuring projects. The best way out is to start a taxonomy of these
project layouts (directory structure themes?) and add this to the
project generator as a theme. At present there are several already
available:

- [[https://github.com/vector-of-bool/vector-of-bool.github.io/blob/master/_drafts/project-layout.md][Project Layout]]: see also discussion in [[https://old.reddit.com/r/cpp/comments/996q8o/prepare_thy_pitchforks_a_de_facto_standard/][reddit]]. Also: [[https://vector-of-bool.github.io/2018/09/16/layout-survey.html][Project
  Layout - Survey Results and Updates]]
- [[https://build2.org/][Build2]]: the packaging system seems to have a preferred directory
  layout. In particular, see [[https://build2.org/build2-toolchain/doc/build2-toolchain-intro.xhtml#proj-struct][Canonical Project Structure]].
- GNU: gnu projects seem to have a well-defined structure, if not the
  most sensible.
- [[https://www.reddit.com/r/cpp/comments/cvuywh/structuring_your_code_in_directories/][Structuring your code in directories]]
- [[https://api.csswg.org/bikeshed/?force=1&url=https://raw.githubusercontent.com/vector-of-bool/pitchfork/develop/data/spec.bs#src.layout][The Pitchfork Layout (PFL)]]
- [[https://www.boost.org/development/requirements.html#Organization][Boost: Organization]]
- [[https://hiltmon.com/blog/2013/07/03/a-simple-c-plus-plus-project-structure/][A Simple C++ Project Structure]]

*Product Model*

Actually we have been going about this all wrong. What we've called
"orchestration" is in fact the product model. It is just lacking all
other entities in the product meta-model such as:

- injection/coding models: injection/coding models are themselves
  modeling elements within the product meta-model. However, to avoid
  having to load an entire coding/injection model, a product coding
  model can contain only the key aspects of the injection/coding
  models we're interested in: a) file or path to the model b)
  references c) labels: these allow us to group models easily such as
  say "pipeline" or "injection" etc. d) references: with this we can
  make a product graph of model dependencies. We can also avoid
  rereading models. we can also figure out what packages needed by the
  model graph.
- build systems: visual studio, msbuild, cmake
- ctest
- CI: travis, appveyor.
- kubernetes support, docker support.
- valgrind
- compiler: clang, gcc, msvc, clang-cl. Version of the compiler. This
  is used in several places such as the scripts, CI, etc.
- operative system: windows, linux. used in installation scripts, CI,
  etc.
- dependencies for install scripts; these are sourced from the
  component models.
- manual: org mode, latex
- org agile: product backlog, sprints, vision, etc.

Notes:

- a product may be associated with one or more primary technical
  spaces (e.g. support for say C# and C++ in the same model). This
  would have an impact at the product level.
- a product could have some simple wale templates so that when you
  initialise a product you would get a trivial dia model with a simple
  entry point (for executables) or a library with maybe no types.
- when generating a product we can generate all models (product and
  component), generate just the product, generate a specific component
  or generate a label (which groups components).
- we need a "init" command that initialises a product. It needs a
  product name and maybe some other parameters to determine what to
  add. Maybe it just makes a product model and asks the user to fill
  it in instead.
- there are several types of component models: 1) models that do not
  generate anything at all. these are useful for defining templates,
  configurations, etc. 2) regular component models 3) product
  models. 4) platform definition models that are used to adapt
  existing libraries into MASD.
- in this sense, we have two different models: product and
  component. Both of these need to be projected into artefact space
  (because we have multiple facets in products as well). This means we
  somehow need to use archetypes from both models.
- the product model should have meta-elements describing the component
  models (perhaps =masd::component_model::target=, with a matching
  =masd::component_model::reference= in the component models).
- See aslo the story about directories in dogen: [[*Move models into the project directory][Move models into the
  project directory]].
- we could create separate chains for product and component
  model. This would imply a need for distinct model types. On the
  product model, we would locate all of the meta-elements representing
  a component model, and for each of these, run the product model
  chain. For other meta-model elements we just run their associated
  transforms - hopefully not many as these are expected to be very
  simple elements. We should also make use of injection model caching
  to avoid reloading models.
- as with component models, we should also have templates for product
  models so that we could simply do a "dogen new product" or some such
  incantation and that would result in the creation of a dogen product
  model and possibly its initial generation. One slight problem is
  that if we do a "dogen new component" we still have to manually add
  the component to the product model.
- we need to have a separate injection adapter for product models so
  that we filter out "invalid" meta-elements for the model
  type. Similarly, in the component injection adapter, we should
  filter out product model meta-elements (travis build files, etc).

Links:

- [[https://github.com/bkaradzic/GENie][GENie - Project generator tool]]
- see [[https://github.com/cginternals/cmake-init][cmake-init]] for ideas.
- [[https://github.com/premake/premake-core][Premake: powerfully simple build configuration.]]
- [[https://jgcoded.github.io/CMakeStarter/][CMake Starter]]: "This website is a simple tool to help C++ developers
  quickly start new CMake-based projects. The tool generates an entire
  C++ project with boiler-plate CMake files and source code, and the
  generated project can be downloaded as a zip file."
- [[https://awfulcode.io/2019/04/13/professional-zero-cost-setup-for-c-projects-part-1-of-n/][Professional, zero-cost setup for C++ projects (Part 1 of N)]]:

*** Formatter meta-model elements                                     :story:

A second approach is to leave this work until we have a way to code
generate meta-model elements. Then we could have a way to supply this
information as meta-data - or perhaps it is derived from the position
of the element in modeling space? The key thing is we need a static
method to determine the meta-name, and a virtual method to allow
access to it via inheritance. Perhaps we need to capture this pattern
in a more generic way. It may even already exist in the patterns
book. Then the elements would become an instance of the pattern. We
should also validate that all descendants provide a value for this
argument (e.g. an element descendant must have the meta-name set). We
could also use this for stereotypes.

The binding of the formatter against the meta-type is interesting, in
this light. The formatter has a type parameter - the type it is
formatting. In fact the formatter may have a number of type
parameters - we need to look at the stitch templates to itemise them
all - and these are then used to generate the formatter's template. We
could take this a level up and say that, at least conceptually, there
is a meta-meta-type for formatters, which is made up of a
parameterisable type. Then we could declare the formatter as an
instance of this meta-meta-type with a well-defined set of
parameters. Then, when a user instantiates a formatter, we can check
that all of the mandatory parameters have been filled in and error if
not. In this case we have something like:

- =masd::structural::parameterisable_type=. This is a meta-type that
  has a list of KVPs. Some are mandatory, some are optional.
- =masd::codegen::meta_formatter=. This defines the parameters needed
  for the formatter, with default values etc.
- =masd::codegen::formatter=. This is the actual formatters. They must
  supply values for the parameters defined by the meta-formatter.

Of course, we do not need a three-level hierarchy for this, and if
this is the only case where these parameters are used, we could just
hard-code the formatter as a meta-element and treat it like we do with
all other meta-types. Interestingly, we could bind formatters to
stereotypes rather than meta-elements. This would allow us to avoid
binding into the dogen implementation, and instead think at the MASD
level (e.g. =dogen::assets::meta_model::structural::enumeration= is a
lot less elegant than =masd::enumeration= or even
=masd::structural::enumeration=).

We could also validate that the wale template exists. In fact, if the
wale template is a meta-model element, we can check for consistency
within resolution. However, we need a generic way to associate a wale
template with any facet. The ideal setup would be for users to define
wale templates as instances of a meta-model element which is
parameterisable (see above). In reality, what we have found here is
another pattern:

- there are templates as model elements. When we create a template we
  are instantiating a template's template.
- we can then constrain the world of possibilities in to a
  well-defined set of parameters which are needed for the specific
  template that we are working on. This has a meta-model element
  associated with it, and a file.
- the file is the template file. In the case of wale, the template
  file is then instantiated. This is done by associating facets with
  the wale templates, and for each facet, supplying the arguments to
  instantiate the template. We then end up with a number of actual
  CPP/HPP files.
- for stitch the process is a bit different. The main problem is
  because we incorrectly "weaved" the arguments into the stitch
  template. It made sense at the time purely because we don't really
  expect to instantiate a given stitch template N times; it is really
  only done once. This was slightly misleading. Because of this we
  hard-coded the behaviour related to certain keys (e.g. includes,
  etc). If instead we somehow handle stitch in exactly the same way as
  we handle wale, we can keep the templates in a common template
  directory; then associate them to specific facets via meta-data, and
  supply the arguments as part of the same meta-data. The template
  would then just contain the code that would be weaved. A formatter
  is then a meta-model element associated with a wale template for the
  header file and - very interestingly - a wale template for the cpp
  file _which generates stitch templates_. The user then manually
  fills in the stitch template, but supplies any parameters (remember
  these are fixed) in the meta-model element. Generation will then
  produce the CPP
- the logical consequence of this approach is that we must reference
  the c++ generation model in order to create new formatters, because
  it will contain the templates. However, because the wale content of
  the template is located in the filesystem, it will not be possible
  to instantiate the template. We need instead to find a way to embed
  the content of the template into the model element itself. Then the
  reference would be sufficient. The downside is that, in the absence
  of org-mode injectors, these templates will be extremely difficult
  to manage (imagine having to update a dia comment with a wale
  template every time you need to change the template). On the plus
  side, we wouldn't have to have a set of files in the filesystem,
  which would make things a bit "neater".
- in fact, we have two use cases: the templates which generate
  generators (e.g. stitch) and so must be loaded into the code
  generator and the templates which are a DSL and so can be
  interpreted. Ultimately these should have a JSON object as
  input. Ultimately there should be a JSON representation of instances
  of the meta-model that can be used as input. However, what we are
  saying is that there is a ladder of flexibility and each has its own
  use cases:

  - code generated;
  - code generated with overrides;
  - DSL templates;
  - generator templates;
  - handcrafted

  Each of these has a role to play.

*** Private and public includes                                       :story:

#+begin_quote
*Story*: As a dogen user, I want to hide some internal types from
users so that I don't increase coupling for no reason.
#+end_quote

NOTE: We should use the terms =internal= and =external= to avoid
confusion with C++ scopes. This follows Microsoft terminology for C#
assemblies.

At present we are making all headers in a model public. However, for
models such as cpp this doesn't make any sense since only one type
should be available to the outside world. What we really need is a
separation between public and private headers, a functionality similar
to =internal= in C#. In conjunction with [[*Build%20shared%20objects%20instead%20of%20dynamic%20libraries][using shared objects]], this
should improve build times.

In order to do this:

- add a new config parameter: default visibility to private or default
  visibility to public. This is just so we don't have to mark all
  types manually - instead we just need to mark the exceptions.
- add two new stereotypes: =public= and =private=.
- add enum to sml: =visibility_type= (check with .Net for
  names). Valid values are =public=, =private=. Objects, enumerations,
  etc will have this enum.
- locator will now respect this value when producing an absolute file
  path. If public files go under =include/public=, if private files go
  under =include/private=.
- CMakelists for the component will add to the include path the
  private directory. Same for the spec CMakelists. Need to check that
  this not add to the global include path.
- CMakelists for the include files will only package the public
  headers.
- mark all the types accordingly in all our models. fix all the
  ensuing breakage. we will probably need to move forward on the IoC
  front in order for this to work as we don't want to expose
  implementations - e.g. =workflow_interface= will be public but
  =workflow= will be private; this means we need some kind of factory
  to generate =workflow_interface=.

More thoughts on this:

- we don't really need to have different directories for this; we
  could just put all the include files in the same directory. At
  packaging time, we should only package the public files (this would
  have to be done using CPack).
- also the GCC/MSVC visibility pragmas should take into account these
  options and only export public types.
- the slight problem with this is that we need some tests to ensure
  the packages we create are actually exporting all public types; we
  could easily have a public type that depends on a private type
  etc. We should also validate yarn to ensure this does not
  happen. This can be done by ensuring that a type marked as external
  only depends on types also marked as external and so forth.
- this could also just be a packaging artefact - we would only package
  public headers. Layout of source code would remain the same.
- when module support is available, we could use this to determine
  what is exported on the module interfaces.

*** Integration of archetypes into assets                             :story:

Up to recently, there was a belief that the archetypes model was
distinct from the assets model. The idea was that the projection of
assets into archetype space could be done without knowledge of the
things we are projecting. However, that is demonstrably false: n order
to project we need a name. That name contains a location. The location
is a point on a one-dimensional asset space.

In reality, what we always had is:

- a first dimension within assets space: "modeling dimension",
  "logical dimension"? It has an associated location.
- a second dimension within assets space: "physical dimension", with
  an associated location. Actually we cannot call it physical because
  physical is understood to mean the filesystem.

So it is that concepts such as archetype, facet and technical space
are all part of assets - they just happen to be part of the
two-dimensional projection. Generation is in effect a collection of
model to text transforms that adapts the two-dimensional element
representation into the extraction meta-model. Formatters are model to
text transforms which bind to locations in the physical dimension.

In this view of the world, we have meta-model elements to declare
archetypes, with their associated physical locations. This then
results in the injection of these meta-elements. Formatters bind to
these locations.

However, note that formatters provide dependencies. This is because
these are implementation dependent. This means we still need some
transforms to occur at the generation level. However, all of the
dependencies which are modeling related should happen within
assets. Only those which are formatter specific should happen in
generation. The problem though is that at present we deem all
dependencies to be formatter specific and each formatter explicitly
names its dependencies against which facets. It does make sense for
these to be together.

Perhaps what we are trying to say is that there are 3 distinct
concepts:

- modeling locations;
- logical locations;
- physical locations.

The first two are within the domain of assets. The last one is in the
domain of generation and extraction. Assets should make the required
data structures available, but it is the job of generation to populate
this information. Thus directory themes, locator, etc are all
generation concepts.

One could, with a hint of humour, call the "logical dimension" the
meta-physical dimension. This is because it provides the meta-concepts
for the physical dimension.

A backend provides a translation into a representation considered
valid according to the rules of a technical space. A backend can be
the primary or secondary backend for a technical space. A component
can only have a primary backend, and any number of secondary
backends. Artefacts produced by a backend must have a unique physical
location. In LAM mode, the component is split into multiple
components, each with their own primary technical space.

*** Replace traits with calls to the formatters                       :story:

Where we are using these traits classes, we should really be including
the formatter and calling for its static name - at least within each
backend.

*** Make creating new facets easier                                   :story:

For types that are stitchable such as formatters, we need to always
copy and paste the template form another formatter and then update
values. It would be great if we could have dogen generate a bare-bones
stitch template. This is pretty crazy so it requires a bit of
concentration to understand what we're doing here:

- detect that the =yarn::object= is annotated as
  =quilt.cpp.types.class_implementation.formatting_style= =stitch=.
- find the corresponding expected stitch file. If none is available,
  /dynamically/ change the =formatting_style= to =stock= and locate a
  well-known stitch formatter.
- the stitch formatter uses a stitch template that generates stitch
  templates. Since we cannot escape stitch markup, we will have to use
  the assistant. One problem we have is that the formatter does not
  state all of the required information such as what yarn types does
  it format and so forth. We probably need a meta-model concept to
  capture the idea of formatters - and this could be in yarn - and
  make sure it has all of this information. This also has the
  advantage of making traits, initialisers etc easier. We can do the
  same for helpers too.
- an additional wrinkle is that we need different templates for
  different languages. However, perhaps these are just wale templates
  in disguise rather than stitch templates? Then we can have the
  associated default wale templates, very much in the same way we have
  wale templates for the header files. They just happen to have stitch
  markup rather than say C++ code.

This is a radically different way from looking at the code. We are now
saying that yarn should have concepts for:

- facets: specialisation of modules with meta-data such as facet name
  etc. This can be done via composition to make our life easier.
- formatters and helpers: elements which belong to a facet and know of
  their archetype, wale templates, associated yarn element and so
  forth.

We then create stereotypes for these just like we did for
=enumeration=. As part of the yarn parsing we instantiate these
meta-objects with all of their required information. In addition, we
need to create what we are calling at present "profiles" to define
their enablement and to default some of its meta-data.

When time comes for code-generation, these new meta-types behave in a
more interesting way:

- if there is no stitch template, we use wale to generate it.
- once we have a stitch template, we use stitch to generate the c++
  code. From then on, we do not touch the stitch template. This
  happens because overwrite is set to false on the enablement
  "profile".

Merged stories:

*Code generate initialisers and traits*

If we could mark the modules containing facets with a stereotype
somehow - say =facet= for example, we could automatically inject two
meta-types:

- =initialzer=: for each type marked as =requires_initialisation=,
  register the formatter. Register the types as a formatter or as a
  helper.
- =traits=: for each formatter in this module (e.g. classes with the
  stereotype of =C++ Artefact Formatter= or =C# Artefact Formatter=),
  ask for their archetype. The formatters would have a meta-data
  parameter to set their archetype. In fact we probably should have a
  separate meta-data parameter (archetype source? archetype?).

We may need to solve the stereotype registration problem though, since
only C++ would know of this facet. Or we could hard-code it in yarn
for now.

Notes:

- how does the initialiser know the formatter is a =quilt.cpp=
  formatter rather than say a C# formatter? this could be done via the
  formatter's archetype - its the kernel.
- users can make use of this very same mechanism to generate their own
  formatters. We can then load up the DLL with boost plugin. Note that
  users are not constrained by the assets meta-model. That is to say,
  they can create new meta-types and inject them into assets. Whilst
  we don't support this use case at present, we should make sure the
  framework does not preclude it. Their DLL then defines the
  formatters which are able to process those meta-types. The only snag
  in all of this is the expansion machinery. We use static visitors
  all over the place, and without somehow dynamically knowing about
  the new types, they will not get expanded. We need to revisit
  expansion in this light to see if there is a way to make it more
  dynamic somehow, or at least have a "default" behaviour for all
  unknown types where we do the generic things to them such as
  computing the file path, etc. This is probably sufficient for the
  vast majority of use cases. The other wrinkle is also locator. We
  are hard-coding paths. If the users limit themselves to creating
  "regular" entities rather than say CMakeLists/msbuild like entities
  which have some special way to compute their names, then we don't
  have a problem. But there should be a generic way to obtain all path
  elements apart from the file name from locator. And also perhaps
  have facets that do not have a facet directory so that we can place
  types above the facet directories such as SLNs, CMakeLists, etc.

*** Consider adding descriptions to feature bundles                   :story:

It would be nice if we could add the feature bundle as an entry into
dumpspecs, with an associated description. For example, say for
=masd.generation.decoration=, explaining what a decoration is.

*** Create the notion of project destinations                         :story:

At present we have conflated the notion of a facet, which is a logical
concept, with the notion of the folders in which files are placed - a
physical concept. We started thinking about addressing this problem by
adding the "intra-backend segment properties", but as the name
indicates, we were not thinking about this the right way. In truth,
what we really need is to map facets (better: archetype locations) to
"destinations".

For example, we could define a few project destinations:

: masd.generation.destination.name="types_headers"
: masd.generation.destination.folder="include/masd.cpp_ref_impl.northwind/types"
: masd.generation.destination.name=top_level (global?)
: masd.generation.destination.folder=""
: masd.generation.destination.name="types_src"
: masd.generation.destination.folder="src/types"
: masd.generation.destination.name="tests"
: masd.generation.destination.folder="tests"

And so on. Then we can associate each formatter with a destination:

: masd.generation.cpp.types.class_header.destination=types_headers

Notes:

- these should be in archetypes models.
- with this we can now map any formatter to any folder, particularly
  if this is done at the element level. That is, you can easily define
  a global mapping for all formatters, and then override it
  locally. This solves the long standing problem of creating say types
  in tests and so forth. With this approach you can create anything
  anywhere.
- we need to have some tests that ensure we don't end up with multiple
  files with the same name at the same destination. This is a
  particular problem for CMake. One alternative is to allow the
  merging of CMake files, but we don't yet have a use case for
  this. The solution would be to have a "merged file flag" and then
  disable all other facets.
- this will work very nicely with profiles: we can create a few out of
  the box profiles for users such as flat project, common facets and
  so on. Users can simply apply the stereotype to their models. These
  are akin to "destination themes". However, we will also need some
  kind of "variable replacement" so we can support cases like
  =include/masd.cpp_ref_impl.northwind/types=. In fact, we also have
  the same problem when it comes to modules. A proper path is
  something like:
  - =include/${model_modules_as_dots}/types/${internal_modules_as_folders}=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_dots}.=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_underscores}_=

  This is *extremely* flexible. The user can now create a folder
  structure that depends on package names etc or choose to flatten it
  and can do so for one or all facets. This means for example that we
  could use nested folders for =include=, not use model modules for
  =src= and then flatten it all for =tests=.
- actually it is a bit of a mistake to think of these destinations as
  purely physical. In reality, we may also need them to contribute to
  namespaces. For example, in java the folders and namespaces must
  match. We could solve this by having a "module contribution" in the
  destination. These would then be used to construct the namespace for
  a given facet. Look for java story on backlog for this.
- this also addresses the issue of having multiple serialisation
  formats and choosing one, but having sensible folder names. For
  example, we could have boost serialisation mapped to a destination
  called =serialisation=. Or we could map it to say RapidJSON
  serialisation. Or we could support two methods of serialisation for
  the same project. The user chooses where to place them.

*** Model "types" and element binding                                 :story:

It seems clear that we will have different "types" of models:

- product models, describing entire products.
- component models, which at present we call "models". These describe
  a given component type such as a library or an executable. Thus,
  they themselves have sub-types.
- profile models: useful to keep the configuration separate. However,
  it may make more sense to place them in the product model, since its
  shared across components?
- PDMs: these describe platforms.

At present there is no concept of model types, so any meta-model
element can be placed in any model. This is convenient, but in the
future it may make things too complicated: users may end up placing
types in PDMs when they didn't meant to do so, etc. What seems to
emerge from here is that, just as with variability, there is a concept
of a binding point at the model level too. That is, meta-model
elements are associated with specific model types (binding element?).

In an ideal world, we should have a class in the meta-model that
represents each model type. We then instantiate this class within one
of the dogen models to register the different model types. Its
code-generation representation is the registration. It also binds to
all the meta-model elements it binds to. This can be done simply by
creating a feature that lists the stereotypes of the elements
(remember that these are then registered too, because we will generate
the meta-class information as we generate the assets model). Then, we
can ask the model type if a given element is valid (check a set of
stereotypes).

Formatters are themselves meta-model elements, and they bind to other
meta-model elements (which raises the question: which meta-model
elements are bindable? we can't allow a formatter to bind to a
formatter...). Perhaps we need another type of model, which is a
"generation model". This is where we can either declare new technical
spaces or add to existing technical spaces; and declare new facets and
formatters. We should be able to add to existing facets and TSs by
allowing users to specify the TS/facet when declaring the
formatter. If not specified, then the user must declare a facet in the
package containing the formatter. Similarly with TSs.

Note also that the formatter binding code is "inserted" directly
during generation into the CPP file. Its not possible to change
it. Same with the includes. This ensures the user cannot bypass the
model type system by mistake. Also, by having a formatter meta-model
type, we can now declare the header file as we please, and ensure the
shape of the implementation. Now, the stitch template can be
restricted to only the formatting function itself; the rest is
code-generated. We no longer need wale templates. This will of course
require the move to PDMs and the removal of the helper code. This also
means that anyone can declare new meta-model elements; they will
register themselves, and correctly expand across archetype
space. However, we do not have the adaption code nor do we have
containers for these modeling elements. We need a separate story for
this use case.

Destinations are meta-model elements too. In the generation.cpp model
we will declare all the available destinations:

- global
- src
- include
- tests

etc. The formaters bind into destinations. Formatters belong to facets
in the archetype space, which express themselves as directories in the
artefact path when we project from archetype space into artefact
space. More generally: assets in asset space are projected into the
multidimensional archetype space. Archetypes are projected into
artefact space, but the dimensions of archetype space are flattened
into the hierarchy of the filesystem.

We also need a concept of artefact types. These mainly are needed for
file extensions, but conceivably could also be used for other
purposes.

Notes:

- the binding should be done at the streotype level, not model
  element.

*** Associate includes with model elements                            :story:

The right solution for the formatter includes is to supply them as
meta-data in the model element. This has the advantage that we can
then make use of profiles. At present we have one way to supply
includes: the primary and secondary includes:

: "masd.generation.cpp.io.class_header.primary_inclusion_directive": "<boost/property_tree/json_parser.hpp>",
: "masd.generation.cpp.io.class_header.secondary_inclusion_directive": "<boost/algorithm/string.hpp>",

This does a part of the job: we can associate up to two include
directives with one facet and element. However:

- by using this machinery we are effectively replacing the original
  include.
- the includes will occur for anyone who references the type. Though
  however, since the includes are applicable only to the class
  implementation this is less of a problem. Technically its still
  incorrect though because these are not the includes needed to use
  the type but the includes needed to define the type.

For formatters, we kind of need to make the includes only happen when
we are building the formatter. If we could have a similar machinery,
but without adding to types referencing the type, this would give us a
way to declare all of the formatters dependencies. Then, we could
switch to building all of the stitch boilerplate outside of stitch and
supplying it as a KVP.

*** Move models into the project directory                            :story:

At present we have a models directory in each component of a
product. However, perhaps it makes more sense to have it as a
subdirectory of the component itself. This is because in an ideal
world, we should create a package for the component with the model and
the header files as well as the binaries, allowing users to consume
it:

- in the Dogen case, it means users can create plugins for Dogen;
- in the PDM case, it means users can make use of the PDM in their own
  models;
- for user models, it means you can consume a product in another
  product by referencing its models.

However, one downside of this approach is that we then need to have
many directories in the include path for models. If we take the
include headers as an example, there are a small number of directories
in the path:

- compiler specific directories
- =/usr/include=
- ...

Maybe we have two separate issues here:

- when creating a product, where should the models be placed? If we
  keep in mind that models are themselves an asset like any other and
  as such require a meta-model representation, it would be logical to
  keep the model with the component it generates (just like we keep
  the product model within the product it generates). This means for
  instance that we could easily initialise a component via the command
  line and create a "template" blank model (in dia or JSON) with a
  number of things already set. We probably also need a way to avoid
  deleting multiple files (e.g. if we have both a dia and a JSON
  model, we need to know to ignore both of them). This means that when
  building a product we need multiple include directories for models,
  just as we do for headers. This work should be done as part of
  adding products to the asset model because models will be in the
  same namespace. The dia and JSON directories are then the facets for
  the model. This also means that we can now add the targets for
  generation, conversion etc directly into each component. So,
  somewhat paradoxically, when we create a model, we need to have a
  model of the model in it (or maybe two models of the model, Dia and
  JSON). Interestingly, now that we have a model of the model, we can
  suddenly move all of the keys that we have placed at the top-level
  into this modeling element. We can aslo associate it with a profile
  via stereotypes, removing the need for
  =masd.variability.profile=. And if we take it to the next leve, then
  perhaps references are themselves also modeling elements. Its not
  clear if this is an advantage though.
- from a "consumption" perspective, perhaps we could have a single
  =shared/dogen/models= directory, just like we will also place all of
  the PDM's includes under =/usr/include= and the SO's under
  =/usr/lib=. We could split it into Dia and JSON if need be.
- the product model itself should be at the top-most directory of the
  git repository. We also need a "models" directory to store models
  which are not expressed as source code (profiles, PDMs, etc). Then,
  for each component, we should have the models at the root directory
  of the component. Whilst this is not in line with our OCD, it is
  required in order for the product model to be able to locate the
  component models. An alternative is to have a convention that we
  always look into a "models" directory (which can be renamed via a
  meta-data parameter) for models, plus any additional directories in
  the "model path". We must inject the model file names to dogen so
  that we do not delete the models.

*** Formatters can only belong to one facet                           :story:

Up to know there was an agreement that generation space was
hierarchical and formatters could only belong to one facet. This has
been true until now, but with the addition of CMake support to tests,
we now have an exception: we need to honour both the tests facet and
the cmake facet. If either of them are off, then we should not emit
the CMake file. This means that we need to somehow map one formatter
to multiple facets. For now we just hacked it and used one of the
facets. It means that if you disable CMake but enable testing you'll
still end up with the testing CMake file.

*** Project layout analysis                                           :story:

We should probably look at the layout of a few projects and see if our
meta-model covers these cases.

Links:

- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1204r0.html][Canonical Project Structure]]
- [[https://www.reddit.com/r/cpp/comments/8qzepa/poll_c_project_layout/][Poll: C++ project layout]]
- [[https://www.reddit.com/r/cpp/comments/996q8o/prepare_thy_pitchforks_a_de_facto_standard/][Prepare thy Pitchforks: A (de facto) Standard Project Layout]]
- [[https://github.com/vector-of-bool/pitchfork][Pitchfork is a Set of C++ Project Conventions]]
- [[https://mariuszbartosik.com/directory-structure-for-a-c-project/][Directory Structure for a C++ Project]]

** Deprecated
*** CANCELLED Consider adding =artefact_set= to extraction model      :story:
    CLOSED: [2020-03-18 Wed 08:16]

*Rationale*: with the recent merge of the physical model, this is no
longer required.

We are using collections of artefacts quite a bit, and it makes sense
to create an abstraction for it such as a =artefact_set=. However, for
this to work properly we need to add at least one basic behaviour: the
ability to merge two artefact sets. Or else we will end up having to
unpack the artefacts, then merging them, then creating a new artefact
set.

Problem is, we either create the artefact set as a non-generatable
type - not ideal - or we create it as generatable and need to add this
as a free function. We need to wait until dogen has support for
merging code generation.

*** CANCELLED Check if enable kernel directories is on extraction     :story:
    CLOSED: [2020-03-18 Wed 08:28]

*Rationale*: this story has bit-rotted.

When we moved the kernel logic into yarn from quilt, we did not rename
the traits.
*** CANCELLED Consider renaming formatter groups and model groups to sets :story:
    CLOSED: [2020-03-20 Fri 11:45]

*Rationale*: with the new physical meta-model we won't need formatter
groups.

We should try to keep the words groups and sets to their mathematical
as much as possible - modulus our limited understanding. As such,
where we are using "group" we probably mean "set" since there is no
associated operation with the set; it is merely a way of gathering
elements.

*** CANCELLED Consider adding support for formatter tags or labels    :story:
    CLOSED: [2020-03-20 Fri 11:45]

*Rationale*: any such properties must be reflected in the physical
meta-model, and should only be added when we have use cases for
them. We should avoid a generic "label" concept unless we have a
really strong use case.

At present there is a presumption that if a formatter belongs to say
=types= it cannot belong to any other facet. This means facets are
used purely for hierarchical purposes. However, in certain cases it
may make sense to "tag" or "label" formatters. For example, we may
need to know of all header or implementation files; or of all build
files, or of all files that belong to the main class, and so
forth. For this tags are more appropriate. We have started to hack
things slightly (such as =file_types=) but a generic solution for this
would be preferable.

*** CANCELLED Initialise formatters in the formatter's translation unit :story:
    CLOSED: [2020-03-20 Fri 11:46]

*Rationale*: formatters should register against facets and facets
against backends. This will be done with the current generation
refactor.

At present we are initialising the formatters in each of the facet
initialisers. However, it makes more sense to initialise them on the
translation unit for each formatter. This will also make life easier
when we move to a mustache world where there may not be a formatter
header file at all.

*** CANCELLED Allow multiple types to go into a single formatter      :story:
    CLOSED: [2020-03-20 Fri 11:47]

*Rationale*: this approach violates the current MASD
thinking. Modeling elements model the entities in a file. If we have
more than one "programming entity", a single model entity should
contain all the information required to generate it. There should
always be a 1-1 mapping.

We have found a number of cases where it may be useful to have more
than one type going into a formatter:

- [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#add-support-for-inner-classes][inner classes]];
- declaring all/some of the following in a single header: exceptions,
  enumerations, built-ins.
- typedefs ([[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#manual-typedef-generation][manual]], [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#automatic-typedef-generation][automatic]])
- [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#include-groups][include groups]] (and to be fair, [[https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#consider-renaming-includers]["master" headers]] too).
- grouping a number of forward declarations into a file.

There are probably a few more in the backlog. What all these use cases
share in common is that in some cases we want to be able to send
several types into a given formatter. This is actually not that hard
to do:

- find a way to "label" types in yarn, perhaps for a given formatter;
- transfer those labels across to CPP's formattables;
- group formattables by label;
- have a separate interface for formatters that take multiple
  formattables; one of the methods of this interface is the label;
- for each formatter, find all types with matching label and pass them
  on.

One thing to bear in mind though is that the labeling is done at the
yarn level; and for a given yarn entity, we may have a number of
formattables. Should all be passed in?

Merged stories:

*Types that share one file*

#+begin_quote
*Story*: As a dogen user, I want to generate a single file for a
number of related classes so that I don't have to deal with lots of
files when they are not needed.
#+end_quote

At present we force all types etc to have their own file. However, in
cases it may be useful to have multiple types sharing the same
file. For instance, one may want to have all enumerations in one file,
or all exceptions, etc.

We could implement this using dynamic extensions.

*** CANCELLED Special purpose formatters                               :epic:
    CLOSED: [2020-03-20 Fri 11:50]

*Rationale*: there is no such thing as a special purpose
formatter. Formatters belong to kernels, which gives them a
"theme". Any formatters which do not fit the MASD kernel should be
placed on an appropriate kernel.

In the future, when the creation of formatters is made easier, we may
start designing formatters that are totally a application specific and
may not have any particular use for any other application. They should
be accepted in mainline Dogen:

- to make sure we don't break this code;
- to allow other people to copy and paste to generate their own
  formatters;
- because sometimes what one thinks is special purpose actually much
  more general.

However, we need to make sure we don't start cluttering the code base
with these formatters. We will also have to start to worry about
things like defining stable interfaces:

- at which point do we decide that some code has bitrot and
  deprecated, so will have to be removed?
- what happens when a formatter moves from version 1 to version 2 of
  some dependent library, must we create a version 1 and version 2
  formatter or just update the existing one? what if it breaks code
  for people using version 1 that do not wish to move to version 2?
- do we mandate compilation tests for all formatters? This would mean
  our build machine would be full of third-party libraries (some
  potentially not available in Debian), and quite hard to
  maintain. Alternatively we could mandate that if you have a
  formatter you must setup a CTest agent with a compilation for that
  formatter and publish the results of the build to dashboard; if your
  build becomes consistently red we are allowed to remove the
  formatter.
- for the diff tests, is it acceptable if someone refactors the code?
  Once "your" formatter is merged in it is now owned by the community
  and it is entirely possible that someone will improve it/extend it,
  etc. In order for this to work they need to be very sure they have
  not broken the original use case.

We probably just need to setup a very simple policy to start off with,
but its best to keep track of these potential pitfalls.

Merged with this story:

*Private formatters*

We should look into code we do in dogen that is highly repetitive and
create "private formatters" for it. For example, field definitions are
more or less exclusive to dogen so it doesn't make it any sense to add
it to the "public" side of dogen; but it would be nice to create a
formatter to generate them so that we don't have to do it
manually. For these "private formatters" we would need to load a SO
with them into a dogen binary.
*** CANCELLED Protect against double-initialisation                   :story:
    CLOSED: [2020-03-20 Fri 14:27]

*Rationale*: we have a more comprehensive solution for this problem
that takes this issue into account.

We need to look into static initialisation and make sure the code can
cope with it being called several times.

At present it seems we would re-register fields, backends, etc so
multiple initialisation would fail.

In addition to this, we should also look into passing the registrars
into the initialisers. At present we are calling the static methods
directly. This is not ideal, because just like with singletons, we are
hiding the dependencies. We should really pass the registrars in the
initialise function so we can see the dependencies at the top-level.

A second related problem is the lack of initialisation. We need to
have some really meaningful exception that tells users when they
forgot to initialise the framework.

This story will eventually become irrelevant once we move to Boost.DI.

*** CANCELLED Generate a feature initialiser for all initialisers     :story:
    CLOSED: [2020-03-20 Fri 14:28]

*Rationale*: we have a more comprehensive solution for this problem
that takes this issue into account.

At present we are code-generating the features and the initialiser for
the features in each model. However, we then need to remember to call
all of the initialisers. This is done, somewhat arbitrarily, in the
context factory:

: variability::meta_model::feature_template_repository
: make_feature_template_repository() {
:     variability::helpers::feature_template_registrar rg;
:     injection::features::initializer::register_templates(rg);
:     assets::features::initializer::register_templates(rg);
:     generation::features::initializer::register_templates(rg);
:     templating::initializer::register_templates(rg);
:     variability::features::initializer::register_templates(rg);
:     archetypes::features::initializer::register_templates(rg);
:     extraction::features::initializer::register_templates(rg);
:     generation::cpp::feature_initializer::register_templates(rg);
:     generation::csharp::feature_initializer::register_templates(rg);
:     features::initializer::register_templates(rg);
:     const auto r(rg.repository());
:     return r;
: }

It would be much better if we could just extend the initialiser to
know of all dependent initialisers and call them. This way the
initialiser in engine would already call all of the initialisers. This
can probably be easily done by:

- allowing more than one initialiser in the merged model
- as part of the merge, keep track of the "dependent" models, and of
  their initialisers. We probably already do something similar for the
  registrar.
- ensure the initialisers only do something the first time they are
  called. We already have a story for this somewhere in the backlog.
