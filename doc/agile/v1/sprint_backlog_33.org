#+title: Sprint Backlog 33
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Sprint Goals

- Finnish getting PlantUML diagrams into a usable state.
- PMM refactoring

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2023-01-10 Tue 21:10]
| <75>                                   |         |       |      |       |
| Headline                               | Time    |       |      |     % |
|----------------------------------------+---------+-------+------+-------|
| *Total time*                           | *16:58* |       |      | 100.0 |
|----------------------------------------+---------+-------+------+-------|
| Stories                                | 16:58   |       |      | 100.0 |
| Active                                 |         | 16:58 |      | 100.0 |
| Edit release notes for previous sprint |         |       | 8:12 |  48.3 |
| Sprint and product backlog refinement  |         |       | 1:10 |   6.9 |
| Fullgen build is not running           |         |       | 0:43 |   4.2 |
| Update vcpkg to latest                 |         |       | 0:11 |   1.1 |
| OSX build is failing                   |         |       | 0:19 |   1.9 |
| Update emacs to latest                 |         |       | 0:54 |   5.3 |
| Nightlies failing with valgrind errors |         |       | 0:45 |   4.4 |
| Check last commit is broken            |         |       | 1:12 |   7.1 |
| Rename codec model                     |         |       | 3:32 |  20.8 |
#+end:

Agenda:

#+begin_src emacs-lisp
(org-agenda-file-to-front)
#+end_src

*** STARTED Edit release notes for previous sprint                    :story:
    :LOGBOOK:
    CLOCK: [2023-01-06 Fri 22:30]--[2023-01-06 Fri 22:50] =>  0:20
    CLOCK: [2023-01-03 Tue 22:02]--[2023-01-03 Tue 23:20] =>  1:18
    CLOCK: [2023-01-02 Mon 08:02]--[2023-01-02 Mon 08:08] =>  0:06
    CLOCK: [2023-01-01 Sun 23:40]--[2023-01-01 Sun 23:49] =>  0:09
    CLOCK: [2022-12-31 Sat 16:12]--[2022-12-31 Sat 17:26] =>  1:14
    CLOCK: [2022-12-31 Sat 14:07]--[2022-12-31 Sat 15:22] =>  1:15
    CLOCK: [2022-12-31 Sat 13:57]--[2022-12-31 Sat 14:06] =>  0:09
    CLOCK: [2022-12-31 Sat 13:39]--[2022-12-31 Sat 13:55] =>  0:16
    CLOCK: [2022-12-30 Fri 16:47]--[2022-12-30 Fri 17:49] =>  1:02
    CLOCK: [2022-12-30 Fri 12:11]--[2022-12-30 Fri 14:34] =>  2:23
    :END:

Add github release notes for previous sprint.

Release announcements:

- [[https://twitter.com/MarcoCraveiro/status/1570851700893941760][twitter]]
- [[https://www.linkedin.com/posts/marco-craveiro-phd-%F0%9F%87%A6%F0%9F%87%B4%F0%9F%87%B5%F0%9F%87%B9-31558919_release-dogen-v1031-exeunt-academia-activity-6976618358418886656-FRBE][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

#+begin_src markdown
![Baía Azul](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/Baia.Azul.Benguela.Angola.Dezembro.2022.jpeg)
_Baía Azul, Benguela, Angola. (C) 2022 Casal  Sampaio._

**DRAFT: Release notes under construction**

# Introduction

As expected, going back into full time employment has had a measurable impact on our open source throughput. If to this one adds the rather noticeable PhD hangover — there were far too many celebratory events to recount — it is perhaps easier to understand why it took nearly four months to nail down the present release. That said, it was a productive effort when measured against its goals. Our primary goal was to finish the CI/CD work commenced [the previous sprint](https://github.com/MASD-Project/dogen/releases/tag/v1.0.31). This we duly completed, though you won't be surprised to find out it was _far more involved_ than anticipated. So much so that the, ahem, _final touches_, have spilled over to the next sprint. Our secondary goal was to resume tidying up the LPS (Logical-Physical Space), but here too we soon bumped into a hurdle: Dogen's PlantUML output was not fit for purpose, so the goal quickly morphed into diagram improvement. Great strides were made in this new front but, as always, progress was hardly linear; to cut a very long story short, when we were half-way through the ask, we got lost on _yet another_ architectural rabbit hole.  A veritable Christmas Tale of a sprint it was, though we are not entirely sure on the moral of the story. Anyway, grab yourself that coffee and let's dive deep into the weeds.

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing them in more detail. Given the stories do not require that much of a demo, we discuss their implications in terms fo the Domain Architecture.

[![Sprint 1.0.32 Demo](https://img.youtube.com/vi/ei8B1Pine34/0.jpg)](https://youtu.be/ei8B1Pine34)
_Video 1_: Sprint 32 Demo.

## Remove Dia and JSON support

The major user facing story this sprint is the deprecation of two of our three codecs, Dia and JSON, and, somewhat more dramatically, the eradication of the _entire_ notion of "codec" as it stood thus far. Such a drastic turn of events demands an explanation, so please bear with. It wasn't _that_ long ago that "codecs" [took the place](https://github.com/MASD-Project/dogen/releases/tag/v1.0.27) of the better-known "injectors". Going further back in time, injectors themselves emerged from [a refactor](https://github.com/MASD-Project/dogen/releases/tag/v1.0.12) of the original "frontends", a legacy of the days when we viewed Dogen more like a traditional compiler. "Frontend" implies a unidirectional transformation, and it belongs to the compiler domain rather than MDE, so the move to injectors was undoubtedly a step in the right direction. Alas, as the release notes [tried to explain then](https://github.com/MASD-Project/dogen/releases/tag/v1.0.27) (section "Rename injection to codec"), we could not settle on this term because Dogen's injectors did not behave like "proper" MDE injectors. Now that the thesis and associated material has been published, this reasoning can be explained in more detail via the  [MDE companion notes](https://zenodo.org/record/5812017#.Y7v_3dLP2XJ) (p. 32):

> In [Béz+03], Bézivin et al. outlines their motivation [for the creation of Technical Spaces (TS)]: ”The notion of TS  allows us to deal more efficiently with the ever-increasing complexity of evolving technologies. There is no uniformly superior technology and each one has its strong and weak points.” The idea is then to engineer bridges between technical spaces, allowing the importing and exporting of artefacts across them. These bridges take the form of adaptors called ”projectors”, as Bézivin explains (emphasis ours):
>
> > _The responsibility to build projectors lies in one space_. The rationale to define them is quite simple: when one facility is available in another space and that building it in a given space is economically too costly, then the decision may be taken to build a projector in that given space. There are two kinds of projectors according to the direction: _injectors_ and _extractors_. Very often we need a couple of injector/extractor [(_sic._)] to solve a given problem. [Béz05a]

In other words, injectors are meant to be  transforms responsible from projecting elements from one TS into another. Our "injectors" behaved like real injectors sometimes (_e.g._ Dia), other times like extractors (_e.g._ PlantUML), but also like "injector-extractors" too (_e.g._ JSON, org-mode). Calling them "injectors" seemed therefore a way to mislead domain experts, which is not ideal since the objective of the Domain Architecture clean up is, _specifically_, to align terminology with MDE. "Codec" is an improvement on both of these but it's not entirely satisfying because the term is close to the Audio/Video domain , as [Wikipedia explains](https://en.wikipedia.org/wiki/Codec):

> A codec is a device or computer program that encodes or decodes a data stream or signal. Codec is a portmanteau [a blend of words in which parts of multiple words are combined into a new word] of coder/decoder.

Nevertheless, we could not think of a better of way of saying "injection and extraction" in one word, nor had anyone defined the appropriate _portemanteau_ within the MDE canon, so we went with the term until something better came along. Unfortunately, the conceptual rigmarole does not end there.

What is even more significant is that the _very notion_ of "injection from multiple sources" was set in stone in MASD theory via the _pervasive integration_ principle, the second of the methodology's six core values. I shan't bother you too much with the remaining five principles, but it is worth reading Principle 2 in full to contextualise our decision making. The [PhD thesis](https://uhra.herts.ac.uk/handle/2299/25708) (p. 61) states:

> **Principle 2**: MASD adapts to users’ tools and workflows, not the converse. Adaptation is achieved via a strategy of pervasive integration.
>
> MASD promotes tooling integration: developers preferred tools and workflows must be leveraged and integrated with rather than replaced or subverted. First and foremost, MASD’s integration efforts are directly aligned with its mission statement (_cf._ Section 5.2.2 [Mission Statement]) because integration infrastructure is understood to be a key source of SRPPs [Schematic and Repetitive Physical Patterns]. Secondly, integration efforts must be subservient to MASD’s narrow focus [Principle 1]; that is, MASD is designed with the specific purpose of being continually extended, but only across a fixed set of dimensions. For the purposes of integration, these dimensions are the projections in and out of MASD’s TS [Technical Spaces], as Figure 5.2 illustrates.

![MASD Pervasive Integration](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/masd_pervasive_integration.png)
_Figure 1_ [orginaly 5.2]: MASD Pervasive integration strategy.

> Within these boundaries, MASD’s integration strategy is one of pervasive integration. MASD encourages mappings from any tools and to any programming languages used by developers — provided there is sufficient information publicly available to create and maintain those mappings, and sufficient interest from the developer community to make use of the functionality. Significantly, the onus of integration is placed on MASD rather than on the external tools, with the objective of imposing minimal changes to the tools themselves. To demonstrate how the approach is to be put in practice, MASD’s research includes both the integration of org-mode (cf. Chapter 7), as well as a survey on the integration strategies of special purpose code generators (Craveiro, 2021d [available [here](https://zenodo.org/record/5790875#.YkoSutDMKXI)]); subsequent analysis generalised these findings so that MASD tooling can benefit from these integration strategies. Undertakings of a similar nature are expected as the tooling coverage progresses.

Whilst _in theory_ this principle sounds great, and whilst we still agree wholeheartedly with it _in spirit_, there are a few practical problems in terms of its current implementation. The first, which to be fair is already hinted at above, is that you need to have an _interested community_ maintaining the injectors into MASD's TS (Technical Space). That is because, even with decent test coverage, it's very easy to break existing workflows when adding new functionality, and the continued maintenance of the tests is costly. Secondly, many of these formats evolve over time, so one needs to keep up-to-date with tooling to remain relevant. Thirdly, as we add formats we will inevitably pickup more and more external dependencies, resulting in a bulking up of Dogen's core only to satisfy some possibly peripheral use case. Finally, each injector adds a large cognitive load because, as we do changes, we now need to revisit _all injectors_ and see how they map to each representation. Advanced mathematics is not required to see that the velocity of coding will decrease as the number of injectors increases; simple extrapolation shows a future where complexity goes through the roof and development slows down to a crawl. The obviousness of this conclusion does leave one wondering why it wasn't spotted earlier. Turns out we had looked into this but the analysis was naively hand-waved away during our PhD research by means of one key assumption: we posited the existence of a "native" format for modeling, whose scope would be a super-set of all functionality required by MASD. [XMI](https://en.wikipedia.org/wiki/XML_Metadata_Interchange) was the main contender, and we even acquired [Mastering XMI: Java Programming with the XMI Toolkit, XML and UML (OMG)](https://www.wiley.com/en-us/Mastering+XMI:+Java+Programming+with+XMI,+XML+and+UML-p-9780471384298) for this purpose.  In this light, mappings were seen as trivial-ish functions to well defined structural patterns, rather than an exploration of an open-ended space.Turns out this assumption was misplaced.

To make matters worse, the more we used org-mode in anger, the more we compared its plasticity to all other formats. Soon, a very important question emerged: what if org-mode is _the_ native format for MASD? That is to say, given our experience with the myriad of input formats (including Dia, JSON, XMI and others), what if org-mode is _the format_ which best embodies MASD's approach to [Literate Modeling](https://discovery.ucl.ac.uk/id/eprint/933/1/10.0_Literate_Modelling.pdf)? Thus far, it certainly has proven to be the format with the lowest impedance mismatch to our conceptual model. And we could already see how the future would play out by looking at some of the stories in this release: there were obvious ways in which to simplify the org-mode representation (with the objective of improving PlantUML output), but these changes lacked an obvious mapping to other codecs such as Dia and JSON. They could of course be done, but in ways that would increase complexity across the board for other codecs. If to this you add resourcing constraints, then it makes sense to refocus the mission and choose a single format as _the_ native MASD input format. Note that it does not mean we are abandoning Principle 2 altogether; one can envision separate repos for tools with mapping code that translates from a specific input format into org-mode, and these can even be loaded into the Dogen binary as shared objects via a plugin interface _a-la_ [Boost.DLL](https://www.boost.org/doc/libs/1_81_0/doc/html/boost_dll.html). In this framework, each format becomes the responsibility of a specific maintainer with its own plugin and set of tests, all of which exogenous to the set of Dogen's core responsibilities, but still falling under the broader MASD umbrella. And most important of all, they can safely be ignored until such time concrete use cases arrive.

![Dia's last stand](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/masd_dia_last_stand.png)
_Figure 2_: After being used continuously for a decade and a half, Dia stands down, for now.

Whilst the analysis required its fair share of white-boarding, the resulting action items did not and were swiftly dealt with at the sprint's end. Post implementation, we could not help but notice its benefits are even broader than originally envisioned because a lot of the complexity in the codec model was related to supporting bits of functionality for disparate codecs. In addition, we trimmed down dependencies to ```libxml``` and ```zlib```, and removed  a lot of testing infrastructure, including the deletion of the infamous frozen repo [described in Sprint 30](https://github.com/MASD-Project/dogen/releases/tag/v1.0.30). It was painful to see Dia going away, having used it for over a decade, but one cannot afford to be sentimental with code bases or else they rot and become unmaintainable. The dust has barely settled, but it already appears we are converging closer to the original vision of injection (Figure 2); next sprint we'll continue to workout the implications of this change, such as moving PlantUML output to regular code generation.

![Dogen pipeline](https://raw.githubusercontent.com/MASD-Project/dogen/master/doc/blog/images/orchestration_pipeline.png)
_Figure 2_: The Dogen pipeline, circa Sprint 12.

The observant reader will not fail to notice that, as it is for injectors, so it is for extractors. In other words, as we increase Dogen's coverage across TS — adding more and more languages, and more and more functionality in each language — we will suffer from a similar complexity explosion to what was described above for injection. However, several mitigating factors come to our rescue, or so we hope. First, whilst injectors are at the mercy of the tooling, which changes often, extractors depend on programming language specifications, idioms and libraries. These change too but not quite as often. The problem is worse for libraries, of course, as these do get released often, but not quite as bad for the programming language itself. Secondly, there is an expectation of backwards compatibility when programming languages change, meaning we can get away with being stale for longer; and for libraries, we should clearly state which versions we support. Existing versions will not bit-rot, though we may be a bit stale with regards to latest-and-greatest. I guess, as it was with injectors, time will tell how well these assumptions hold up.

## Improve note placement in PlantUML for namespaces

A minor user facing change was the improvement on how we generate PlantUML notes for namespaces. In the past these were generated as follows:

```plantuml
namespace entities #F2F2F2 {
    note top of variability
        Houses all of the meta-modeling elements related to variability.
    end note
```

The problem with this approach is that the notes end up floating above the namespace with an arrow, making it hard to read. A better approach is a floating note:

```plantuml
namespace entities #F2F2F2 {
    note variability_1
        Houses all of the meta-modeling elements related to variability.
    end note
```

The note is now declared inside the namespace. To  ensure the note has a unique name, we simply append the note count.

## Try a different layout engine in PlantUML

This change is strictly speaking not user facing in the sense that nothing has changed for users, unless they follow the same approach as Dogen. But since it has had an impact in improving our PlantUML diagrams, it is certainly worth shouting about it. As part of our efforts in making PlantUML diagrams more readable, we played a bit with different [layout engines](https://graphviz.org/docs/layouts/). In the end we settled on ELK, the [Eclipse Layout Kernel](https://plantuml.com/elk). If you are interested, we were greatly assisted in our endeavours by the PlantUML community:

- [Alternative layout engines from graphviz #1110](https://github.com/plantuml/plantuml/issues/1110)
- [Class diagrams: how to make best use of space in large diagrams #1187](https://github.com/plantuml/plantuml/issues/1187)

The change itself is fairly minor from a Dogen perspective, _e.g._ in CMake we added:

```cmake
    message(STATUS "Found PlantUML: ${PLANTUML_PROGRAM}")
    set(WITH_PLANTUML "on")
    set(PLANTUML_ENVIRONMENT PLANTUML_LIMIT_SIZE=65536 PLANTUML_SECURITY_PROFILE=UNSECURE)
    set(PLANTUML_OPTIONS -Playout=elk -tsvg)
```

The operative part being ```-Playout=elk```.  Whilst it did not solve all of our woes, it certainly made diagrams a tad neater as Figure 3 shows.

![Codec model](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/codec_plantuml_diagram.png)
_Figure 3_: Codec model in PlantUML with the ELK layout.

Note also that you need to install the ELK jar, as per instructions in the PlantUML site.

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the sprint log. As usual, for all the gory details of the work carried out this sprint, see [the sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_32.org).

## Milestones and Éphémérides

There were no particular events to celebrate.

## Significant Internal Stories

This was yet another sprint focused on internal engineering work, completing the move to the new CI environment that was started in [Sprint 31](https://github.com/MASD-Project/dogen/releases/tag/v1.0.31). This work can be split into three distinct epics: continuous builds, nightly builds and general improvements. Finally, we also spent a fair bit of time improving PlantUML diagrams.

### CI Epic 1: Continuous Builds

The main task this sprint was to get the Reference Products up to speed in terms of Continuous builds. We also spent some time ironing out messaging coming out of CI.

![Continuous reference model](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/masd_continuous_reference.png)
_Figure 4_: Continuous builds for the C++ Reference Product.

The key stories under this epic can be summarised as follows:

- **Add continuous builds to C++ reference product**: CI has been restored to the C++ reference product, via [github workflows](https://github.com/MASD-Project/cpp_ref_impl/actions).
- **Add continuous builds to C# reference product**: CI has been restored to the C# reference product, via [github workflows](https://github.com/MASD-Project/csharp_ref_impl/actions).
- **Gitter notifications for builds are not showing up**: some work was required to reinstate basic Gitter support for GitHub workflows. It the end it was worth it, especially because we can see everything from within Emacs!
- **Create a GitHub account for MASD BOT**: closely related to the previous story, it was a bit annoying to have the GitHub account writing messages to gitter as oneself because you would not see these (presumably the idea being that you send the message so you don't need to see it). Turns out its really easy to create a github account for a bot, just use your existent email address and add ```+something```, for example ```+masd-bot```. With this we now see the messages as coming from the MASD bot.

### CI Epic 2: Nightly Builds

This sprint was focused on bringing Nightly builds up-to-speed. The work was difficult due to the strange nature of our nightly builds. We basically do two types things with our nightlies:

- run valgrind on the existing CI, to check for any memory issues. In the future one can imagine adding fuzzing etc and other long running tasks that are not suitable for every commit.
- perform a "full generation" for all Dogen code, called internally "fullgen". This is a setup whereby we generate all facets across physical space, even though many of them are disabled for regular use. It serves as a way to validate that we generate good code. We also generate tests for these facets. Ideally we'd like to valgrind all of this code too.

At the start of this sprint we were in a bad state because all of the changes done to support CI in GitHub didn't work too well with our current setup. In addition, because nightlies took too long to run on Travis, we were running them on our own PC. Our first stab was to simply move nightlies into GitHub workflow. We soon found out that a naive approach would burst GitHub limits, generous as they are, because fullgen plus valgrind equal a long time running tests. Eventually we settled on the final approach of splitting fullgen from the plain nightly. This, plus the deprecation of vast  swathes of the Dogen codebase meant that we could run fullgen.

![Nightly Dogen builds](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/masd_nightly_fullgen.png)
_Figure 5_: Nightly builds for Dogen. ```fg``` stands for ```fullgen```.

In terms of detail, the following stories were implemented to get to the bottom of this epic:

- **Improve diffing output in tests**: It was not particularly clear why some tests were failing on nightlies but passing on continuous builds. We spent some time making it clearer.
- **Nightly builds are failing due to missing environment var**: A ridiculously large amount of time was spent in understanding why the locations of the reference products were not showing up in nightly builds. In the end, we ended up changing the way reference products are managed altogether, making life easier for all types of builds. See this story under "General Improvements".
- **Full generation support in tests is incorrect**: Nightly builds require "full generation"; that is to say, generating all facets across physical space. However, there were inconsistencies on how this was done because our unit tests relied on "regular generation".
- **Tests failing with filesystem errors**:  yet another fallout of the complicated way in which we used to do nightlies, with lots of copying and moving of files around. We somehow managed to end up in a complex race condition when recreating the product directories and initialising the test setup. The race condition was cleaned up and we are more careful now in how we recreate the test data directories.
- **Add nightly builds to C++ reference product**: We are finally building the C++ reference implementation once more.
- **Investigate nightly issues**: this was an hilarious problem: we were still running nightlies on our desktop PC, and after a Debian update they stopped appearing. Reason: for some reason sleep mode was set to a different default and the PC was now falling asleep after a certain time without use. However, the correct solution is to move to GitHub and not depend on local PCs so we merely deprecated local nightlies. It also saves us on electricity bills!
- **Create a nightly github workflow**: as per the previous story, all nightlies are now in GitHub! this is both for ["plain" nightlies](https://github.com/MASD-Project/dogen/actions/workflows/nightly-linux.yml) as well as ["fullgen" builds](https://github.com/MASD-Project/dogen/actions/workflows/nightly-fullgen-linux.yml), with full CDash integration.
- **Run nightlies only when there are changes**: we now only build nightlies if there was a commit in the previous 24 hours, which hopefully will keep GitHub happy.
- **Consider creating nightly branches**: with the move to GitHub actions, it made sense to create a real branch that is persisted in GitHub rather than a temporary throw away one. This is because its very painful to investigate issues: one has to recreate the "fullgen" code first, then redo the build, etc. With the new approach, the branch for the current nightly is created and pushed into GitHub, and then the nightly runs off of it. This means that, if the nightly fails, one simply has to pull the branch and build it locally. Quality of life improved dramatically.
- **Nightly builds are taking too long**: unfortunately, we burst the GitHub limits when running fullgen builds under valgrind. This was a bit  annoying because we really wanted to see if all of the generated code was introducing some memory issues, but alas it just takes too long. Anyways, as a result of this, and as alluded to in other stories, we split "plain" nightlies from "fullgen" nightlies, and used valgrind only on plain nightlies.

### CI Epic 3: General Improvements

Some of the work did not fall under Continuous or Nightly builds, so we are detailing it here:

- **Update boost to latest in vcpkg**: Dogen is now using Boost [v1.80](https://www.boost.org/users/history/version_1_80_0.html). In addition, given how trivial it is to update dependencies, we shall now perform an update at the start of every new sprint.
- **Remove deprecated uses of boost bind**: Minor tidy-up to get rid of annoying warnings that resulted from using latest Boost.
- **Remove` uses of mock configuration factory**: as part of the tidy-up around configuration, we rationalised some of the infrastructure to create configurations.
- **Cannot access binaries from release notes**: annoyingly it seems the binaries generated on each workflow are only visible to certain GitHub users. As a mitigation strategy, for now we are appending the packages directly to the release note. A more lasting solution is required, but it will be backlogged.
- **Enable CodeQL**: now that LGTM [is no more](https://github.blog/2022-08-15-the-next-step-for-lgtm-com-github-code-scanning), we started looking into its next iteration. First bits of support have been added via [GitHub actions](https://github.com/MASD-Project/dogen/actions/workflows/codeql-analysis.yml), but it seems more is required in order to visualise its output. Sadly, as this is not urgent, it will remain on the backlog.
- **Code coverage in CDash has disappeared**: as part of the CI work, we seemed to have lost code coverage. It is still not clear why this was happening, but after some other changes, the code coverage came back. Not ideal, clearly there is something stochastic somewhere on our CTest setup but, hey-ho, nothing we can do until the problem resurfaces.
- **Make reference products git sub-modules**: in the past we had a complicated set of scripts that downloaded the reference products, copied them to well-known locations and so on. It was... not ideal. As we had already mentioned in [the previous release](https://github.com/MASD-Project/dogen/releases/tag/v1.0.31), it also meant we had to expose end users to our quirky directory structure because the CMake presets are used by all. With this release we had a moment of enlightenment: what if the reference products were moved to git submodules? We've had such success with vcpkg in the previous sprint that it seemed like a no-brainer. And indeed it was. We are now not exposing any of the complexities of our own personal choices in directory structures, and due to the magic of git, the specific version of the reference product is pinned on the commit and commited into git. This is a much better approach altogether.

![Submodules](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/masd_test_data_submodules.png)
_Figure 6_: Reference products are now git sub-modules of Dogen.

###  PlantUML Epic: Improvements to diagrams of Dogen models

We were hoping to start the PMM refactor this sprint, but once we tried using the PlantUML diagrams in anger, it became clear we could not see the woods for the trees. Some smaller diagrams such as the identification model were fairly straightforward, and could be understood. However, key diagrams such as the logical model, or the text model were in an unusable state. It became clear that, before we could get on with real coding, we had to make the diagrams at least "barely usable", to borrow an expression from Ambler [Ambler, Scott W (2007). “Agile Model driven development (AMDD)". In the previous sprint we had already added a simple way to express relationships, like so:

```org-mode
 ** Taggable                                                         :element:
   :PROPERTIES:
   :custom_id: 8BBB51CE-C129-C3D4-BA7B-7F6CB7C07D64
   :masd.codec.stereotypes: masd::object_template
   :masd.codec.plantuml: Taggable <|.. comment
   :END:
```

Any expression under ```masd.codec.plantuml``` is transported verbatim to the PlantUML diagram. We decided to annotate all Dogen models with such expressions to see how that would impact diagrams in terms of readability. Of course, the right thing would be to automate such relationships but, as per [previous sprint's discussions](https://github.com/MASD-Project/dogen/releases/tag/v1.0.31), this is easier said than done: you'd move from a world of no relationships to a world of far too many relationships, making the diagram equally unusable. So hand-holding it was. This, plus the move to ELK as explained above allowed us to successfully update a large chunk of Dogen models:

- ```dogen```
- ```dogen.cli```
- ```dogen.codec```
- ```dogen.identification```
- ```dogen.logical```
- ```dogen.modeling```
- ```dogen.orchestration```
- ```dogen.org```
- ```dogen.physical```

However, we hit a limitation with ```dogen.text```. The model is just too unwieldy in its present form. Part of the problem stems from the fact that there are just no relations to add: templates are not related to anything. So, by default, PlantUML makes one long (and I do mean _long_) line. Here is a small fragment of the model:

![Text Model](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/masd_text_model.png)
_Figure 7_: Partial representation of Dogen's text model in PlantUML.

Tried as we might we could not get this model to work. Then we noticed something interesting: some parts of the model where classes are slightly smaller were being rendered in a more optimal way, as you can see in the picture above; smaller classes cluster around a circular area whereas very long classes are lined up horizontally. We took our findings to PlantUML:

- [Class diagrams: how to make best use of space in large diagrams #1187](https://github.com/plantuml/plantuml/issues/1187)

We are still investigating what can be done from a PlantUML perspective, but it seems having very long stereotypes is confusing the layout engine. Reflecting on this, it seems this is also less readable for humans too. For example:

```org-mode
**** builtin header                                                 :element:
     :PROPERTIES:
     :custom_id: ED36860B-162A-BB54-7A4B-4B157F8F7846
     :masd.wale.kvp.containing_namespace: text.transforms.hash
     :masd.codec.stereotypes: masd::physical::archetype, dogen::builtin_header_configuration
     :END:
```

Using ```stereotypes``` in this manner is a legacy from Dia, because that is what is expected of a UML diagram. However, since org-mode does not suffer from these constraints, it seemed logical to create different properties to convey different kinds of information. For instance, we could split out configurations into its own entry:

```org-mode
**** enum header                                                    :element:
     :PROPERTIES:
     :custom_id: F2245764-7133-55D4-84AB-A718C66777E0
     :masd.wale.kvp.containing_namespace: text.transforms.hash
     :masd.codec.stereotypes: masd::physical::archetype
     :masd.codec.configurations: dogen::enumeration_header_configuration
     :END:
```

And with this, the mapping into PlantUML is also simplified, since perhaps the configurations are not needed from a UML perspective. Figure 6 shows side by side both of these approaches:

![Long stereotypes](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/masd_long_stereotypes.png)
_Figure 8_: Removal of some stereotypes.

Next sprint we need to update all models with this approach and see if this improves diagram generation.

This epic was composed of a number of stories, as follows:

- **Add PlantUML relationships to diagrams**: manually adding each relationship to each model was a lengthy (and somewhat boring) operation, but improved the generated diagrams dramatically.
- **Upgrade PlantUML to latest**: it seems latest is always greatest with PlantUML, so we spent some time understanding how we can manually update it rather than depend on the slightly older version in Debian. We ended up settling on a massive hack, just drop the JAR in the same directory as the packaged version and then symlink it. Not great, but it works.
- **Change namespaces note implementation in PlantUML**: See user visible stories above.
- **Consider using a different layout engine in PlantUML**: See user visible stories above.

### Video series of Dogen coding

We have been working on a long standing series of videos on the PMM refactor. However, as you probably guessed, they have had nothing to do with refactoring with the PMM so far, because the CI/CD work has dominated all our time for several months now. To make matters more confusing, we had recorded a series of videos on CI previously ([MASD - Dogen Coding: Move to GitHub CI](https://www.youtube.com/playlist?list=PLwfrwe216gF2qlIWKsBrL7UEsrEg8dXC0), but in an extremely optimistic step, we concluded that series because we thought the work that was left was fairly trivial - famous last words hey. If that wasn't enough, our Debian PC has been upgraded to Pipewire which - whilst a possibly superior option to Pulse Audio - lacks a noise filter that we can work with.

To cut a long and somewhat depressing story short, our videos were in a big mess and we didn't quite know how to get out of it. So this sprint we decided to start from a clean slate:

- the existing series on PMM refactor was renamed to "MASD - Dogen Coding: Move to GitHub Actions". It seems best rather than append these 3 videos to the existing "MASD - Dogen Coding: Move to GitHub CI" playlist because it would probably make it even more confusing.
- we well, "completed it" as is, even though it missed all of the work in the previous sprint. This is just so we can get it out of the way.
I guess once noise-free sound is working again we could add an addendum and do a quick tour of our new CI/CD infrastructure, but given our present time constraints it is hard to tell when that will be.

Anyways, hopefully all of that makes some sense. Here are the videos we recorded so far.

[![Move to GitHub Actions](https://img.youtube.com/vi/WeUBvf_SLSU/0.jpg)](https://www.youtube.com/playlist?list=PLwfrwe216gF0_1KPp_ir7ZCYLwnhdmBGJ)
_Video 2_: Playlist for  "MASD - Dogen Coding: Move to GitHub Actions".

The next table shows the individual parts of the video series.

|Video | Description |
|---------|-----------------|
| [Part 1](https://www.youtube.com/watch?v=WeUBvf_SLSU)|In this video we start off with some boring tasks left over from the previous sprint. In particular, we need to get nightlies to go green before we can get on with real work.|
| [Part 2](https://www.youtube.com/watch?v=wTGTcSz1vgM)|This video continues the boring work of sorting out the issues with nightlies and continuous builds. We start by revising what had been done offline to address the problems with failing tests in the nightlies and then move on to remove the mock configuration builder that had been added recently.|
| [Part 3](https://www.youtube.com/watch?v=1VmGUEN-6eI)|With this video we finally address the remaining CI problems by adding GitHub Actions support for the C# Reference Product.|

_Table 1_: Video series for "MASD - Dogen Coding: Move to GitHub Actions".

## Resourcing

The resourcing picture is, shall we say, _nuanced_. On the plus side, utilisation is down significantly when compared to the previous sprint — we did take four months this time round instead of a couple of years, so that undoubtedly helped. On the less positive side, we still find ourselves well outside the expected bounds for this particular metric; given a sprint is _circa_ 80 hours, one would expect to clock that much time in a month or two of side-coding. We are hoping next sprint will compress some of the insane variability we have experienced of late with regards to the cadence of our sprints.

![Sprint 32 stories](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/sprint_32_pie_chart.jpg)
_Figure 9_: Cost of stories for sprint 32.

The per-story data forms an ever so slightly flatterer picture. Around 23% of the overall spend was allocated towards non-coding tasks such as writing the release notes (~12.5%), backlog refinement (~8%) and demo related activities. Worrying, it was up around 5% from the previous  sprint, which was itself already an extremely high number historically. Given the resource constraints, it would be wise to compress time spent on management activities such as these to free up time for _actual_ work, and buck the trend of these two or three sprints. Engineering activities where bucketed into three main topics, with CI/CD taking around 30% of the total ask (22% for Nightlies and 10% for Continuous), roughly 30% taken on PlantUML work and the remaining 15% used in miscellaneous engineering activities — including  a fair portion of analysis on the "native" format for MASD.

## Roadmap

With Sprint 32 we decided to decommission the Project Roadmap. It had served us well up to the end of the PhD thesis, as it was a useful — if albeit vague — way to see what was was coming up the road. Now that we have finished commitments with firm dead lines we can rely on a pure agile approach and see where each sprint takes us. Besides, it is one less task to worry about when writing up the release notes.

## Binaries

| Operative System | Binaries|
|--------------------------|------------|
| Linux Debian/Ubuntu | [dogen_1.0.32_amd64-applications.deb](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/dogen_1.0.32_amd64-applications.deb) |
| Windows | [DOGEN-1.0.32-Windows-AMD64.msi](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/DOGEN-1.0.32-Windows-AMD64.msi) |
| Mac OSX | [DOGEN-1.0.32-Darwin-x86_64.dmg](https://github.com/MASD-Project/dogen/releases/download/v1.0.32/DOGEN-1.0.32-Darwin-x86_64.dmg) |

_Table 2_: Binary packages for Dogen.

A few important notes:

- **Linux**: the Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this a trivial undertaking.
- **OSX and Windows**: we are not testing the OSX and Windows builds (_e.g._ validating the packages install, the binaries run, _etc._). If you find any problems with them, please report an issue.
- **64-bit**: as before, all binaries are 64-bit. For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available in [zip](https://github.com/MASD-Project/dogen/archive/v1.0.30.zip) or [tar.gz](https://github.com/MASD-Project/dogen/archive/v1.0.30.tar.gz) format.
- **Assets on release note**: these are just pictures and other items needed by the release note itself. We found that referring to links on the internet is not a particularly good idea as we now have lots of 404s for older releases. Therefore, from now on, the release notes will be self contained. Assets are otherwise not used.

# Next Sprint

Now that we are finally out of the woods of CI/CD engineering work, expectations for the next sprint are running high. We may actually be able to devote most of the resourcing towards real coding. Having said that, we still need to mop things up with the PlantUML representation, which will probably not be the most exciting of tasks.

That's all for this release. Happy Modeling!
#+end_src


*** Create a demo and presentation for previous sprint                :story:

Time spent creating the demo and presentation.

**** Presentation

***** Dogen v1.0.32, "Natal na Baía Azul"

    Marco Craveiro
    Domain Driven Development
    Released on 30th December 2022

*** STARTED Sprint and product backlog refinement                     :story:
    :LOGBOOK:
    CLOCK: [2023-01-10 Tue 20:10]--[2023-01-10 Tue 21:10] =>  1:00
    CLOCK: [2022-12-31 Sat 10:39]--[2022-12-31 Sat 10:49] =>  0:10
    :END:

Updates to sprint and product backlog.

*** COMPLETED Update github actions to build from tags                :story:

At present it seems we only build from master. We need to build from tags for
releases.

*** COMPLETED Fullgen build is not running                            :story:
    :LOGBOOK:
    CLOCK: [2022-12-31 Sat 13:12]--[2022-12-31 Sat 13:30] =>  0:18
    CLOCK: [2022-12-31 Sat 13:10]--[2022-12-31 Sat 13:11] =>  0:01
    CLOCK: [2022-12-31 Sat 12:45]--[2022-12-31 Sat 13:09] =>  0:24
    :END:

It seems we are looking for a non-existent commit:

: test -z $(git rev-list --after="24 hours" 74b36a13e1bce390582d9ae3a661c8aae7909911) && echo "::set-output name=should_run::false"
: fatal: bad object 74b36a13e1bce390582d9ae3a661c8aae7909911

The problem seems to be related to how =github.sha= works:

#+begin_quote
*github.sha*: type: string. The commit SHA that triggered the workflow. The
value of this commit SHA depends on the event that triggered the workflow. For
more information, see "Events that trigger workflows." For example,
ffac537e6cbbf934b08745a378932722df287a53.
#+end_quote

Its not obvious what the SHA is when you are triggered from a scheduler, plus we
are creating new commits and deleting commits when we recreate the branch. Best
to ask git for the latest commit instead.

Links:

- [[https://docs.github.com/en/actions/learn-github-actions/contexts#github-context][github context]]

*** COMPLETED Update vcpkg to latest                                  :story:
    :LOGBOOK:
    CLOCK: [2022-12-31 Sat 14:06]--[2022-12-31 Sat 14:07] =>  0:01
    CLOCK: [2022-12-31 Sat 13:55]--[2022-12-31 Sat 13:57] =>  0:02
    CLOCK: [2022-12-31 Sat 13:37]--[2022-12-31 Sat 13:39] =>  0:02
    CLOCK: [2022-12-31 Sat 13:30]--[2022-12-31 Sat 13:36] =>  0:06
    :END:

Boost 1.81 is now available, check to see if its in vcpkg.

*** COMPLETED OSX build is failing                                    :story:
    :LOGBOOK:
    CLOCK: [2023-01-01 Sun 20:25]--[2023-01-01 Sun 20:44] =>  0:19
    :END:

Error:

: [347/1212] Building CXX object projects/dogen.variability/src/CMakeFiles/dogen.variability.lib.dir/types/helpers/template_instantiator.cpp.o
: FAILED: projects/dogen.variability/src/CMakeFiles/dogen.variability.lib.dir/types/helpers/template_instantiator.cpp.o
: /usr/local/bin/sccache /Applications/Xcode_14.0.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -DBOOST_BIND_GLOBAL_PLACEHOLDERS -I/.../dogen/build/output/macos-clang-debug/stage/include -I/.../dogen/projects/dogen/include : -I/.../dogen/projects/dogen.identification/include -I/.../dogen/projects/dogen.physical/include -I/.../dogen/projects/dogen.cli/include -I/.../dogen/projects/dogen.utility/include -I/.../dogen/projects/dogen.variability/include -I/.../dogen/projects/dogen.org/include -I/.../dogen/projects/dogen.codec/include -I/.../dogen/projects/dogen.tracing/include -I/.../dogen/projects/dogen.logical/include -I/.../dogen/projects/dogen.orchestration/include -I/.../dogen/projects/dogen.templating/include -I/.../dogen/projects/dogen.text/include -isystem /.../dogen/build/output/macos-clang-debug/vcpkg_installed/x64-osx/include -fprofile-arcs -ftest-coverage -Wall -Wextra -Wconversion -gdwarf-4 -Wno-mismatched-tags -Qunused-arguments -pedantic -Werror -Wno-system-headers -Woverloaded-virtual -Wwrite-strings  -frtti -fvisibility=default  -g -isysroot /Applications/Xcode_14.0.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk -fPIC -std=c++17 -MD -MT projects/dogen.variability/src/CMakeFiles/dogen.variability.lib.dir/types/helpers/template_instantiator.cpp.o -MF projects/dogen.variability/src/CMakeFiles/dogen.variability.lib.dir/types/helpers/template_instantiator.cpp.o.d -o projects/dogen.variability/src/CMakeFiles/dogen.variability.lib.dir/types/helpers/template_instantiator.cpp.o -c /.../dogen/projects/dogen.variability/src/types/helpers/template_instantiator.cpp
: In file included from projects/dogen.variability/src/types/helpers/template_instantiator.cpp:21:
: In file included from build/output/macos-clang-debug/vcpkg_installed/x64-osx/include/boost/lexical_cast.hpp:30:
: In file included from build/output/macos-clang-debug/vcpkg_installed/x64-osx/include/boost/range/iterator_range_core.hpp:26:
: In file included from build/output/macos-clang-debug/vcpkg_installed/x64-osx/include/boost/iterator/iterator_traits.hpp:10:
: In file included from /Applications/Xcode_14.0.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk/usr/include/c++/v1/iterator:5: 78:
: In file included from /Applications/Xcode_14.0.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk/usr/include/c++/v1/__functional_base:26:
: In file included from /Applications/Xcode_14.0.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk/usr/include/c++/v1/utility:221:
: /Applications/Xcode_14.0.1.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk/usr/include/c++/v1/__utility/pair.h:51:9: error: implicit instantiation of undefined template 'std::vector<std::string>'

This seems to be an OSX specific problem. Add includes to these classes prior to
boost lexical and see if it fixes the problem.

*** COMPLETED Update emacs to latest                                  :story:
    :LOGBOOK:
    CLOCK: [2023-01-01 Sun 19:30]--[2023-01-01 Sun 20:24] =>  1:14
    :END:

We should try to update all packages, the emacs version etc to latest version.

*** STARTED Nightlies failing with valgrind errors                    :story:
    :LOGBOOK:
    CLOCK: [2023-01-06 Fri 22:50]--[2023-01-06 Fri 23:05] =>  0:15
    CLOCK: [2023-01-04 Wed 19:24]--[2023-01-04 Wed 19:27] =>  0:00
    CLOCK: [2022-12-31 Sat 11:00]--[2022-12-31 Sat 11:19] =>  0:19
    CLOCK: [2022-12-31 Sat 10:30]--[2022-12-31 Sat 10:38] =>  0:08
    :END:

Error:

: ### unhandled dwarf2 abbrev form code 0x25
: ### unhandled dwarf2 abbrev form code 0x25
: ### unhandled dwarf2 abbrev form code 0x25
: ### unhandled dwarf2 abbrev form code 0x23
: ==15594== Valgrind: debuginfo reader: ensure_valid failed:
: ==15594== Valgrind:   during call to ML_(img_get)
: ==15594== Valgrind:   request for range [346666073, +4) exceeds
: ==15594== Valgrind:   valid image size of 25293440 for image:
: ==15594== Valgrind:   "/home/runner/work/dogen/dogen/build/output/linux-gcc-debug/stage/bin/dogen.cli.tests"
: ==15594==
: ==15594== Valgrind: debuginfo reader: Possibly corrupted debuginfo file.
: ==15594== Valgrind: I can't recover.  Giving up.  Sorry.
: ==15594==

Seems like we are using a non-supported version of DWARF. We need to ensure
Clang and GCC emit a version valgrind supports, e.g.: =-gdwarf-4=.

Alternatively we could look into updating valgrind to latest, if it supports
DWARF 5. Actually looking at the linked ticket seems like its not yet supported.

Links:

- [[https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html][3.10 Options for Debugging Your Program - gstrict-dwarf]]
- [[https://bugzilla.mozilla.org/show_bug.cgi?id=1758782][Valgrind run fails when building with clang 14]]
- [[https://bugs.kde.org/show_bug.cgi?id=452758][Valgrind does not read properly DWARF5 as generated by Clang14]]

*** STARTED Check last commit is broken                               :story:
    :LOGBOOK:
    CLOCK: [2023-01-04 Wed 19:02]--[2023-01-04 Wed 19:23] =>  0:21
    CLOCK: [2023-01-03 Tue 23:20]--[2023-01-03 Tue 23:46] =>  0:26
    CLOCK: [2023-01-02 Mon 07:50]--[2023-01-02 Mon 08:01] =>  0:11
    CLOCK: [2023-01-01 Sun 23:25]--[2023-01-01 Sun 23:38] =>  0:13
    CLOCK: [2023-01-01 Sun 20:46]--[2023-01-01 Sun 20:47] =>  0:01
    :END:

At present we are not checking the last commit correctly.

Notes:

- remove the use of two steps, retrieve and use the latest commit in a single
  step.

Links:

- [[https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idoutputs][jobs.<job_id>.outputs]]
- [[https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#environment-files][Environment files]]

*** STARTED Rename codec model                                        :story:
    :LOGBOOK:
    CLOCK: [2023-01-09 Mon 19:20]--[2023-01-09 Mon 19:50] =>  0:30
    CLOCK: [2023-01-07 Sat 16:25]--[2023-01-07 Sat 16:50] =>  0:25
    CLOCK: [2023-01-07 Sat 15:02]--[2023-01-07 Sat 15:23] =>  0:21
    CLOCK: [2023-01-07 Sat 14:16]--[2023-01-07 Sat 14:31] =>  0:15
    CLOCK: [2023-01-07 Sat 13:12]--[2023-01-07 Sat 14:00] =>  0:48
    CLOCK: [2023-01-07 Sat 13:00]--[2023-01-07 Sat 13:12] =>  0:12
    CLOCK: [2023-01-07 Sat 12:54]--[2023-01-07 Sat 13:00] =>  0:06
    CLOCK: [2023-01-07 Sat 12:16]--[2023-01-07 Sat 12:40] =>  0:24
    CLOCK: [2023-01-07 Sat 11:44]--[2023-01-07 Sat 12:15] =>  0:31
    :END:

Now that we no longer support codecs, we should rename the model appropriately.
The model is now responsible for the following:

- loading files from the filesystem.
- converting org-mode representation into an intermediate representation.
- find references to other models and loading them.
- performing any initial transformations required in order to do the loading.

Notes:

- actually come to think about it, we now have a proper set of injectors because
  its only responsibility is to inject elements into Dogen.


 Ever since org-mode support was productionised in [Sprint 30](https://github.com/MASD-Project/dogen/releases/tag/v1.0.30), we've been reflecting and rethinking


*** Ignore vcpkg path length warning                                  :story:

#+begin_example
Building boost-system[core]:x64-windows...
CMake Warning at scripts/cmake/vcpkg_buildpath_length_warning.cmake:4 (message):
  boost-system's buildsystem uses very long paths and may fail on your
  system.

  We recommend moving vcpkg to a short path such as 'C:\src\vcpkg' or using
  the subst command.
Call Stack (most recent call first):
  ports/boost-system/portfile.cmake:3 (vcpkg_buildpath_length_warning)
  scripts/ports.cmake:147 (include)
#+end_example

Clues about path length:

#+begin_example
-- Downloading https://github.com/boostorg/system/archive/boost-1.80.0.tar.gz -> boostorg-system-boost-1.80.0.tar.gz...
-- Extracting source D:/a/dogen/dogen/vcpkg/downloads/boostorg-system-boost-1.80.0.tar.gz
#+end_example

Links:

- [[https://github.com/microsoft/vcpkg/issues/11119][[vcpkg_buildpath_length_warning] Please add advice to enable long paths on
  Windows 10 #11119]]
- [[https://github.com/microsoft/vcpkg/discussions/19141][[vcpkg_buildpath_length_warning] Please add advice to enable long paths on
  Windows 10 #19141]]
- [[https://learn.microsoft.com/en-gb/windows/win32/fileio/maximum-file-path-limitation?tabs=registry][Maximum Path Length Limitation]]
- [[https://github.com/actions/runner-images/issues/1052][MAX_PATH lengths on Windows environment #1052]]

*** Windows package is broken                                         :story:

When we install the windows package under wine, it fails with:

: E0fc:err:module:import_dll Library boost_log-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library boost_filesystem-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library boost_program_options-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library libxml2.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library boost_thread-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:LdrInitializeThunk Importing dlls for L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe" failed, status c0000135

This will probably be fixed when we move over to the new way of specifying
dependencies in CMake. Do that first and revisit this problem.

Actually, this did not help. We then used the new VCPKG macro (see links) which
now includes all of boost. We are failing on:

: 00fc:err:module:import_dll Library MSVCP140_CODECVT_IDS.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\boost_log-vc143-mt-x64-1_78.dll") not found
: 00fc:err:module:import_dll Library boost_log-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found

Notes:

- Check if we are on latest MSVC.

Links:

- [[https://github.com/microsoft/vcpkg/issues/1653][CMake: provide option to deploy DLLs on install() like VCPKG_APPLOCAL_DEPS
  #1653]]
- [[https://gitlab.kitware.com/cmake/cmake/-/issues/22623][InstallRequiredSystemLibraries MSVCP140.dll is missing]]
- [[https://stackoverflow.com/questions/4134725/installrequiredsystemlibraries-purpose][InstallRequiredSystemLibraries purpose]]
- [[https://gitlab.kitware.com/cmake/cmake/-/issues/20228][IRSL should install MSVCP140_CODECVT_IDS.dll]]: CMake versions after 3.16 should
  install this DLL.

*** Warning on OSX build                                              :story:

We seem to have a single warning on OSX:

#+begin_example
ld: warning: direct access in function

'boost::archive::basic_text_oprimitive<
    std::__1::basic_ostream<char,
                            std::__1::char_traits<char>
                            >
>
::~basic_text_oprimitive()'

from file

'vcpkg_installed/x64-osx/debug/lib/libboost_serialization.a(basic_text_oprimitive.o)'

to global weak symbol

'std::__1::basic_ostream<
    char, std::__1::char_traits<char>
>&
std::__1::endl<char, std::__1::char_traits<char> >(
    std::__1::basic_ostream<char, std::__1::char_traits<char> >&
)'

from file 'projects/dogen.utility/tests/CMakeFiles/dogen.utility.tests.dir/indenter_filter_tests.cpp.o'

means the weak symbol cannot be overridden at runtime. This was likely caused by
different translation units being compiled with different visibility settings.
#+end_example

The flags that control this behaviour are:

: cxxflags=-fvisibility=hidden
: cxxflags=-fvisibility-inlines-hidden

Compare our settings with Boost.

By removing the current settings for OSX we get over 50 warnings:

: ld: warning: direct access in function 'boost::test_tools::tt_detail::print_log_value<char [48]>::operator()(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const (&) [48])' from file 'projects/dogen.identification/tests/CMakeFiles/dogen.identification.tests.dir/legacy_logical_name_tree_parser_tests.cpp.o' to global weak symbol 'boost::test_tools::tt_detail::static_const<boost::test_tools::tt_detail::impl::boost_test_print_type_impl>::value' from file 'vcpkg_installed/x64-osx/debug/lib/libboost_unit_test_framework.a(framework.o)' means the weak symbol cannot be overridden at runtime. This was likely caused by different translation units being compiled with different visibility settings.

In addition it also causes failures in tests:

: dogen.utility.tests/resolver_tests/resolver_returns_test_data_directory_for_empty_path
: dogen.utility.tests/resolver_tests/validating_resolver_returns_test_data_directory_for_empty_paths

Notes:

- try removing special handling for boost.

#+begin_src markdown
Since every single warning on my debug builds is related to ```~basic_text_oprimitive```, I decided to investigate how this symbol is exported in boost. We start with macro ```BOOST_SYMBOL_VISIBLE``` which is defined as follows [1]:

> Defines the syntax of a C++ language extension that indicates a symbol is to be globally visible. If the compiler has no such extension, the macro is defined with no replacement text. Needed for classes that are not otherwise exported, but are used by RTTI. Examples include class for objects that will be thrown as exceptions or used in dynamic_casts, across shared library boundaries.

This appears sensible enough. We can see ```basic_text_oprimitive``` making use of it [2]:

```c++
// class basic_text_oprimitive - output of prmitives to stream
template<class OStream>
class BOOST_SYMBOL_VISIBLE basic_text_oprimitive
{
```

In GCC [3] this macro is defined as follows:

```
#define BOOST_SYMBOL_VISIBLE __attribute__((__visibility__("default")))
```

In Clang too [4]:

```
 define BOOST_SYMBOL_VISIBLE __attribute__((__visibility__("default")))
```

The general conclusion is that by setting visibility to default we should match the symbols definition. We now turn our attention to the destructor [2]:

```c++
    BOOST_ARCHIVE_OR_WARCHIVE_DECL
    basic_text_oprimitive(OStream & os, bool no_codecvt);
    BOOST_ARCHIVE_OR_WARCHIVE_DECL
    ~basic_text_oprimitive();
```

The macro ```BOOST_ARCHIVE_OR_WARCHIVE_DECL``` is defined as follows:

```c++
    #if defined(BOOST_WARCHIVE_SOURCE) || defined(BOOST_ARCHIVE_SOURCE)
        #define BOOST_ARCHIVE_OR_WARCHIVE_DECL BOOST_SYMBOL_EXPORT
    #else
        #define BOOST_ARCHIVE_OR_WARCHIVE_DECL BOOST_SYMBOL_IMPORT
    #endif
```

The macros ```BOOST_SYMBOL_EXPORT``` and ```BOOST_SYMBOL_IMPORT``` are cousins of BOOST_SYMBOL_VISIBLE. Once more, clang and GCC are identical. GCC [3]:

```c++
#    define BOOST_SYMBOL_EXPORT __attribute__((__visibility__("default")))
#    define BOOST_SYMBOL_IMPORT
```

Whereas Clang says [4]:

```c++
#  define BOOST_SYMBOL_EXPORT __attribute__((__visibility__("default")))
...
#  define BOOST_SYMBOL_IMPORT
```

This means when we are importing, visibility is not defined. We now need to find out if that is a good thing or bad.

[1] https://www.boost.org/doc/libs/master/libs/config/doc/html/boost_config/boost_macro_reference.html
[2] https://www.boost.org/doc/libs/1_80_0/boost/archive/basic_text_oprimitive.hpp
[3] https://www.boost.org/doc/libs/1_80_0/boost/config/compiler/gcc.hpp
[4] https://www.boost.org/doc/libs/1_80_0/boost/config/compiler/clang.hpp
#+end_src

Sent email to boost users.

Actually a really easy way to test this is to hack a script that overwrites this
file in OSX with the fixes and see what happens to the warnings. We can even
leave it in for now until the PR is merged.

We were patching the wrong file it seems, the problem is not with =oarchive=,
its with =oprimitive=.

Links:

- [[https://stackoverflow.com/questions/36567072/why-do-i-get-ld-warning-direct-access-in-main-to-global-weak-symbol-in-this][Why do I get "ld: warning: direct access in _main to global weak symbol" in
  this simple code? [duplicate]​]]
- [[https://stackoverflow.com/questions/8685045/xcode-with-boost-linkerid-warning-about-visibility-settings/11879361#11879361][xcode with boost : linker(Id) Warning about visibility settings]]
- [[https://github.com/Microsoft/vcpkg/issues/4497][Boost linker warnings on OSX #4497]]
- [[https://github.com/boostorg/serialization/issues/265][Strange "direct access" warning on OSX for basic_text_oprimitive #265]]

*** Use clang format to format the code base                          :story:

It seems clang-format is being used by quite a lot of people to save
time with the formatting of the code. More info:

- http://clang.llvm.org/docs/ClangFormat.html

Emacs support:

- https://github.com/llvm-mirror/clang/blob/master/tools/clang-format/clang-format.el

Links:

- [[https://github.com/marketplace/actions/clang-format-check][clang-format-check]]: GitHub Action for clang-format checks. Note that this
  Action does NOT format your code for you - it only verifies that your
  repository's code follows your project's formatting conventions. [[https://github.com/search?o=desc&q=uses%3A+jidicula%2Fclang-format-action+-user%3Ajidicula&s=indexed&type=Code][Example
  repos]].
- [[https://github.com/STEllAR-GROUP/hpx/blob/master/.clang-format][HPX clang format]]
- [[https://engineering.mongodb.com/post/succeeding-with-clangformat-part-1-pitfalls-and-planning][Succeeding With ClangFormat, Part 1: Pitfalls And Planning]]
- [[https://github.com/basiliscos/cpp-rotor/blob/master/.clang-format][example: clang format in rotor]]
- [[https://github.com/jbapple-cloudera/clang-format-infer][clang-format-infer GH]]
- [[https://zed0.co.uk/clang-format-configurator/][clang-format-configurator]]
- http://clangformat.com/
- [[https://github.com/johnmcfarlane/unformat][Unformat]]: Python3 utility to generate a .clang-format file from
  example code-base.
- [[https://www.reddit.com/r/cpp/comments/pnli5r/cc_precommit_hooks_for_static_analyzers_and/][C/C++ pre-commit hooks for static analyzers and linters]]
- [[https://github.com/lballabio/QuantLib/blob/master/.clang-format][quant lib]] clang format.
- [[https://github.com/OpenSourceRisk/Engine/blob/master/.clang-format][ORE clang format]]

*** Add PlantUML relationships to diagrams                            :story:

We need to go through each and every model and add the relations we add in Dia
to make diagrams more readable. Models:

- dogen: done
- dogen.cli: done
- dogen.codec: done
- dogen.identification: done
- dogen.logical: done
- dogen.modeling: no changes
- dogen.orchestration: done
- dogen.org: done
- dogen.physical: done
- dogen.text: started

Links:

- [[https://github.com/plantuml/plantuml/issues/1187][Class diagrams: attaining a more "square-like" use of space in large diagrams
  #1187]]
- [[https://plantuml.com/class-diagram][Section "Help on layout" in manual]]
- [[https://plantuml.com/elk][Using ELK layout engine]]
- [[https://crashedmind.github.io/PlantUMLHitchhikersGuide/layout/layout.html]["The Hitchhiker's Guide to PlantUML", section 6. "Layout"]]
- [[https://www.augmentedmind.de/2021/01/17/plantuml-layout-tutorial-styles/]["PlantUML layout and styles tutorial"]]
- [[https://isgb.otago.ac.nz/infosci/mark.george/Wiki/wiki/PlantUML%20GraphViz%20Layout]["PlantUML GraphViz Layout"]]

*** Update CMakeLists to match latest                                 :story:

We have modified locally the CMakeLists to match the modern approach, but we
never updated the templates. As part of doing this, we should remove ODB
support. This is because:

- we don't use ODB at present;
- when we do look into ODB again, it will be done as part of a cartridge
  framework rather than via build files.

Actually this is a big ask. We have a lot of missing requirements in order to do
this:

- component type: library or executable. However, if its executable, we are
  still building a library and we need to supply dependencies for both.
- missing parts: we need a part for modeling and another for generated tests.
- features: we need a templatised feature which expands across the parts. The
  feature will carry dependencies.
- the dependency needs to have the following information:
  - include: public or private
  - standard dogen model or exogenous?
  - link
- official location for generated files:
  : PRIVATE ${stage_inc_dir}/ # generated header files

One possible approach is to create a model element for references which contains
all of the required information. Example:

: * some reference                                      :reference:
:   :PROPERTIES:
:   :masd.codec.reference: dogen.tracing
:   :masd.logical.reference.type: public
:   :masd.logical.reference.link: boost::boost
:   :END:
:
: [[../../dogen.tracing/modeling/dogen.tracing.org]]

Notes:

- we do not want to handle transitive references in this way; from a dogen
  perspective we want to load these models, but from a code generation
  perspective we do not want to add references recursively. We want instead to
  rely on transitivity.

*** Configuration as stereotype causes noise                          :story:

At present we have very large classes (in terms of width) because they have
configuration associated with them as stereotypes. This is a particular problem
in the text model. Nothing stops us from having a separate way of handling
configuration - for example a different property which is not a stereotype. It
could be expressed differently in PlantUML - perhaps a separate section as per
"Advanced class body". We could name the section "Configuration" or "Profiles".

Notes:

- at present we have several different "kinds" of information in the stereotypes
  field:
  - the meta-type (e.g. enumeration, object, etc). This is probably the most in
    keeping with UML's notion of stereotypes.
  - the associated object templates used by the class.
  - the associated configurations.

  We could have two fields for each of these (e.g. templates, configurations)
  and then combine them all as stereotypes in logical model. This allows us to
  express them as different groups within PlantUML.
- we should express =masd::object= in the UML diagrams even though its the
  default. This would make diagrams clearer.
- we could create a named section for enumerators, fields, etc.
- we could express the type of an enumeration, if supplied.
- we could express the type of a primitive, if supplied.
- meta information could appear in a group called "meta-information".
- consider using =struct= or =entity= for =masd::object= and =annotation= for
  =masd::object_template=.
- if class is abstract, use =abstract=.
- check why feature model is not available on codec to codec transform and see
  how hard it is to get it.

*** Consider using meta-data within codec model                       :story:

At present we are hooking directly into the tags within the codec model in order
to access meta-data. This is because we only read in profiles etc later on in
the transform graph. In fact the problem is somewhat recursive: the fundamental
problem is that we did not expect to bootstrap a full context at the codec
level; instead we relied on a "minimal context" bootstrapped within the codec
model itself, allowing us to run the conversions without needing orchestration.
However, this has now proven to be incorrect: we need meta-data in the codec
model therefore we should bootstrap a full context before we perform conversion.
This requires a fair bit of surgery.

Notes:

- we need to move the conversion tests back to orchestration.

*** Thoughts on refactoring variability                               :story:

Originally, we introduced tagged values in Dia because we needed to add
meta-data to types which was not directly supported by the tool. We soon
extended it to all sorts of annotations. But now that we are no longer
constrained to Dia, we need to revisit this decision. Fundamentally, there are
two kinds of datum modeled as features:

- data which has a functional dependency on the geometry of physical space; and
- data which does not.

The first case involves the use of templates which expand over physical space,
and this cannot be avoided (e.g. whether a facet is =enabled= or not). The
second case however is quite trivial. In fact, org-mode does not suffer from the
same limitations as Dia; one can add all necessary properties as tags, and these
can be deserialised (manually?) into what we call the codec model at present. In
this particular case, variability is a bit of an overkill: we know precisely
what needs to be read, and where to put it. We could simply add logic around
codec object creation to read these properties in.

Having said that, we would still end up with something looking like the
features. This is because we still need code to loop through the list of KVPs,
convert them to a well-defined type, etc. So we need variability as-is; its just
that we have 2 use cases (regular/static and template-expansion/dynamic). In
reality, the key problem we have is that we do not want to pull in the physical
model into the codec model. There are two cases:

- in "the real world": this is a full blown instantiation of Dogen. We do not
  need to worry about this since the context is created in orchestration.
  Therefore we do not need to add a dependency because of this, just refactor
  how context bootstrapping works so that we can have a feature model in the
  codec context.
- for testing purposes: since in the intermediate model we do not rely on
  features that depend on template instantiation, we can just use any old set of
  template instantiation domains. This should be sufficient for the tests.

Notes:

- update the existing workflow for conversion to bootstrap a complete context.

*** Conversion as code generation                                     :story:

We may have made a modeling error when we created a distinction between
"generation" and "conversion". Generation was the full blown code generation of
a project and conversion was taking a model file in one representation and
writing it as another (/e.g./ Dia to JSON, org to JSON /etc/). That then
resulted in a "conversion" workflow inside of codec, which made things strange -
it was as if we were duplicating functionality. In reality, conversion is
generation, we just modeled it wrong. We should have an entity that represents a
PlantUML diagram and another for org-mode documents inside the LPS. When we read
in a model, we must use it to generate these entities. We then need to create a
template that generates these files.

Notes:

- now that we do not need to convert (org files are final) we can probably get
  rid of the org-to-org conversion. Having said that, it may be useful to
  regenerate the org file for other purposes. Round-tripping was considered
  important in the past, but the reasons for it have been lost in the mists of
  time. Since we do not have a use case for it yet, maybe we should just remove
  it.
- once this is done for PlantUML, the codec model is now clean from all
  codec-related responsibilities and becomes a true "intermediate
  representation".
- we no longer need the CMake targets to generate PlantUML, two steps to run
  tests, etc. which seems to imply we are going in the right direction.

** Deprecated
*** CANCELLED Create a local resolver in codec to support PlantUML use case :story:

*Rationale*: PlantUML output will eventually work in the same way as all Dogen
output, out of the LPS.

We can create most of the links in PlantUML via local resolution. We could
create a local resolver, which only looks at types for the current model. It
lives inside of codec. For those types, if it resolves, we can create the
PlantUML link. In addition, we could also resolve operations in the same way
(once they have been modeled).

Ideally, we should implement this resolve in such a way that it can be used for
merged models and stand alone models. We created a story on the new approach for
the resolver; we need to have a look at that and see if it can be implemented as
part of this work. For example, we could flatten all names prior to calling
resolver; use a GUID against each type, read from custom ID in org mode.

The resolver needs to be primed with all of the existing model names and
namespaces, without depending on the qualified names data structure. It should
have its own data structures. It could live in =identification=. The output of
the resolver should be the GUID of the type the name points to, or nothing if it
could not resolve.

In the calling models, we need a transform that decomposes a type into the names
it references. Then, for each name, we call the resolver.

Notes:

- we could also create two step resolution. We could resolve all local names
  first, and mark them as resolved (for example by adding the UUID of the
  resolved type to the name) and leaving unresolved types unmarked. Then, the
  second merged model resolution would only resolve types which are not yet
  resolved. This would probably speed things up because we may end up with
  smaller containers.

*** CANCELLED Codec classification                                    :story:

*Rationale*: codecs are being decommissioned.

It seems we have three types of codecs:

- input only, Dia
- output only: PlantUML
- roundtrip: org-mode, JSON.

We should make this explicit in the listing of

*** CANCELLED Look at warnings in lgtm                                :story:

*Rationale*: lgtm was deprecated.

We seem to have a number of code quality warnings, check them and fix the
important ones.

Links:

- [[https://lgtm.com/projects/g/MASD-Project/dogen/alerts/?mode=list][lgtm warnings]]

*** CANCELLED org-to-org transform removes custom id's from attributes :story:

*Rationale*: org-to-org transform is likely going to be removed.

Diff of =share/library/masd.org= after transform:

:
:  *** mode                                                          :attribute:
:      :PROPERTIES:
: -    :custom_id: 4e770d9b-44b4-4a6d-8504-49629f4d29c1
:      :masd.codec.value: c++
:      :END:

We should always preserve all attributes.

*** CANCELLED org-to-org transform creates mustache templates incorrectly :story:

*Rationale*: org-to-org transform is likely going to be removed.

At present we determine the language for the block by looking at the name of the
property. If its called =content= we use =mustache=. However, if a user creates
a field called =content= this should not kick in. We should have a configuration
option at the model level that enables this "intelligent" behaviour, and enable
it only for the =dogen.text= model.

: *** content                                                       :attribute:
:    :PROPERTIES:
:    :masd.codec.type: std::string
:    :END:
:
: #+begin_src mustache
: Contents of the artefact.
: #+end_src

*** Limitations of the current org-mode approach                      :story:

*Rationale*: there is no one format which supports all of the required
functionality for Dogen. org-mode is the closest.

At present we decided to implement org-mode support directly in Dogen, which has
some advantages:

- we can process org-mode documents without having Emacs - e.g. you can create
  documents in other editors and still benefit from Dogen support.

However, there are also problems:

- org-mode is very complex. We don't want to support only a partial subset of
  the format; we want users to create regular org-mode documents with all of the
  org-mode functionality and then have them exported into code. This means our
  parser has to be very clever.
- we want to be able to export the model to other formats such as HTML / PDF
  etc. This means the document may contain all sorts of weird and wonderful
  config for those formats which we do not care about.
- we want users to add sections for documentation purposes such as images, etc.
  These won't necessarily be used for Dogen.

All of this makes us think we may have taken the wrong approach. We should
instead support some well-defined intermediary format which can be understood by
Dogen unequivocally, and which has a "trivial" parsing implementation, similar
to Dia XML. Then:

- we can then create an org-mode exporter in elisp which uses the existing
  org-mode functionality to determine what should be converted and what should
  be ignored.
- with this we can be confident that the org-mode document is handled correctly.

There are downsides though:

- third-party editors will no longer be supported - they will require their own
  exporter code, and most do not support this.
- there are more steps to the process, which is particularly annoying when you
  have several models. You now need to ensure you have exported them all, else
  you may get strange errors.
- the format must support mapping to the original locations so that we can still
  support LSP with proper errors, etc.

*** Org roundtrip does not add blank line at the end of document      :story:

*Rationale*: org-to-org transform is likely going to be removed.

Errors:

#+begin_src
FAILED: projects/dogen.orchestration/tests/CMakeFiles/run_dogen.orchestration.tests
cd /work/DomainDrivenConsulting/masd/dogen/integration/build/output/clang11/Release/stage/bin && /work/DomainDrivenConsulting/masd/dogen/integration/build/output/clang11/Release/stage/bin/dogen.orchestration.tests --log_level=error
Running 156 test cases...
Conversion generated differences: "/work/DomainDrivenConsulting/masd/dogen/integration/projects/dogen/modeling/dogen.org"
@@ -494,3 +494,4 @@
   :END:

 An error ocurred when dumping dogen's specs.
+

../../../../projects/dogen.orchestration/tests/dogen_org_product_tests.cpp(188): error: in "dogen_product_org_tests/dogen_org_conversion_has_no_diffs": check diff.empty() has failed
Conversion generated differences: "/work/DomainDrivenConsulting/masd/dogen/integration/projects/dogen.cli/modeling/dogen.cli.org"
@@ -181,3 +181,4 @@
    :END:

 Which style to use when dumping the specs.
+

../../../../projects/dogen.orchestration/tests/dogen_org_product_tests.cpp(202): error: in "dogen_product_org_tests/dogen_cli_org_conversion_has_no_diffs": check diff.empty() has failed
Conversion generated differences: "/work/DomainDrivenConsulting/masd/dogen/integration/projects/dogen.logical/modeling/dogen.logical.org"
@@ -4668,3 +4668,4 @@
    :END:

 An error has occurred while formatting.
+

../../../../projects/dogen.orchestration/tests/dogen_org_product_tests.cpp(244): error: in "dogen_product_org_tests/dogen_logical_org_conversion_has_no_diffs": check diff.empty() has failed

3 failures are detected in the test module "dogen.orchestration.tests"
ninja: build stopped: subcommand failed.
Error running CMake.

Compilation exited abnormally with code 1 at Sun Apr 11 20:15:01
Elapsed: 00:00:13.738
#+end_src

*** Partial information extraction from logical model                 :story:

*Rationale*: the codec model is going to become a simple injector.

Since we just need generalisation and association information, we could probably
create a specialised chain in the logical model that gives us the bare minimum
to populate the relevant properties; we can then extract those and map them to
the codec ID. We should also make the codec ID mandatory and use a map for the
codec elements. The orchestration model is responsible for stitching the
information.
