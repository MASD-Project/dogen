#+title: Sprint Backlog 32
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) spike(p) }

* Sprint Goals

- Finish restoring support for CI/CD.
- Get PlantUML diagrams into a usable state.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2022-11-29 Tue 09:16]
| <75>                                                      |           |         |       |       |
| Headline                                                  | Time      |         |       |     % |
|-----------------------------------------------------------+-----------+---------+-------+-------|
| *Total time*                                              | *3d 3:20* |         |       | 100.0 |
|-----------------------------------------------------------+-----------+---------+-------+-------|
| Stories                                                   | 3d 3:20   |         |       | 100.0 |
| Active                                                    |           | 3d 3:20 |       | 100.0 |
| Edit release notes for previous sprint                    |           |         | 10:13 |  13.6 |
| Create a demo and presentation for previous sprint        |           |         |  2:06 |   2.8 |
| Update boost to latest in vcpg                            |           |         |  0:20 |   0.4 |
| Improve diffing output in tests                           |           |         |  0:19 |   0.4 |
| Sprint and product backlog grooming                       |           |         |  5:16 |   7.0 |
| Nightly builds are failing due to missing environment var |           |         |  1:07 |   1.5 |
| Remove deprecated uses of boost bind                      |           |         |  0:09 |   0.2 |
| Full generation support in tests is incorrect             |           |         |  3:18 |   4.4 |
| Tests failing with filesystem errors                      |           |         |  2:10 |   2.9 |
| Remove uses of mock configuration factory                 |           |         |  0:54 |   1.2 |
| Add continuous builds to C++ reference product            |           |         |  2:13 |   2.9 |
| Add continuous builds to C# reference product             |           |         |  1:50 |   2.4 |
| Remove ODB support from Dogen                             |           |         |  0:45 |   1.0 |
| Model significant relations at a higher level             |           |         |  1:05 |   1.4 |
| Create a mock configuration builder                       |           |         |  1:58 |   2.6 |
| Enable CodeQL                                             |           |         |  0:21 |   0.5 |
| Change namespaces note implementation in PlantUML         |           |         |  1:48 |   2.4 |
| Consider using a different layout engine in PlantUML      |           |         |  1:31 |   2.0 |
| Upgrade PlantUML to latest                                |           |         |  0:44 |   1.0 |
| Code coverage in CDash has disappeared                    |           |         |  0:23 |   0.5 |
| Investigate nightly issues                                |           |         |  0:15 |   0.3 |
| Gitter notifications for builds are not showing up        |           |         |  2:07 |   2.8 |
| Create a nightly github workflow                          |           |         |  5:39 |   7.5 |
| Nightly builds are taking too long                        |           |         |  4:21 |   5.8 |
| Ignore vcpkg path length warning                          |           |         |  0:12 |   0.3 |
| Windows package is broken                                 |           |         |  0:20 |   0.4 |
| Warning on OSX build                                      |           |         |  2:23 |   3.2 |
| Use clang format to format the code base                  |           |         |  0:58 |   1.3 |
| Add PlantUML relationships to diagrams                    |           |         | 17:19 |  23.0 |
| Configuration as stereotype causes noise                  |           |         |  2:09 |   2.9 |
| Update CMakeLists to match latest                         |           |         |  0:45 |   1.0 |
| Thoughts on refactoring variability                       |           |         |  0:22 |   0.5 |
#+end:

Agenda:

#+begin_src emacs-lisp
(org-agenda-file-to-front)
#+end_src

*** COMPLETED Edit release notes for previous sprint                  :story:
    :LOGBOOK:
    CLOCK: [2022-09-23 Fri 08:53]--[2022-09-23 Fri 09:02] =>  0:09
    CLOCK: [2022-09-23 Fri 07:58]--[2022-09-23 Fri 08:18] =>  0:20
    CLOCK: [2022-09-16 Fri 09:35]--[2022-09-16 Fri 11:06] =>  1:31
    CLOCK: [2022-09-14 Wed 18:00]--[2022-09-14 Wed 18:41] =>  0:41
    CLOCK: [2022-09-13 Tue 17:47]--[2022-09-13 Tue 18:20] =>  0:33
    CLOCK: [2022-09-13 Tue 08:18]--[2022-09-13 Tue 09:02] =>  0:44
    CLOCK: [2022-09-12 Mon 22:00]--[2022-09-12 Mon 22:41] =>  0:41
    CLOCK: [2022-09-11 Sun 22:25]--[2022-09-11 Sun 23:35] =>  1:10
    CLOCK: [2022-09-11 Sun 11:58]--[2022-09-11 Sun 12:33] =>  0:35
    CLOCK: [2022-09-10 Sat 22:44]--[2022-09-10 Sat 23:40] =>  0:56
    CLOCK: [2022-09-10 Sat 20:21]--[2022-09-10 Sat 20:35] =>  0:14
    CLOCK: [2022-09-10 Sat 19:02]--[2022-09-10 Sat 20:20] =>  1:18
    CLOCK: [2022-09-10 Sat 15:49]--[2022-09-10 Sat 17:10] =>  1:21
    :END:

Add github release notes for previous sprint.

Release announcements:

- [[https://twitter.com/MarcoCraveiro/status/1570851700893941760][twitter]]
- [[https://www.linkedin.com/posts/marco-craveiro-phd-%F0%9F%87%A6%F0%9F%87%B4%F0%9F%87%B5%F0%9F%87%B9-31558919_release-dogen-v1031-exeunt-academia-activity-6976618358418886656-FRBE][linkedin]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

#+begin_src markdown
![Graduation](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/phd_graduation.jpg)
_Graduation day for the PhD programme of Computer Science at the University of Hertfordshire, UK. (C) 2022 Shahinara Craveiro._

# Introduction

After an hiatus of almost ten months, we've finally managed to get another Dogen release out. When looked at purely from a software engineering perspective, this wasn't exactly the most compelling of releases since almost all our stories are infrastructural. More specifically, the majority of resourcing was shifted towards getting Continuous Integration (CI) to work again, in the wake of the carnage left by Travis CI's decommission. However, the _true_ focus of the last few months lays outside the bounds of software engineering; our time was spent mainly on completing the PhD thesis, getting it past a myriad of red-tape processes and, perhaps most significantly of all, on passing the final exam called _the viva_. And so we did. Given it has taken some eight years to complete the PhD programme, you'll forgive us for the break with the tradition in naming releases after Angolan places or events; regular service will resume on the next release, for this as well as in the engineering front ```<knocks on wood, nervously>```. So grab a cupper, sit back, relax, and get ready for the release notes that mark the end of academic life in the Dogen project.

# User visible changes

This section covers stories that affect end users, with the video providing a quick demonstration of the new features, and the sections below describing them in more detail. However, as we've only had a couple of those - and even then, as these are fairly minor - the demo spends some time reflecting on the PhD programme overall.

[![Sprint 1.0.31 Demo](https://img.youtube.com/vi/ei8B1Pine34/0.jpg)](https://youtu.be/ei8B1Pine34)
_Video 1_: Sprint 31 Demo.

## Deprecate support for dumping tracing to a relational database

It wasn't _that_ long ago Dogen was extended to dump tracing information into relational databases such as [PostgreSQL](https://www.postgresql.org/) and their ilk. In fact, [v1.0.20](https://github.com/MASD-Project/dogen/releases/tag/v1.0.20)'s release notes announced this new feature with great fanfare, and we genuinely had high hopes for its future. You are of course forgiven if you fail to recall what the fuss was all about, so it is perhaps worthwhile doing a quick recap. Tracing - or _probing_ as it was known then - was introduced in the long forgotten days of [Dogen v1.0.05](https://github.com/MASD-Project/dogen/releases/tag/v1.0.05), the idea being that it would be useful to inspect model state as the transform graph went through its motions. Together with log files, this treasure trove of information enabled us to understand where things went wrong quickly, more often than not without necessitating a debugger. And it was indeed incredibly useful to begin with, but we soon got bored of manually inspecting trace files. You see, the trouble with these crazy critters is that they are rather plump blobs of JSON, thus making it difficult to understand "before" and "after" diffs for the state of a given model transform - even when allowing for [json-diff](https://github.com/andreyvit/json-diff) and the like. To address the problem we doubled-down on our usage of [JQ](https://stedolan.github.io/jq/), but the more we did so, the clearer it became that JQ queries competed in the readability space with computer science classics like regular expressions and perl. A few choice data points should give a flavour of our troubles:

```bash
# JQ query to obtain file paths:
$ jq .models[0].physical.regions_by_logical_id[0][1].data.artefacts_by_archetype[][1].data.data.file_path
# JQ query to sort models by elements:
$ jq '.elements|=sort_by(.name.qualified)'
# JQ query for element names in generated model:
$ jq ."elements"[]."data"."__parent_0__"."name"."qualified"."dot"
```

It is of course deeply unfair to blame JQ for all our problems, since "meaningful" names such as ```__parent_0__``` fall squarely within Dogen's sphere of influence. Moreover, as a tool JQ is extremely useful for what it is _meant_ to do, as well as being incredibly fast at it. Nonetheless, we begun to accumulate more and more of these query fragments, glued them up with complex UNIX shell pipelines that dumped information from trace files into text files, and then dumped diffs of said information to other text files which where then... - well, you get the drift. These scripts were extremely brittle and mostly "one-off" solutions, but at least the direction of travel was obvious: what was needed was a way to build up a number of queries targeting the "before" and "after" state of any given transform, such that we could ask a series of canned questions like "has object X gone missing in transform T0?" or "did we update field Y incorrectly in transform T1?",  and so on. One can easily conceive that a large library of these queries would accumulate over time, allowing us to see at a glance what changed between transforms and, in so doing, make routine investigations several orders of magnitude faster. Thus far, thus logical. We then investigated PostgreSQL's JSON support and, at first blush, found it to be [very comprehensive](https://www.postgresql.org/docs/current/functions-json.html). Furthermore, given that Dogen always had basic support for [ODB](https://www.codesynthesis.com/products/odb/), it was "easy enough" to teach it to dump trace information into a relational database - which we did in the [aforementioned release](https://github.com/MASD-Project/dogen/releases/tag/v1.0.20).

Alas, after the initial enthusiasm, we soon realised that expressing our desired questions as database queries was _far_ more difficult than anticipated. Part of it is related to the complex graph that we have on our JSON documents, which could be helped by creating a more relational-database-friendly model; and part of it is the inexperience with PostgreSQL's JSON query extensions. Sadly, we do not have sufficient time address either question properly, given the required engineering effort. To make matters worse, even though it was not being used in anger, the maintenance of this code was become increasingly expensive due to two factors:

- its reliance on a beta version of ODB ([v2.5](https://www.codesynthesis.com/pipermail/odb-users/2021-October/004696.html)), for which there are no DEBs readily available; instead, one is expected to build it from source using [Build2](https://build2.org/), an extremely interesting but rather _suis generis_ build tool; and
- its reliance on either manual install of the ODB C++ libraries or a patched version of [vcpkg](https://vcpkg.io/en/getting-started.html) with support for v2.5. As vcpkg undergoes constant change, this means that every time we update it, we then need to spend ages porting our code to the new world.

Now, one of the rules we've had for the longest time in Dogen is that, if something is not adding value (or worse, _subtracting_ value) then it should be deprecated and removed until such time it can be proven to add value. As with any spare time project, time is extremely scarce, so we barely have enough of it to be confused with the real issues at hand - let alone speculative features that may provide a pay-off one day. So it was that, with great sadness, we removed all support for the relational backend on this release. Not all is lost though. We use [MongoDB](https://www.mongodb.com/) a fair bit at work, and got the hang of its query language. A much simpler alternative is to dump the JSON documents into MongoDB - a shell script would do, at least initially - and then write Mongo queries to process the data. This is an approach we shall explore next time we get stuck investigating an issue using trace dumps.

## Add "verbatim" PlantUML extension

Since we moved away from [Dia](https://wiki.gnome.org/Apps/Dia), the quality of our diagrams degraded considerably. This is to be expected; when we originally added PlantUML support in the [previous release](https://github.com/MASD-Project/dogen/releases/tag/v1.0.30), it was as much a feasibility study as it was the implementation of a new feature. So the understanding was that we'd spend a number of sprints adding improvements to this new codec, until it got to the point where the diagrams where of comparable quality to the Dia ones. However, this sprint it dawned on us just how much machinery would be required to properly model relations in the rich way we had in Dia. Worse: it is not necessarily possible to merely record relations between entities in the input codec and then map those to a UML diagram, the reason being that, in Dia, we cleverly choose which relations are of significance and ignore those we deemed to be less interesting when conveying meaning on a diagram. To make matters more concrete, imagine a [vocabulary type](https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p2125r0.pdf) such as ```entities::name``` in model ```dogen::identification```. It is used throughout the whole of Dogen, and any entity with a representation in the LPS (Logical-Physical Space) will use it. A blind approach of modeling each and every relation to a core type such as this would result in a mess of inter-crossing lines, removing any meaning from the resulting diagram.

After a great deal of pondering, we decided that the PlantUML output needs two kinds of data sources: _automated_, where the relationship is obvious and uncontroversial, such as say the attributes that make up a class; and _manual_, where the relationship requires hand-holding by a human. This is useful for example in the above case, where one would like to suppress the relationships with a basic vocabulary type. This feature was implemented by means of adding a  PlantUML  _verbatim_  attribute to models. It is called "verbatim" because we merely add **exactly** what you put in there into the final PlantUML output. By convention, these statements are placed straight after the entity they were added to. It is perhaps easier to understand this feature by means of an example. Say in the ```dogen.codec``` model one wishes to add a relationship between ```model``` and ```element```. One could go about it as follows:

![Dogen.Codec model](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/add_plantuml_relationships_via_verbatim.png)
_Figure 1_: Use of the verbatim PlantUML property in the ```dogen.codec``` model.

As you can see, the property ```masd.codec.plantuml``` is extremely simple: it merely allows one to enter valid PlantUML statements, which are subsequently transported into the generated source code without modification, _e.g._:

![PlantUML generated source](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/plantuml_source_with_verbatim_attribute.png)
_Figure 2_: PlantUML source code for ```dogen.codec``` model.

For good measure, we can observe the final (graphical) output produced by PlantUML, with the two relations. Its worth highlighting a couple of things here. The first is that we added a relationship with the object template ```Element```. Now, it is not entirely clear this is the correct way in UML to model relationships with object templates - the last expert I consulted was not entirely pleased with this approach - but no matter. The salient point is not whether this specific representation is correct or incorrect, but that one can choose to use this or any other representation quite easily, as desired. Secondly and similarly, the aggregation between ```model_set```, ```model``` and ```element``` is something that one would like to highlight in this model, and it is possible to do so trivially by means of this feature. Each of these classes is composed of a number of attributes which are not  particularly interesting from a relationship perspective, and adding relations for all of those would greatly increase the amount of noise in the diagram.

![PlantUML output](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/graphical_representation_of_plantuml_model.png)
_Figure 3_: Graphical output produced by PlantUML from Dogen-generated sources.

This feature is a great example of how often one needs to think of a problem from many different perspectives before arriving at a solution; and that, even though the problem may appear extremely complex at the start, sometimes all it takes is to view it from a completely different angle. All and all, the feature was implemented in just over two hours; we had originally envisioned lots of invasive changes at the lowers levels of Dogen just to propagate this information, and likely an entire sprint dedicated to it. To be fair, the jury is not out yet on whether this is really the correct approach. Firstly, because we now need to go through each and every model and compare the relations we had in Dia to those we see in PlantUML, and implement them if required. Secondly, we have no way of knowing if the PlantUML input is correct or not, short of writing a parser for their syntax - which we won't consider. This means the user will only find out about syntax errors after running PlantUML - and given it will be within generated code, it is entirely likely the error messages will be less than obvious as to what is causing the problem. Finally and somewhat related:  the _verbatim_ nature of this attribute entails bypassing the Dogen type system entirely, by design. This means that  if this information is useful for purposes other than PlantUML generation - say for example for regular source code generation - we would have no access to it.

A possibly better way of modeling this property is to add a non-verbatim attribute such as "significant relationship" or "user important relationship" or some such. Whatever its name, said attribute would model the notion of there being an important relationship between some types within the Dogen type system, and it could then be used by the PlantUML codec to output it in its syntax. However, before we get too carried away, its important to remember that we always take the simplest possible approach first and wait until use cases arrive, so all of this analysis has been farmed off to the backlog for some future use.

## Video series on MDE and MASD

In general, we tend to place our YouTube videos under the Development Matters section of the release notes because these tend to be about coding within the narrow confines of Dogen. As with so many items within this release, an exception was made for one of the series because it is likely to be of interest to Dogen developers and users alike. The series in question is called "MASD: An introduction to Model Assisted Software Development", and it is composed of 10 parts as of this writing. Its main objective was to prepare us for the _viva_, so the long arc of the series builds up to why one would want to create a new methodology and ends with an explanation of what that methodology might be. However, as we were unsure as to whether we could use material directly from [the thesis](https://uhra.herts.ac.uk/handle/2299/25708), and given our shortness of time to create new material specifically for the series, we opted for a high-level description of the methodology which is somewhat unsatisfactory due to a lack of visuals. We are therefore considering an additional 11th part which reviews a couple of key chapters from the thesis, namely Chapters 5 and 6.

At any rate, the individual videos are listed on Table 1, with a short description. They are also available as a playlist, as per link below.

![MASD: An introduction to Model Assisted Software Development](https://img.youtube.com/vi/yRFjSegsC_s/0.jpg)
_Video 2_: Playlist "MASD: An introduction to Model Assisted Software Development".

|Video | Description |
|---------|-----------------|
| [Part 1](https://www.youtube.com/watch?v=yRFjSegsC_s) | This lecture is the start of an overview of Model Driven Engineering (MDE), the approach that underlies MASD.|
| [Part 2](https://www.youtube.com/watch?v=Q-5Ic_gOd0Y)|In this lecture we conclude our overview of MDE by discussing Platforms and Technical Spaces, and we start to look at the field in more detail, critiquing its foundations.|
| [Part 3](https://www.youtube.com/watch?v=P20uEmc0wtc)|In this lecture we discuss the two fundamental concepts of MDE: Models and Transformations.|
| [Part 4](https://www.youtube.com/watch?v=_x5Wnab8Ipk)|In this lecture we take a large detour to think about the philosophical implications of modeling. In the detour we discuss Russell, Whitehead, Wittgenstein and Meyers amongst others.|
| [Part 5](https://www.youtube.com/watch?v=w1ZH4v8UiJU)|In this lecture we finish our excursion into the philosophy of modeling and discuss two core topics: Technical Spaces (TS) and Platforms.|
|[Part 6](https://www.youtube.com/watch?v=TcCNNpH4EfM)|In this video we take a detour and talk about research, and how our programme in particular was carried out - including all the bumps and bruises we faced along the way.|
|[Part 7](https://www.youtube.com/watch?v=r33MbmOv2ag)|In this lecture we discuss Variability and Variability Management in the context of Model Driven Engineering (MDE).|
|[Part 8](https://www.youtube.com/watch?v=AAvopzFQm9Q)|In this lecture we start a presentation of the material of the thesis itself, covering state of the art in code generation, and the requirements for a new approach.|
|[Part 9](https://www.youtube.com/watch?v=EFPMWq5SNGQ)|In this lecture we outline the MASD methodology: its philosophy, processes, actors and modeling language. We also discuss the domain architecture in more detail.|
|[Part 10](https://www.youtube.com/watch?v=EFPMWq5SNGQ)|In this final lecture we discuss Dogen, introducing its architecture.|

_Table 1_: Video series for "MASD: An introduction to Model Assisted Software Development".

# Development Matters

In this section we cover topics that are mainly of interest if you follow Dogen development, such as details on internal stories that consumed significant resources, important events, etc. As usual, for all the gory details of the work carried out this sprint, see the sprint log. As usual, for all the gory details of the work carried out this sprint, see [the sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_31.org).

## Milestones and Éphémérides

This sprint marks the end of the PhD programme that started in 2014.

![PhD Thesis](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/phd_thesis_in_uh_archive.png)
_Figure 3_: PhD thesis within the University of Hertfordshire archives.

## Significant Internal Stories

From an engineering perspective, this sprint had one goal which was to restore our CI environment. Other smaller stories were also carried out.

### Move CI to GitHub actions

A great number of stories this sprint (listed below) were connected with returning to a sane world of continuous integration, which we had lost with the demise of the open source support for [Travis CI](https://www.travis-ci.org). First and foremost, I'd like to give a huge shout out to Travis CI for all the years of supporting open source projects, even when perhaps it did not make huge financial sense. Prior to this decision, we had relied on Travis CI quite a lot, and in general it just worked. To my knowledge, they were the first ones to introduce the simple YAML markup for their IaC language, and it still supports features that we could not map to in our new approach (_e.g._  the infamous issue [#399](https://github.com/actions/toolkit/issues/399)). So it was not without sadness that we lost Travis CI support and found ourselves needing to move on to a new, hopefully stable, home. As we have support for [GitHub](https://github.com/MASD-Project/dogen), [BitBucket](https://bitbucket.org/MASD-Project/dogen/src/master/) and [GitLab](https://gitlab.com/DomainDrivenConsulting/dogen) as Git clones, we considered these three providers. In the end, we settled on GitHub actions, mainly because of the wealth of example projects using C++. All things considered, the move was remarkably easy, though not without its challenges. At present we seem to have all Dogen builds across Linux, Windows and OSX working reliably - though, as always, much work still remains such as porting all of our reference products.

![GitHub Actions](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/github_actions_for_dogen.png)
_Figure 4_: GitHub actions for the Dogen project.

,**Related Stories**: "Move build to GitHub", "Can't see build info in github builds", "Update the test package scripts for the GitHub CI", "Remove deprecated travis and appveyor config files", "Create clang build using libc++", "Add packaging step to github actions", "Setup MSVC Windows build for debug and release", "Update build instructions in readme", "Update the test package scripts for the GitHub CI", "Comment out clang-cl windows build", "Setup the laptop for development", "Windows package is broken", "Rewrite CTest script to use github actions".

### Improvements to vcpkg setup

As part of the move to GitHub actions, we decided to greatly simplify our builds. In the past we had relied on a hack: we built all our third party dependencies and placed them, as a zip, on DropBox. This worked, but it meant that updating these dependencies was a major pain. In particular, we often forgot how exactly those builds had been done and where we had sourced all of the libraries. As part of the research on GitHub actions, it became apparent that all the cool kids had moved on to using [vcpkg](https://vcpkg.io/en/getting-started.html) within the CI itself, with a set of supporting actions that made this use case much easier than before. This is highly advantageous because it means that updating the third party dependencies means merely having to update a git submodule. We took this opportunity and simplified all of our dependencies, which meant that sadly we had to remove our support for ODB since v2.5 is not available on vcpkg (see above). Nonetheless, the new setup is an improvement of several orders of magnitude, especially because in the past we had to have our own OSX and Windows Physicals/VM's to build the dependencies whereas now we rely only on vcpkg.

,**Related Stories**:  "Update vcpkg to latest", "Remove third-party dependencies outside vcpkg",  "Update nightly builds to use new vcpkg setup".

### Improvements to CTest and CMake scripts

Closely related to the work on vcpkg and GitHub actions was a number of fundamental changes to our CMake and CTest setup. First and foremost, we like to point out the move to use CMake Presets. This is a great little feature in CMake that enables one to pack all of the CMake configuration into a preset file, removing the need for the old ```build.*``` scripts that had littered our build directory. It also means that building from Emacs - as well as other editors and IDEs which support presets, of course - is now really easy. In the past we had to supply a number o environment variables and other swuch incantations to the build script in order to setup the required environment. With presets all of that is encapsulated into a self comntained ```CMakePresets.json``` file, making the build much simpler:


```
cmake --preset linux-clang-release
cmake --build --preset linux-clang-release
```

You can also list the available presets very easily:

```
$ cmake --list-presets
Available configure presets:

  "linux-clang-debug"             - Linux clang debug
  "linux-clang-release"           - Linux clang release
  "linux-gcc-debug"               - Linux gcc debug
  "linux-gcc-release"             - Linux gcc release
  "windows-msvc-debug"            - Windows x64 Debug
  "windows-msvc-release"          - Windows x64 Release
  "windows-msvc-clang-cl-debug"   - Windows x64 Debug
  "windows-msvc-clang-cl-release" - Windows x64 Release
  "macos-clang-debug"             - Mac OSX Debug
  "macos-clang-release"           - Mac OSX Release
```

This ensures a high degree of regularity of Dogen builds if you wish to stick to the defaults, which is the case for almost all our use cases. The exception had been nightlies, but as we explain elsewhere, with this release we also managed to make those builds conform to the same overall approach.

The release also saw a general clean up of the CTest script, now called ```CTest.cmake```, which supports both continuous as well as nighly builds with minimal complexity. Sadly, the integration of presets with CTest is not exactly perfect, so it took us a fair amount of time to work out how to best get these two to talk to each other.

,**Related Stories**: "Rewrite CTest script to use github actions", "Assorted improvements to CMake files"

### Smaller stories

In addition to the big ticket items, a number of smaller stories was also worked om.

- **Fix broken org-mode tests**: due to the _ad-hoc_ nature of our org-mode parser, we keep finding weird and wonderful problems with code generation, mainly related to the introduction of spurious whitelines. This sprint we fixed yet another group of these issues. Going forward, the right solution is to remove org-mode support from within Dogen, since we can't find a third party library that is rock solid, and add instead an XMI-based codec. We can then extend Emacs to generate this XMI output. There are downsides to this approach - for example, the loss of support to non-Emacs based editors such as VI and VS Code.
- **Generate doxygen docs and add to site**: Every so often we update manually the Doxygen docs available [on our site](https://mcraveiro.github.io/dogen/doxygen/index.html). This time we also added a badge linking back to the documentation. Once the main bulk of work is finished with GitHub actions, we need to consider adding an action to regenerate documentation.
- **Update build instructions in README***: This sprint saw a raft of updates to our [REAMDE file](https://github.com/MASD-Project/dogen/blob/master/README.md), mostly connected with the end of the tesis as well as all the build changes related to GitHub actions.
- **Replace Dia IDs with UUIDs**: Now that we have removed Dia models from within Dogen, it seemed appropriate to get rid of some of its vestiges such as Object IDs based on Dia object names. This is yet another small step towards making the org-mode models closer to their native representation. We also begun work on supporting proper capitalisation of org-mode headings ("Capitalise titles in models correctly"), but sadly this proved to be much more complex than expected and has since been returned to the product backlog for further analysis.
- **Tests should take full generation into account**: Since time immemorial, our nightly builds have been, welll, _different_, from regular CI builds. This is because we make use of a feature called "full generation". Full generation forces the instantiation of model elements across all facets of physical space regardless of the requested configuration within the user model. This is done so that we exercise generated code to the fullest, and also has the great benefit of valgrinding the generated tests, hopefully pointing out any leaks we may have missed. One major down side of this approach was the need to somehow "fake" the contents of the Dogen directory, to esnure the generated tests did not break. We did this via the "pristine" hack: we kept two checkouts of Dogen, and pointed the tests of the main build towards this printine directory, so that the code geneation tests did not fail. It was ugly but just about worked. That is, until we introduced CMake Presets. Then, it caused all sorts of very annoying issues. In this sprint, after the longest time of trying to extend the hack, we finally saw the obvious: the easiest way to address this issue is to extend the tests to also use full generation. This was very easy to implement and made the nightlies regular with respect to the continuous builds.

### Video series of Dogen coding

This sprint we recorded a series of videos titled "MASD - Dogen Coding: Move to GitHub CI". It is somewhat more generic than the name implies, because it includes a lot of the side-tasks needed to make GitHub actions work such as removing third party dependencies, fixing CTest scripts, _etc._ The video series is available as a playlist, in the link below.

[![Move to GitHub CI](https://img.youtube.com/vi/l13FwDpvcA8/0.jpg)](https://youtu.be/ei8B1Pine34)
_Video 3_: Playlist for  "MASD - Dogen Coding: Move to GitHub CI".

The next table shows the individual parts of the video series.

|Video | Description |
|---------|-----------------|
| [Part 1](https://www.youtube.com/watch?v=l13FwDpvcA8)|In this part we start by getting all unit tests to pass.|
| [Part 2](https://www.youtube.com/watch?v=v7ebzs6XIf4)|In this video we update our vcpkg fork with the required libraries, including ODB. However, we bump into problems getting Dogen to build with the new version of ODB.|
| [Part 3](https://www.youtube.com/watch?v=JOQPzueENB0)|In this video we decide to remove the relational model altogether as a way to simplify the building of Dogen. It is a bittersweet decision as it took us a long time to code the relational model, but in truth it never lived up to its original promise.|
| [Part 4](https://www.youtube.com/watch?v=zu-YeZ6akcM)|In this short video we remove all uses of Boost DI. Originally, we saw Boost DI as a solution for our dependency injection needs, which are mainly rooted in the registration of M2T (Model to Text) transforms.|
| [Part 5](https://www.youtube.com/watch?v=OdDDQlV72BA)|In this video we update vcpkg to use latest and greatest and start to make use of the new standard machinery for CMake and vcpkg integration such as CMake presets. However, ninja misbehaves at the end.|
| [Part 6](https://www.youtube.com/watch?v=aY_OLBtkEHY)|In this part we get the core of the workflow to work, and iron out a lot of the kinks across all platforms.|
| [Part 7](https://www.youtube.com/watch?v=gtV9frKFZTw)|In this video we review the work done so far, and continue adding support for nightly builds using the new CMake infrastructure.|
| [Part 8](https://www.youtube.com/watch?v=Pf-nD5UpLT8)|This video concludes the series. In it, we sort out the few remaining problems with nightly builds, by making them behave more like the regular CI builds.|

_Table 2_: Video series for "MASD - Dogen Coding: Move to GitHub CI".

## Resourcing

At over ten months duration, this sprint was characterised mainly by its irregularity, rendering metrics such as utilisation rate rather meaningless. It would of course be an unfair comment if we stopped at that - given how much was achieved on the PhD front -  but alas these are not resourcing concerns, given its sole focus on engineering effort. Looking at the sprint as a whole, it must be classified was very productive, weighing in at just over 85 hours and haing largely achieved our sprint goals. It is of course very disappointing to spend this much effort just to get back to where we were in terms of CI/CD in the Travis CI golden days, but it is what it is, and if anything our new setup is certainly a step up in terms of functionality when compared to the Travis/AppVeyor approach.

The most expensive story, by far, was the rewrite of the CTest scripts, at almost 16% of total effort, and it was closely followed by our series of lectures on MDE and MASD (11%). We also spent an uncharacteristic large amount of time refining our sprint and product backlogs: 10% versus the 7% of sprint 30 and the 3.5% of sprint 29. Of course, in the context of ten months with very little coding, it does make sense that we spent a lot of time having ideas about coding. All told, just under 60% of the sprint's total resourcing was directly related to its missing

![Sprint 31 stories](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/sprint_31_pie_chart.jpg)
_Figure 5_: Cost of stories for sprint 31.

## Roadmap

![Project plan](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/sprint_31_project_plan.png)


![Resource allocation](https://github.com/MASD-Project/dogen/releases/download/v1.0.31/sprint_31_resource_allocation_graph.png)

# Binaries

You can download binaries from either [Bintray](https://bintray.com/masd-project/main/dogen/1.0.30) or [GitHub](https://github.com/MASD-Project/dogen/releases/tag/v1.0.30), as per Table 3. All binaries are 64-bit. For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available in [zip](https://github.com/MASD-Project/dogen/archive/v1.0.30.zip) or [tar.gz](https://github.com/MASD-Project/dogen/archive/v1.0.30.tar.gz) format.

| Operative System | Debug | Release |
|--------------------------|------------|-----------|
| Linux Debian/Ubuntu (Deb) | [linux-clang-debug](https://github.com/MASD-Project/dogen/suites/8228081571/artifacts/359021758) | [linux-clang-release](https://github.com/MASD-Project/dogen/suites/8228081571/artifacts/359021759) |
| Linux Debian/Ubuntu (Deb) | [linux-gcc-debug](https://github.com/MASD-Project/dogen/suites/8228081571/artifacts/359021760) | [linux-gcc-release](https://github.com/MASD-Project/dogen/suites/8228081571/artifacts/359021761) |
| Windows (MSI) | [windows-msvc-debug](https://github.com/MASD-Project/dogen/suites/8228081572/artifacts/359031416) | [windows-msvc-release](https://github.com/MASD-Project/dogen/suites/8228081572/artifacts/359031417) |
| Mac OSX (DMG) | [macos-clang-debug](https://github.com/MASD-Project/dogen/suites/8228081569/artifacts/359027762) | [macos-clang-release](https://github.com/MASD-Project/dogen/suites/8228081569/artifacts/359027763) |

_Table 3_: Binary packages for Dogen.

,**Note 1:** The Linux binaries are not stripped at present and so are larger than they should be. We have [an outstanding story](https://github.com/MASD-Project/dogen/blob/master/doc/agile/product_backlog.org#linux-and-osx-binaries-are-not-stripped) to address this issue, but sadly CMake does not make this a trivial undertaking.

,**Note 2:** Due to issues with Travis CI, we did not manage to get OSX to build, so and we could not produce a final build for this sprint. The situation with Travis CI is rather uncertain at present so we may remove support for OSX builds altogether next sprint.

# Next Sprint

That's all for this release. Happy Modeling!
#+end_src

*** COMPLETED Create a demo and presentation for previous sprint      :story:
    :LOGBOOK:
    CLOCK: [2022-09-16 Fri 19:15]--[2022-09-16 Fri 20:13] =>  0:58
    CLOCK: [2022-09-16 Fri 13:55]--[2022-09-16 Fri 14:53] =>  0:58
    CLOCK: [2022-09-14 Wed 18:42]--[2022-09-14 Wed 18:52] =>  0:10
    :END:

Time spent creating the demo and presentation.

**** Presentation

***** Dogen v1.0.31, "Exeunt Academia"

    Marco Craveiro
    Domain Driven Development
    Released on 4th September 2022

*** COMPLETED Update boost to latest in vcpg                          :story:
    :LOGBOOK:
    CLOCK: [2022-09-17 Sat 13:00]--[2022-09-17 Sat 13:20] =>  0:20
    :END:

Boost 1.80 is now available.

*** COMPLETED Improve diffing output in tests                         :story:
    :LOGBOOK:
    CLOCK: [2022-09-19 Mon 08:04]--[2022-09-19 Mon 08:23] =>  0:19
    :END:

When a test fails with differences, we get the following output:

#+begin_example
Differences found. Outputting head of first 5 diffs.
diff -u include/dogen.identification/io/entities/name_io.hpp include/dogen.identification/io/entities/name_io.hpp
Reason: Changed generated file.
---  include/dogen.identification/io/entities/name_io.hpp
+++  include/dogen.identification/io/entities/name_io.hpp
@@ -1,11 +1,5 @@
 /* -*- mode: c++; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*-
  *
- * These files are code-generated via overrides to test dogen. Do not commit them.
- *
- * Generation timestamp: 2022-09-19T00:04:25
- * WARNING: do not edit this file manually.
- * Generated by MASD Dogen v1.0.32
- *
  * Copyright (C) 2012-2015 Marco Craveiro <marco.craveiro@gmail.com>
  *
  * This program is free software; you can redistribute it and/or modify
#+end_example

There are problems with this:

- it appears as if the generated files are missing these lines. However, when we
  look at the filesystem, they are absent from the original files. So it may be
  the generated files are generating this and shouldn't. We should always check
  from the perspective of the files in the filesystem.
- the =---= and =+++= should say what they mean.
- actually upon investigation, the test files did contain the output:

#+begin_example
 * These files are code-generated via overrides to test dogen. Do not commit them.
 *
 * Generation timestamp: 2022-09-19T00:04:25
 * WARNING: do not edit this file manually.
 * Generated by MASD Dogen v1.0.32
 *
 * Copyright (C) 2012-2015 Marco Craveiro <marco.craveiro@gmail.com>
#+end_example

Something went wrong with full generation. The problem appears to be that full
generation overrides the decoration settings.

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2022-11-19 Sat 15:06]--[2022-11-19 Sat 15:11] =>  0:05
    CLOCK: [2022-11-19 Sat 14:31]--[2022-11-19 Sat 14:59] =>  0:28
    CLOCK: [2022-11-19 Sat 14:06]--[2022-11-19 Sat 14:07] =>  0:01
    CLOCK: [2022-11-19 Sat 13:57]--[2022-11-19 Sat 14:06] =>  0:09
    CLOCK: [2022-11-13 Sun 08:27]--[2022-11-13 Sun 08:30] =>  0:03
    CLOCK: [2022-11-12 Sat 22:53]--[2022-11-12 Sat 23:06] =>  0:13
    CLOCK: [2022-11-12 Sat 22:46]--[2022-11-12 Sat 22:52] =>  0:06
    CLOCK: [2022-11-12 Sat 22:28]--[2022-11-12 Sat 22:44] =>  0:16
    CLOCK: [2022-11-12 Sat 21:54]--[2022-11-12 Sat 22:01] =>  0:29
    CLOCK: [2022-10-09 Sun 11:35]--[2022-10-09 Sun 11:47] =>  0:12
    CLOCK: [2022-10-04 Tue 08:21]--[2022-10-04 Tue 08:29] =>  0:08
    CLOCK: [2022-10-03 Mon 19:41]--[2022-10-03 Mon 19:51] =>  0:10
    CLOCK: [2022-10-03 Mon 18:30]--[2022-10-03 Mon 18:47] =>  0:17
    CLOCK: [2022-10-01 Sat 16:48]--[2022-10-01 Sat 16:54] =>  0:06
    CLOCK: [2022-09-30 Fri 11:10]--[2022-09-30 Fri 11:15] =>  0:05
    CLOCK: [2022-09-27 Tue 08:28]--[2022-09-27 Tue 08:40] =>  0:12
    CLOCK: [2022-09-27 Tue 07:39]--[2022-09-27 Tue 07:48] =>  0:09
    CLOCK: [2022-09-26 Mon 08:25]--[2022-09-26 Mon 08:45] =>  0:20
    CLOCK: [2022-09-25 Sun 19:38]--[2022-09-25 Sun 19:59] =>  0:21
    CLOCK: [2022-09-25 Sun 17:31]--[2022-09-25 Sun 17:35] =>  0:04
    CLOCK: [2022-09-23 Fri 14:54]--[2022-09-23 Fri 14:59] =>  0:05
    CLOCK: [2022-09-23 Fri 14:42]--[2022-09-23 Fri 14:53] =>  0:11
    CLOCK: [2022-09-23 Fri 10:29]--[2022-09-23 Fri 10:33] =>  0:04
    CLOCK: [2022-09-22 Thu 08:47]--[2022-09-22 Thu 08:56] =>  0:09
    CLOCK: [2022-09-20 Tue 08:15]--[2022-09-20 Tue 08:21] =>  0:06
    CLOCK: [2022-09-19 Mon 11:42]--[2022-09-19 Mon 11:50] =>  0:08
    CLOCK: [2022-09-19 Mon 08:29]--[2022-09-19 Mon 08:29] =>  0:00
    CLOCK: [2022-09-19 Mon 08:24]--[2022-09-19 Mon 08:28] =>  0:04
    CLOCK: [2022-09-18 Sun 07:37]--[2022-09-18 Sun 07:39] =>  0:02
    CLOCK: [2022-09-17 Sat 21:24]--[2022-09-17 Sat 21:27] =>  0:03
    CLOCK: [2022-09-14 Wed 18:53]--[2022-09-14 Wed 19:02] =>  0:09
    CLOCK: [2022-09-06 Tue 12:07]--[2022-09-06 Tue 12:24] =>  0:17
    CLOCK: [2022-09-06 Tue 11:40]--[2022-09-06 Tue 12:06] =>  0:26
    :END:

Updates to sprint and product backlog.

*** COMPLETED Nightly builds are failing due to missing environment var :story:
    :LOGBOOK:
    CLOCK: [2022-09-19 Mon 07:35]--[2022-09-19 Mon 08:02] =>  0:27
    CLOCK: [2022-09-18 Sun 18:21]--[2022-09-18 Sun 18:40] =>  0:19
    CLOCK: [2022-09-18 Sun 07:29]--[2022-09-18 Sun 07:34] =>  0:05
    CLOCK: [2022-09-17 Sat 20:55]--[2022-09-17 Sat 21:11] =>  0:16
    :END:

We have a few tests failing with the following error:

#+begin_example
/home/marco/nightly/dogen/master/projects/dogen.utility/src/types/environment/variable_reader.cpp(96): Throw in function strict_read_environment_variable
Dynamic exception type: boost::wrapexcept<dogen::utility::environment::environment_exception>
std::exception::what: Environment variable is empty or not defined: DOGEN_PROJECTS_DIRECTORY
unknown location(0): fatal error: in "Test setup": std::runtime_error: Error during test
/home/marco/nightly/dogen/master/projects/dogen.codec/tests/main.cpp(35): last checkpoint: initializer
Running 1 test case...

 *** No errors detected
Test setup error:
#+end_example

We do not seem to be using presets in the nightly for some reason.

Notes:

- this is due to a bug on the CTest script which is resetting the CMake
  arguments for nightlies.
- it appears we are not using parallel builds during nightly, we are taking over
  8h for a single build. This has now been fixed.
- one of the tests is now timing out:

  : dogen.logical.generated_tests/entities_input_model_set_tests/xml_roundtrip_produces_the_same_entity         Failed  10m 10ms        Completed (Timeout)

  We need to find out how to increase the timeout.
- clang builds have the wrong DWARF2 format:

  : unhandled dwarf2 abbrev form code 0x25

Links:

- [[https://cmake.org/cmake/help/latest/command/ctest_build.html][ctest_build]]
- [[https://bugzilla.mozilla.org/show_bug.cgi?id=1758782][FireFox: Valgrind run fails when building with clang 14]]
- [[https://bugs.kde.org/show_bug.cgi?id=452758][kde: Valgrind does not read properly DWARF5 as generated by Clang14]]
- [[https://stackoverflow.com/questions/45009595/how-to-overwrite-ctest-default-timeout-1500-in-cmakelists-txt][SO: How to overwrite Ctest default timeout 1500 in CMakeLists.txt]]

Merged stories:

*Nightly builds are failing due to missing variable*

At present we are getting the following error:

: /home/marco/nightly/dogen/master/projects/dogen.utility/src/types/environment/variable_reader.cpp(96): Throw in function static std::string dogen::utility::environment::variable_reader::strict_read_environment_variable(const std::string&)
: Dynamic exception type: boost::wrapexcept<dogen::utility::environment::environment_exception>
: std::exception::what: Environment variable is empty or not defined: DOGEN_PROJECTS_DIRECTORY
: unknown location(0): fatal error: in "Test setup": std::runtime_error: Error during test
: /home/marco/nightly/dogen/master/projects/dogen.codec/tests/main.cpp(35): last checkpoint: initializer
: Running 1 test case...

*Fix errors in nightly builds*

*** COMPLETED Remove deprecated uses of boost bind                    :story:
    :LOGBOOK:
    CLOCK: [2022-09-17 Sat 21:14]--[2022-09-17 Sat 21:23] =>  0:09
    :END:

#+begin_example
[56/2312] Building CXX object projects/dogen/generated_tests/CMakeFiles/dogen.generated_tests.dir/spec_category_tests.cpp.o
In file included from /usr/include/boost/smart_ptr/detail/sp_thread_sleep.hpp:22,
                 from /usr/include/boost/smart_ptr/detail/yield_k.hpp:23,
                 from /usr/include/boost/smart_ptr/detail/spinlock_gcc_atomic.hpp:14,
                 from /usr/include/boost/smart_ptr/detail/spinlock.hpp:42,
                 from /usr/include/boost/smart_ptr/detail/spinlock_pool.hpp:25,
                 from /usr/include/boost/smart_ptr/shared_ptr.hpp:29,
                 from /usr/include/boost/shared_ptr.hpp:17,
                 from /usr/include/boost/test/tools/assertion_result.hpp:21,
                 from /usr/include/boost/test/tools/old/impl.hpp:20,
                 from /usr/include/boost/test/test_tools.hpp:46,
                 from /usr/include/boost/test/unit_test.hpp:18,
                 from /home/marco/nightly/dogen/master/projects/dogen/generated_tests/spec_category_tests.cpp:29:
/usr/include/boost/bind.hpp:36:1: note: ‘#pragma message: The practice of declaring the Bind placeholders (_1, _2, ...) in the global namespace is deprecated. Please use <boost/bind/bind.hpp> + using namespace boost::placeholders, or define BOOST_BIND_GLOBAL_PLACEHOLDERS
#+end_example

Links:

- [[https://stackoverflow.com/questions/63084695/note-when-building-cgal-code-the-practice-of-declaring-the-bind-placeholders][SO: Note when building CGAL code: The practice of declaring the Bind
  placeholders (_1, _2, ...) in the global namespace is deprecated]]

*** COMPLETED Full generation support in tests is incorrect           :story:
    :LOGBOOK:
    CLOCK: [2022-09-25 Sun 08:57]--[2022-09-25 Sun 09:12] =>  0:15
    CLOCK: [2022-09-23 Fri 07:50]--[2022-09-23 Fri 08:01] =>  0:11
    CLOCK: [2022-09-22 Thu 07:30]--[2022-09-22 Thu 07:39] =>  0:09
    CLOCK: [2022-09-21 Wed 23:20]--[2022-09-21 Wed 23:53] =>  0:33
    CLOCK: [2022-09-21 Wed 08:24]--[2022-09-21 Wed 08:48] =>  0:24
    CLOCK: [2022-09-21 Wed 07:40]--[2022-09-21 Wed 07:47] =>  0:07
    CLOCK: [2022-09-20 Tue 18:51]--[2022-09-20 Tue 19:05] =>  0:14
    CLOCK: [2022-09-20 Tue 08:34]--[2022-09-20 Tue 08:59] =>  0:25
    CLOCK: [2022-09-19 Mon 11:18]--[2022-09-19 Mon 11:28] =>  0:10
    CLOCK: [2022-09-19 Mon 08:29]--[2022-09-19 Mon 09:19] =>  0:50
    :END:

Nightly build now uses full generation for tests. The problem is that full
generation expresses decoration as well:

#+begin_example
 * These files are code-generated via overrides to test dogen. Do not commit them.
 *
 * Generation timestamp: 2022-09-19T00:04:25
 * WARNING: do not edit this file manually.
 * Generated by MASD Dogen v1.0.32
 *
 * Copyright (C) 2012-2015 Marco Craveiro <marco.craveiro@gmail.com>
#+end_example

We need a way to set decoration to false in the model and respect that somehow.
Actually, it seems the problem is we are not honouring the variability overrides
in the tests.

The issue was we were supplying the command line incorrectly:

: --variability-override masd.variability.profile,masd.variability.profile,"

The command line argument =--variability-override= is not necessary. However,
when we fixed this we then created a whole raft of problems:

- we are now fully generating *everything*, including all reference products.
- for some reason the profile cannot be found for the c++ reference product:

  : std::exception::what: Configuration references a profile that could not be found: dogen.profiles.base.test_all_facets

- not clear why we do not throw on an invalid variability override. One for the
  backlog.

The quick hack is to only use the overrides on Dogen tests somehow.

With the builder changes we now get the following error:

#+begin_example
Running 1 test case...
Differences found. Outputting head of first 5 diffs.
/home/marco/nightly/dogen/master/projects/dogen.orchestration/tests/dogen_org_product_tests.cpp(83): error: in "dogen_product_org_tests/dogen_cli_org_produces_expected_model": check mg::check_for_differences(od, m) has failed

 *** 1 failure is detected in the test module "dogen.orchestration.tests"
#+end_example

This appears to reveal some bug in the diffing logic given that we do not see
any differences.

Notes:

- its not obvious what is causing this difference but it seems there is some
  logic error in the check for differences method. We must be falling through
  the cracks on some unforeseen case.

The problem is we had disabled diffing. Enabling diffing we now see:

#+begin_src diff
Unexpected write: dogen.identification/include/dogen.identification/types/entities/name_fwd.hpp
Reason: { "__type__": "operation_reason", "value": "changed_generated" }
diff -u include/dogen.identification/types/entities/name_fwd.hpp include/dogen.identification/types/entities/name_fwd.hpp
Reason: Changed generated file.
---  include/dogen.identification/types/entities/name_fwd.hpp
+++  include/dogen.identification/types/entities/name_fwd.hpp
@@ -2,7 +2,7 @@
  *
  * These files are code-generated via overrides to test dogen. Do not commit them.
  *
- * Generation timestamp: 2022-09-21T00:04:26
+ * Generation timestamp: not-a-date-time
  * WARNING: do not edit this file manually.
  * Generated by MASD Dogen v1.0.32
  *
/home/marco/nightly/dogen/master/projects/dogen.orchestration/tests/dogen_org_product_tests.cpp(204): error: in "dogen_product_org_tests/dogen_identification_org_produces_expected_model": check mg::check_for_differences(od, m) has failed
#+end_src

There are now two problems:

- why are we not generating a timestamp?
- if we did, we would still have a diff. We need a way to force the timestamp to
  a known value.

Links:

- [[https://github.com/MASD-Project/dogen/releases/tag/v1.0.19][v1.0.19]]: "Add support for variability overrides in Dogen"

*** COMPLETED Tests failing with filesystem errors                    :story:
    :LOGBOOK:
    CLOCK: [2022-09-24 Sat 08:35]--[2022-09-24 Sat 08:49] =>  0:14
    CLOCK: [2022-09-23 Fri 09:03]--[2022-09-23 Fri 10:13] =>  1:10
    CLOCK: [2022-09-22 Thu 19:40]--[2022-09-22 Thu 20:00] =>  0:20
    CLOCK: [2022-09-22 Thu 08:20]--[2022-09-22 Thu 08:46] =>  0:26
    :END:

The next batch of test failures is related to filesystem errors:

#+begin_example
Running 1 test case...
/home/marco/nightly/dogen/master/projects/dogen.orchestration/tests/code_generation_chain_tests.cpp(222): error: in "code_generation_chain_tests/empty_folders_are_not_deleted_when_delete_empty_folders_flag_is_off": check exists(first_empty_folders) has failed
/home/marco/nightly/dogen/master/projects/dogen.orchestration/tests/code_generation_chain_tests.cpp(223): error: in "code_generation_chain_tests/empty_folders_are_not_deleted_when_delete_empty_folders_flag_is_off": check exists(second_empty_folders) has failed
#+end_example

#+begin_example
/home/marco/nightly/dogen/master/projects/dogen.utility/src/types/test_data/dogen_product.cpp(125): Throw in function initialize
Dynamic exception type: boost::wrapexcept<dogen::utility::test_data::test_data_exception>
std::exception::what: Failed to delete output directory.
unknown location(0): fatal error: in "Test setup": std::runtime_error: Error during test
/home/marco/nightly/dogen/master/projects/dogen.orchestration/tests/main.cpp(39): last checkpoint: initializer
Running 1 test case...
#+end_example

#+begin_example
D:\a\dogen\dogen\projects\dogen.utility\src\types\test_data\dogen_product.cpp(125): Throw in function initialize
Dynamic exception type: struct boost::wrapexcept<class dogen::utility::test_data::test_data_exception>
std::exception::what: Failed to delete output directory.
unknown location(0): fatal error: in "Test setup": class std::runtime_error: Error during test
D:\a\dogen\dogen\projects\dogen.codec\tests\main.cpp(35): last checkpoint: initializer
Running 1 test case...
#+end_example

The problem is a race condition on how we are using the filesystem. The product
initialisers are recreating the top-level product directories, and this causes a
race condition between the tests generating code and the initialiser. We need to
have a way to setup / clean each test so that they do not affect each other.

We only seem to have three tests that actually write to the filesystem. So to
fix this:

- remove the recreation of directories from the product classes. Add it to
  utilities.
- add a unique prefix to each test's output directory and recreate that
  directory.
- add comments on the tests where we do not write to the filesystem to make it
  more obvious.

*** COMPLETED Remove uses of mock configuration factory               :story:
    :LOGBOOK:
    CLOCK: [2022-09-23 Fri 10:33]--[2022-09-23 Fri 11:27] =>  0:54
    :END:

We don't really need a builder and a factory. Also remove the various flags we
left scattered to handle diffing, reporting etc.

*** COMPLETED Add nightly builds to C++ reference product             :story:

Since we list travis we lost support for nightlies.

*** COMPLETED Add continuous builds to C++ reference product          :story:
    :LOGBOOK:
    CLOCK: [2022-09-25 Sun 12:15]--[2022-09-25 Sun 12:31] =>  0:16
    CLOCK: [2022-09-23 Fri 16:54]--[2022-09-23 Fri 18:51] =>  1:57
    :END:

Since we list travis we lost support for CI.

*** COMPLETED Add continuous builds to C# reference product           :story:
    :LOGBOOK:
    CLOCK: [2022-09-26 Mon 07:40]--[2022-09-26 Mon 07:52] =>  0:12
    CLOCK: [2022-09-25 Sun 13:45]--[2022-09-25 Sun 15:11] =>  1:26
    CLOCK: [2022-09-25 Sun 13:30]--[2022-09-25 Sun 13:42] =>  0:12
    :END:

Since we list travis we lost support for CI.

Merged stories:

*Add github actions build for C#*

We need to build on .Net 6.

*** COMPLETED CI error: Failed to delete output directory             :story:

*Rationale*: the changes to test structure resolved this issue.

We are experiencing a strange CI error:

#+begin_example
D:\a\dogen\dogen\projects\dogen.utility\src\types\test_data\dogen_product.cpp(125): Throw in function initialize
Dynamic exception type: struct boost::wrapexcept<class dogen::utility::test_data::test_data_exception>
std::exception::what: Failed to delete output directory.
unknown location(0): fatal error: in "Test setup": class std::runtime_error: Error during test
D:\a\dogen\dogen\projects\dogen.orchestration\tests\main.cpp(39): last checkpoint: initializer
Running 1 test case...

 *** No errors detected
Test setup error:
#+end_example

We also have this related error:

#+begin_example
Running 1 test case...
/home/runner/work/dogen/dogen/projects/dogen.orchestration/tests/code_generation_chain_tests.cpp(169): fatal error: in "code_generation_chain_tests/empty_folders_are_deleted_when_delete_empty_folders_flag_is_on": critical check are_generated_files_healthy(od, t, 60 ) has failed

 *** 1 failure is detected in the test module "dogen.orchestration.tests"
#+end_example

*** CANCELLED Remove ODB support from Dogen                           :story:
    :LOGBOOK:
    CLOCK: [2022-09-23 Fri 15:49]--[2022-09-23 Fri 16:34] =>  0:45
    :END:

*Rationale*: Actually it seems we are not compiling this code as it stands so
for now its OK to leave it as is.

Last sprint we removed the relational model from Dogen. This sprint we need to g
one step further and remove ODB support. Now, we may not need to remove it
entirely: the headers Dogen generates are simple C++ headers that do not require
ODB libraries to compile, /e.g./:

#+begin_src c++
#ifdef ODB_COMPILER

#pragma db object(categories) schema("NORTHWIND")

#pragma db member(categories::category_id_) id
#pragma db member(categories::description_) null
#pragma db member(categories::picture_) null

#endif
#+end_src

We could conceivably continue to generate these, but we must not add the
associated ODB files (generated by ODB) because then we pull in the ODB C++
libraries and these are not supported by vcpkg. If we leave the pragmas we at
least know we are not making ODB support any worse. This is still useful as we
may return to it in the future. It also ensure some variation in the logical
model (in particular in the cartridges domain).

Merged stories:

*Reference implementation build is borked*

We need to upgrade the ODB version of the reference implementation. Annoyingly
this will mean hitting the usual issues with vcpkg. We should probably consider
deprecating ODB from the reference implementation as well, or at least disabling
the building of the generated ODB code.

*** COMPLETED Cannot access binaries from release notes               :story:

At present the URLs for the binaries are 404ing. We need to upload binaries
manually to the release.

- [[https://github.com/MASD-Project/dogen/releases/download/v1.0.31/DOGEN-1.0.31-Darwin-x86_64.dmg][DOGEN-1.0.31-Darwin-x86_64.dmg]]
- [[https://github.com/MASD-Project/dogen/releases/download/v1.0.31/DOGEN-1.0.31-Windows-AMD64.msi][DOGEN-1.0.31-Windows-AMD64.msi]]
- [[https://github.com/MASD-Project/dogen/releases/download/v1.0.31/dogen_1.0.31_amd64-applications.deb][dogen_1.0.31_amd64-applications.deb]]

Release notes have been updated:

- https://github.com/MASD-Project/dogen/releases/tag/v1.0.31

*** COMPLETED Add support for relations in codec model                :story:
    :PROPERTIES:
    :CUSTOM_ID: 1ECCD69A-EE17-BAE4-7FE3-DA5F2E6E01FB
    :END:

*Rationale*: this story and associated tasks have all been implemented.

One very simple way to improve diagrams is to allow users to associate a
fragment of PlantUML code with a class, for example:

: masd.codec.plantuml: myclass <>-- other_class : test

This fragments are added after the class, verbatim. Its up to the users to
annotate diagrams as they see fit, we merely copy and paste these annotations.

In the future, we may spot patterns of usage that can be derived from meta-data,
but for now we just need the diagrams to be usable like they were in Dia.

Notes:

- notes are not indented at present.
- we are not leaving a space after inheritance.
- empty classes still have brackets.
- no top-level namespace for model. We didn't have this in Dia either.

 Tasks:

- add new feature in codec model.
- add properties in model and element to store the data.
- when converting into PlantUML, output the new properties after dumping the
  class.
- move codec to codec tests from orchestration to codec component.
- codec needs to have a way to bootstrap its context without requiring
  orchestration.

*** COMPLETED Add models directory to each component                  :story:

*Rationale*: this has been done in Dogen.

Instead of a product level models directory, we should have separate component
level directories. We can't do the PMM implementation just yet but we can use
regexes to get the directory in the correct shape and then use it to target the
changes in the PMM. The directory should be called =modeling= to reflect the
fact that it will contain more than models.

Notes:

- when we do this we will break the dogen product unit tests.
- we need to add the targets to each component (generation, conversion).


*** CANCELLED Model significant relations at a higher level           :story:
    :LOGBOOK:
    CLOCK: [2022-09-27 Tue 08:10]--[2022-09-27 Tue 08:27] =>  0:17
    CLOCK: [2022-09-25 Sun 18:02]--[2022-09-25 Sun 18:50] =>  0:48
    :END:

Last sprint we added the PlantUML verbatim property, /i.e./:

:   :masd.codec.plantuml: model o-- element : composed of
:   :masd.codec.plantuml: Element <|.. model

This was meant to allow us to add the missing relations in the PlantUML
diagrams. However, there are issues with this approach:

- we may enter invalid PlantUML syntax, and will only find out at diagram
  generation time. The error will probably be very hard to figure out as well.
- we need to know the exact element name. Given the "spaces for underscores"
  approach, this is not very nice (/e.g./ we replace "a model type" with
  "a_model_type").
- if you rename a type, this will fail.

Seems like a better approach is to name the relations and add them as codec
attributes:

:   :masd.codec.composition: 294DC761-8784-3D74-824B-48E7BCC2CFB2, description
:   :masd.codec.aggregation: 294DC761-8784-3D74-824B-48E7BCC2CFB2, another description
:   :masd.codec.association: 294DC761-8784-3D74-824B-48E7BCC2CFB2, yet another description

These relations then give rise to a mapping to the element name during
resolution. This copes with renames.

Notes:

- actually, this story is related to the modeling of relationships in general.
  We need to look through the backlog to find out what analysis had been done on
  this and see how much of it is needed in order to implement this
  functionality.
- we need to split out two different activities. The current activity is just to
  get the PlantUML diagrams into a usable state. If we get side-tracked into
  solving relations in general, this will take too long. Also, by manually
  updating diagrams with Verbatim we will get a much better handle on the use
  cases, and we can then replace those over time. For now, unwind any changes we
  did for this and put this story in the backlog.

Links:

- [[https://www.ibm.com/docs/en/rational-soft-arch/9.5?topic=diagrams-relationships-in-class][UML: Relationships in class diagrams]]
- [[https://www.omg.org/spec/MOF/2.5.1/PDF][MOF 2.5 specification]]

Merged stories:

*Consider modeling relations at a higher level of abstration*
    :PROPERTIES:
    :CUSTOM_ID: E19AC760-A5C5-CC84-61DB-E6D7B9562ECF
    :END:

Note: this story captures the high-level analysis for implementing relations
across dogen. We then need to create specific stories for its implementation.

At present we model relations in logical model as two object templates:

- =Generalisable= for inheritance (implements and extends).
- =Associatable= for composition.

In reality, we should have created the UML relationships as a top-level
construct:

- association: composition, aggregation
- dependency
- generalisation
- realisation

Relationships should have an associated comment or description.

This story implements the functionality described in [[file:/work/DomainDrivenConsulting/masd/dogen/integration/doc/agile/product_backlog.org::#E19AC760-A5C5-CC84-61DB-E6D7B9562ECF][this story]] but only as far
as the codec model is concerned.

Notes:

- relationships should already exist in the codec model. These exist for "local"
  relationships only (that is, elements in the same model). They can be used for
  generalisation. This does mean generalisation could be "remote" though as we
  some times inherit from other diagrams. We need a way to distinguish between
  local and remote relations, which could be by "resolving" the GUID into an
  element.
- relationships can be user-annotated, and used for UML diagram generation.
- generalisation and realisation remove the need for the parent meta-data.
- relationships can be derived from attributes. This is what the "resolver"
  does. It is in fact not a resolver but a transform that converts properties in
  the element into relationships.
- relationships should use the GUID as well as the qualified name.
- relationships should really be modeled as org-mode headings. However, one
  downside of this approach is that we will create a lot of noise when
  generating documentation. However, given we will only use them for local
  relationships (generalisation, UML purposes), maybe the noise is not that bad.
- transparent and opaque associations as well associative container keys need to
  be mapped to the appropriate UML stereotypes. Leaves and root parents as well.
  If none is appropriate we should create them.
- add a new type of relationship to codec model. We probably also need an enum
  to capture the type of relationship. This can be supplied in org-mode as
  meta-data. Relationships belong to elements.
- object templates are incorrectly modeled as stereotypes. These are
  realisations.
- profiles are also incorrectly modeled as stereotypes. These are also
  relations. However, the problem will be that once we remove them from
  stereotypes we cannot see them in UML. We need to have a section in the
  documentation which shows these properties for an element.
- The name of the relation is its description, e.g. "throws". We can have
  duplicate relation names.
- for now, do a hack in the logical model that takes relations of certain types
  (say realisation) and adds them to stereotypes in the logical model. However,
  we must be able to ignore other types (say attributes annotated by the user).
- best mapping for org-mode is:
  - title is the type we point to.
  - description is the name of the relationship.
  - attribute =relationship= to denote codec type.
  - meta-data to denote relationship type.
  - add GUID if you want the relationship to show up in PlantUML.
- make object templates interfaces. Modeling a concept is a realisation.

Links:

- [[https://www.guru99.com/uml-relationships-with-example.html#:~:text=Relationships%20in%20UML%20are%20used,Dependency%20%2C%20Generalization%20%2C%20and%20Realization][UML Relationships Types: Association, Dependency, Generalization]].

*** CANCELLED Create a mock configuration builder                     :story:
    :LOGBOOK:
    CLOCK: [2022-09-20 Tue 19:06]--[2022-09-20 Tue 19:15] =>  0:09
    CLOCK: [2022-09-19 Mon 18:19]--[2022-09-19 Mon 18:40] =>  0:21
    CLOCK: [2022-09-19 Mon 16:31]--[2022-09-19 Mon 17:50] =>  1:19
    CLOCK: [2022-09-19 Mon 11:28]--[2022-09-19 Mon 11:37] =>  0:09
    :END:

*Rationale*: a better approach was implemented by adding state to the model
producer.

At present we are using a factory for creating mock configurations. This was
fine because we only had one or two variations, so it was easy enough to
construct the configuration in one call. However, with variability overrides we
now have several different scenarios. It would be easier to have a builder, with
sensible defaults, that returns a full configuration which is then supplied to
the model generator.

Notes:

- consider adding all variables to the result of the builder, to make the code a
  bit less repetitive.

*** COMPLETED Enable CodeQL                                           :story:
    :LOGBOOK:
    CLOCK: [2022-09-25 Sun 17:53]--[2022-09-25 Sun 17:57] =>  0:04
    CLOCK: [2022-09-25 Sun 17:45]--[2022-09-25 Sun 17:52] =>  0:07
    CLOCK: [2022-09-25 Sun 17:36]--[2022-09-25 Sun 17:41] =>  0:05
    CLOCK: [2022-09-25 Sun 17:25]--[2022-09-25 Sun 17:30] =>  0:05
    :END:

GitHub seems to have new security tooling. Enabled but not quite sure what it
does.

Links:

- [[https://github.com/MASD-Project/dogen/security/code-scanning][code-scanning]]

*** COMPLETED Change namespaces note implementation in PlantUML       :story:
    :LOGBOOK:
    CLOCK: [2022-10-04 Tue 19:19]--[2022-10-04 Tue 20:00] =>  0:41
    CLOCK: [2022-10-04 Tue 08:30]--[2022-10-04 Tue 08:50] =>  0:20
    CLOCK: [2022-10-04 Tue 08:08]--[2022-10-04 Tue 08:20] =>  0:21
    CLOCK: [2022-10-02 Sun 08:27]--[2022-10-02 Sun 08:38] =>  0:11
    CLOCK: [2022-10-02 Sun 08:21]--[2022-10-02 Sun 08:26] =>  0:05
    CLOCK: [2022-10-01 Sat 17:11]--[2022-10-01 Sat 17:30] =>  0:19
    :END:

At present we are adding notes to namespaces like so:

#+begin_src plantuml
    note top of  variability
        Houses all of the meta-modeling elements related to variability.
    end note
#+end_src

The problem with this approach is that the notes end up floating above the
namespace with an arrow, making it hard to read. A better approach is a floating
note:

#+begin_src plantuml
    note A1
        Houses all of the meta-modeling elements related to variability.
    end note
#+end_src

The note is declared inside the namespace. We probably need to ensure the note
has a unique name. We probably need to use a GUID for the note. Actually maybe
we can use the ID of the namespace in the note.

At present model level comments look dodgy:

#+begin_src c++
        os << "note as N1" << std::endl
           << m.comment().documentation() << std::endl
           << "end note" << std::endl << std::endl;
#+end_src

We should also use GUIDs here.

Links:

- [[https://stackoverflow.com/questions/59934882/plantuml-and-notes-on-packages][PlantUML and notes on packages]]

*** COMPLETED Consider using a different layout engine in PlantUML    :story:
    :LOGBOOK:
    CLOCK: [2022-10-08 Sat 20:25]--[2022-10-08 Sat 21:07] =>  0:42
    CLOCK: [2022-10-08 Sat 20:10]--[2022-10-08 Sat 20:24] =>  0:14
    CLOCK: [2022-10-08 Sat 10:50]--[2022-10-08 Sat 11:05] =>  0:15
    CLOCK: [2022-10-08 Sat 10:13]--[2022-10-08 Sat 10:23] =>  0:10
    CLOCK: [2022-10-08 Sat 10:02]--[2022-10-08 Sat 10:12] =>  0:10
    :END:

At present PlantUML is rendering using the basic dot engine. This results in
very horizontal diagrams. It also seems to crash in some cases (not sure if this
is dot or not). It would be great to experiment with other layout engines, if
they exist.

#+begin_example
COMMAND PLANTUML_LIMIT_SIZE=65536 ${PLANTUML_PROGRAM} -Playout=smetana
#+end_example

Smetana fails to generate the diagram, even without namespace to namespace
relations:

#+begin_example
java.lang.ArrayIndexOutOfBoundsException: 3
        at gen.lib.dotgen.mincross__c.left2right(mincross__c.java:369)
        at gen.lib.dotgen.mincross__c.transpose_step(mincross__c.java:522)
        at gen.lib.dotgen.mincross__c.transpose(mincross__c.java:571)
        at gen.lib.dotgen.mincross__c.mincross_step(mincross__c.java:1645)
        at gen.lib.dotgen.mincross__c.mincross_(mincross__c.java:627)
        at gen.lib.dotgen.mincross__c.dot_mincross(mincross__c.java:194)
        at gen.lib.dotgen.dotinit__c.dotLayout(dotinit__c.java:370)
        at gen.lib.dotgen.dotinit__c.doDot(dotinit__c.java:492)
        at gen.lib.dotgen.dotinit__c.dot_layout(dotinit__c.java:547)
        at gen.lib.dotgen.dotinit__c$2.exe(dotinit__c.java:539)
        at gen.lib.gvc.gvlayout__c.gvLayoutJobs(gvlayout__c.java:153)
        at net.sourceforge.plantuml.sdot.CucaDiagramFileMakerSmetana.createFileLocked(CucaDiagramFileMakerSmetana.java:381)
        at net.sourceforge.plantuml.sdot.CucaDiagramFileMakerSmetana.createFile(CucaDiagramFileMakerSmetana.java:336)
        at net.sourceforge.plantuml.cucadiagram.CucaDiagram.exportDiagramInternal(CucaDiagram.java:620)
        at net.sourceforge.plantuml.classdiagram.ClassDiagram.exportDiagramInternal(ClassDiagram.java:188)
        at net.sourceforge.plantuml.UmlDiagram.exportDiagramNow(UmlDiagram.java:135)
        at net.sourceforge.plantuml.AbstractPSystem.exportDiagram(AbstractPSystem.java:179)
        at net.sourceforge.plantuml.PSystemUtils.exportDiagramsDefault(PSystemUtils.java:209)
        at net.sourceforge.plantuml.PSystemUtils.exportDiagrams(PSystemUtils.java:93)
        at net.sourceforge.plantuml.SourceFileReaderAbstract.getGeneratedImages(SourceFileReaderAbstract.java:186)
        at net.sourceforge.plantuml.Run.manageFileInternal(Run.java:518)
        at net.sourceforge.plantuml.Run.processArgs(Run.java:401)
        at net.sourceforge.plantuml.Run.manageAllFiles(Run
#+end_example

It was also suggested we try ELK:

#+begin_example
COMMAND PLANTUML_LIMIT_SIZE=65536 ${PLANTUML_PROGRAM} -Playout=elk
#+end_example

This layout does appear to be superior to the regular PlantUML dot layout.

Links:

- [[https://github.com/plantuml/plantuml/issues/1110][GH: Alternative layout engines from graphviz #1110]]
- [[https://github.com/plantuml/plantuml/issues/1078][GH: Allow Arrows in any direction #1078]]
- [[https://plantuml.com/smetana02][Context of the Smetana project]]
- [[https://graphviz.org/docs/layouts/][graphviz: Layout Engines]]
- [[https://plantuml.com/elk][Eclipse Layout Kernel]]

*** COMPLETED Upgrade PlantUML to latest                              :story:
    :LOGBOOK:
    CLOCK: [2022-10-08 Sat 14:26]--[2022-10-08 Sat 14:39] =>  0:13
    CLOCK: [2022-10-08 Sat 09:30]--[2022-10-08 Sat 10:01] =>  0:31
    :END:

At present, when we add a relation between classes in inner namespaces, it
crashes PlantUML. Before we submit a ticket we should update to latest and try
to add the relation.

It seems we did some kind of hack to get latest on Debian:

#+begin_example
$ cd /usr/share/plantuml/
$ ls -l
total 18251
-rw-r--r--   1 root           root      8618641 2020-03-10  2020 plantuml-1.2020.02.jar
-rw-r--r--   1 marco          marco    10070645 2022-04-09 09:47 plantuml-1.2022.3.jar
lrwxrwxrwx   1 root           root           21 2022-04-09 09:49 plantuml.jar -> plantuml-1.2022.3.jar
 #+end_example

 We should follow the same pattern. We also need to update it in the laptop to
 avoid oscillation between the two versions and rewriting diagrams each time.

 Notes:

 - updated to [[https://github.com/plantuml/plantuml/releases][v1.2022.10]]

*** COMPLETED Code coverage in CDash has disappeared                  :story:
    :LOGBOOK:
    CLOCK: [2022-10-09 Sun 21:58]--[2022-10-09 Sun 22:07] =>  0:09
    CLOCK: [2022-10-09 Sun 21:30]--[2022-10-09 Sun 21:44] =>  0:14
    :END:

We do not seem to have code coverage any longer.

*** CANCELLED Investigate nightly issues                              :story:
    :LOGBOOK:
    CLOCK: [2022-10-09 Sun 09:15]--[2022-10-09 Sun 09:17] =>  0:02
    CLOCK: [2022-10-09 Sun 09:02]--[2022-10-09 Sun 09:15] =>  0:13
    :END:

First we started to have very strange errors in the nightly: 3816 failed tests,
with errors such as:

#+begin_example
valgrind:  Fatal error at startup: a function redirection
valgrind:  which is mandatory for this platform-tool combination
valgrind:  cannot be set up.  Details of the redirection are:
valgrind:
valgrind:  A must-be-redirected function
valgrind:  whose name matches the pattern:      strlen
valgrind:  in an object with soname matching:   ld-linux-x86-64.so.2
valgrind:  was not found whilst processing
valgrind:  symbols from the object with soname: ld-linux-x86-64.so.2
valgrind:
valgrind:  Possible fixes: (1, short term): install glibc's debuginfo
valgrind:  package on this machine.  (2, longer term): ask the packagers
valgrind:  for your Linux distribution to please in future ship a non-
valgrind:  stripped ld.so (or whatever the dynamic linker .so is called)
valgrind:  that exports the above-named function using the standard
valgrind:  calling conventions for this platform.  The package you need
valgrind:  to install for fix (1) is called
valgrind:
valgrind:    On Debian, Ubuntu:                 libc6-dbg
valgrind:    On SuSE, openSuSE, Fedora, RHEL:   glibc-debuginfo
valgrind:
valgrind:  Note that if you are debugging a 32 bit process on a
valgrind:  64 bit system, you will need a corresponding 32 bit debuginfo
valgrind:  package (e.g. libc6-dbg:i386).
valgrind:
valgrind:  Cannot continue -- exiting now.  Sorry.
#+end_example

This is likely related to some =dist-upgrade=. However, the biggest problem is
that the nightly has now gone missing altogether. Nothing in the logs, no email
in cron. Checked job, looks fine. Will try again next day.

Builds are missing because since the last dist-upgrade our PC switches off into
sleep mode. However, the right fix is to move nightlies to github (separate
story).

*** COMPLETED Consider creating nightly branches                      :story:

It may be useful to commit and push all of the generated code on a nightly. That
way we can look at the code when there are problems. The downsides are:

- we need to delete these branches.
- CI is both building and pushing into git, which is not ideal.

Notes:

 Perhaps a better way would be to have 2 separate workflows: one just for full
generation and one for building that branch.

*** COMPLETED Gitter notifications for builds are not showing up      :story:
    :LOGBOOK:
    CLOCK: [2022-11-18 Fri 19:00]--[2022-11-18 Fri 19:15] =>  0:15
    CLOCK: [2022-11-14 Mon 08:40]--[2022-11-14 Mon 08:51] =>  0:11
    CLOCK: [2022-11-13 Sun 14:02]--[2022-11-13 Sun 14:09] =>  0:07
    CLOCK: [2022-11-13 Sun 09:49]--[2022-11-13 Sun 09:56] =>  0:07
    CLOCK: [2022-11-13 Sun 09:25]--[2022-11-13 Sun 09:29] =>  0:04
    CLOCK: [2022-11-13 Sun 08:31]--[2022-11-13 Sun 08:35] =>  0:04
    CLOCK: [2022-11-12 Sat 23:07]--[2022-11-12 Sat 23:22] =>  0:15
    CLOCK: [2022-11-12 Sat 22:44]--[2022-11-12 Sat 22:45] =>  0:01
    CLOCK: [2022-11-12 Sat 22:02]--[2022-11-12 Sat 22:27] =>  0:25
    CLOCK: [2022-09-18 Sun 08:05]--[2022-09-18 Sun 08:15] =>  0:10
    CLOCK: [2022-09-18 Sun 07:20]--[2022-09-18 Sun 07:29] =>  0:09
    CLOCK: [2022-09-17 Sat 21:29]--[2022-09-17 Sat 21:48] =>  0:19
    :END:

We used to see travis and appveyor build notifications. We stopped seeing them
after moving to github actions. This is useful because we can see them from
Emacs in IRC.

Notes:

- it seems the settings have an option for this in webhooks. Redo the hook to
  see if it helps.
- its now working, but the problem is we are posting as ourselves rather than a
  BOT. This is very confusing. Also, the details are not very useful:

#+begin_quote
Marco Craveiro @mcraveiro 22:59
Ref: refs/heads/master
Event: push
Action: https://github.com/MASD-Project/dogen/commit/17e2909dcdbec19753b72c1669dd7dd02497396f/checks
Message: - success
Ref: refs/heads/master
Event: push
Action: https://github.com/MASD-Project/dogen/commit/17e2909dcdbec19753b72c1669dd7dd02497396f/checks
Message: - success
#+end_quote

- created a BOT account.

Links:

- [[https://gitlab.com/gitterHQ/webapp/-/blob/develop/docs/integrations.md][Gitter: github integrations]]
- [[https://github.com/juztcode/gitter-github-action][Gitter notify - Github action]]
- [[https://developer.gitter.im/docs/authentication][GitHub Developer - Authentication]]
- [[https://developer.gitter.im/docs/rest-api][REST API]]
- [[https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment][GH: Using an environment]]
- [[https://stackoverflow.com/questions/58858429/how-to-run-a-github-actions-step-even-if-the-previous-step-fails-while-still-f][How to run a github-actions step, even if the previous step fails, while still
  failing the job]]

*** COMPLETED Create a nightly github workflow                        :story:
    :LOGBOOK:
    CLOCK: [2022-11-20 Sun 18:41]--[2022-11-20 Sun 18:59] =>  0:18
    CLOCK: [2022-11-13 Sun 08:11]--[2022-11-13 Sun 08:26] =>  0:15
    CLOCK: [2022-11-13 Sun 08:10]--[2022-11-13 Sun 08:22] =>  0:12
    CLOCK: [2022-11-12 Sat 13:42]--[2022-11-12 Sat 14:03] =>  0:21
    CLOCK: [2022-11-12 Sat 13:00]--[2022-11-12 Sat 13:12] =>  0:12
    CLOCK: [2022-11-12 Sat 10:55]--[2022-11-12 Sat 11:40] =>  0:45
    CLOCK: [2022-10-17 Mon 19:51]--[2022-10-17 Mon 20:09] =>  0:18
    CLOCK: [2022-10-16 Sun 18:30]--[2022-10-16 Sun 18:44] =>  0:14
    CLOCK: [2022-10-15 Sat 10:30]--[2022-10-15 Sat 11:05] =>  0:35
    CLOCK: [2022-10-14 Fri 08:00]--[2022-10-14 Fri 08:09] =>  0:09
    CLOCK: [2022-10-12 Wed 08:18]--[2022-10-12 Wed 08:59] =>  0:41
    CLOCK: [2022-10-12 Wed 07:41]--[2022-10-12 Wed 07:55] =>  0:14
    CLOCK: [2022-10-11 Tue 07:43]--[2022-10-11 Tue 08:28] =>  0:45
    CLOCK: [2022-10-10 Mon 07:50]--[2022-10-10 Mon 08:03] =>  0:13
    CLOCK: [2022-10-09 Sun 16:00]--[2022-10-09 Sun 16:27] =>  0:27
    :END:

It seems we have all that is required to run nightlies in github. Its better to
do it there because it avoids issues with local PC such as PC is off, cron
issues etc.

Notes:

- For some reason we are not setting up vcpkg path in the includes, Try merging
  steps to see if that makes any difference.
- new warning: "Node.js 12 actions are deprecated. For more information see:
  https://github.blog/changelog/2022-09-22-github-actions-all-actions-will-begin-running-on-node16-instead-of-node12/.
  Please update the following actions to use Node.js 16: lukka/get-cmake,
  lukka/run-vcpkg, lukka/run-vcpkg". Bump versions to latest.
- build is now failing because we need to reconfigure to find the code
  generator:

  : 2022-10-14T02:12:45.8641079Z ninja: error: unknown target 'gao', did you mean 'rat'?
- we still have the problem of linking:

  : projects/dogen/generated_tests/configuration_tests.cpp:29:10: fatal error: boost/test/unit_test.hpp: No such file or directory

  The best thing may be to split the nightly into two workflow steps.
- deleted the existing nightly/YYYYMMDD branch which was causing issues:

  : Switched to a new branch 'nightly'
  : To https://github.com/MASD-Project/dogen
  : ! [remote rejected] nightly -> nightly (cannot lock ref 'refs/heads/nightly': 'refs/heads/nightly/20221112' exists; cannot create 'refs/heads/nightly')
  : error: failed to push some refs to 'https://github.com/MASD-Project/dogen'

- nightly builds are failing to compile because the generated tests are using a
  CMakeLists with globals:

  #+begin_src cmake
  target_link_libraries(${tests_target_name}
    ${lib_target_name}
    ${CMAKE_REQUIRED_LIBRARIES}
    ${CMAKE_THREAD_LIBS_INIT}
    ${Boost_LIBRARIES})
  #+end_src

  This means we can only fix the nightlies by updating CMake templates to take
  into account dependencies. However, since we managed to compile the nightly
  locally, this means the main story is complete. We need a separate story to
  get the nightly to go green.

Links:

- [[https://unix.stackexchange.com/questions/49053/how-do-i-add-x-days-to-date-and-get-new-date][How do I add X days to date and get new date?]]
- [[https://iramykytyn.dev/how-to-integrate-valgrind-into-github-actions][How To Integrate Valgrind into GitHub Actions?]]

*** COMPLETED Add additional reference directories                    :story:

*Rationale*: already implemented.

 At present we expect the reference models to be either on the data directory
 (for system models) or on the same directory as the target. Presumably, users
 may also want to have models on other directories. For example, if one were to
 extend Dogen with a different project, it would be required to load models from
 the dogen directory.

 We could simply add a command line argument for reference directories; if the
 reference is not found in the target model directory, we would then try all
 available reference directories.

 We should implement this as a =-I= parameter akin to compilers. We should also
 have a command to output the current include path - check GCC to see what
 command they use for this.

 This will be a requirement in order to support PDMs because we shall have many
 directories with models. We will need the concept of "system include
 directories" for this. We need to look into how compilers do this. We must also
 dump these in dumpspecs.

 Merged Stories:

 *Add additional data files directories*

 #+begin_quote
 *Story*: As a dogen user, I want dogen to use my own private data libraries so
 that I don't have to supply them as diagrams.
 #+end_quote

 Users should be able to provide directories for their own JSON models. We just
 need to add a new parameter to the knitter and transport it all the way to OM's
 workflow.

 In the future, when everything is a model, data file directories and reference
 directories will become one and the same.



*** COMPLETED Run nightlies only when there are changes               :story:

*Rationale*: implemented.

We should not need to run nightlies every night if there are no commits.

Links:

- [[https://github.com/sunglasses-ai/classy/blob/main/.github/workflows/ci.yml#L10][GH: ci.yml]] Example action with check.
- [[https://stackoverflow.com/questions/63014786/how-to-schedule-a-github-actions-nightly-build-but-run-it-only-when-there-where][SO: How to schedule a GitHub Actions nightly build, but run it only when there
  where recent code changes?]]

*** COMPLETED Create a GitHub account for MASD BOT                    :story:

*Rationale*: implemented.

At present we are sending gitter out notifications as ourselves, which is very
confusing. We need a separate BOT account for this. Unfortunately, this entails
creating a new email account, a new GitHub account etc.

Links:

- [[https://stackoverflow.com/questions/29177623/what-is-a-bot-account-on-github][What is a "bot account" on github?]]

*** COMPLETED Nightly builds are taking too long                      :story:
    :LOGBOOK:
    CLOCK: [2022-11-29 Tue 08:30]--[2022-11-29 Tue 08:53] =>  0:23
    CLOCK: [2022-11-27 Sun 10:11]--[2022-11-27 Sun 10:26] =>  0:15
    CLOCK: [2022-11-27 Sun 10:00]--[2022-11-27 Sun 10:10] =>  0:10
    CLOCK: [2022-11-26 Sat 14:01]--[2022-11-26 Sat 14:50] =>  0:49
    CLOCK: [2022-11-26 Sat 11:21]--[2022-11-26 Sat 12:09] =>  0:48
    CLOCK: [2022-11-25 Fri 08:23]--[2022-11-25 Fri 08:40] =>  0:17
    CLOCK: [2022-11-24 Thu 07:49]--[2022-11-24 Thu 08:00] =>  0:11
    CLOCK: [2022-11-23 Wed 08:01]--[2022-11-23 Wed 08:29] =>  0:28
    CLOCK: [2022-11-22 Tue 08:49]--[2022-11-22 Tue 08:59] =>  0:10
    CLOCK: [2022-11-22 Tue 08:31]--[2022-11-22 Tue 08:49] =>  0:18
    CLOCK: [2022-11-21 Mon 22:40]--[2022-11-21 Mon 22:47] =>  0:07
    CLOCK: [2022-11-21 Mon 07:49]--[2022-11-21 Mon 08:14] =>  0:25
    :END:

It seems we are bursting the 6h limit for nightlies now. We are running around
900 tests in 6h. We need to reduce the total number of tests run under valgrind.

The solution is to have 2 nightlies:

- a full generation build which runs all tests (e.g. 3.6k).
- a normal build from master which only runs the ~600 tests.

In order to support this use case we just need a flag to disable memcheck.

Notes:

- all builds pass but the fullgen ones did not make it into CTest. Rename
  builds. This will also help with coverage.
- issues with badges.
- valgrind fails due to CMake version update.

**** CDash Issue

Submit issue to CDash project: https://github.com/Kitware/CDash/issues

#+begin_src markdown
Hi CDash developers,

Thanks very much for a great tool, which I have relied on daily for more than a decade. I have a strange problem, and not quite sure how to debug it further. My open source project  [1] makes use of the community CDash instance [2]. In general, all my builds have worked just fine but of late I am having a strange problem with a new set of builds. To the layperson, the CTest submission logs look normal, e.g. [3]:

```
SetCTestConfiguration:BuildDirectory:/home/runner/work/dogen/dogen/build/output/linux-clang-debug
SetCTestConfiguration:SourceDirectory:/home/runner/work/dogen/dogen
SetCTestConfigurationFromCMakeVariable:DropMethod:CTEST_DROP_METHOD
SetCTestConfiguration:DropMethod:http
SetCTestConfigurationFromCMakeVariable:DropSite:CTEST_DROP_SITE
SetCTestConfiguration:DropSite:my.cdash.org
SetCTestConfigurationFromCMakeVariable:DropLocation:CTEST_DROP_LOCATION
SetCTestConfiguration:DropLocation:/submit.php?project=MASD+Project+-+Dogen
Submit files
   SubmitURL: http://my.cdash.org/submit.php?project=MASD+Project+-+Dogen
   Upload file: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Update.xml to http://my.cdash.org/submit.php?project=MASD+Project+-+Dogen&FileName=github___linux-clang-debug___20221126-0000-Nightly___XML___Update.xml&build=linux-clang-debug&site=github&stamp=20221126-0000-Nightly&MD5=56f692fc3ddcd82bf709978b14f82f8d Size: 553
   Uploaded: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Update.xml
   Upload file: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Configure.xml to http://my.cdash.org/submit.php?project=MASD+Project+-+Dogen&FileName=github___linux-clang-debug___20221126-0000-Nightly___XML___Configure.xml&build=linux-clang-debug&site=github&stamp=20221126-0000-Nightly&MD5=6ccfef78f7a4fa53aff8bcb633109666 Size: 20947
   Uploaded: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Configure.xml
   Upload file: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Build.xml to http://my.cdash.org/submit.php?project=MASD+Project+-+Dogen&FileName=github___linux-clang-debug___20221126-0000-Nightly___XML___Build.xml&build=linux-clang-debug&site=github&stamp=20221126-0000-Nightly&MD5=bbf8a77fc3c9de8f93be2f2ce769714d Size: 1120
   Uploaded: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Build.xml
   Upload file: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Test.xml to http://my.cdash.org/submit.php?project=MASD+Project+-+Dogen&FileName=github___linux-clang-debug___20221126-0000-Nightly___XML___Test.xml&build=linux-clang-debug&site=github&stamp=20221126-0000-Nightly&MD5=10192675e22f3562805fa0e8e4eb2981 Size: 8212762
   Uploaded: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Test.xml
   Upload file: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Done.xml to http://my.cdash.org/submit.php?project=MASD+Project+-+Dogen&FileName=github___linux-clang-debug___20221126-0000-Nightly___XML___Done.xml&build=linux-clang-debug&site=github&stamp=20221126-0000-Nightly&MD5=c8da7710474223551534d3969a8c4d98 Size: 107
   Uploaded: /home/runner/work/dogen/dogen/build/output/linux-clang-debug/Testing/20221126-0000/Done.xml
   Submission successful
```

However, nothing comes up in the dashboard. Other builds (both nightly and continuous) display just fine.

[1] https://github.com/MASD-Project/dogen
[2] https://my.cdash.org/index.php?project=MASD+Project+-+Dogen
[3] https://github.com/MASD-Project/dogen/actions/runs/3551520273/jobs/5965773135
#+end_src
*** STARTED Ignore vcpkg path length warning                          :story:
    :LOGBOOK:
    CLOCK: [2022-09-29 Thu 19:45]--[2022-09-29 Thu 19:57] =>  0:12
    :END:

#+begin_example
Building boost-system[core]:x64-windows...
CMake Warning at scripts/cmake/vcpkg_buildpath_length_warning.cmake:4 (message):
  boost-system's buildsystem uses very long paths and may fail on your
  system.

  We recommend moving vcpkg to a short path such as 'C:\src\vcpkg' or using
  the subst command.
Call Stack (most recent call first):
  ports/boost-system/portfile.cmake:3 (vcpkg_buildpath_length_warning)
  scripts/ports.cmake:147 (include)
#+end_example

Clues about path length:

#+begin_example
-- Downloading https://github.com/boostorg/system/archive/boost-1.80.0.tar.gz -> boostorg-system-boost-1.80.0.tar.gz...
-- Extracting source D:/a/dogen/dogen/vcpkg/downloads/boostorg-system-boost-1.80.0.tar.gz
#+end_example

Links:

- [[https://github.com/microsoft/vcpkg/issues/11119][[vcpkg_buildpath_length_warning] Please add advice to enable long paths on
  Windows 10 #11119]]
- [[https://github.com/microsoft/vcpkg/discussions/19141][[vcpkg_buildpath_length_warning] Please add advice to enable long paths on
  Windows 10 #19141]]
- [[https://learn.microsoft.com/en-gb/windows/win32/fileio/maximum-file-path-limitation?tabs=registry][Maximum Path Length Limitation]]
- [[https://github.com/actions/runner-images/issues/1052][MAX_PATH lengths on Windows environment #1052]]

*** STARTED Windows package is broken                                 :story:
    :LOGBOOK:
    CLOCK: [2022-09-20 Tue 08:21]--[2022-09-20 Tue 08:33] =>  0:12
    CLOCK: [2022-09-18 Sun 07:39]--[2022-09-18 Sun 07:47] =>  0:08
    :END:

When we install the windows package under wine, it fails with:

: E0fc:err:module:import_dll Library boost_log-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library boost_filesystem-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library boost_program_options-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library libxml2.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:import_dll Library boost_thread-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found
: 00fc:err:module:LdrInitializeThunk Importing dlls for L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe" failed, status c0000135

This will probably be fixed when we move over to the new way of specifying
dependencies in CMake. Do that first and revisit this problem.

Actually, this did not help. We then used the new VCPKG macro (see links) which
now includes all of boost. We are failing on:

: 00fc:err:module:import_dll Library MSVCP140_CODECVT_IDS.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\boost_log-vc143-mt-x64-1_78.dll") not found
: 00fc:err:module:import_dll Library boost_log-vc143-mt-x64-1_78.dll (which is needed by L"C:\\Program Files\\DOGEN\\bin\\dogen.cli.exe") not found

Notes:

- Check if we are on latest MSVC.

Links:

- [[https://github.com/microsoft/vcpkg/issues/1653][CMake: provide option to deploy DLLs on install() like VCPKG_APPLOCAL_DEPS
  #1653]]
- [[https://gitlab.kitware.com/cmake/cmake/-/issues/22623][InstallRequiredSystemLibraries MSVCP140.dll is missing]]
- [[https://stackoverflow.com/questions/4134725/installrequiredsystemlibraries-purpose][InstallRequiredSystemLibraries purpose]]
- [[https://gitlab.kitware.com/cmake/cmake/-/issues/20228][IRSL should install MSVCP140_CODECVT_IDS.dll]]: CMake versions after 3.16 should
  install this DLL.

*** STARTED Warning on OSX build                                      :story:
    :LOGBOOK:
    CLOCK: [2022-09-30 Fri 10:43]--[2022-09-30 Fri 11:00] =>  0:17
    CLOCK: [2022-09-30 Fri 10:30]--[2022-09-30 Fri 10:37] =>  0:07
    CLOCK: [2022-09-30 Fri 08:19]--[2022-09-30 Fri 08:27] =>  0:08
    CLOCK: [2022-09-29 Thu 22:40]--[2022-09-29 Thu 22:47] =>  0:07
    CLOCK: [2022-09-29 Thu 19:35]--[2022-09-29 Thu 19:44] =>  0:09
    CLOCK: [2022-09-29 Thu 17:40]--[2022-09-29 Thu 17:50] =>  0:10
    CLOCK: [2022-09-29 Thu 08:40]--[2022-09-29 Thu 08:59] =>  0:19
    CLOCK: [2022-09-28 Wed 08:01]--[2022-09-28 Wed 09:00] =>  0:59
    CLOCK: [2022-09-27 Tue 07:48]--[2022-09-27 Tue 07:55] =>  0:07
    :END:

We seem to have a single warning on OSX:

#+begin_example
ld: warning: direct access in function

'boost::archive::basic_text_oprimitive<
    std::__1::basic_ostream<char,
                            std::__1::char_traits<char>
                            >
>
::~basic_text_oprimitive()'

from file

'vcpkg_installed/x64-osx/debug/lib/libboost_serialization.a(basic_text_oprimitive.o)'

to global weak symbol

'std::__1::basic_ostream<
    char, std::__1::char_traits<char>
>&
std::__1::endl<char, std::__1::char_traits<char> >(
    std::__1::basic_ostream<char, std::__1::char_traits<char> >&
)'

from file 'projects/dogen.utility/tests/CMakeFiles/dogen.utility.tests.dir/indenter_filter_tests.cpp.o'

means the weak symbol cannot be overridden at runtime. This was likely caused by
different translation units being compiled with different visibility settings.
#+end_example

The flags that control this behaviour are:

: cxxflags=-fvisibility=hidden
: cxxflags=-fvisibility-inlines-hidden

Compare our settings with Boost.

By removing the current settings for OSX we get over 50 warnings:

: ld: warning: direct access in function 'boost::test_tools::tt_detail::print_log_value<char [48]>::operator()(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const (&) [48])' from file 'projects/dogen.identification/tests/CMakeFiles/dogen.identification.tests.dir/legacy_logical_name_tree_parser_tests.cpp.o' to global weak symbol 'boost::test_tools::tt_detail::static_const<boost::test_tools::tt_detail::impl::boost_test_print_type_impl>::value' from file 'vcpkg_installed/x64-osx/debug/lib/libboost_unit_test_framework.a(framework.o)' means the weak symbol cannot be overridden at runtime. This was likely caused by different translation units being compiled with different visibility settings.

In addition it also causes failures in tests:

: dogen.utility.tests/resolver_tests/resolver_returns_test_data_directory_for_empty_path
: dogen.utility.tests/resolver_tests/validating_resolver_returns_test_data_directory_for_empty_paths

Notes:

- try removing special handling for boost.

#+begin_src markdown
Since every single warning on my debug builds is related to ```~basic_text_oprimitive```, I decided to investigate how this symbol is exported in boost. We start with macro ```BOOST_SYMBOL_VISIBLE``` which is defined as follows [1]:

> Defines the syntax of a C++ language extension that indicates a symbol is to be globally visible. If the compiler has no such extension, the macro is defined with no replacement text. Needed for classes that are not otherwise exported, but are used by RTTI. Examples include class for objects that will be thrown as exceptions or used in dynamic_casts, across shared library boundaries.

This appears sensible enough. We can see ```basic_text_oprimitive``` making use of it [2]:

```c++
// class basic_text_oprimitive - output of prmitives to stream
template<class OStream>
class BOOST_SYMBOL_VISIBLE basic_text_oprimitive
{
```

In GCC [3] this macro is defined as follows:

```
#define BOOST_SYMBOL_VISIBLE __attribute__((__visibility__("default")))
```

In Clang too [4]:

```
 define BOOST_SYMBOL_VISIBLE __attribute__((__visibility__("default")))
```

The general conclusion is that by setting visibility to default we should match the symbols definition. We now turn our attention to the destructor [2]:

```c++
    BOOST_ARCHIVE_OR_WARCHIVE_DECL
    basic_text_oprimitive(OStream & os, bool no_codecvt);
    BOOST_ARCHIVE_OR_WARCHIVE_DECL
    ~basic_text_oprimitive();
```

The macro ```BOOST_ARCHIVE_OR_WARCHIVE_DECL``` is defined as follows:

```c++
    #if defined(BOOST_WARCHIVE_SOURCE) || defined(BOOST_ARCHIVE_SOURCE)
        #define BOOST_ARCHIVE_OR_WARCHIVE_DECL BOOST_SYMBOL_EXPORT
    #else
        #define BOOST_ARCHIVE_OR_WARCHIVE_DECL BOOST_SYMBOL_IMPORT
    #endif
```

The macros ```BOOST_SYMBOL_EXPORT``` and ```BOOST_SYMBOL_IMPORT``` are cousins of BOOST_SYMBOL_VISIBLE. Once more, clang and GCC are identical. GCC [3]:

```c++
#    define BOOST_SYMBOL_EXPORT __attribute__((__visibility__("default")))
#    define BOOST_SYMBOL_IMPORT
```

Whereas Clang says [4]:

```c++
#  define BOOST_SYMBOL_EXPORT __attribute__((__visibility__("default")))
...
#  define BOOST_SYMBOL_IMPORT
```

This means when we are importing, visibility is not defined. We now need to find out if that is a good thing or bad.

[1] https://www.boost.org/doc/libs/master/libs/config/doc/html/boost_config/boost_macro_reference.html
[2] https://www.boost.org/doc/libs/1_80_0/boost/archive/basic_text_oprimitive.hpp
[3] https://www.boost.org/doc/libs/1_80_0/boost/config/compiler/gcc.hpp
[4] https://www.boost.org/doc/libs/1_80_0/boost/config/compiler/clang.hpp
#+end_src

Sent email to boost users.

Actually a really easy way to test this is to hack a script that overwrites this
file in OSX with the fixes and see what happens to the warnings. We can even
leave it in for now until the PR is merged.

We were patching the wrong file it seems, the problem is not with =oarchive=,
its with =oprimitive=.

Links:

- [[https://stackoverflow.com/questions/36567072/why-do-i-get-ld-warning-direct-access-in-main-to-global-weak-symbol-in-this][Why do I get "ld: warning: direct access in _main to global weak symbol" in
  this simple code? [duplicate]​]]
- [[https://stackoverflow.com/questions/8685045/xcode-with-boost-linkerid-warning-about-visibility-settings/11879361#11879361][xcode with boost : linker(Id) Warning about visibility settings]]
- [[https://github.com/Microsoft/vcpkg/issues/4497][Boost linker warnings on OSX #4497]]
- [[https://github.com/boostorg/serialization/issues/265][Strange "direct access" warning on OSX for basic_text_oprimitive #265]]

*** STARTED Use clang format to format the code base                  :story:
    :LOGBOOK:
    CLOCK: [2022-10-07 Fri 08:01]--[2022-10-07 Fri 08:49] =>  0:48
    CLOCK: [2022-10-06 Thu 20:11]--[2022-10-06 Thu 20:16] =>  0:05
    CLOCK: [2022-10-06 Thu 20:05]--[2022-10-06 Thu 20:10] =>  0:05
    :END:

It seems clang-format is being used by quite a lot of people to save
time with the formatting of the code. More info:

- http://clang.llvm.org/docs/ClangFormat.html

Emacs support:

- https://github.com/llvm-mirror/clang/blob/master/tools/clang-format/clang-format.el

Links:

- [[https://github.com/marketplace/actions/clang-format-check][clang-format-check]]: GitHub Action for clang-format checks. Note that this
  Action does NOT format your code for you - it only verifies that your
  repository's code follows your project's formatting conventions. [[https://github.com/search?o=desc&q=uses%3A+jidicula%2Fclang-format-action+-user%3Ajidicula&s=indexed&type=Code][Example
  repos]].
- [[https://github.com/STEllAR-GROUP/hpx/blob/master/.clang-format][HPX clang format]]
- [[https://engineering.mongodb.com/post/succeeding-with-clangformat-part-1-pitfalls-and-planning][Succeeding With ClangFormat, Part 1: Pitfalls And Planning]]
- [[https://github.com/basiliscos/cpp-rotor/blob/master/.clang-format][example: clang format in rotor]]
- [[https://github.com/jbapple-cloudera/clang-format-infer][clang-format-infer GH]]
- [[https://zed0.co.uk/clang-format-configurator/][clang-format-configurator]]
- http://clangformat.com/
- [[https://github.com/johnmcfarlane/unformat][Unformat]]: Python3 utility to generate a .clang-format file from
  example code-base.
- [[https://www.reddit.com/r/cpp/comments/pnli5r/cc_precommit_hooks_for_static_analyzers_and/][C/C++ pre-commit hooks for static analyzers and linters]]
- [[https://github.com/lballabio/QuantLib/blob/master/.clang-format][quant lib]] clang format.
- [[https://github.com/OpenSourceRisk/Engine/blob/master/.clang-format][ORE clang format]]

*** STARTED Add PlantUML relationships to diagrams                    :story:
    :LOGBOOK:
    CLOCK: [2022-11-12 Sat 17:11]--[2022-11-12 Sat 18:40] =>  1:29
    CLOCK: [2022-10-13 Thu 08:00]--[2022-10-13 Thu 08:13] =>  0:13
    CLOCK: [2022-10-12 Wed 19:33]--[2022-10-12 Wed 20:25] =>  0:52
    CLOCK: [2022-10-11 Tue 21:50]--[2022-10-11 Tue 22:17] =>  0:27
    CLOCK: [2022-10-11 Tue 08:29]--[2022-10-11 Tue 08:57] =>  0:28
    CLOCK: [2022-10-10 Mon 19:35]--[2022-10-10 Mon 19:49] =>  0:14
    CLOCK: [2022-10-10 Mon 18:35]--[2022-10-10 Mon 19:10] =>  0:35
    CLOCK: [2022-10-10 Mon 08:47]--[2022-10-10 Mon 08:58] =>  0:11
    CLOCK: [2022-10-10 Mon 08:39]--[2022-10-10 Mon 08:46] =>  0:07
    CLOCK: [2022-10-10 Mon 08:04]--[2022-10-10 Mon 08:38] =>  0:34
    CLOCK: [2022-10-09 Sun 23:27]--[2022-10-09 Sun 23:36] =>  0:09
    CLOCK: [2022-10-09 Sun 23:03]--[2022-10-09 Sun 23:20] =>  0:17
    CLOCK: [2022-10-09 Sun 22:35]--[2022-10-09 Sun 23:02] =>  0:27
    CLOCK: [2022-10-09 Sun 22:20]--[2022-10-09 Sun 22:31] =>  0:11
    CLOCK: [2022-10-09 Sun 09:17]--[2022-10-09 Sun 09:25] =>  0:08
    CLOCK: [2022-10-08 Sat 17:46]--[2022-10-08 Sat 18:12] =>  0:26
    CLOCK: [2022-10-08 Sat 17:30]--[2022-10-08 Sat 17:45] =>  0:15
    CLOCK: [2022-10-06 Thu 19:20]--[2022-10-06 Thu 20:04] =>  0:44
    CLOCK: [2022-10-06 Thu 08:02]--[2022-10-06 Thu 08:55] =>  0:53
    CLOCK: [2022-10-05 Wed 19:09]--[2022-10-05 Wed 19:41] =>  0:32
    CLOCK: [2022-10-05 Wed 18:45]--[2022-10-05 Wed 19:07] =>  0:22
    CLOCK: [2022-10-05 Wed 08:14]--[2022-10-05 Wed 08:56] =>  0:42
    CLOCK: [2022-10-01 Sat 16:39]--[2022-10-01 Sat 16:48] =>  0:09
    CLOCK: [2022-10-01 Sat 15:36]--[2022-10-01 Sat 15:45] =>  0:09
    CLOCK: [2022-10-01 Sat 15:19]--[2022-10-01 Sat 15:25] =>  0:17
    CLOCK: [2022-10-01 Sat 14:50]--[2022-10-01 Sat 15:07] =>  0:17
    CLOCK: [2022-10-01 Sat 13:20]--[2022-10-01 Sat 13:30] =>  0:10
    CLOCK: [2022-10-01 Sat 11:44]--[2022-10-01 Sat 12:09] =>  0:25
    CLOCK: [2022-10-01 Sat 11:32]--[2022-10-01 Sat 11:43] =>  0:11
    CLOCK: [2022-10-01 Sat 11:00]--[2022-10-01 Sat 11:31] =>  0:31
    CLOCK: [2022-09-30 Fri 20:48]--[2022-09-30 Fri 21:14] =>  0:26
    CLOCK: [2022-09-30 Fri 19:42]--[2022-09-30 Fri 20:15] =>  0:33
    CLOCK: [2022-09-30 Fri 19:07]--[2022-09-30 Fri 19:20] =>  0:13
    CLOCK: [2022-09-30 Fri 13:57]--[2022-09-30 Fri 16:04] =>  2:07
    CLOCK: [2022-09-30 Fri 11:41]--[2022-09-30 Fri 12:10] =>  0:29
    CLOCK: [2022-09-30 Fri 11:16]--[2022-09-30 Fri 11:24] =>  0:08
    CLOCK: [2022-09-30 Fri 11:02]--[2022-09-30 Fri 11:09] =>  0:07
    CLOCK: [2022-09-30 Fri 09:00]--[2022-09-30 Fri 09:58] =>  0:58
    CLOCK: [2022-09-19 Mon 11:38]--[2022-09-19 Mon 11:42] =>  0:04
    :END:

We need to go through each and every model and add the relations we add in Dia
to make diagrams more readable. Models:

- dogen: done
- dogen.cli: done
- dogen.codec: done
- dogen.identification: done
- dogen.logical: done
- dogen.modeling: no changes
- dogen.orchestration: done
- dogen.org: done
- dogen.physical: done
- dogen.text: started

Links:

- [[https://github.com/plantuml/plantuml/issues/1187][Class diagrams: attaining a more "square-like" use of space in large diagrams
  #1187]]
- [[https://plantuml.com/class-diagram][Section "Help on layout" in manual]]
- [[https://plantuml.com/elk][Using ELK layout engine]]
- [[https://crashedmind.github.io/PlantUMLHitchhikersGuide/layout/layout.html]["The Hitchhiker's Guide to PlantUML", section 6. "Layout"]]
- [[https://www.augmentedmind.de/2021/01/17/plantuml-layout-tutorial-styles/]["PlantUML layout and styles tutorial"]]
- [[https://isgb.otago.ac.nz/infosci/mark.george/Wiki/wiki/PlantUML%20GraphViz%20Layout]["PlantUML GraphViz Layout"]]

*** STARTED Configuration as stereotype causes noise                  :story:
    :LOGBOOK:
    CLOCK: [2022-11-13 Sun 21:24]--[2022-11-13 Sun 22:44] =>  1:20
    CLOCK: [2022-11-13 Sun 20:58]--[2022-11-13 Sun 21:10] =>  0:12
    CLOCK: [2022-11-13 Sun 20:21]--[2022-11-13 Sun 20:37] =>  0:16
    CLOCK: [2022-11-13 Sun 10:02]--[2022-11-13 Sun 10:23] =>  0:21
    :END:

At present we have very large classes (in terms of width) because they have
configuration associated with them as stereotypes. This is a particular problem
in the text model. Nothing stops us from having a separate way of handling
configuration - for example a different property which is not a stereotype. It
could be expressed differently in PlantUML - perhaps a separate section as per
"Advanced class body". We could name the section "Configuration" or "Profiles".

Notes:

- at present we have several different "kinds" of information in the stereotypes
  field:
  - the meta-type (e.g. enumeration, object, etc). This is probably the most in
    keeping with UML's notion of stereotypes.
  - the associated object templates used by the class.
  - the associated configurations.

  We could have two fields for each of these (e.g. templates, configurations)
  and then combine them all as stereotypes in logical model. This allows us to
  express them as different groups within PlantUML.
- we should express =masd::object= in the UML diagrams even though its the
  default. This would make diagrams clearer.
- we could create a named section for enumerators, fields, etc.
- we could express the type of an enumeration, if supplied.
- we could express the type of a primitive, if supplied.
- meta information could appear in a group called "meta-information".
- consider using =struct= or =entity= for =masd::object= and =annotation= for
  =masd::object_template=.
- if class is abstract, use =abstract=.
- check why feature model is not available on codec to codec transform and see
  how hard it is to get it.

*** STARTED Update CMakeLists to match latest                         :story:
    :LOGBOOK:
    CLOCK: [2022-11-19 Sat 14:59]--[2022-11-19 Sat 15:00] =>  0:01
    CLOCK: [2022-11-19 Sat 14:07]--[2022-11-19 Sat 14:30] =>  0:23
    CLOCK: [2022-11-19 Sat 12:13]--[2022-11-19 Sat 12:34] =>  0:21
    :END:

We have modified locally the CMakeLists to match the modern approach, but we
never updated the templates. As part of doing this, we should remove ODB
support. This is because:

- we don't use ODB at present;
- when we do look into ODB again, it will be done as part of a cartridge
  framework rather than via build files.

Actually this is a big ask. We have a lot of missing requirements in order to do
this:

- component type: library or executable. However, if its executable, we are
  still building a library and we need to supply dependencies for both.
- missing parts: we need a part for modeling and another for generated tests.
- features: we need a templatised feature which expands across the parts. The
  feature will carry dependencies.
- the dependency needs to have the following information:
  - include: public or private
  - standard dogen model or exogenous?
  - link
- official location for generated files:
  : PRIVATE ${stage_inc_dir}/ # generated header files

One possible approach is to create a model element for references which contains
all of the required information. Example:

: * some reference                                      :reference:
:   :PROPERTIES:
:   :masd.codec.reference: dogen.tracing
:   :masd.logical.reference.type: public
:   :masd.logical.reference.link: boost::boost
:   :END:
:
: [[../../dogen.tracing/modeling/dogen.tracing.org]]

Notes:

- we do not want to handle transitive references in this way; from a dogen
  perspective we want to load these models, but from a code generation
  perspective we do not want to add references recursively. We want instead to
  rely on transitivity.

*** STARTED Thoughts on refactoring variability                       :story:
    :LOGBOOK:
    CLOCK: [2022-11-29 Tue 08:54]--[2022-11-29 Tue 09:16] =>  0:22
    :END:

Originally, we introduced tagged values in Dia because we needed to add
meta-data to types which was not directly supported by the tool. We soon
extended it to all sorts of annotations. But now that we are no longer
constrained to Dia, we need to revisit this decision. Fundamentally, there are
two kinds of datum modeled as features:

- data which has a functional dependency on the geometry of physical space; and
- data which does not.

The first case involves the use of templates which expand over physical space,
and this cannot be avoided (e.g. whether a facet is =enabled= or not). The
second case however is quite trivial. In fact, org-mode does not suffer from the
same limitations as Dia; one can add all necessary properties as tags, and these
can be deserialised (manually?) into what we call the codec model at present. In
this particular case, variability is a bit of an overkill: we know precisely
what needs to be read, and where to put it. We could simply add logic around
codec object creation to read these properties in.

Notes:

- remove all Dia support. Delete frozen.
- remove all JSON support.
- update all reference products to use org-mode only.

*** Add support for MSVC with CL                                      :story:

At present this build is broken and GitHub does not support allow failures. The
following was removed from the workflow:

#+begin_src yaml
          # FIXME: can't use this until allow_failures is added to github.
          # See https://github.com/actions/toolkit/issues/399
          # - name: windows-msvc-clang-cl
          #   os: windows
          #   family: windows
          #   compiler: msvc-clang-cl
          #   experimental: true
          ...
          if ("${{matrix.experimental}}" -eq "true")
          {
              $build_group = "Experimental"
          }
#+end_src

*** Consider exporting =WITH_FULL_GENERATION= in CMakeFile            :story:

At present we have to do two things for full generation:

#+begin_src yaml
      - name: Run CTest workflow
        env:
          WITH_FULL_GENERATION: ON
          preset: ${{matrix.family}}-${{matrix.compiler}}-${{matrix.buildtype}}
#+end_src

And:

#+begin_src yaml
        run: |
          export DOGEN_BUILD_PROVIDER="github"
          export DOGEN_BUILD_COMMIT="${GITHUB_SHA}"
          export DOGEN_BUILD_NUMBER="${GITHUB_RUN_NUMBER}"
          export DOGEN_BUILD_TIMESTAMP=`date "+%Y/%m/%d %H:%M:%S"`
          export cmake_args="build_group=Nightly,build_name=${preset},with_full_generation=ON"
#+end_src

This is because the unit tests need the environment variable to pass and the
code generator needs the CMake parameter. However, it does not make sense to use
these in isolation. We should probably export the environment variable from
within CTest or CMake.

Actually the situation is even more confusing:

- we need to run =with_full_generation=ON= for the original build;
- but it must be =OFF= for the fullgen build, else we just recreate the branch.

*** Mapping of third-party dependencies (PDMs)                        :story:

System models should follow the physical structure of dependencies. That is, we
should not have a "boost" system model, but instead a boost-test etc. Each of
these can then have mappings (e.g. vcpkg name, build2 name, etc). Users must
declare these references just like they do with user models. Dogen can then
create code for:

- cmake targets, properly linking against libraries;
- vcpkg install, at product level, by de-duplicating component dependencies;
- possibly distro dependencies.

We should only have a mandatory dependency, which is the STL. In addition, we
need different models for each version (e.g. c++ 03, etc). This makes it easier
to include the right types.

Note that each model must have an associated version. The version should be part
of the file name. However, maybe we need to distinguish between TS version (11,
17, etc) from library version.

One way of solving this regularity problem (e.g. having masd::std::string is the
real name but its annoying to have to have users typing this) is to support
"using" statements at the model level. If a user could type something like
=using masd= and this would allow us to find all types as if we had typed
=masd::= then users could still type =std::string= and find =masd::std::string=.
We'd solve both the regularity and the "look and feel". The downside is that
this could have important ramifications in resolution:

- how does this work in the presence of merged models? do we merge usings? will
  this find types we're not supposed to find?

Needs some more thinking.

Notes:

- utility model must be split into multiple models: the PDMs should go with
  their own models. The real utilities should stay on a utility product. This
  can then be reused by dogen (for example, if logging is enabled it uses the
  logging setup from this model). Users should be able to switch it off though.

Merged stories:

*Platform description models*

We should consider integrating all of the information regarding "platforms" into
platform description models (PDMs). These include:

- all types available in a library (proxy models) and their mapping in terms of
  aspects. We could make the mapping a bit clearer by designing platform
  description models that are not part of modeling. We don't really need support
  for attributes, operations etc. However, we need some kind of "adaptor" that
  extracts all the type information (or some other way of making resolution work
  across different model types).
- name of the library, supported language (e.g. for Boost, C++ etc), versions of
  the library. We must associate the types with a version (e.g. introduced on
  version X, deprecated on version Y) so that when the user is using a given
  version it errors if a type is not available.
- packaging support: mapping to the name used on most common packaging systems
  such as DEB, RPM etc. Also, mapping on language specific package managers such
  as build2, vcpkg, conan, nuget, etc. User can decide what package manager to
  use overall or for a specific library (e.g. possible to mix-and-match package
  manager). It should also have a mapping for CMake support that includes "in
  CMake library from version X" and "available on an external source" with a
  URL. If the user selects the latter, Dogen can download the CMake file (or
  maybe Dogen should include the external CMake files to guarantee a stable
  behaviour).

Notes:

- with this we can now move away from the bad modeling used with proxy models,
  where it was not quite clear what they were. We can create a different file
  format (e.g. *.pdm* )that can share some similarities with existing JSON
  models but is not stuck with all the baggage needed to represent user models.
- however, PDMs are just regular models. Users should be able to make use of the
  PDM stereotypes to define PDM types in a regular UML model. Interestingly, a
  PDM from this perspective is similar to a named configuration. The difference
  is that it introduces a new type into the type system rather than a new
  stereotype. But with this we can now make use of named configurations, making
  PDMs a lot less verbose (e.g. define a named config and share it across
  multiple types, like SmartPointer etc). We could even share it across multiple
  platform models!
- PDMs should be organised by language (e.g. folder for cpp, etc). Top level
  directory is PDM rather than library.
- users declare references to PDMs just like normal models. PDMs have an
  attribute for "auto-loading". If on, we load regardless. Else it must be
  referenced. This is useful for say STL, hardware types.
- users can supply their own PDMs, although they are encouraged to submit them
  if they are general enough.
- if users choose CMake as the build system, we automatically add all the
  boilerplate required to add the library to CMake. In an ideal world, the PDM
  should contain the CMake snippets, with "macros" where required (e.g. version,
  etc). This would mean we wouldn't have to change templates when new libraries
  are added. We could also have a "standard" CMake snippet that works for most
  libraries.
- different libraries may have different types of support (e.g. a library may
  not exist in a package manager, etc)
- we could now create an installation script that sets up all dependencies (e.g.
  DEB/RPM). It cannot be a target because the build would fail (e.g. CMake would
  not find all dependencies).
- now that =library= can be renamed to =pdm=, we could call the top-level
  directory =library=.

*Adding linking libraries is not handled*

#+begin_quote
*Story*: As a dogen user, I want to link against libraries without having to
manually generate CMakeFiles.
#+end_quote

At present whenever a model requires additional link library targets we need to
disable CMake generation and do it by hand. However:

- for well-known dependencies such as boost we could create a convention (e.g.
  assume/require that the CMake boost libraries flags are set via find boost).
  Alternatively, the types should contain meta-data that has information about
  linking requirements; e.g. if you use a type from a boost model, it should
  provide you with linking information in its meta-data. Each boost type could
  have different information depending on which boost library they come from.
- for user level dependencies we should add dynamic extensions at the model
  level. Also, references provide sufficient information to link against other
  dogen models.


*** Model "types" and element binding                                 :story:

It seems clear that we will have different "types" of models:

- product models, describing entire products.
- component models, which at present we call "models". These describe a given
  component type such as a library or an executable. Thus, they themselves have
  sub-types.
- profile models: useful to keep the configuration separate. However, it may
  make more sense to place them in the product model, since its shared across
  components?
- PDMs: these describe platforms.
- generative models: generate extensions to code generators.

At present there is no concept of model types, so any meta-model element can be
placed in any model. This is convenient, but in the future it may make things
too complicated: users may end up placing types in PDMs when they didn't meant
to do so, etc. What seems to emerge from here is that, just as with variability,
there is a concept of a binding point at the model level too. That is,
meta-model elements are associated with specific model types (binding element?).

In an ideal world, we should have a class in the meta-model that represents each
model type. We then instantiate this class within one of the dogen models to
register the different model types. Its code-generation representation is the
registration. It also binds to all the meta-model elements it binds to. This can
be done simply by creating a feature that lists the stereotypes of the elements
(remember that these are then registered too, because we will generate the
meta-class information as we generate the assets model). Then, we can ask the
model type if a given element is valid (check a set of stereotypes).

Formatters are themselves meta-model elements, and they bind to other meta-model
elements (which raises the question: which meta-model elements are bindable? we
can't allow a formatter to bind to a formatter...). Perhaps we need another type
of model, which is a "generation model". This is where we can either declare new
technical spaces or add to existing technical spaces; and declare new facets and
formatters. We should be able to add to existing facets and TSs by allowing
users to specify the TS/facet when declaring the formatter. If not specified,
then the user must declare a facet in the package containing the formatter.
Similarly with TSs.

Note also that the formatter binding code is "inserted" directly during
generation into the CPP file. Its not possible to change it. Same with the
includes. This ensures the user cannot bypass the model type system by mistake.
Also, by having a formatter meta-model type, we can now declare the header file
as we please, and ensure the shape of the implementation. Now, the stitch
template can be restricted to only the formatting function itself; the rest is
code-generated. We no longer need wale templates. This will of course require
the move to PDMs and the removal of the helper code. This also means that anyone
can declare new meta-model elements; they will register themselves, and
correctly expand across archetype space. However, we do not have the adaption
code nor do we have containers for these modeling elements. We need a separate
story for this use case.

Destinations are meta-model elements too. In the generation.cpp model we will
declare all the available destinations:

- global
- src
- include
- tests

etc. The formaters bind into destinations. Formatters belong to facets in the
archetype space, which express themselves as directories in the artefact path
when we project from archetype space into artefact space. More generally: assets
in asset space are projected into the multidimensional archetype space.
Archetypes are projected into artefact space, but the dimensions of archetype
space are flattened into the hierarchy of the filesystem.

We also need a concept of artefact types. These mainly are needed for file
extensions, but conceivably could also be used for other purposes.

Notes:

- the binding should be done at the streotype level, not model element.

Merged stories:

*Model types and element binding*

Once we introduce the concept of model types (e.g. product, component, possibly
different types of components), we should also take into account that some model
types don't support some model elements. It should be possible to declare which
model types support which element types such that if a user tries to use an
invalid model type, we get a sensible error. In effect, this is a binding
problem at the meta-model level.

One of the component model types should be "generative components". These are
used to augment the code generator. We need to look at the thesis on generative
development to look for terminology.

We should also have model types for terraform, etc.

One way to implement this is to add a feature at the element (once we have a way
of expressing dogen meta-elements) that lists the supported model types for a
given element.

*logical models could have a model classification*

Consider creating an enumeration for model classification (e.g. type of the
model):

- relational model
- core domain model
- generic sub-domain model
- segregated core model

This still requires a lot of analysis work. This is kind of a model level
stereotype which can be used by the code generator for example to determine
which models are compatible. It could also be used to determine what facets can
be enabled/disabled.

*Merged with modes of operation story:*

Create "modes" of operation: relational, object-oriented and procedural. they
limit the types available in yarn. relational only allows built-ins plus
relational commands (FK and PK; FK is when using a model type, PK is a marker on
a property). procedural only allows built-ins plus model types. we will need
pointer support for this. object oriented is the current mode. the modes are
validated in the middle end.

*** Make executable a trivial target                                  :story:

In order to simplify the make targets, we should make the executable just a
simple invocation of a class within the library; this would then mean we just
need to link against the library for the component, making CMake infrastructure
trivial.

*** Create a standard directory for generated files                   :story:

Dogen projects will require at least one generated file for the version. We
should have a standard way of referring to that directory, e.g.:

: DOGEN_GENERATED_HEADER_DIR

This directory should be private.

Notes:

- how do we know which components need access to this directory?

*** org-model and links to references                                 :story:

At present we are adding links as part of the drawers in org-models. However,
this is not ideal. We want to use regular org-mode constructs. There are several
use cases for links:

- we want org-mode links so that we can make use of tools such as org-roam.
- we need to use the links for dogen to resolve the model. Both of these use
  cases happen whilst the code is in its normal shape in the git repository.
- when we generate doxygen documentation we need links that can work when the
  site has been published. At this point the models have all been copied to a
  different location by doxygen.

We need to find out a way to make links work for all of these use cases.

Merged stories

*Add references as links to org documents*

Try adding a =references= tag. Content is a list of links to org models.
However, because of the way our referencing works in dogen, we need to do some
kind of hack. Perhaps the "text" of the link could be the simple path to the
file and the link the relative path. To start off with, it can be ignored and
managed manually. This will be spun into its own story for the future.

*** Generated CMakefiles do not take tests into account               :story:

At present most models have:

: masd::build::cmakelists, dogen::handcrafted::cmake

One of the reasons is because the template is not taking generated
tests into account. It should have:

: if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/generated_tests)
:    add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/generated_tests)
: endif()

*** =CMakeFiles= do not reference dogen models                        :story:

At present we cannot test cross-model referencing because our
CMakeFiles are not adding the linking references to these models. This
needs to be fixed before we can test cross model serialisation.

Notes:

- in order to map references to models, we need to create a modeling
  element for a reference. For this we have two cases: for proxy
  models/PDMs, we need to read from the meta-data the name of the lib
  the model generates. For dogen models we can create it from the
  model name.
- this is a variation of the "exports and imports" pattern: we import
  a set of libraries (these can either be macros or actual library
  names) and we export (for now) a single library. When we support
  facets in libraries, we may need to export more than one, so we
  should cope with this scenario now. We need to keep track of the
  exports for a reference, and then use those as the imports for the
  model.
- in an ideal world, all imports come via this mechanism. However,
  this means we now have to create PDMs/proxies just to setup the
  imports. For example, for LibXML we will not need to define any of
  the types, but we need the import. However, If we do force the
  definition of the PDM, the advantage is that we now have the right
  place to put the definition, and is done only once and shared by all
  models.

*** Add operations to PlantUML diagrams                               :story:

At present its not possible to add operations to PlantUML diagrams. This means
the diagrams are not as useful as the Dia representation. We could add basic
support for operations and then dump these in PlantUML.

This can be done by adding operations all the way to the logical model, but then
not expressing them. Or we can just stop at the codec model and then create a
story for the rest of the work.

Merged stories:

*Consider adding operations for PlantUML*

We don't need full operation support for it to be useful. We could
have minimal support and use it to update the UML diagrams. It would
then later on be extended for merging code generation.

*** Add grouping for top-level classes in PlantUML                    :story:

It is possible to group classes that have no relationship with =together=.
Experiment with this to see if it improves the layout for top-level classes.

#+begin_src plantuml
together {
  class Together1
  class Together2
  class Together3
}
#+end_src

Links:

- [[https://plantuml.com/class-diagram][See "Help on layout" section]]

Merged stories:

*Consider creating a "top-level" group in PlantUML*

There are a number of classes that always show up at the top:

- registrar
- cmakelists
- main
- etc

These could also be part of a top-level group.

We could also have a group for logic-less templates if they are at the
top-level.

*** Consider adding support for archimate                             :story:

#+begin_quote
ArchiMate is an open and independent enterprise architecture modeling language
to support the description, analysis and visualization of architecture within
and across business domains in an unambiguous way.

ArchiMate offers a common language for describing the construction and operation
of business processes, organizational structures, information flows, IT systems,
and technical infrastructure. This is just like an architectural drawing in
classical building where the architecture describes the various aspects of the
construction and use of a building. This insight helps the different
stakeholders to design, assess, and communicate the consequences of decisions
and changes within and between these business domains.
#+end_quote

Links:

- [[https://plantuml.com/archimate-diagram][PlantUML: Archimate Diagram]]
- [[https://github.com/plantuml-stdlib/Archimate-PlantUML][GH: Archimate-PlantUML]]

*** Add links to PlantUML diagrams                                    :story:

PlantUML supports org-mode style links in diagrams, e.g.:

#+begin_src
[[http://plantuml.com]]
#+end_src

Links:

- [[https://plantuml.com/link][PlantUML: Format definition]]

*** Consider renaming assemblers with weavers                         :story:

Weaving is the MDE term. We have a few transforms with "assembly" on the name,
we should rename these or at least clarify the difference between assembling and
weaving.

*** Consider adding support for clang-tidy                            :story:

As [[http://clang.llvm.org/extra/clang-tidy/][per docs]]:

#+begin_quote
clang-tidy is a clang-based C++ “linter” tool. Its purpose is to provide an
extensible framework for diagnosing and fixing typical programming errors, like
style violations, interface misuse, or bugs that can be deduced via static
analysis. clang-tidy is modular and provides a convenient interface for writing
new checks.
#+end_quote

As with clang-format, we should create a meta-model element to generate this
file.

See also:

- [[https://github.com/lballabio/QuantLib/blob/master/.github/workflows/tidy.yml][QL clang tidy GH workflow]]
- [[https://github.com/polysquare/clang-tidy-target-cmake][clang-tidy-target-cmake]]
- [[https://www.kdab.com/clang-tidy-part-1-modernize-source-code-using-c11c14/?utm_source%3DMaster%2520List%252006-16&utm_campaign%3Dd11fea20e3-EMAIL_CAMPAIGN_2017_03_23&utm_medium%3Demail&utm_term%3D0_bdde4cdc11-d11fea20e3-101553725&goal%3D0_bdde4cdc11-d11fea20e3-101553725][Clang-Tidy, part 1: Modernize your source code using C++11/C++14]]
- [[https://github.com/STEllAR-GROUP/hpx/blob/master/.clang-tidy][hpx clang tidy file]]
- [[https://github.com/lballabio/QuantLib/pull/1369][Split .clang-tidy into one for fixes and one for checks #1369]]

*** Split pre-assembly chain                                          :story:

We have several chains inside a massive chain. Notes:

- initial steps (variability, mappings, dynamic stereotypes). This is a chain on
  its own right.
- a top-level chain that processes model sets.
- the pre-assembly chain then calls these two sub-chains.

*** Add support for titles in PlantUML                                :story:

As per docs:

#+begin_quote
Title

The title keywords is used to put a title. You can add newline using \n in the
title description.
#+end_quote

Consider also:

#+begin_quote
Footer and header

You can use the commands header or footer to add a footer or a header on any
generated diagram.

Legend the diagram

The legend and end legend are keywords is used to put a legend.
#+end_quote

Links:

- https://plantuml.com/commons

*** Allow moving PlantUML documentation                               :story:

At present the documentation is always generated at the top of an element:

#+begin_src plantuml
    note top of references_resolver
Trivial struct that records all the data we need for our little DFS excursion.
    end note
#+end_src

However, in many cases, there are better places to put the docs. It would be
nice if we could supply a hint to the codec.

*** Codec ID validation                                               :story:

We are now relying on the codec ID for a lot of things. We need to make sure:

- it has been supplied for all elements;
- it is unique across a model.

In the future we should also check its unique across all loaded models.

*** Core generation rules                                             :story:

These are some ideas that need further work. We seem to have figured out a
couple of core principles related to generation:

- all artefacts must be modeled. We should not ignore anything. If it requires
  ignoring, we must ignore it at the model level - e.g. we should not use things
  such as ignore regexes. Corollary: dogen models must be modeled.
- do not have more than one representation of the same data. For example, take
  protected regions; we can't have these on the output files and on the model
  itself. This is because if we do we now have a synchronisation problem. The
  use case is to edit the protected region from the file itself. The only
  problem with this approach is that one cannot take the model and regenerate
  everything. For this we could have a rule - the region in the model always
  reflects the last run. That is, the region in the model is read-only and will
  be updated with generation.
- it would be extremely useful to have a list of files in the model which is
  updated after each generation. This would allow us to jump quickly from model
  elements to files. It would also allow us to use org-roam.
- models should refer to documentation for detailed explanation, via org-roam.
- there should only be one generation pass. At present we rely on a multi-pass
  approach for generation. For example, we may generate some code using Dogen,
  then use ODB to generate more code and so on. The right approach is to have a
  cartridge for ODB and a way to model the expected artefacts coming out of ODB
  (or perhaps detect them post-facto?). This means the build system only has to
  call Dogen. Note that cartridges can call cartridges and so on. We need a way
  to model this chain of calls as a model element. A cartridge such as protobuf
  may generate many files. We need a way to capture this.
- a model element may be associated with a directory which then has a model:
  product models make use of this feature to contain component models. This
  should also allow for adding new component models: dogen creates the directory
  structure and the empty component model.
 - models have an associated "type" which determines the valid set of logical
   entities that can be instantiated within the model, and by implication the
   set of projections onto physical space.
- it should be possible to generate a model either at the product level or at
  the component level.
- when working at a higher level than the product, org-roam is used to weave
  knowledge across the product set. But should there be a higher-level model
  above product? This would allow for the creation of the correct filesystem
  structure for products, as well as cater for documentation specific to this
  higher level entity.

*** Consider using templates for codec conversion                     :story:

At present we are generating org-mode and PlantUML output by hand. It seems
likely using templates would be an improvement. However, we should probably do
this after we move away from stitch and into mustache. This will also make life
easier because stitch may pull in unnecessary dependencies.

*** Ignorable headlines in org-mode                                   :story:

It should be possible to add documents that do not get expressed as source code
in models, but still come out in the documentation. At present it seems any
headline which does not have a MASD attribute is ignored.

There are two use cases:

- adding content that is intended only for documentation purposes. For this we
  can make use of the MASD attributes (or its absence).
- ignoring a MASD element temporarily. For this we can remove manually the
  attribute?

Notes:

- apparently org-mode supports headlines marked with =COMMENT=.

Links:

- [[https://www.reddit.com/r/emacs/comments/lxwo61/quick_tip_for_those_with_a_literate_emacs_config/][reddit: Quick tip for those with a literate emacs config]]

*** Add support for notes and comments in models                      :story:

In dia we used to use UML notes to add additional commentary in diagrams. We can
still do that in PlantUML:

#+begin_src plantuml
@startuml
class Foo
note left: On last defined class

note top of Object
  In java, <size:18>every</size> <u>class</u>
  <b>extends</b>
  <i>this</i> one.
end note

note as N1
  This note is <u>also</u>
  <b><color:royalBlue>on several</color>
  <s>words</s> lines
  And this is hosted by <img:sourceforge.jpg>
end note
@enduml
#+end_src

We just need a model element that propagates into the logical model and gets
converted into PlantUML output:

#+begin_example
 * object note :note:

  In java, <size:18>every</size> <u>class</u>
  <b>extends</b>
  <i>this</i> one.
#+end_example

Links:

- [[https://plantuml.com/class-diagram#59c91a18bcc97bb0][Class diagram: notes]]

*** Upload release to github on tags                                  :story:

At present we are manually uploading binaries on a release to github. It would
be nice to integrate this with CI.

This has been added to workflow, check at the end of the sprint if it works.

Links:

- [[https://gist.github.com/stefanbuck/ce788fee19ab6eb0b4447a85fc99f447][upload-github-release-asset.sh]]
- [[https://developer.github.com/v3/repos/releases/#upload-a-release-asset][Upload a release asset]]


*** Add model name as title in PlantUML diagram                       :story:

At present its not possible to know the name of the model by looking at the SVG.
We should have a comment with the title. It could be added to the existing model
level comment with documentation.

*** Fix PlantUML indentation                                          :story:

At present if we indent a diagram in emacs we get a different
indentation compared to the generated one. We need to either get emacs
to indent correctly, or copy the emacs indentation. The main problem
at present is that we are indenting classes correctly in generated
code:

#+begin_src plantuml
namespace entities #F2F2F2 {
    class section #F7E5FF {
        +{field} blocks std::list<block>
    }
}
#+end_src

Whereas emacs does not:

#+begin_src plantuml
@startuml

namespace entities #F2F2F2 {
    class section #F7E5FF {
        +{field} blocks std::list<block>
    }
}
@enduml
#+end_src

Note that the behaviour seems different in org-babel. Note also that
notes have incorrect indentation (text model, inside a namespace):

#+begin_src plantuml
note top of  transformation_error
An error occurred whilst applying a transformation.
end note
#+end_src

Note also the extra space before exception name, as well as the
missing indentation inside the note.

Actually what is causing the problem with emacs indentation is the use
of top-level notes:

#+begin_src plantuml
note as N1
Contains all of the M2T transforms for all supported backends.
end note
#+end_src

We need to investigate why this causes indentation problems. Seems
like its valid syntax.

Note also that all comments are not indented correctly.

*** Create a local resolver in codec to support PlantUML use case     :story:

We can create most of the links in PlantUML via local resolution. We
could create a local resolver, which only looks at types for the
current model. It lives inside of codec. For those types, if it
resolves, we can create the PlantUML link. In addition, we could also
resolve operations in the same way (once they have been
modeled).

Ideally, we should implement this resolve in such a way that it can be
used for merged models and stand alone models. We created a story on
the new approach for the resolver; we need to have a look at that and
see if it can be implemented as part of this work. For example, we
could flatten all names prior to calling resolver; use a GUID against
each type, read from custom ID in org mode.

The resolver needs to be primed with all of the existing model names
and namespaces, without depending on the qualified names data
structure. It should have its own data structures. It could live in
=identification=. The output of the resolver should be the GUID of the
type the name points to, or nothing if it could not resolve.

In the calling models, we need a transform that decomposes a type into
the names it references. Then, for each name, we call the resolver.

Notes:

- we could also create two step resolution. We could resolve all local
  names first, and mark them as resolved (for example by adding the
  UUID of the resolved type to the name) and leaving unresolved types
  unmarked. Then, the second merged model resolution would only
  resolve types which are not yet resolved. This would probably speed
  things up because we may end up with smaller containers.

*** Fix PlantUML namespace comments                                   :story:

At present the comments appear as a link to the namespace. We should
try to do whatever it is we did for the model comment, which looks
like a regular note at the top of the namespace.

In addition, sub-namespaces seem to generate both a class with the
comment as well as the namespace itself (see text model).

*** Consider adding PlantUML verbatim inside element                  :story:

One simple way of adding operations is to extend verbatim to support inside
class. This can easily be achieved via =.before=, =.inside= and =.after= fields.
The =.inside= can be replaced by operations later.

*** Empty classes do not need brackets in PlantUML                    :story:

At present we are still adding start and end brackets to empty classes:

#+begin_src plantuml
class string_to_document_transform <<dogen::handcrafted::typeable>> #FFFACD {
}
#+end_src

This takes space for no reason. We could just not print those brackets.



*** Make use of association relationships                             :story:

When we start having to create elements such as visitor etc., it would
be nice to rely on the association between visitor and visitable to
figure out what the visitor is visiting. This and other simple cases
can be inferred simply by looking at the end points of the
association. However, we should still allow supplying this
meta-parameter as meta-data because it may not be practical to have
the association. And we need a way to express this in JSON as well.

*** Consider replacing the associations against object templates      :story:

Object templates are really a higher level concept when compared to
objects, etc. We should not be using associations to denote the notion
of an object instantiating an object template. Perhaps the "implements
an interface" relationship is more appropriate. Check the UML books.

*** Make =parent= feature a CSV collection                            :story:

At present we declare multiple parents like so:

:    :masd.codec.parent: entities::Taggable, entities::Stereotypable, entities::Nameable, entities::Configurable, entities::DeterminableOrigin, entities::TaggableOverridable, entities::Commentable

This is a remnant of the Dia stereotypes field, which was one long CSV string.
However, in the Dia world it makes more sense for us to have:

:    :masd.codec.parent: entities::Taggable
:    :masd.codec.parent: entities::Stereotypable
:    :masd.codec.parent: entities::Nameable
:    :masd.codec.parent: entities::Configurable
: ...

This would make the org-mode document more readable. For this to work, we
probably just need to:

- make the field a CSV collection to allow for the transition without breaking
  anything.
- add processing in codec to handle the collection.



*** Consider renaming plantuml property                               :story:

In the previous sprint we introduced =masd.codec.plantuml=. this name is
questionable, given that the codec name is =plantum=. Consider renaming it.
Suggestions:

: masd.codec.plantuml.verbatim

*** Add full and relative path processing to PM                       :story:

We need to be able to generate full paths in the PM. This will require access to
the file extensions. For this we will need new decoration elements. This must be
done as part of the logical model to physical model conversion. While we're at
it, we should also generate the relative paths. Once we have relative paths we
should compute the header guards from them. These could be generalised to
"unique identifiers" or some such general name perhaps. That should be a
separate transform.

Notes:

- we are not yet populating the archetype kind in archetypes so we cannot locate
  the extensions. Also we did not create all of the required archetype kinds in
  the text models. The populating should be done via profiles.
- we must first figure out the number of enabled backends. The meta-model
  properties will always contain all backends, but not all of them are enabled.
- we need to populate the part directories. For this we need to know what parts
  are available for each backend (PMM), and then ensure the part properties have
  been created. We also need a directory for the part in variability. It is not
  clear we have support for this in the template instantiation domains - we
  probably only have backend, facet, archetype.
- guiding principle: there should be a direct mapping between the two
  hierarchical spaces: the definition meta-model of the physical space and its
  instances in the file-system.

Merged stories:

*Map archetypes to labels*

We need to add support in the PMM for mapping archetypes to labels. We may need
to treat certain labels more specially than others - its not clear. We need a
container with:

- logical model element ID
- archetype ID
- labels

*Implement locator in physical model*

Use PMM entities to generate artefact paths, within =m2t=.

*Create a archetypes locator*

We need to move all functionality which is not kernel specific into yarn for the
locator. This will exist in the helpers namespace. We then need to implement the
C++ locator as a composite of yarn locator.

*Other Notes*

At present we have multiple calls in locator, which are a bit ad-hoc. We could
potentially create a pattern. Say for C++, we have the following parameters:

- relative or full path
- include or implementation: this is simultaneously used to determine the
  placement (below) and the extension.
- meta-model element:
- "placement": top-level project directory, source directory or "natural"
  location inside of facet.
- archetype location: used to determine the facet and archetype postfixes.

E.g.:

: make_full_path_for_enumeration_implementation

Interestingly, the "placement" is a function of the archetype location (a given
artefact has a fixed placement). So a naive approach to this seems to imply one
could create a data driven locator, that works for all languages if supplied
suitable configuration data. To generalise:

- project directory is common to all languages.
- source or include directories become "project sub-directories". There is a
  mapping between the artefact location and a project sub-directory.
- there is a mapping between the artefact location and the facet and artefact
  postfixes.
- extensions are a slight complication: a) we want to allow users to override
  header/implementation extensions, but to do it so for the entire project
  (except maybe for ODB files). However, what yarn's locator needs is a mapping
  of artefact location to extension. It would be a tad cumbersome to have to
  specify extensions one artefact location at a time. So someone has to read a
  kernel level configuration parameter with the artefact extensions and expand
  it to the required mappings. Whilst dealing with this we also have the issue
  of elements which have extension in their names such as visual studio projects
  and solutions. The correct solution is to implement these using element
  extensions, and to remove the extension from the element name.
- each kernel can supply its configuration to yarn's locator via the kernel
  interface. This is fairly static so it can be supplied early on during
  initialisation.
- there is still something not quite right. We are performing a mapping between
  some logical space (the modeling space) and the physical space (paths in the
  filesystem). Some modeling elements such as the various CMakeLists.txt do not
  have enough information at the logical level to tell us about their location;
  at present the formatter itself gives us this hint ("include cmakelists" or
  "source cmakelists"?). It would be annoying to have to split these into
  multiple archetypes just so we can have a function between the archetype
  location and the physical space. Although, if this is the only case of a
  modeling element not mapping uniquely, perhaps we should do exactly this.
- However, we still have inclusion paths to worry about. As we done with the
  source/include directories, we need to somehow create a concept of inclusion
  path which is not language specific; "relative path" and "requires relative
  path" perhaps? These could be a function of archetype location.

Merged stories:

*Generate file paths as a transform*

We need to understand how file paths are being generated at present; they should
be a transform inside generation.

*Create the notion of project destinations*

At present we have conflated the notion of a facet, which is a logical concept,
with the notion of the folders in which files are placed - a physical concept.
We started thinking about addressing this problem by adding the "intra-backend
segment properties", but as the name indicates, we were not thinking about this
the right way. In truth, what we really need is to map facets (better: archetype
locations) to "destinations".

For example, we could define a few project destinations:

: masd.generation.destination.name="types_headers"
: masd.generation.destination.folder="include/masd.cpp_ref_impl.northwind/types"
: masd.generation.destination.name=top_level (global?)
: masd.generation.destination.folder=""
: masd.generation.destination.name="types_src"
: masd.generation.destination.folder="src/types"
: masd.generation.destination.name="tests"
: masd.generation.destination.folder="tests"

And so on. Then we can associate each formatter with a destination:

: masd.generation.cpp.types.class_header.destination=types_headers

Notes:

- these should be in archetypes models.
- with this we can now map any formatter to any folder, particularly if this is
  done at the element level. That is, you can easily define a global mapping for
  all formatters, and then override it locally. This solves the long standing
  problem of creating say types in tests and so forth. With this approach you
  can create anything anywhere.
- we need to have some tests that ensure we don't end up with multiple files
  with the same name at the same destination. This is a particular problem for
  CMake. One alternative is to allow the merging of CMake files, but we don't
  yet have a use case for this. The solution would be to have a "merged file
  flag" and then disable all other facets.
- this will work very nicely with profiles: we can create a few out of the box
  profiles for users such as flat project, common facets and so on. Users can
  simply apply the stereotype to their models. These are akin to "destination
  themes". However, we will also need some kind of "variable replacement" so we
  can support cases like =include/masd.cpp_ref_impl.northwind/types=. In fact,
  we also have the same problem when it comes to modules. A proper path is
  something like:
  - =include/${model_modules_as_dots}/types/${internal_modules_as_folders}=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_dots}.=
  - =include/${model_modules_as_dots}/types/${internal_modules_as_underscores}_=

  This is *extremely* flexible. The user can now create a folder structure that
  depends on package names etc or choose to flatten it and can do so for one or
  all facets. This means for example that we could use nested folders for
  =include=, not use model modules for =src= and then flatten it all for
  =tests=.
- actually it is a bit of a mistake to think of these destinations as purely
  physical. In reality, we may also need them to contribute to namespaces. For
  example, in java the folders and namespaces must match. We could solve this by
  having a "module contribution" in the destination. These would then be used to
  construct the namespace for a given facet. Look for java story on backlog for
  this.
- this also addresses the issue of having multiple serialisation formats and
  choosing one, but having sensible folder names. For example, we could have
  boost serialisation mapped to a destination called =serialisation=. Or we
  could map it to say RapidJSON serialisation. Or we could support two methods
  of serialisation for the same project. The user chooses where to place them.

*** Update github actions to build from tags                          :story:

At present it seems we only build from master. We need to build from tags for
releases.

*** Assorted improvements to CMake files                               :epic:

#+begin_src cmake
include(CheckIPOSupported)
check_ipo_supported(RESULT result)
if(result)
  set_target_properties(foo PROPERTIES INTERPROCEDURAL_OPTIMIZATION TRUE)
endif()

LINK_WHAT_YOU_USE
set(CMAKE_CXX_CLANG_TIDY "clang-tidy" "-checks=*")
<LANG>_CLANG_TIDY: CMake 3.6+
<LANG>_CPPCHECK
<LANG>_CPPLINT
<LANG>_INCLUDE_WHAT_YOU_USE

install(TARGETS MyLib
        EXPORT MyLibTargets
        LIBRARY DESTINATION lib
        ARCHIVE DESTINATION lib
        RUNTIME DESTINATION bin
        INCLUDES DESTINATION include
        )
#+end_src

*Previous understanding*

It seems we are not using proper CMake idioms to pick up compiler features, as
explained here:

- [[http://unclejimbo.github.io/2018/06/08/Modern-CMake-for-Library-Developers/][Modern CMake for Library Developers]]
- [[https://cliutils.gitlab.io/modern-cmake/][An Introduction to Modern CMake]]
- [[http://www.slideshare.net/DanielPfeifer1/cmake-48475415][CMake - Introduction and best practices]]
- [[https://datascience.dsscale.org/wp-content/uploads/2016/06/151208-LANL-Hoffman-Science.pdf][Building Science with CMake]]
- [[https://github.com/crezefire/cxp][CXP: C++ Cross Platform]]: A template project for creating a cross
  platform C++ CMake project using modern CMake syntax and transitive
  dependencies.
- [[https://cgold.readthedocs.io/en/latest/][CGold: The Hitchhiker’s Guide to the CMake]]
- [[https://polly.readthedocs.io/en/latest/index.html][Polly: Collection of CMake toolchains]]
- [[https://github.com/sblumentritt/cmake_modules][GH cmake_modules]]: "This repository provides a wide range of CMake
  helper files."

We need to implement this using proper CMake idioms.

Notes:

- Add version and language to project.
- start using [[https://cmake.org/cmake/help/v3.3/command/target_compile_options.html][target compile options]] for each target. We will have to repeat the
  same flags; this could be avoided by passing in a variable. See also [[http://stackoverflow.com/questions/23995019/what-is-the-modern-method-for-setting-general-compile-flags-in-cmake][What is
  the modern method for setting general compile flags in CMake?]]
- define qualified aliases for all libraries, including nested aliasing for
  =dogen::test_models=. Ensure all linking is done against qualified names.
- use target include directories for each target and only add the required
  include directories to each target. Mark them with the appropriate visibility,
  including using =interface=. We should then remove all duplication of
  libraries in the specs.
- try replacing calls to =-std=c++-14= with compiler feature detection. We need
  to create a list of all C++-14 features we're using.
- remove all of the debug/release compilation options and start using
  =CMAKE_BUILD_TYPE= instead. See [[http://pastebin.com/jCDW5Aa9][this]] example. We added build type support to
  our builds, but as a result, the binaries moved from =stage/bin= to =bin=.
  There is no obvious explanation for this.
- remove =STATIC= on all libraries and let users specify which linkage to use.
  We already have a story to capture this work.
- remove the stage folder and use the traditional CMake directories. This will
  also fix the problems we have with BUILD_TYPE.
- consider buying the CMake book: https://crascit.com/professional-cmake/.

Merged stories:

*Usage of external module path in cmakelists*                       :story:

It seems like we are not populating the target names
properly. Originally the target name for test model all built-ins was:

: dogen_all_builtins

When we moved the test models into =test_models= the target name did
not change. It should have changed to:

: dogen_test_models_all_builtins

*** Capitalise titles in models correctly                             :story:

We still have models with lower case titles:

: * initializer                                                       :element:

Capitalise these correctly.

When we tried to do this to the dogen model, generation failed with the
following error:

: Error: Object has attribute with undefined type: spec_category

We are probably not normalising to lower case.

In addition

Merged stories:

*Capitalise model headers correctly*

At present most models still use the "all lower case" notation, copied from Dia.
We need to capitalise headers correctly so that when we generate documentation
they come out correctly.

*** Consider creating a small paper summarising MASD                  :story:

At present we have the thesis, that explains the entire methodology and its
rationale in great detail. This is not suitable for new users. We should have a
small paper, 3 or 4 pages long, that summarises the argument.

*** Missing PlantUML features                                         :story:

This story keeps track of features that we need to implement when
exporting into PlantUML.

- generalisation via meta-data does not work. This would be very
  complicated: we need to resolve =masd.generalization.parent= into
  the value and then look for all classes with the stereotype of the
  concept (e.g. =meta_element= for logical model).
- we need to unpack all properties and resolve those into types for
  the current model in order to model associations. Also, we need a
  simple resolver to find the types in the right namespaces. The
  logic would be very similar to the existing resolver. One possible
  solution is to move name trees into the codec model and perform
  these transforms earlier in the pipeline.
- add tests to check that PlantUML output has not changed. We need to
  have support for diffing on conversion, which we may not yet
  have. Also the existing tests do not take into account the action.
- add plantuml representations to reference models.
- improve layout: [[https://crashedmind.github.io/PlantUMLHitchhikersGuide/layout/layout.html][layout]]. See also [[https://stackoverflow.com/questions/61639970/align-packages-vertically-in-plantuml][SO: Align packages vertically in
  PlantUML]]
- add comments on relations:

: someclass o-- otherclass : a description of the relation

  This means the org-mode model must contain the comment. We could
  either use the comment on the association itself, or have a field in
  properties:

: :masd.codec.relation_comment: a description of the relation

  This is then copied as is into the UML diagram. By separating the
  two means we can still make "proper" comments at the attribute
  level.
- there should be meta-data to determine if a comment will show up in
  the diagram as a note or not. It should apply to both classes and
  methods.
- we should allow users to set the location of the comment for classes
  etc (bottom, top, left, right).
- we should allow enabling comments on methods and attributes, with
  direction (left or right).
- notes are not indented at present.
- we are not leaving a space after inheritance.
- empty classes still have brackets.
- no top-level namespace for model. We didn't have this in Dia either.
- inner modules have incorrect names, as documentation is appearing at the
  top-level namespace. See logical model for an example.

Notes:

- we need to move features such as name trees and generalisation into
  the codec model.
- actually perhaps we need to allow users to "tweak" the
  relationships. We probably don't want every single relationship to
  become a link in UML because then we can't see the wood for the
  trees. A better approach may be to force users to declare the
  relationships as meta-data, like we suggested above for comments,
  and then transport those annotations to the UML diagram. This also
  means we could add a full reference to the types, which solves the
  issue of having to merge models. Finally, it means we can declare
  any kind of relation, as per the plant uml syntax - in fact we
  should probably expose the plant UML syntax directly, e.g. the user
  is expected to declare a native plantuml relation:

: :masd.codec.plantuml_relation: AS_IS_RELATION

  We could perhaps default to the current class, but that's about
  it. This would also allow for things like hidden links, etc. In
  effect the only relation you get for free is inheritance. Actually
  we should also have a "raw" way of inserting relations in a
  namespace. These are placed at the end of the namespace.
- logical model namespaces are not documented correctly.

Links:

- [[https://forum.plantuml.net/8770/how-to-force-vertical-arrangement-of-packages][How to force vertical arrangement of packages?]]

*** Consider standardising all templates as mustache templates        :story:

At present we have a somewhat complex story with regards to templating:

1. we use a mustache-like approach called wale, built in-house. It is used for
   some header files such as the M2T transforms.
2. we use a t4-like approach called stitch, also in-house. It is used for the
   implementation of the M2T transforms.

What would be really nice is if we could use the same approach for both, and if
that approach was not part of Dogen. The purpose of this story is to explore the
possibility of replacing both with a standard implementation of mustache,
ideally available on vcpkg. We already have a story for replacing wale with
mustache in the backlog, so see that for the choice of implementation. This
story concerns itself mainly with the second item in the above list; that is,
can we replace stitch with mustache.

In order to answer this question we first must try to figure out what the
differences between T4 and mustache are. T4 is a "generator generator". That is,
the text template generates C# code that generates the ultimate target of the
template. This means it is possible to embed any logic within the T4 template as
required, to do complex processing. It also means the processing is "fast"
because we generate C# code rather than try to introspect at run time. Stitch
uses the same approach. However, after many years of using both T4 and Stitch,
the general conclusion has been that the templates should be kept as simple as
possible. The main reason is that "debugging" through the templates is
non-trivial, even though it is simple C++ code (in the case of stitch).

Mustache on the other hand puts forward an approach of logic-less templates.
That is, the templates are evaluated dynamically by the templating engine, and
the engine only allows for a very limited number of constructs. In some
implementations, the so called "template hash", that is the input to the
template, is a JSON object. All the template can do is refer to entries in the
JSON object and replace tokens with the values of those entries.

Until recently we deemed mustache to be too simple for our needs because Dogen
templates were very complex. However, several things have changed:

- we do not want the templates to have any indentation at all; this should be
  left to clang-format as a subsequent T2T transform. This removes a lot of
  functionality we had in Stitch.
- we do not want the logical model objects to be processed any further in the
  template. As explained above this leads to a lot of complications. We want the
  object to be in its final form.
- we want all relationships etc to be encoded in the logical model object prior
  to M2T transformation.

In other words, we have slowly been converging towards logic-less templates,
though we are not yet there. The main stumbling blocks are:

- epilogue and prologue are at present handled by assistants:

#+begin_src
    text::formatters::assistant ast(lps, e, a, true/*requires_header_guard*/);
    const auto& o(ast.as<logical::entities::structural::object>(e));

    {
        auto sbf(ast.make_scoped_boilerplate_formatter(o));
        {
            const auto ns(ast.make_namespaces(o.name()));
            auto snf(ast.make_scoped_namespace_formatter(ns));
#>

class <#= o.name().simple() #>;

<#+
        } // snf
#>

<#+
    } // sbf

#+end_src

   Ideally we should just have a way to ask for the values of these fields.
- we need to investigate all templates and see if a JSON representation of a
  logical model element is sufficient to capture all required information.
  However the best way to do this is to have an incremental approach: provide a
  mustache based M2T and then incrementally move each M2T at a time.

If we do move to mustache, there are lots of advantages:

- remove all of templating code.
- we could allow users to supply their own mustache templates in a model. We can
  even allow for the dynamic creation of PMM elements and then the association
  of those elements with templates. End users cannot of course extend the LMM,
  but even just extending the LMM gives them a lot of power.
- we could create a stand alone tool that allows users to play with templates.
  All they need is a dump of the JSON representation of the objects in their
  model (this could be an option in Dogen). Then the tool can take the template
  and the JSON and render it to =std::out=. This makes template development much
  easier. If we integrate it with Emacs, we could even have a view where we
  do: 1) JSON 2) template 3) output. Users can then change 1) and 2) and see the
  results in 3). We don't even have to extend emacs for this, we could just use
  the compilation command.

Notes:

- if we could create JSON schemas for the LMM, we could then allow users to
  create their own JSON representations. Not sure how useful this would be.
- we need JSON support in Dogen for this.
- we need to measure how much slower Dogen would be with this approach.
  Presumably mustache is a lot slower that Stitch.
- from this perspective, the PMM is fixed but the PM then becomes a dynamic
  entity. We can supply a PM model with Dogen but that is just Dogen's
  interpretation of the physical space; users could supply their own PM's as
  required. The PMs need to bind to the PMM: either the user supplies its own
  TS, part etc or it must bind (via meta-data) to existing parts, TS etc. We
  also need to support two styles of declaring PM entities: inline (e.g. nested)
  or outline (e.g. we want to bind a given facet, part etc to an already
  existing TS, etc).
- we could hash both the mustache template and the JSON object used as input,
  and save those two hashes in the generated file. If the hashes match, don't
  bother regenerating.

Links:

- [[https://en.wikipedia.org/wiki/Text_Template_Transformation_Toolkit][wikipedia: Text Template Transformation Toolkit]]

Merged stories:

*Implement wale in terms of existing template libraries*

Originally we implemented wale as a quick hack, but we stated:

#+begin_quote
A second point is the use of [[https://github.com/jamboree/bustache][bustache]] vs rolling our own trivial mustache-like
implementation:

- if we use bustache we can, in the future, start to make use of
  complex mustache templates. We don't have a use case for this now,
  but there is no reason to preclude it either.
- however, with bustache as a third-party dependency we now have to
  worry about generating OSX and windows binaries for the
  library. Until we do, the builds will break.

For now, to make life easier we will roll our own. As soon as we have
a stable windows environment we will move to bustache.
#+end_quote

We should really move to one of these mustache implementations. Inja
seems to be the most sensible one, even though it depends on a JSON
library. We will need JSON internally anyway, so it may be the time to
add a dependency. We should also have a way to associate an arbitrary
JSON document with a formatter so that users can create their own
templates with their own parameters and the model is merely used for
pass-through.

We should also start to create a standard set of variables that dogen
exports into inja such as object name, namespaces, etc. These are
"system variables" and do not require any action from the user. In
fact, if we use the JSON based approach, we could define a JSON schema
for meta-model elements which is MASD specific. These are used by the
templates.

Note that stitch only makes sense when we are creating a code
generator (at least given the use cases we have so far) whereas inja
makes sense even for regular models and can be applied to items in any
technical space.

Links:

- [[https://github.com/cierelabs/boostache/tree/develop][boostache]]
- [[https://github.com/no1msd/mstch][mstch]]
- [[https://github.com/mrtazz/plustache][plustache]] (in vcpkg)
- [[https://github.com/melpon/ginger][ginger]]
- [[https://github.com/qicosmos/render][render]]
- [[https://github.com/pantor/inja][inja]]: in vcpkg, needs JSON library. [[https://github.com/paradoxxxzero/jinja2-mode][Emacs mode]]. "Inja is a template engine for
  modern C++, loosely inspired by jinja for python. It has an easy and yet
  powerful template syntax with all variables, loops, conditions, includes,
  callbacks, and comments you need, nested and combined as you like. Inja uses
  the wonderful json library by nlohmann for data input."
- [[https://github.com/jrziviani/amps][amps]]
- [[https://github.com/OlafvdSpek/ctemplate][ctemplate]]: This library provides an easy to use and lightning fast
  text templating system to use with C++ programs. It was originally
  called Google Templates, due to its origin as the template system
  used for Google search result pages.
- [[https://github.com/moneymanagerex/ctpp][ctpp GH]]: See also [[http://ctpp.havoc.ru/en/][homepage]]. Seems a bit unmaintained but may have
  some good ideas. See [[http://ctpp.havoc.ru/en/whatis.html][What is CTPP?]]
- [[https://github.com/blockspacer/CXXCTP][CXXCTP GH]]: "Add custom features to C++ language, like metaclasses,
  Rust-like traits, reflection and many more. A fully open source,
  powerful solution for modification and generation of C++ source
  code. Reduce the amount of boilerplate code in your C++ projects."
- [[https://github.com/flexferrum/autoprogrammer][autoprogrammer GH]]: "Welcome to Autoprogrammer, the C++ code
  generation tool! This tool helps you dramatically reduce the amount
  of boilerplate code in your C++ projects. Based on clang frontend,
  the 'autoprogrammer' parses your C++ source files and generates new
  set C++ sources. For instance, it generates enum-to-string
  converting functions for you. Instead of you."
- [[https://github.com/TheLongRunSmoke/utility-boilerplate-qt][utility-boilerplate-qt GH]]: "Template for creating simple
  cross-platform application with GUI based on Qt."

*Consider renaming =wale= to =mustache=*

We need to rename all of the wale templates to mustache.

*Consider renaming =wale= to =tangle=*

Wale and stitch are remnant from the sewing days. Whilst stitch is
still vaguely appropriate, we can't even remember what wale stands
for. We should use a more domain-specific term such as weave or
tangle. In fact, we probably should rename =stitch= to =weave= given
it weaves text with code, and find a better name for wale. Its not
"tangling" (given tangling, as we understand it from org-mode, is just
another name for weaving). We need to look into logic-less templates
terminology.

Actually this is a mistake. Wale is just a poor-person's mustache and
will be replaced by a proper implementation of mustache as soon as we
can. We should instead start calling it mustache and explain this is
just a temporary fix.

*Consider renaming logic-less templates*

Originally we though this was a good name because it was used by some
domain experts, but it seems it generates more confusion than
anything. It may just be a term used by mustache and other niche
template groups. We should probably rename it to text templates given
most domain experts know what that means.

In addition, the templates should be specific to their types; we need
to know if its a mustache template or a stitch template because the
processing will be very different. The templates should be named after
their type in the logical model. Rename these to wale templates.

Actually its not yet clear if the existing logic could not be extended
to other template types. We should wait until we implement it front to
back and then make a decision.

The most obvious thing is just to call the templates after their
actual name: mustache.

** Deprecated
*** CANCELLED Add nightly builds to C# reference product              :story:

Since we list travis we lost support for nightlies.

Actually we never did have nightlies for C++.

*** CANCELLED Create a map between UML/MOF terminology and yarn        :epic:

*Rationale*: this story is too ambitious. We don't really have a use case for
this and it would be non-trivial.

It would be helpful to know what a yarn type means in terms of UML/MOF, and
perhaps even explain why we have chosen certain names instead of the UML ones.
We should also cover the modeling of relationships and the relation between yarn
concepts and UML/MOF classes. This will form a chapter in the manual.

The UML specification is available [[http://www.omg.org/spec/UML/2.5/][here]] and MOF specification is available [[http://www.omg.org/spec/MOF/2.5][here]].

We need a way to uniquely identify a property. This could be done by appending
the containing type's qualified name to the property name.

See also [[http://www.uml-diagrams.org/][The Unified Modeling Language]] for a more accessible treatment.

See [[http://www-01.ibm.com/support/knowledgecenter/SS5JSH_9.1.2/com.ibm.xtools.transform.uml2.cpp.doc/topics/rucppprofile.html][Stereotypes of the UML-to-C++ transformation profile]] for ideas.

*** CANCELLED Analysis of MDE papers to read                          :story:

*Rationale*: with the end of academic work these stories no longer apply.

Links:

- [[https://ulir.ul.ie/bitstream/handle/10344/2126/2007_Botterweck.pdf;jsessionid=AC6FF39BA414E6065602C7851860C43D?sequence=2][Model-Driven Derivation of Product Architectures]]
- [[https://madoc.bib.uni-mannheim.de/993/1/abwl_02_05.pdf][A Taxonomy of Metamodel Hierarchies]]
