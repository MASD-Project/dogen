#+title: Sprint Backlog 11
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- resolve the refactoring problem with lots of duplicated types;
- work on the masd profile.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2019-02-22 Fri 19:31]
| <75>                                                      |          |        |        |       |
| Headline                                                  | Time     |        |        |     % |
|-----------------------------------------------------------+----------+--------+--------+-------|
| *Total time*                                              | *221:10* |        |        | 100.0 |
|-----------------------------------------------------------+----------+--------+--------+-------|
| Stories                                                   | 221:10   |        |        | 100.0 |
| Active                                                    |          | 221:10 |        | 100.0 |
| Edit release notes for previous sprint                    |          |        |   2:03 |   0.9 |
| Sprint and product backlog grooming                       |          |        |   5:32 |   2.5 |
| Work on MASD theory                                       |          |        | 113:38 |  51.4 |
| Rename models to fit MASD architecture                    |          |        |   4:56 |   2.2 |
| Read up on framework and API design                       |          |        |   0:57 |   0.4 |
| Analyse the state of the mess of refactors                |          |        |   6:27 |   2.9 |
| Create the =orchestration= model                          |          |        |   4:15 |   1.9 |
| Update vcpkg across all operative systems                 |          |        |   4:35 |   2.1 |
| Update C# reference models to latest dogen                |          |        |   0:42 |   0.3 |
| Convert the utility model into a regular dogen model      |          |        |   7:27 |   3.4 |
| Improve support for clang-cl builds                       |          |        |   2:02 |   0.9 |
| Move generation properties into meta-data                 |          |        |   9:26 |   4.3 |
| Disabling facet globally and enabling locally fails       |          |        |   2:21 |   1.1 |
| References to types in top-level namespace do not resolve |          |        |   0:51 |   0.4 |
| Create a colour palette test model                        |          |        |   1:09 |   0.5 |
| Create a single binary for all of dogen                   |          |        |  37:30 |  17.0 |
| Setup a nightly build for Dogen                           |          |        |   3:47 |   1.7 |
| Great meta-data rename                                    |          |        |   6:00 |   2.7 |
| Throw on profiles that refer to invalid fields            |          |        |   0:21 |   0.2 |
| Dogen's vcpkg export for OSX was created from master      |          |        |   0:46 |   0.3 |
| Fix clang-cl warnings                                     |          |        |   5:07 |   2.3 |
| Use tracing options in existing code                      |          |        |   0:57 |   0.4 |
| Model references are not transitive                       |          |        |   0:21 |   0.2 |
#+TBLFM: $5='(org-clock-time%-mod @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2018-10-29 Mon 10:46]
     :LOGBOOK:
     CLOCK: [2018-10-29 Mon 10:47]--[2018-10-29 Mon 10:52] =>  0:05
     CLOCK: [2018-10-29 Mon 10:24]--[2018-10-29 Mon 10:46] =>  0:22
     CLOCK: [2018-10-29 Mon 09:27]--[2018-10-29 Mon 10:23] =>  0:56
     CLOCK: [2018-10-29 Mon 08:46]--[2018-10-29 Mon 09:26] =>  0:40
     :END:

 Add github release notes for previous sprint.

 Title: Dogen v1.0.10, "Lucira"

#+begin_src markdown
![Lucira](http://www.redeangola.info/wp-content/uploads/2016/06/roteiro_lucira_pedro-carreno_5-580x361.jpg)
_Lucira fishing village, Namibe province, Angola. [(C) 2016 Rede Angola](http://www.redeangola.info/roteiros/lucira/)_.

# Overview

This sprint brought the infrastructural work to a close. Much was achieved, though mainly relevant to the development process. As always, you can get the gory details in [the sprint log](https://github.com/MASD-Project/dogen/blob/master/doc/agile/v1/sprint_backlog_10.org), but the below has the highlights.

## Complete the vcpkg transition

There were still a number of issues to mop-up including proper OSX build support, removing all references to conan (the previous packaging system) and fixing a number of warnings that resulted from the build settings on vcpkg. We have now fully transitioned to vcpkg and we're already experiencing the benefits of the new package management system: adding new packages across all operative systems now takes a couple of hours (the time it takes to rebuild the vcpkg export in three VMs). However, not all packages are available in vcpkg and not all packages that are available build cleanly on all our supported platforms, so we haven't reached nirvana just yet.

## Other build improvements

In parallel to the vcpkg transition we also cleaned up most warnings, resulting in very clean builds on [CDash](https://my.cdash.org/index.php?project=MASD+Project+-+Dogen). The only warnings we see are real warnings that need to be addressed. We have tried moving to ```/W4``` and even ```Wall``` on MSVC but quickly discovered that [it isn't feasible at present](https://github.com/Microsoft/vcpkg/issues/4577), so we are using the compiler default settings until the issues we raised are addressed.

Sadly, we've had to ignore all failing tests across all platforms for now (thus taking a further hit on code coverage). This had to be done because at present the tests do not provide enough information for us to understand why they are failing when looking at the Travis/AppVeyor logs. Since reproducing things locally is just too expensive, we need to rewrite these tests to make them easy to troubleshoot from CI logs. This will be done as part of the code generation of model tests.

A final build "improvement" was the removal of facets that were used only to test the code generator, such as hashing, serialisation etc. This has helped immensely in terms of the build time outs but the major downside is we've lost yet another significant source of testing. It seems the only way forward is to create a nightly build that exercises all features of the code generator and runs on our machines - we just do not have enough time on Travis / AppVeyor to compile non-essential code. We still appear to hit occasional timeouts, but these are much less frequent.

## Code coverage

We've lacked code coverage for a very long time, and this has been a pressing need because we need to know which parts of the generated code are not being exercised. We finally managed to get it working thanks to the amazing [kcov](https://github.com/SimonKagstrom/kcov). It is far superior to gcov and previous alternative approaches, requiring very little work to set up. Unfortunately how coverage numbers are very low now due to the commenting out of many unit tests to resolve the build times issues. However, the great news is we can now monitor the coverage as we re-introduce the tests. Sadly, the code coverage story on C# is still weak as we do not seem to be able to generate any information at present (likely due to NUnit shadowing). This will have to be looked at in the future.

We now have support for both [Codecov](https://codecov.io/gh/MASD-Project/dogen) and [Coveralls](https://coveralls.io/github/MASD-Project/dogen?branch=master), which appear to give us different results.

##  C++ 17 support

One of the long time desires has been to migrate from C++ 14 to C++ 17 so that we can use the new features. However, this migration was blocked due to the difficulties of upgrading packages across all platforms. With the completion of the vcpkg story, we finally had all the building blocks in place to move to C++ 17, which was achieved successfully this sprint. This now means we can start to make use of ```ranges```, ```string_view``` and all the latest developments. The very first feature we've introduced is nested namespaces, described below.

## Project naming clean-up

Now we've settled on the new standard namespaces structure, as defined by the [Framework Design Guidelines](https://docs.microsoft.com/en-us/dotnet/standard/design-guidelines/names-of-namespaces), we had to update all projects to match. We've also made the build targets match this structure, as well as the folders in the file system, making them all consistent. Since we had to update the CMake files, we started to make them a bit more modern - but we only scratched the surface.

## Defining a Dogen API

As part of the work with Framework Design Guidelines, we've created a model to define the product level API and tested it via scenarios. The API is much cleaner and suitable for interoperability (e.g. SWIG) as well as for the code generation of the remotable interfaces.

# User visible changes

The main feature added this sprint was the initial support for C++ 17. You can now set your standard to this version:

```
#DOGEN quilt.cpp.standard=c++-17
```

At present the only difference is how nested namespaces are handled. Using our annotations class as an example, prior to enabling C++ 17 we had:

```
namespace masd
namespace dogen
namespace annotations {
<snip>
} } }
```

Now we generate the following code:

```
namespace masd::dogen::annotations {
<snip>
}
```

# Next Sprint

We have reached a bit of a fork in Dogen's development: we have got some good ideas on how to address the fundamental architectural problems, but these require very significant surgery into the core of Dogen and its not yet clear if this can be achieved in an incremental manner. On the other hand, there are a number of important stories that need to be implemented in order to get us in a good shape (such as sorting out the testing story). Hard decisions will have to be made in the next sprint.

# Binaries

You can download binaries from [Bintray](https://bintray.com/masd-project/main/dogen) for OSX, Linux and Windows (all 64-bit):

- [dogen_1.0.10_amd64-applications.deb](https://dl.bintray.com/masd-project/main/1.0.10/dogen_1.0.10_amd64-applications.deb)
- [dogen-1.0.10-Darwin-x86_64.dmg](https://dl.bintray.com/masd-project/main/1.0.10/dogen-1.0.10-Darwin-x86_64.dmg)
- [dogen-1.0.10-Windows-AMD64.msi](https://dl.bintray.com/masd-project/main/dogen-1.0.10-Windows-AMD64.msi)

For all other architectures and/or operative systems, you will need to build Dogen from source. Source downloads are available below.#+end_src

 - [[https://twitter.com/MarcoCraveiro/status/1051785972206247936][Tweet]]
 - [[https://www.linkedin.com/feed/update/urn:li:activity:6457553749215899648/][LinkedIn]]
 - [[https://gitter.im/MASD-Project/Lobby][Gitter]]
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/1056856688983187456][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6462624611979841536][LinkedIn]]
- [[https://gitter.im/MASD-Project/Lobby][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
     :LOGBOOK:
     CLOCK: [2019-02-21 Thu 13:01]--[2019-02-21 Thu 13:20] =>  0:19
     CLOCK: [2019-02-19 Tue 13:05]--[2019-02-19 Tue 13:16] =>  0:11
     CLOCK: [2019-02-15 Fri 17:45]--[2019-02-15 Fri 17:52] =>  0:07
     CLOCK: [2019-02-15 Fri 11:46]--[2019-02-15 Fri 11:55] =>  0:09
     CLOCK: [2019-02-15 Fri 09:50]--[2019-02-15 Fri 09:53] =>  0:03
     CLOCK: [2019-02-13 Wed 10:42]--[2019-02-13 Wed 11:04] =>  0:22
     CLOCK: [2019-02-13 Wed 09:53]--[2019-02-13 Wed 10:03] =>  0:10
     CLOCK: [2019-02-11 Mon 11:28]--[2019-02-11 Mon 11:37] =>  0:09
     CLOCK: [2019-02-09 Sat 06:50]--[2019-02-09 Sat 06:54] =>  0:04
     CLOCK: [2019-02-09 Sat 06:11]--[2019-02-09 Sat 06:19] =>  0:08
     CLOCK: [2019-02-08 Fri 13:41]--[2019-02-08 Fri 14:02] =>  0:21
     CLOCK: [2019-02-06 Wed 11:51]--[2019-02-06 Wed 12:10] =>  0:19
     CLOCK: [2018-11-22 Thu 10:21]--[2018-11-22 Thu 11:25] =>  1:01
     CLOCK: [2018-11-19 Mon 09:10]--[2018-11-19 Mon 11:01] =>  1:51
     CLOCK: [2018-10-29 Mon 08:30]--[2018-10-29 Mon 08:45] =>  0:15
     :END:

 Updates to sprint and product backlog.

*** COMPLETED Work on MASD theory                                     :story:
    CLOSED: [2018-11-19 Mon 11:35]
    :LOGBOOK:
    CLOCK: [2018-11-16 Fri 16:07]--[2018-11-16 Fri 17:07] =>  1:00
    CLOCK: [2018-11-16 Fri 13:28]--[2018-11-16 Fri 15:00] =>  1:32
    CLOCK: [2018-11-16 Fri 09:12]--[2018-11-16 Fri 12:54] =>  3:42
    CLOCK: [2018-11-15 Thu 18:02]--[2018-11-15 Thu 19:29] =>  1:27
    CLOCK: [2018-11-15 Thu 13:20]--[2018-11-15 Thu 17:05] =>  3:45
    CLOCK: [2018-11-15 Thu 09:20]--[2018-11-15 Thu 12:06] =>  2:46
    CLOCK: [2018-11-15 Thu 09:11]--[2018-11-15 Thu 09:18] =>  0:07
    CLOCK: [2018-11-14 Wed 13:15]--[2018-11-14 Wed 18:15] =>  5:00
    CLOCK: [2018-11-14 Wed 08:12]--[2018-11-14 Wed 12:15] =>  4:03
    CLOCK: [2018-11-13 Tue 14:02]--[2018-11-13 Tue 17:02] =>  3:00
    CLOCK: [2018-11-13 Tue 09:00]--[2018-11-13 Tue 12:09] =>  3:09
    CLOCK: [2018-11-12 Mon 13:25]--[2018-11-12 Mon 17:35] =>  4:10
    CLOCK: [2018-11-12 Mon 09:04]--[2018-11-12 Mon 12:09] =>  4:10
    CLOCK: [2018-11-10 Sat 14:10]--[2018-11-10 Sat 17:50] =>  3:40
    CLOCK: [2018-11-09 Fri 14:05]--[2018-11-09 Fri 18:10] =>  4:05
    CLOCK: [2018-11-09 Fri 08:22]--[2018-11-09 Fri 12:30] =>  4:08
    CLOCK: [2018-11-08 Thu 13:40]--[2018-11-08 Thu 17:22] =>  3:42
    CLOCK: [2018-11-08 Thu 08:15]--[2018-11-08 Thu 12:22] =>  9:07
    CLOCK: [2018-11-07 Wed 13:10]--[2018-11-07 Wed 18:29] =>  5:19
    CLOCK: [2018-11-07 Wed 08:15]--[2018-11-07 Wed 12:21] =>  4:06
    CLOCK: [2018-11-06 Tue 08:15]--[2018-11-06 Tue 10:39] =>  2:24
    CLOCK: [2018-11-05 Mon 14:25]--[2018-11-05 Mon 18:20] =>  3:55
    CLOCK: [2018-11-05 Mon 08:30]--[2018-11-05 Mon 12:20] =>  3:50
    CLOCK: [2018-11-04 Sun 14:00]--[2018-11-04 Sun 18:00] =>  4:00
    CLOCK: [2018-11-03 Sat 07:30]--[2018-11-03 Sat 11:20] =>  3:50
    CLOCK: [2018-11-02 Fri 13:50]--[2018-11-02 Fri 18:20] =>  4:30
    CLOCK: [2018-11-02 Fri 10:01]--[2018-11-02 Fri 12:19] =>  2:18
    CLOCK: [2018-11-01 Thu 15:45]--[2018-11-01 Thu 18:16] =>  2:31
    CLOCK: [2018-11-01 Thu 10:10]--[2018-11-01 Thu 12:43] =>  2:33
    CLOCK: [2018-10-31 Wed 16:27]--[2018-10-31 Wed 17:45] =>  1:18
    CLOCK: [2018-10-31 Wed 15:00]--[2018-10-31 Wed 15:08] =>  0:08
    CLOCK: [2018-10-31 Wed 12:02]--[2018-10-31 Wed 14:59] =>  2:57
    CLOCK: [2018-10-31 Wed 08:28]--[2018-10-31 Wed 11:07] =>  2:39
    CLOCK: [2018-10-30 Tue 17:14]--[2018-10-30 Tue 18:31] =>  1:17
    CLOCK: [2018-10-30 Tue 13:36]--[2018-10-30 Tue 15:08] =>  1:32
    CLOCK: [2018-10-30 Tue 08:55]--[2018-10-30 Tue 12:05] =>  3:10
    CLOCK: [2018-10-29 Mon 15:32]--[2018-10-29 Mon 18:10] =>  2:38
    CLOCK: [2018-10-29 Mon 12:35]--[2018-10-29 Mon 14:08] =>  1:33
    CLOCK: [2018-10-29 Mon 10:53]--[2018-10-29 Mon 11:35] =>  0:42
    :END:

Work on defining the theory for MASD:

- update latex templates.
- update API scenarios.
- finish foundations chapter.

*** COMPLETED Rename input models directory to models                 :story:
    CLOSED: [2018-11-22 Thu 10:30]

*Rationale*: Already done.

We need to move the dogen project to the new directory layout whereby
all models are kept in the =models= directory.

*** COMPLETED ODB source files are generated when ODB is off          :story:
    CLOSED: [2019-02-04 Mon 11:49]

Even when the ODB facet is off, we still get the following in CMake:

: set(odb_files "")
: file(GLOB_RECURSE odb_files RELATIVE
:   "${CMAKE_CURRENT_SOURCE_DIR}/"
:   "${CMAKE_CURRENT_SOURCE_DIR}/*.cxx")
: set(files ${files} ${odb_files})

This should only be generated if ODB is on.

Actually the problem is slightly more complicated. We are only adding
these lines if ODB is on, but however, we may have switched ODB on but
not defined classes with ODB stereotypes. In this case we do not
generate any pragmas, and thus no ODB files. However, the ODB flag is
still on so we add the above file inclusion. To make this in the most
clean possible manner, we'd have to check to see if any ODB files were
generated to determine if there is a need to add them. However, this
is probably non-trivial because we only have a list of files after
template expansion. The simplest way may be to do a transform that
looks for ODB stereotypes and marks a flag at model level.

Actually we already had solved this problem:

:        if (a.is_odb_facet_enabled() && !c.odb_targets().targets().empty()) {

We can reuse this machinery.

*** COMPLETED Split ODB executable from ODB libraries in CMake        :story:
    CLOSED: [2019-02-04 Mon 11:49]

In order to compile on Travis using vcpkg, we need to detect the ODB
executable separately from the ODB libraries. We have the following
cases:

- if ODB facet is off, no ODB related code should be emitted.
- if ODB facet is on, it is the responsibility of the containing
  project to ensure that at least the ODB libraries have been found
  (or that the project has been excluded from the build). We should
  refuse to continue if they are not present.
- if the ODB compiler has not been found, we should not include the
  ODB targets.

*** COMPLETED SQLite backend is misspelled                            :story:
    CLOSED: [2019-02-04 Mon 12:08]

At present we are calling SQLite =sqllite=. Fix this.

*** COMPLETED Rename models to fit MASD architecture                  :story:
    CLOSED: [2019-02-06 Wed 09:42]
    :LOGBOOK:
    CLOCK: [2018-11-23 Fri 11:40]--[2018-11-23 Fri 11:57] =>  0:17
    CLOCK: [2018-11-23 Fri 10:19]--[2018-11-23 Fri 11:39] =>  1:20
    CLOCK: [2018-11-23 Fri 09:02]--[2018-11-23 Fri 10:18] =>  1:16
    CLOCK: [2018-11-22 Thu 15:16]--[2018-11-22 Thu 15:17] =>  0:01
    CLOCK: [2018-11-22 Thu 13:14]--[2018-11-22 Thu 15:16] =>  2:02
    :END:

We now have the following top-level models:

- injection
- coding
- generation
- extraction
- tracing

We need to update the models to match this.

*** COMPLETED Read up on framework and API design                     :story:
    CLOSED: [2019-02-06 Wed 09:42]
    :LOGBOOK:
    CLOCK: [2018-11-22 Thu 09:55]--[2018-11-22 Thu 10:20] =>  0:25
    CLOCK: [2018-11-19 Mon 11:02]--[2018-11-19 Mon 11:34] =>  0:32
    :END:

Now that we are creating a top-level API for Dogen we should really
read up on books about good API design.

Namespacing guideline:

- company | project
- product | technology
- feature
- subnamespace

So in our case, =masd::dogen= and =masd::cpp_ref_impl=. We are
violating the guideline on no abbreviations with ref_impl but
=cpp_reference_implementation= seems a tad long.

It seems we have several types of classes:

- interfaces
- abstract base classes
- values
- objects where data dominates and behaviours are small or trivial
- objects where behaviour dominates and data is small or trivial
- static classes

These should be identifiable at the meta-model level, with appropriate
names.

*** COMPLETED Analyse the state of the mess of refactors              :story:
    CLOSED: [2019-02-06 Wed 12:02]
    :LOGBOOK:
    CLOCK: [2019-02-06 Wed 14:40]--[2019-02-06 Wed 16:01] =>  1:21
    CLOCK: [2019-02-06 Wed 09:39]--[2019-02-06 Wed 11:50] =>  2:11
    CLOCK: [2018-11-29 Thu 09:46]--[2018-11-29 Thu 10:02] =>  0:16
    CLOCK: [2018-11-28 Wed 14:55]--[2018-11-28 Wed 16:22] =>  1:27
    CLOCK: [2018-11-28 Wed 14:30]--[2018-11-28 Wed 14:54] =>  0:24
    CLOCK: [2018-11-28 Wed 13:10]--[2018-11-28 Wed 13:43] =>  0:33
    CLOCK: [2018-11-27 Tue 11:48]--[2018-11-27 Tue 12:03] =>  0:15
    :END:

The first task is to try to abort the OOP refactors that we made in
the past.

Notes:

- some properties were moved into element and are now being used. They
  no longer exist in the formatters types.
- some properties were moved into the generation model but are not
  being used.
- the best approach is to unwind *all* of the refactoring work. If we
  can get to a place were generation space is again totally decoupled
  from coding space, we can then at least start to work towards
  finding commonalities between generation space models.

Tasks:

- delete all types that are not being used at present.
- move all properties that were moved from formattables into element
  back to formattables. Actually this cannot be done because we
  refactored these types a fair bit. They are no longer compatible
  with formatables without a lot of surgery.
- move dynamic transforms back to formattables / fabric transforms.

Important conclusions:

- there is no such thing as "fabric". All metamodel elements that were
  defined at the generation level are really coding entities. It does
  not matter that some of them may be specific to a TS, because TSs
  are cross-cutting concerns; they will appear at every point in the
  pipeline. The key thing is the metamodel elements are not
  "generational concepts". That is, they do not appear only after we
  moved from coding space into generation space (facet expansion).
- the generational model has a dependency on the coding model, but its
  a "soft-dependency". Generational model deals with all concepts from
  generational space. Some of these may require information from
  coding space, but that's the only connection.
- the extractional model takes the generational representation and
  instantiates artefacts. Again, TSs are part of the extractional
  model. There is a "conversion model" that takes us from generational
  space to extractional space.

*** COMPLETED Create the =orchestration= model                        :story:
    CLOSED: [2019-02-06 Wed 15:36]
    :LOGBOOK:
    CLOCK: [2018-11-27 Tue 08:51]--[2018-11-27 Tue 11:47] =>  3:56
    CLOCK: [2018-11-26 Mon 17:26]--[2018-11-26 Mon 18:22] =>  0:56
    CLOCK: [2018-11-26 Mon 17:02]--[2018-11-26 Mon 17:25] =>  0:23
    :END:

Create a model with the top-level transforms.

*** COMPLETED Create the =generation= model                           :story:
    CLOSED: [2019-02-08 Fri 13:51]

*Rationake*: model has been created. The approach has changed and we
have stories to cover it.

Create a new model called =generation= and move all code-generation
related class to it.

We need to create classes for element properties and make model have a
collection that is a pair of element and element properties. We need a
good name for this pair:

- extended element
- augmented element
- decorated element: though not using the decorator pattern; also, we
  already have decoration properties so this is confusing.

Alternatively we could just call it =element= and make it contain a
modeling element.

Approach:

- create a new generation model, copying across all of the meta-model
  and transform classes from yarn. Get the model to transform from
  endomodel to generation model.
- augment formattables with the new element properties. Supply this
  data via the context or assistant.

Problems:

- all of the transforms assume access to the modeling element means
  access to the generation properties. However, with the introduction
  of the generation element we now have a disconnect. For example, we
  sometimes sort and bucket the elements, and then modify them; this
  no longer works with generation elements because these are not
  pointers. It would be easier to make the generation properties a
  part of the element. This is an ongoing discussion we've had since
  the days of formattables. However, in formattables we did write all
  of the transforms to take into account the formattable contained
  both the element and the formattable properties, whereas now we need
  to update all transforms to fit this approach. This is a lot more
  work. The quick hack is to slot in the properties directly into the
  element as some kind of "opaque properties". We could create a base
  class =opaque_properties= and then have a container of these in
  element. However, to make it properly extensible, the only way is to
  make it a unordered set of pointers.
- actually the right solution for this is to use multiple
  inheritance. For each modeling element we need to create a
  corresponding generation version of it, which is the combination of
  the modeling element and a generation element base class. Them the
  generation model is made up of pointers to generation elements and
  it dispatches into generation elements descendants in the
  formatter. The key point is to preserve the distinction between
  modeling (single element) vs generation (projection across facet
  space).

*** COMPLETED Rename core models                                      :story:
    CLOSED: [2019-02-08 Fri 13:52]

*Rationale*: this has been implemented.

The more we catch up with the literature, the more the current model
names look weird, particularly =modeling= and =generation=. In reality
all of the models relate to "modeling" and to generation. We should
just bite the bullet and use the compiler related names: frontend,
middleend and backend.

Interestingly, eCore/EMF also take the same approach of having a model
that is then enriched for generation. This means we could have:

- frontend/interop/external.
- middleend/modeling
- backend/generation

*** COMPLETED Update vcpkg across all operative systems               :story:
    CLOSED: [2019-02-09 Sat 06:46]
    :LOGBOOK:
    CLOCK: [2019-02-08 Fri 12:25]--[2019-02-08 Fri 13:30] =>  1:05
    CLOCK: [2019-02-08 Fri 08:35]--[2019-02-08 Fri 12:05] =>  3:30
    :END:

Now that we have updated linux to latest vcpkg, we need to do the same
for windows and osx. Hopefully latest boost.di and boost will fix the
errors we are experiencing there.

*** COMPLETED Update C# reference models to latest dogen              :story:
    CLOSED: [2019-02-11 Mon 09:08]
    :LOGBOOK:
    CLOCK: [2019-02-11 Mon 09:09]--[2019-02-11 Mon 09:25] =>  0:16
    CLOCK: [2019-02-11 Mon 08:42]--[2019-02-11 Mon 09:08] =>  0:26
    :END:

At present the C# reference models do not work with latest dogen.

*** COMPLETED Convert the utility model into a regular dogen model    :story:
    CLOSED: [2019-02-12 Tue 12:25]
    :LOGBOOK:
    CLOCK: [2019-02-12 Tue 09:19]--[2019-02-12 Tue 12:25] =>  3:06
    CLOCK: [2019-02-11 Mon 17:39]--[2019-02-11 Mon 17:45] =>  0:06
    CLOCK: [2019-02-11 Mon 17:15]--[2019-02-11 Mon 17:38] =>  0:23
    CLOCK: [2019-02-11 Mon 14:49]--[2019-02-11 Mon 17:14] =>  2:25
    CLOCK: [2019-02-11 Mon 13:54]--[2019-02-11 Mon 14:48] =>  0:54
    CLOCK: [2019-02-11 Mon 11:51]--[2019-02-11 Mon 12:11] =>  0:20
    CLOCK: [2019-02-11 Mon 11:37]--[2019-02-11 Mon 11:50] =>  0:13
    :END:

Up to now we have manually created utility. However, as part of the
CLI cleanup we should really have high-level constructs to represent
logging etc. It makes no sense to create these types
manually. Instead, we need to create a utility model and mark all of
the existing types as either hand-crafted or regenerate them via dogen
(for example for enums).

*** COMPLETED Improve support for clang-cl builds                     :story:
    CLOSED: [2019-02-13 Wed 10:03]
    :LOGBOOK:
    CLOCK: [2019-02-13 Wed 09:20]--[2019-02-13 Wed 09:52] =>  0:32
    CLOCK: [2019-02-12 Tue 08:28]--[2019-02-12 Tue 08:47] =>  0:19
    CLOCK: [2019-02-09 Sat 16:19]--[2019-02-09 Sat 16:37] =>  0:18
    CLOCK: [2019-02-09 Sat 15:25]--[2019-02-09 Sat 16:18] =>  0:53
    :END:

We have added preliminary support for building with clang-cl on
windows, but the build is not green. Most of the errors seem to be on
boost.

With boost 1.69 we now have mostly green builds. The only problem is
that one of the ref impl tests is failing:

: Running 1 test case...
: unknown location(0): fatal error: in "boost_model_tests/validate_serialisation": class boost::archive::archive_exception: unregistered void cast class masd::cpp_ref_impl::boost_model::class_derived<-class masd::cpp_ref_impl::boost_model::class_base
: ..\..\..\..\projects\masd.cpp_ref_impl.test_model_sanitizer\tests\boost_model_tests.cpp(56): last checkpoint: validate_serialisation
:
: *** 1 failure is detected in the test module "test_model_sanitizer_tests"

Its not obvious why it is failing as the debug tests are passing. We
should just open a story for this.

Links:

- [[https://ci.appveyor.com/project/mcraveiro/dogen/builds/19463961/job/6bnv6ppljlklu2ag][Release build]]
- [[https://ci.appveyor.com/project/mcraveiro/dogen/builds/19463961/job/45yhn8sdhexvsdmi][Debug build]]
- [[https://github.com/Kitware/CDash/issues/733][CDash reporting problems]]

*** COMPLETED Simplify split configuration configuration              :story:
    CLOSED: [2019-02-15 Fri 09:43]

*Rationale*: implemented as part of moving extraction options into
meta-data.

At present we have two separate command line parameters to configure
the main output directory and the directory for header files. The
second parameter is used for split configurations. The problem is that
we now need to treat split configuration projects specially because of
this. It makes more sense to force the header directory to be relative
to the output path and make it a meta-data parameter.

*** COMPLETED Make "ignore regexes" a model property                  :story:
    CLOSED: [2019-02-15 Fri 09:44]

*Rationale*: implemented as part of moving extraction options into
meta-data.

At present we have a command line option:
=--ignore-files-matching-regex=. It is used to ignore files in a
project. However, the problem is, because it is a command line option,
it must be supplied with each invocation of Dogen. This means that if
we want to run dogen from outside the build system, we need to know
what options were set in the build scripts or else we will have
different results. This is a problem for testing. We should make it a
meta-data option, which is supplied with each model and even more
interesting, can be used with profiling. This means we can create
profiles for specific purposes (ODB, lisp, etc) and then reuse them in
different projects.

We should do the same thing for =--delete-extra-files=.

*** COMPLETED Fix the northwind model                                 :story:
    CLOSED: [2019-02-15 Fri 09:45]

*Rationale*: implemented as part of the ref impl / vcpkg clean up.

There are numerous problems with this model:

- at present we have oracle support on ODB. Oracle libs are not
  distributed with debian. If we do not find oracle we do not compile
  northwind. This is not ideal. We should remove oracle support from
  northwind, and install odb support in the build machine (hopefully
  available as debs).
- the tests are commented out and require a clean up.
- the tests require a database to be up.

Notes:

- it is possible to setup [[https://docs.travis-ci.com/user/database-setup/#postgresql][postgres on travis]]

*** COMPLETED Move generation properties into meta-data               :story:
    CLOSED: [2019-02-15 Fri 11:23]
    :LOGBOOK:
    CLOCK: [2019-02-15 Fri 09:38]--[2019-02-15 Fri 09:49] =>  0:11
    CLOCK: [2019-02-15 Fri 08:51]--[2019-02-15 Fri 09:37] =>  0:46
    CLOCK: [2019-02-14 Thu 21:04]--[2019-02-14 Thu 21:10] =>  0:06
    CLOCK: [2019-02-14 Thu 17:45]--[2019-02-14 Thu 18:29] =>  0:44
    CLOCK: [2019-02-14 Thu 16:41]--[2019-02-14 Thu 17:15] =>  0:34
    CLOCK: [2019-02-14 Thu 16:30]--[2019-02-14 Thu 16:40] =>  0:10
    CLOCK: [2019-02-14 Thu 16:25]--[2019-02-14 Thu 16:29] =>  0:04
    CLOCK: [2019-02-14 Thu 15:33]--[2019-02-14 Thu 16:24] =>  0:51
    CLOCK: [2019-02-14 Thu 14:40]--[2019-02-14 Thu 15:32] =>  0:52
    CLOCK: [2019-02-14 Thu 14:01]--[2019-02-14 Thu 14:39] =>  0:38
    CLOCK: [2019-02-14 Thu 09:54]--[2019-02-14 Thu 11:46] =>  1:52
    CLOCK: [2019-02-13 Wed 17:53]--[2019-02-13 Wed 18:34] =>  0:41
    CLOCK: [2019-02-13 Wed 17:02]--[2019-02-13 Wed 17:52] =>  0:50
    CLOCK: [2019-02-12 Tue 18:30]--[2019-02-12 Tue 18:43] =>  0:13
    CLOCK: [2019-02-12 Tue 17:35]--[2019-02-12 Tue 18:29] =>  0:54
    :END:

We have a number of properties that are in the configuration of the
code generator but which are really part of the model. We need to move
these into the model to avoid having to add them to the new CLI
interface.

Notes:

- rename "yarn." transforms in log to "masd." - done.

*** COMPLETED Disabling facet globally and enabling locally fails     :story:
    CLOSED: [2019-02-18 Mon 13:02]
    :LOGBOOK:
    CLOCK: [2019-02-18 Mon 14:30]--[2019-02-18 Mon 14:51] =>  0:21
    CLOCK: [2019-02-18 Mon 11:02]--[2019-02-18 Mon 13:02] =>  2:00
    :END:

We tried to disable hash globally and then enable it just for the
types that require it, but it was not expressed. Interestingly,
disabling an archetype globally and then enabling it locally does work
(e.g. forward declarations).

*** COMPLETED References to types in top-level namespace do not resolve :story:
    CLOSED: [2019-02-19 Tue 10:58]
    :LOGBOOK:
    CLOCK: [2019-02-19 Tue 10:35]--[2019-02-19 Tue 11:04] =>  0:29
    CLOCK: [2019-02-18 Mon 17:49]--[2019-02-18 Mon 17:56] =>  0:07
    CLOCK: [2019-02-15 Fri 16:01]--[2019-02-15 Fri 16:16] =>  0:15
    :END:

When referring to =weaving_styles= defined in =masd::dogen= from
within =masd::dogen::cli=, dogen failed to resolve the
type. Qualifying it as =masd::dogen::weaving_styles= solved the
problem. Resolver is not walking up the path correctly.

We also need to take into account the case where the name is used
within a inner module.

*** COMPLETED Create a colour palette test model                      :story:
    CLOSED: [2019-02-20 Wed 10:56]
    :LOGBOOK:
    CLOCK: [2019-02-20 Wed 10:57]--[2019-02-20 Wed 11:12] =>  0:15
    CLOCK: [2019-02-20 Wed 10:02]--[2019-02-20 Wed 10:56] =>  0:54
    :END:

Thus far we have been updating the colour palette in a ad-hoc
fashion. The problem is, since we don't have a model that uses all
colours, we do not know how they look together. The idea with colours
is that we can look at a model and quickly find meta-information; if
we are using the same colours with multiple meanings, the approach no
longer works.

Create a simple "colour palette" test model that exercises all
stereotypes which are expressed as colours and ensure there is some
kind of useful pattern.

*** COMPLETED Add support for header-only types                       :story:
    CLOSED: [2019-02-21 Thu 13:13]

*Rationale*: this was already implemented.

Sometimes we may just want to generate a simple header only class. By
default we always get a cpp. We could suppress the cpp by having a
stereotype:

: masd::header_only

This can be a simple profile like handcrafted. It can even be a
superset of handcrafted.

*** COMPLETED Create a single binary for all of dogen                 :story:
    CLOSED: [2019-02-21 Thu 14:54]
    :LOGBOOK:
    CLOCK: [2019-02-22 Fri 16:20]--[2019-02-22 Fri 17:42] =>  1:22
    CLOCK: [2019-02-22 Fri 15:42]--[2019-02-22 Fri 16:19] =>  0:37
    CLOCK: [2019-02-22 Fri 14:45]--[2019-02-22 Fri 15:41] =>  0:56
    CLOCK: [2019-02-21 Thu 14:55]--[2019-02-21 Thu 15:43] =>  0:48
    CLOCK: [2019-02-21 Thu 14:09]--[2019-02-21 Thu 14:54] =>  0:45
    CLOCK: [2019-02-21 Thu 13:21]--[2019-02-21 Thu 14:08] =>  0:47
    CLOCK: [2019-02-21 Thu 10:53]--[2019-02-21 Thu 11:23] =>  0:30
    CLOCK: [2019-02-21 Thu 10:28]--[2019-02-21 Thu 10:52] =>  0:24
    CLOCK: [2019-02-21 Thu 10:07]--[2019-02-21 Thu 10:27] =>  0:20
    CLOCK: [2019-02-21 Thu 08:50]--[2019-02-21 Thu 10:06] =>  1:16
    CLOCK: [2019-02-20 Wed 20:09]--[2019-02-20 Wed 20:21] =>  0:12
    CLOCK: [2019-02-20 Wed 19:45]--[2019-02-20 Wed 20:08] =>  0:23
    CLOCK: [2019-02-20 Wed 18:41]--[2019-02-20 Wed 19:00] =>  0:19
    CLOCK: [2019-02-20 Wed 15:42]--[2019-02-20 Wed 18:40] =>  2:58
    CLOCK: [2019-02-20 Wed 15:34]--[2019-02-20 Wed 15:41] =>  0:07
    CLOCK: [2019-02-20 Wed 14:41]--[2019-02-20 Wed 15:33] =>  0:52
    CLOCK: [2019-02-20 Wed 11:13]--[2019-02-20 Wed 12:52] =>  1:39
    CLOCK: [2019-02-20 Wed 09:45]--[2019-02-20 Wed 10:01] =>  0:31
    CLOCK: [2019-02-19 Tue 15:47]--[2019-02-19 Tue 16:25] =>  0:38
    CLOCK: [2019-02-19 Tue 14:12]--[2019-02-19 Tue 15:46] =>  1:34
    CLOCK: [2019-02-19 Tue 11:19]--[2019-02-19 Tue 13:04] =>  1:45
    CLOCK: [2019-02-15 Fri 16:28]--[2019-02-15 Fri 17:13] =>  0:45
    CLOCK: [2019-02-15 Fri 11:24]--[2019-02-15 Fri 11:45] =>  0:21
    CLOCK: [2019-02-15 Fri 09:54]--[2019-02-15 Fri 11:23] =>  1:29
    CLOCK: [2019-02-13 Wed 10:04]--[2019-02-13 Wed 10:41] =>  0:37
    CLOCK: [2019-02-11 Mon 10:01]--[2019-02-11 Mon 11:27] =>  1:26
    CLOCK: [2019-02-09 Sat 16:38]--[2019-02-09 Sat 18:15] =>  1:37
    CLOCK: [2019-02-09 Sat 06:55]--[2019-02-09 Sat 07:40] =>  0:45
    CLOCK: [2019-02-08 Fri 17:04]--[2019-02-08 Fri 17:29] =>  0:25
    CLOCK: [2019-02-08 Fri 16:20]--[2019-02-08 Fri 17:03] =>  0:43
    CLOCK: [2019-02-08 Fri 14:03]--[2019-02-08 Fri 16:00] =>  1:57
    CLOCK: [2019-02-08 Fri 13:31]--[2019-02-08 Fri 13:40] =>  0:19
    CLOCK: [2019-02-07 Thu 16:20]--[2019-02-07 Thu 17:03] =>  0:43
    CLOCK: [2019-02-07 Thu 14:05]--[2019-02-07 Thu 15:40] =>  1:35
    CLOCK: [2019-02-07 Thu 09:21]--[2019-02-07 Thu 12:05] =>  2:44
    CLOCK: [2019-02-07 Thu 08:36]--[2019-02-07 Thu 09:20] =>  0:44
    CLOCK: [2019-02-06 Wed 16:01]--[2019-02-06 Wed 19:03] =>  3:02
    :END:

As per analysis, we need to create a single dogen binary, like so:

: dogen.cli COMMAND COMMAND_SPECIFIC_OPTIONS

Where =COMMAND= is:

- =transform=: functionality that is currently in tailor.
- =generate=: functionality that is currently in knitter.
- =expand=: functionality that is currently in stitcher plus expansion
  of wale templates.
- =make=: functionality in darter: create project, structure etc.

In order to support sub-commands we need to do a lot of hackery with
program options:

- [[https://gist.github.com/randomphrase/10801888][cmdoptions.cpp]]: Demonstration of how to do subcommand option
  processing with boost program_options
- [[https://stackoverflow.com/questions/15541498/how-to-implement-subcommands-using-boost-program-options][How to implement subcommands using Boost.Program_options?]]

Notes:

- create a top-level code generation transform that uses the API
  options; internally it converts them to legacy options and calls the
  coding workflow.
- add methods to application to execute each activity. Then create a
  boost visitor for each of the activities that calls each method.
- move the hand-crafted configuration defaults in program options
  parser into configuration builder.
- logs from generation get overridden with conversion
- log should start with app details, including command line options so
  we can see what command we're executing.

*Merged Stories*

We started off by creating lots of little executables: knitter,
darter, tailor, stitcher. Each of these has its own project,
command-line options etc. However, now that we are concentrating all
of the domain knowledge in yarn, it seems less useful to have so many
executables that are simply calling yarn transforms. Instead, it may
make more sense to use an approach similar to git and have a
"sub-command":

: dogen knit
: dogen tailor

And so forth. Of course, we could also take this opportunity and clean
up these names to making them more meaningful to end users. Perhaps:

: dogen codegen
: dogen transform

Each of these sub-commands or modes would have their own set of
associated options. We need to figure out how this is done using boost
program options. We also need to spend a bit of time working out the
sub-commands to make sure they make sense across the board.

In terms of names, we can't really call the project "dogen". We should
call it something allusive to the command line, such as cli. However,
the final binary should be called dogen or perhaps, =dogen.cli=. This
fits in with other binaries such as =dogen.web=, =dogen.http=,
=dogen.gui= etc.

*** COMPLETED Setup a nightly build for Dogen                         :story:
    CLOSED: [2019-02-22 Fri 14:44]
    :LOGBOOK:
    CLOCK: [2019-02-22 Fri 14:10]--[2019-02-22 Fri 14:44] =>  0:34
    CLOCK: [2019-02-22 Fri 13:45]--[2019-02-22 Fri 14:09] =>  0:24
    CLOCK: [2019-02-22 Fri 12:29]--[2019-02-22 Fri 12:37] =>  0:08
    CLOCK: [2019-02-22 Fri 12:01]--[2019-02-22 Fri 12:28] =>  0:27
    CLOCK: [2019-02-22 Fri 11:01]--[2019-02-22 Fri 12:00] =>  0:59
    CLOCK: [2019-02-22 Fri 09:45]--[2019-02-22 Fri 11:00] =>  1:15
    :END:

We haven't had nightlies with valgrind for a long time. We need these
for both Dogen and the C++ ref impl.

*** COMPLETED Update annotation profiles and stereotypes to masd namespace :story:
    CLOSED: [2019-02-22 Fri 18:52]

*Rationale*: this has been implemented as part of the great meta-data
rename..

We should rename all annotation profiles and all stereotypes into the
MASD namespace.

We should also rename the artefact formatters to a compliant names,
e.g. instead of =C# Artefact Formatter= maybe
=dogen::csharp_artefact_formatter=. Note its dogen not MASD because
these are dogen specific profiles. We need to create a model for
dogen, separate from the MASD standard profile.

*** COMPLETED Great meta-data rename                                  :story:
    CLOSED: [2019-02-22 Fri 19:09]
    :LOGBOOK:
    CLOCK: [2019-02-22 Fri 18:52]--[2019-02-22 Fri 19:09] =>  0:17
    CLOCK: [2019-02-22 Fri 18:02]--[2019-02-22 Fri 18:51] =>  0:49
    CLOCK: [2018-11-26 Mon 10:01]--[2018-11-26 Mon 11:02] =>  1:01
    CLOCK: [2018-11-23 Fri 21:43]--[2018-11-23 Fri 22:20] =>  0:37
    CLOCK: [2018-11-23 Fri 20:55]--[2018-11-23 Fri 21:42] =>  0:47
    CLOCK: [2018-11-23 Fri 16:57]--[2018-11-23 Fri 18:08] =>  1:11
    CLOCK: [2018-11-23 Fri 16:50]--[2018-11-23 Fri 16:56] =>  0:06
    CLOCK: [2018-11-23 Fri 15:37]--[2018-11-23 Fri 16:49] =>  1:12
    :END:

All of the existing stereotypes and meta-data need to be moved from
the existing names (e.g. =quilt=, =yarn=, etc) into
=masd=. Interestingly, we can take this opportunity to make dia
diagrams a bit more readable. Instead of

: #DOGEN a.b.c=d

we can now just do:

: masd.a.b.c=4

It is very unlikely dia users will need lines starting with =masd.=.

We should probably try to tackle this rename sooner rather than later
since it badly breaks model-compatibility.

We should use the new names as part of this rename, e.g.:

: masd.injection.dia.comment
: masd.extraction.cpp.enabled

Rename =is_proxy_model= to =platform_definition_model=.

Notes:

- decoration etc are still not using the =masd.= prefix.

Merged stories:

*Update all stereotypes to masd*

We need to start distinguishing MASD from dogen. The profile for UML
is part of MASD rather than dogen, so we should update all stereotypes
to match. We need to make a decision regarding the "dia extensions" -
its not clear if its MASD or dogen.

*Clean up UML profiles and meta-data*

- we should wait until we rename =quilt= too so we can clean up the
  quilt meta-data at the same time.
- rename references too since they belong to external, i.e.:

: #DOGEN yarn.reference=annotations.dia

  should be:

: #DOGEN external.reference=annotations.dia

- similarly with:

: #DOGEN yarn.dia.comment=true

  should instead be:

: #DOGEN external.dia.comment=true

  in fact, should we mention "tagged values" instead of "comment"?

*** COMPLETED Throw on profiles that refer to invalid fields          :story:
    CLOSED: [2019-02-22 Fri 19:31]
    :LOGBOOK:
    CLOCK: [2019-02-22 Fri 19:10]--[2019-02-22 Fri 19:31] =>  0:21
    :END:

At present during profile instantiation, if we detect a field which
does not exist we skip the profile. This was done in the past because
we had different binaries for stitch, knit etc, which meant that we
could either split profiles by application or skip errors
silently. Now we have a single binary, we could enable this
validation. However, the stitch tests still rely on this
behaviour. The right solution for this is to have some kind of
override flag ("compatibility mode" springs to mind) which is off by
default but can be used (judiciously).

*** STARTED Dogen's vcpkg export for OSX was created from master      :story:
    :LOGBOOK:
    CLOCK: [2019-02-09 Sat 06:20]--[2019-02-09 Sat 06:46] =>  0:26
    CLOCK: [2019-02-09 Sat 05:50]--[2019-02-09 Sat 06:10] =>  0:20
    :END:

Problems:

- we have built it from master instead of masd branch.
- installing libodb et al. from master fails due to a config error. We
  need to check that master has our fix. We need to check that the
  config.h workaround works for OSX as well.
- when building using the masd branch, we can't download ODB from git
  due to a hash mismatch. This may be something to do with the git
  version (2.7).

*** STARTED Fix clang-cl warnings                                     :story:
    :LOGBOOK:
    CLOCK: [2019-02-19 Tue 09:32]--[2019-02-19 Tue 09:53] =>  0:21
    CLOCK: [2019-02-18 Mon 14:52]--[2019-02-18 Mon 17:48] =>  2:56
    CLOCK: [2019-02-15 Fri 15:20]--[2019-02-15 Fri 16:01] =>  0:41
    CLOCK: [2019-02-15 Fri 14:21]--[2019-02-15 Fri 14:59] =>  0:38
    CLOCK: [2019-02-14 Thu 11:47]--[2019-02-14 Thu 12:18] =>  0:31
    :END:

We also have a number of warnings left to clean up, all related to
boost.log:

: masd.dogen.utility.lib(lifecycle_manager.cpp.obj) : warning LNK4217: locally defined symbol
: ?get_tss_data@detail@boost@@YAPEAXPEBX@Z (void * __cdecl boost::detail::get_tss_data(void const *))
: imported in function "public: struct boost::log::v2s_mt_nt6::sinks::basic_formatting_sink_frontend<char>::formatting_context * __cdecl boost::thread_specific_ptr<struct boost::log::v2s_mt_nt6::sinks::basic_formatting_sink_frontend<char>::formatting_context>::get(void)const " (?get@?$thread_specific_ptr@Uformatting_context@?$basic_formatting_sink_frontend@D@sinks@v2s_mt_nt6@log@boost@@@boost@@QEBAPEAUformatting_context@?$basic_formatting_sink_frontend@D@sinks@v2s_mt_nt6@log@2@XZ)

Notes:

- opened issue: [[https://github.com/Microsoft/vcpkg/issues/5336][Building with clang-cl on windows generates warnings
  from vcpkg-installed libraries]]
- it seems that the log files show a lot more warnings than those
  reported by cdash,
- Updated issue on CDash parsing problems for clang-cl: [[https://github.com/Kitware/CDash/issues/733][Parsing of
  errors and warnings from clang-cl]]
- Sent email to clang mailinglist: [[http://lists.llvm.org/pipermail/cfe-dev/2019-February/061326.html][Clang-cl - errors and warning
  messages slightly different from MSVC]]. Clang [[http://lists.llvm.org/pipermail/cfe-dev/2019-February/061339.html][have patched]] the diffs
  now.

*** STARTED Use tracing options in existing code                      :story:
    :LOGBOOK:
    CLOCK: [2019-02-19 Tue 11:05]--[2019-02-19 Tue 11:18] =>  0:13
    CLOCK: [2019-02-18 Mon 18:50]--[2019-02-18 Mon 18:52] =>  0:02
    CLOCK: [2019-02-18 Mon 14:52]--[2019-02-18 Mon 15:04] =>  0:12
    CLOCK: [2019-02-15 Fri 17:14]--[2019-02-15 Fri 17:44] =>  0:30
    :END:

Tasks:

- read the byproduct directory and supply it to probing somehow.
- add dependency to API from tracing.
- implement a tracer constructor that takes in tracing configuration.
- add tracing configuration to coding options.
- update knitter to generate tracing options.
- delete probing options from configuration.
- delete probing options from tracer.

*** STARTED Model references are not transitive                       :story:
    :LOGBOOK:
    CLOCK: [2019-02-18 Mon 18:29]--[2019-02-18 Mon 18:50] =>  0:21
    :END:

For some reason we do not seem to be following references of
referenced models. We should load them automatically, now that they
are part of the meta-data. However, the =yarn.json= model breaks when
we remove the reference to annotation even though it does not use this
model directly and =yarn= is referencing it correctly.

The reason why is that we load up references to all intermediate
models, but then on merge we only take target references. What we
really need to do is to combine the reference containers on merge. For
this we need to create a method that loops through the map and inserts
all keys which have not yet been inserted. Something like "merge
references".

We should address this issue when we introduce two-phase parsing of
models. This is because, as with the new meta-model elements, we also
need to do a first pass across the target and all reference models to
obtain all the paths for all referenced models. We then need to obtain
the unique set of referenced models and load those. To put in this
logic in the code at present (i.e. without a two-phase approach) would
mean we'd have to load the same models several times (or heavily
rewrite existing code, resulting in a two-phase approach, anyway).

*** Implement configuration validator                                 :story:

At present we are not performing any validation.

*** Create transforms for templating                                  :story:

At present we are using workflows to convert stitch and wale
templates. In reality, these are just tranforms. We need to figure out
if there should just be a high-level transform in orchestrator that
encapsulates these or if the templating model itself should follow the
naming convention.

*** Remove the need for =dia.comment= tag                             :story:

At present we are detecting the presence of =masd.dogen.dia.comment=
in a UML comment to determine if it is to be processed as a comment
for the model module. However, we could just as well look for the
presence of meta-data parameters instead. Similarly, we could say that
it is an error to have more than one comment with meta-data parameters
(as hopefully with do at present with dia.comment). This is a
usability papercut.

While we're there we could also remove the need for =#DOGEN= and state
that all meta-data keys must start with =masd.=. For user specific
keys we could namespace them: =masd.user.=.

*** Add support for "directory mode" in conversion                    :story:

The real use case we have for conversion is to point it to a directory
with models and give it a destination "type" (e.g. json) and a output
directory, and then have it convert all models to that type and place
them in the output directory.

A second but related use case is to point it to a model, supply a
destination "type" and then output it into a directory, without having
to supply a destination file.

In effect, this is a common use case for all commands (generate and
weaving as well). We could probably deduce it: if the user supplied a
directory as a target, we should do it in directory mode.

*** Add stereotype for IoC containers                                 :story:

At present we are marking IoC containers with either handcrafted or
header only. In reality, they should have their own stereotype and
colours as, in the future, we want to code generate them. However, we
can only do this once we get rid of the initialisers because they are
also a form of IoC containers, but with different requirements.

Proposed stereotype: =masd::ioc::container=.

Actually, this is not quite right. We are not creating the IoC
containers themselves, but the wiring code that sets up these
containers. We need to figure out the correct term for
these. Suggestions:

- [[https://github.com/avao/Qart/blob/master/Src/Qart.CyberTester/Bootstrapper.cs][bootstrapper]], with a =Bootstrapper.CreateContainer= method;
  e.g. =masd::ioc::bootstrapper=.

*** Handling of forward declarations on generated types               :story:

At present, if we disable forward declarations globally (in a profile,
say), the code fails to build with errors on visitors. This is because
we need forward declarations for:

- the visitable type;
- all of its descendants;
- the visitor.

This is a hard requirement because, without these the code does not
make sense. We need some way of "forcing" enablement for some features
where there is such a hard dependency. This is probably something we
need to look at when we implement "computable enablement". We then
need some way of telling the system about these dependencies:
e.g. visitor requires enablement x, y, z.

A second problem is that, at present, there is no way to manually
enable (force) forward declarations on visitors. We can enable them on
all model elements but not on the generated type. Because of this we
are generating forward declarations for all types, for no reason.

*** Disable global hashing on coding                                  :story:

We are generating hash for all types at present in coding but we only
need it for two types: name and location. Try to switch it off
globally and on just for those two types.

*** Rename the =transform= method to =apply=                          :story:

Its a bit silly to name classes =x_transform= and then to have their
main method also called =transform=. We should rename these to
something like =apply=.

*** Move text model into extraction model                             :story:

We started this work but stopped half-way. This is required in order
to move to the new pipeline orchestration.

Tasks:

- copy the current state of all types into extraction as they have
  moved on.
- make coding refer to extraction to start off with. Eventually the
  transforms can be moved over to =generation.extraction=.

*** Stitch does not have a force write flag                           :story:

At present the stitch workflow is hardcoded not to force write. The
correct solution is to allow the template to have a force write
parameter.

*** Contents change check is done twice                               :story:

We seem to check twice if a file has changed:

: 2015-04-26 12:37:28.451464 [DEBUG] [formatters.filesystem_writer] File contents have not changed, and force write is false so not writing.
: 2015-04-26 12:37:28.451486 [DEBUG] [formatters.filesystem_writer] File contents have not changed, and force write is false so not writing.

This is in stitch but it should be the same for knit.

*** Convert utility exceptions into dogen exceptions                  :story:

At present the utility model has a number of hand-crafted
exceptions. We need to convert them to dogen exceptions. We also need
to get rid of the invalid enum exception and use the
=std::argument...= exception instead.

*** Rename profile header only                                        :story:

This profile only applies to C++ so it should be:

: masd::cpp::header_only

*** JSON models in dogen are out of sync                              :story:

Problems:

- tailor generation results in files with the wrong name (=dia.json=)
- input models were copied into test data.

*** Add option for northwind tests                                    :story:

  At present, when we detect ODB and associated libraries, we build and
  run the northwind tests. However, not all build agents have postgres
  installed. We need an option that can be used to stop the inclusion of
  the northwind tests - or ideally, to build the tests but not run it.

*** Update dogen's windows vcpkg export                               :story:

- ensure we built it from masd and not master
- check master builds libodb 2.4
- build libodb 2.5 from masd and re-export.

*** Move top-level transforms into orchestration                      :story:

- clear up the existing orchestration model We don't really know what
  its current state is. Keep it as a backup as we may need to go back
  to it.
- copy the top-level chains into orchestration, into a well
  defined namespace (say =dirty=). This must include the model to text
  model and registration. Remove all of these types from coding. At
  this point coding should only depend on injectors.

*** Move generation model out of coding                               :story:

- then copy the model from coding into generation and all associated
  transforms.
- then add support in each generation model (cpp, csharp) for
  converting from the generation model to the formattables model.
- then create a model generation chain that uses the generation model.
- then delete the model and transforms from coding; delete the
  adaptors from generation models (cpp, csharp).
- then move the model to text model chain into generation.

*** Move generation element properties back into formattables         :story:

We moved a number of properties out of formattables. Move them
back. By the end of this refactor we should end up with no references
to facets in coding.

*** Move injection processing out of coding                           :story:

- add model source into injection
- add model set into injection, with target and references
- add workflow that takes in a string, path, etc and creates a model
  set. It will need to read references and language from the model
  annotations.
- add model set into coding.
- add a new model: coding.injection. Create a class that converts from
  one model set to another.
- add a chain in orchestration that does the new injection workflow
  and passes the model set into coding.
- delete injection related classes in coding.

*** Move fabric types into generation                                 :story:

- copy across the fabric types from cpp and csharp into generation.
- update formatters to use the types from generation.
- delete them from original models.

*** Move formattables into generation                                 :story:

- first, update the generation model with formattable properties from
  cpp: add a formattable type to the generation model and container
  for it, add the formattable population logic. Then remove the
  formattable logic from cpp.
- repeat the exercise with csharp. We should end up with two new
  namespaces in generation handling the fabric meta-types and their
  processing.
- by the end of this refactor, cpp and csharp should contain only the
  formatters.

*** Create =generation.extraction= model                              :story:

- rename =generation.cpp= to =generation.extraction=.
- rename =formatters= namespace to =cpp=.
- ensure the logic for processing one tech space will work for
  multiple tech spaces. For example, we could move the existing
  workflow into the =cpp= namespace and register the text generation
  chain from there.
- repeat the exercise with the csharp model.
- by the end of this refactor we should end up with a single
  =generation.extraction= containing both the csharp and cpp
  formatters.
- consider renaming formatters to model to text transforms.

*** Inheriting from oneself causes segfault                           :story:

If you set an object to inherit from itself, say via metadata:

: #DOGEN masd.generalization.parent=in_memory_weaver

Dogen segfaults due to recursion. We need to test this via UML
inheritance as well.

*** Implement the new dogen product API                               :story:

Now the API has been designed and generated, we need to implement it.

*** Fix cmake emacs variable for tab width                            :story:

We need to replace uses of =tab-width= in cmake files with
=cmake-tab-width=, as explained here:

[[http://stackoverflow.com/questions/25751408/controlling-the-indent-offset-for-cmake-in-emacs][Controlling the indent/offset for CMake in emacs]]

We need to do this for both code generated and manually generated
files.

*** Fix =cp= error on cmake with local third-party packages           :story:

We are getting strange errors in cmake:

: cp: cannot stat /usr/lib/i386-linux-gnu/libpthread.so.1.54.0: No such file or directory

*** Assorted improvements to CMake files                               :epic:

It seems we are not using proper CMake idioms to pick up compiler
features, as explained here:

- [[http://unclejimbo.github.io/2018/06/08/Modern-CMake-for-Library-Developers/][Modern CMake for Library Developers]]
- [[http://www.slideshare.net/DanielPfeifer1/cmake-48475415][CMake - Introduction and best practices]]
- [[https://datascience.lanl.gov/data/151208-LANL-Hoffman-Science.pdf][Building Science with CMake]]
- [[http://voices.canonical.com/jussi.pakkanen/2013/03/26/a-list-of-common-cmake-antipatterns/][A list of common CMake antipatterns]]
- [[https://rix0r.nl/blog/2015/08/13/cmake-guide/][The Ultimate Guide to Modern CMake]]
- [[https://github.com/crezefire/cxp][CXP: C++ Cross Platform]]: A template project for creating a cross
  platform C++ CMake project using modern CMake syntax and transitive
  dependencies.

We need to implement this using proper CMake idioms.

Notes:

- Add version and language to project.
- start using [[https://cmake.org/cmake/help/v3.3/command/target_compile_options.html][target compile options]] for each target. We will have to
  repeat the same flags; this could be avoided by passing in a
  variable. See also [[http://stackoverflow.com/questions/23995019/what-is-the-modern-method-for-setting-general-compile-flags-in-cmake][What is the modern method for setting general
  compile flags in CMake?]]
- define qualified aliases for all libraries, including nested
  aliasing for =dogen::test_models=. Ensure all linking is done
  against qualified names.
- use target include directories for each target and only add the
  required include directories to each target. Mark them with the
  appropriate visibility, including using =interface=. We should then
  remove all duplication of libraries in the specs.
- try replacing calls to =-std=c++-14= with compiler feature
  detection. We need to create a list of all C++-14 features we're
  using.
- remove all of the debug/release compilation options and start using
  =CMAKE_BUILD_TYPE= instead. See [[http://pastebin.com/jCDW5Aa9][this]] example. We added build type
  support to our builds, but as a result, the binaries moved from
  =stage/bin= to =bin=. There is no obvious explanation for this.
- remove =STATIC= on all libraries and let users specify which linkage
  to use. We already have a story to capture this work.
- remove the stage folder and use the traditional CMake
  directories. This will also fix the problems we have with
  BUILD_TYPE.
- consider buying the CMake book: https://crascit.com/professional-cmake/.

Merged stories:

*Usage of external module path in cmakelists*                       :story:

It seems like we are not populating the target names
properly. Originally the target name for test model all built-ins was:

: dogen_all_builtins

When we moved the test models into =test_models= the target name did
not change. It should have changed to:

: dogen_test_models_all_builtins

*** Support for cmake components and groups                           :story:

#+begin_quote
*Story*: As a dogen user, I need to integrate the generated models
with my existing packaging code.
#+end_quote

We recently added support for creating multiple packages from a single
source tree. We need generated models to have a new top-level cmake file:

: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/src)
: add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/tests)
:
: install(
:     DIRECTORY include/
:     DESTINATION include
:     COMPONENT headers
:     FILES_MATCHING PATTERN "*.hpp")

And the =src= cmake file:

: install(TARGETS dia ARCHIVE DESTINATION lib COMPONENT libraries)

*** Mop-up nested namespaces using legacy syntax                      :story:

It seems we still have a number of places in the templates where we
are using the legacy nested namespaces. Its probably only in
serialisation, given that's the only place where we've hard-coded the
namespaces and they are more than one level deep (we have a lot of
=std= but that's not affected):

: namespace boost {
: namespace serialization {

We need to wrap these in if's for C++ 17 and add nested namespaces.

*** Add DTL to vcpkg                                                  :story:

DTL seems to be the easiest library to work with in terms of
generating diffs. However, its not on vcpkg.

Tasks:

- add CMake support to DTL. Not strictly needed but seems like an easy
  thing to do and will make vcpkg easier. It also means we can build
  tests and examples to make sure it all works in isolation. Actually
  this was tried before and not accepted by the maintainer.
- add DTL port.

Links:

- [[https://github.com/google/diff-match-patch/tree/master/cpp][diff-match-patch]]: interesting diff library but requires QT.
- [[https://github.com/Martinsos/edlib#usage-and-examples][edlib]]: interesting library but seems to be more for Levehnstein
  diffs. Also not on vcpkg.
- [[https://github.com/cubicdaiya/dtl/pull/2][Add cmake support]]: PR to add CMake support to DTL, not accepted by
  the maintainer. See also [[https://github.com/chino540off/dtl][the repo]].
- [[https://github.com/Microsoft/vcpkg/tree/master/ports/libodb][libodb]]: example of a project with a vcpkg specific CMake support.
- [[https://stackoverflow.com/questions/13438547/linux-c-or-c-library-to-diff-and-patch-strings][Linux C or C++ library to diff and patch strings?]]

*** Add tests for external and model modules                          :story:

At present we do not have tests exercising different combinations of
external and model modules.

Tests:

- 0-3 levels of external modules
- 1-3 levels of model modules

*** Rewrite name resolution in terms of lists                         :story:

Even since we did the external modules / model modules change we broke
code generation; this is because we do not go up the model modules
during name resolution. We did a quick hack to fix this but it needs
to be done properly.

Let's walk through a simple example:. Name cames in as:

- model module: =probing=
- simple: =prober=

We are in model:

- model module: =dogen.external=

Expected behaviour is to try all combinations of model modules:

- =dogen.external.probing=
- =dogen.probing
- =probing=

This highlights a fundamental problem with resolution: we view the
{external, model, internal} modules as if they are separate entities
but in reality, for the purposes of resolution, there is only one
thing that is relevant: the module path. If it matches because of
{external, model, internal} modules, well that is not relevant to
resolution. Other users of =name= do need to know this information
(for example to generate directories or file names) but not the
resolver.

Interestingly, because we are only looking for an id, it doesn't
really matter how we get to it (in terms of the internal composition
of the name), as long as it matches bitwise. This means we can look at
the process slightly differently:

- start off with the name as the user provided it. Extract all strings
  from it to create a list, in order: external, model, internal,
  simple. Try to resolve that. Call it user list.
- then create a second list from model / context: external, model,
  internal. Call it model list.
- try concantenating model list and user list, pretty printing and
  resolving it. If it fails, pop model list and concatenate again. Try
  until model list is empty.

Tasks:

- first add a quick hack just to get the code generator working
  again. For example, take the first model module of the model and try
  resolving with that. Then worry about fixing this properly.
- split the conversion of name into list from pretty printer. Printer
  should merely take a string or list of strings and do its thing. We
  need to find a good location for this method, since (for now) we
  cannot place it in the right location which is the name class
  itself.
- change resolver to obtain the lists as per above. The to list
  machinery can be used for this, though we need to handle model names
  somehow. We can copy the =model_name_mode= logic from printer.
- drop all of the logic in resolver at present and use the list logic
  as per above. Do not check references, etc.

Notes:

- there are a few useful functions here:
  - subtraction: given a base list, subtract another list. Fro
    example, given =masd::dogen::annotations::annotation=, subtract
    =masd::dogen::annotations=. This is useful when determining the
    right qualification inside a class.
  - addition: concatenate a list with another.
  - combination: given a base list, create all possible permutations
    for a second list. For example: =masd::dogen::annotations= and
    =some::type=, we want =masd::dogen::annotations::some::type=,
    =masd::dogen::some::type=, =masd::some::type=, =some::type=. We
    are iterating upwards the first list.
  - make id: given a list, generate an ID. This was we don't even need
    to go though the whole "name building" exercise, we simply go from
    lists into ID's and check the containers.
- we probably should introduce a type for this: =flat_location=?
  something that can be converted from a =location= (but not the
  opposite) and has the properties defined above. Or we could have a
  "location flattener" that performs these actions, but this is less
  clean as we now need a few of these helpers.
- there are two fundamental concepts: a path (which is what we call a
  location) and an address (which is what we call an ID). Path implies
  an hierarchical space, which is what modeling and generation space
  are. Address is flat and unique. There is a function to go from
  paths to addresses but not vice-versa. Given two paths we can
  generate all possible addresses by performing a "climb" in the
  hierarchical space.
- we could make addresses URIs, and preserve almost all of the
  information: =masd://some.model.name/a/b.c=. The problem is we
  cannot tell the difference between model modules and external
  modules. However, we could simplify this and say model modules and
  external modules are all the same thing; users can choose to express
  external modules as part of the file name or not. (e.g. "express
  full path" or some such flag). We can also choose to express
  external modules as directories or as a dotted path. URIs may not be
  the best of ideas because models exist in contexts (workspaces,
  servers, users) rather than in one universal space. However, we
  could use URLs as a way to identify resources once we clear up the
  REST story.

*** Default model modules from filename                               :story:

It would be nice to be able to not have to supply model modules when
its obvious from the filename.

*** Nested external model path results in strange references          :story:

Note: we have probably already implemented a solution for this, need
to check the resolver.

The external model path does not contribute to path resolution in a
model. Up til now that has actually been a feature; it would have been
annoying to have to dype =dogen::= on every type for every
model. Instead, we refer to say =dogen::a::b= as simply =a::b= in all
models that use =a=. However this masks a deeper problem: this is not
the desired behaviour at all times. We saw this problem when we
created multiple models under dynamic: =dynamic::schema= and
=dynamic::expansion=. In this case, users of these models referred to
them as =schema= and =expansion= respectively, and this was not
ideal. In general:

- external module path should contribute to references just like
  internal module path does - there should be no difference;
- dogen should be clever enough to determine if two models share a
  top-level namespace (regardless if it was obtained from the external
  or internal module path) that there is no need to have an absolute
  path. So in the case of =dogen=, since every model has =dogen= as
  their external module path, according to this rule we should not
  have to type it.

*** Remove hello world model                                          :story:

 It is confusing to have it mixed up with product models. Use a regular
 dogen model to test the package. We could have it on the reference
 model as a stand alone example, or we could create a "hello dogen"
 product for a trivial example of dogen usage.

*** Move from doxygen to standardese                                  :story:

We should try to use standardese to generate the documentation for
dogen. Seems easier to use and CMake friendly. Also, it seems more c++
compliant because it uses libclang.

Once the move is done, we should update dogen to generate comments in
either markup via a meta-data parameter (documentation markup?).

Links:

- https://github.com/foonathan/standardese

*** Update ref impl namespaces to match the new specification         :story:

Perform the namespace update to the reference implementation.

*** New approach to model testsing                                    :story:

In the beginning we generated all models with all facets, even the
dogen core models. The idea was to test the generator even though
these facets were not useful for the product. This was really useful
because the dogen models are much more realistic than the test models
and due to this we picked up a number of bugs. However, we have now
hit the maximum build times on travis and we need to start removing
all ballast. This will mean we lose these valuable tests. The
alternative is to create these tests on the fly:

- create a new override flag that forces all facets to be emitted.
- create a new test facet with templates that are dependent on the
  enabled facets; each test tests the dependent facet.
- create a ctest nightly build that generates code using these new
  facets, compiles it and runs all tests.
- we need some meta-data to "ignore" some modeling elements for
  certain facets such as composition which are known to be broken. Or
  maybe we should just leave the tests as red so we know.
- the tests should be designed not to use templates etc to make the
  debug dumps really obvious (unlike the existing tests). It may even
  make more sense to test each type individually so that when the test
  fails its really obvious:

: MY_TYPE_serialisation_roundtrips_correctly

  this way when we look at CDash we know exactly which types failed to
  serialise.

During the transition phase, we will remove all of the existing tests.

*** Add support for multiple profile binds per modeling element       :story:

At present we can only bind an element to one profile. The reason why
is because we've already expanded the profile graphs into a flat
annotation and if we were to apply two of these expanded annotations
with common parents, the second application would overwrite the
first. Of course, we bumped into the exact same problem when doing
profile inheritance; there it was solved by ensuring each parent
profile is applied only once for each graph.

One possible solution for this problem is to consider each model
element as a "dynamic profile" (for want of a better name; on the fly
profile?). We would create a profile which is named after each of the
profiles it includes, e.g. say we include =dogen::hashable= and
=dogen::pretty_printable= for model element e0. Then the "on the fly
profile" would be:

: dogen::hashable_dogen::pretty_printable

It would be generated by the profiler, with parents =dogen::hashable=
and =dogen::pretty_printable=, and cached so that if anyone shows up
with that same profile we can reuse it. Because of the additive nature
of profile graphs this would have the desired result. Actually we
could probably have a two pass-process; first identify all of the
required dynamic profiles and generate them; then process them. This
way we can rely on a const data structure.

This will all be made easier when we have a two-pass pipeline because
we can do the profile processing on the first pass, and we can even
generate the "dynamic profiles" as real meta-model elements, created
on the fly.

*** Facet enablement and model references is buggy                    :story:

At present we are processing enablement as part of the
post-processing. This means that we are using the target model's
annotation profile in order to determine the facet enablement. This
can cause problems as follows: say we enable hashing on a model via
the model profile of M0. We then consume that model as a reference and
disable hashing on M1. When processing types from M0 for M1 we will
disable hashing for them as well. Thus, no includes for hashing will
be generated even if a hash map is used.

Actually this is not quite right. We are expanding annotations at the
external model transform level; this means the enablement on the
reference must be correct. However, somehow we seem to be looking at
the element on the target model when deciding to include the hash
file from reference model.

*** Consider creating a test build for all facets                     :story:

In the past we had enabled a lot of facets on the dogen models to
serve as part of the testing infrastructure. However, its no longer
feasible to do this because the build is taking too long. However, the
reference models just can't capture all of the complexity of a
codebase like dogen's so we lost some testability with this move. What
would be really nice is if we could create "test builds":

- given a set of test models, copy them somewhere, generate a product
  configuration with some kind of override that enables all facets
  everywhere. some will just not come through like ORM.
- build the product. all handcrafted code is now blank but all facets
  are coming though.
- this could be part of the ctest script, as a "mode" - product
  generation test. Every time there is a commit to a product the build
  kicks in.

Notes:

- one way to achieve this would be to force the profile of the
  model. However, we are moving away from profiles, and in the future
  there will be a list of stereotypes associated with the model. Then
  it will be much harder to figure out what stereotypes do what and to
  overwrite them.
- an alternative would be to have some kind of "test mode"; when
  handling enablement, we'd check the "mode". If we're in test mode,
  we simply enable all and ignore any other settings. We could have a
  "force enable" flag or some such like we do for
  overwriting. However, we may then hit another problem: enabling all
  facets may result in non-buildable models:
  - facets may be incompatible. This is not a problem at present.
  - handcrafted classes may result in code that does not
    compile. Shouldn't though because we are still checking the status
    of the attributes.
- the key thing though is the overall build time must be below the
  threshold. Maybe we can have this on a nightly, running on our own
  hardware.

Conclusions:

- create a new flag: =force-enablement=. When set to true, we ignore
  all enablement settings and generate all facets. We do not generate
  all kernels though (e.g. the kernel must be on in the model).
- create a script that copies the models to a new product and
  generates them with fore-enablement. This will only work when we can
  generate products.
- as facets are enabled, tests are automatically generated for them.
- build the result and run all tests.

*** Create some basic naming guidelines                               :story:

As per Framework Design Guidelines, we need some basic guidelines for
naming in Dogen. We don't need to go overboard, we just need something
to get us started and evolve it as we go along.

Links:

- [[https://isocpp.org/wiki/faq/coding-standards][C++ Coding Standards]]
- [[http://wiki.c2.com/?CapitalizationRules][Capitalization Rules]]
- [[https://en.wikipedia.org/wiki/Snake_case][Snake Case]]
- [[http://cs.smu.ca/~porter/csc/ref/stl/naming_conventions.html][Naming Conventions for these STL Reference Pages]]
- [[https://style-guides.readthedocs.io/en/latest/cpp.html][C++ coding style guide]]
- [[https://stxxl.org/tags/1.4.1/coding_style.html][Coding Style Guidelines]]
- [[https://www.fluentcpp.com/2018/04/24/following-conventions-stl/][Make Your Containers Follow the Conventions of the STL]]

*** Consider generating program options code                          :story:

If there was a syntax to describe boost program options, we should be
able to generate most of the code for it:

- the code that initialises the options;
- the domain objects that will store the options;
- the copying of values from program options objects into domain
  objects.

This would mean that creating a command line tool would be a matter of
just supplying an options file. We could then have a stereotype for
this (name to be yet identified). Marking a type with this stereotype
and supplying the appropriate meta-data so one could locate the
options file would cause dogen to emit the program options binding
code.

A similar concept seems to exist for python: [[http://docopt.org/][docopt]]. We should keep
the same syntax. We just need to have a well defined domain object for
these. The aim would be to replace config.

For models such as these, the dia representation is just overhead. It
would be great if we could do it using just JSON.

Actually even better would be if we could have a text file in docopt
format and parse it and then use it to generate the code described
above.

Actually maybe we are just making this too complicated. We probably
just need some very trivial meta-data extensions that express the
required concept:

- create a yarn element to model this new meta-class. We basically
  need to model the structure of program options with option groups
  and options.
- define a stereotype for the new yarn elements, say
  =CommandLineOptionGroup=.
- for types facet we simply generate the regular c++ code. But in
  addition, we also generate a new facet that: a) injects the
  propertties into boost program options b) instantiates the c++
  objects from boost program options.
- this means that instead of creating a new meta-type, we need to
  augment =yarn::object= with command line options stuff.

Notes:

- create stereotypes for options group, options; allow users to define
  members of type options in options group. Or should the options just
  be member variables? In which case we could have
  =command_line::options= as the stereotype.
- generate the options classes.
- inject a hand-crafted validator or consider generating the validator
  given the meta-data supplied by the user (mandatory, at most X
  times, etc).
- generate an options builder that takes on the building
  responsibilities from the parser.
- generate a parser that hooks the builder and copies data from the
  options map into the options.
- allow users to supply the help text and the version text as
  parameters; these should probably be done in a similar way to what
  we do with the modeline etc.
- allow users to set default values in the options attributes and set
  them in generated code. This is probably just adding default value
  support to dogen, for which we have a separate story.
- one very useful way in which to use program options is via
  projections. That is a given model M0 defines the configuration and
  a second model M1 defines the options parsing. In this case the
  options defined in M0 already has the required shape:
  - there is a top-level class housing all options, traditionally
    called "configuration";
  - the top-level class contains meta-data with the product blurb;
  - attributes of that class can be annotated as "modes", "groups" or
    nothing. A mode will result in a modal CLI interface. Groups
    result in top-level groupings of options. Nothing means the
    attribute must be of a simple type and will be a global option
    (e.g. =help=, =version=, etc).
  - attributes have a description, etc associated as meta-data. They
    also have other useful annotations such as optional, mandatory
    etc. These are used in validation. Interestingly this may mean we
    can also automatically generate a validator.
  - dogen generates in M1 a set of chained program option parsers
    (assuming a modal interface; otherwise just one) which generate
    the M0 options.
  - in M1, users define a class with attribute
    =masd::command_line_options=, associated with an options class.
  - users can choose the "backend": boost program options, etc. Each
    is implemented as a separate template.
  - dogen generates a parser with an associated exception
    (parser_validation_error). The exception is simply injected as a
    type.

Links:

- [[https://github.com/abolz/CmdLine2][CmdLine2]]: alternative library to program options.

*** Exclude profiles from stereotypes processing                      :story:

At present we are manually excluding profiles from the stereotypes
transform. This was just a quick hack to get us going. We need to
replace this with a call to annotations to get a list of profile names
and exclude those.

We should also rename =is_stereotype_handled_externally= to something
more like "is profile" or "matches profile name".

Actually the right thing may even be to just remove all of the profile
stereotypes during annotations processing. However, we should wait
until we complete the exomodel work since that will remove scribble
groups, etc. Its all in the annotations transform.

*** Problems in conversion of dogen models                            :story:

 Regenerated all models, got the following errors:

 - we are adding the extension to the dia filename because of how CMake
   works. We should probably remove the output parameter or at least
   allow defaulting it to a replacement of the extension.
 - we are removing the dependencies due to duplicates in JSON keys.
 - we are looking for .dia diagrams instead of .json for references.

 *Previous Understanding*

 We converted all of dogen's models from dia into JSON using tailor and
 code-generated them to see if there were any differences.

 Issues to address:

 - problems with =quilt.cpp= and =yarn.dia= / =yarn.json=: the
   conversion of the model path did not work as expected - we do not
   know of the "."  separator. Fixed it manually and then it all worked
   (minus CMakeLists, see below). We could possibly fix the builder to
   automatically use the "." to separate model paths. Actually with the
   latest changes we now seem to only be looking at the first model
   module, so for =yarn.dia= we only have =yarn=.
 - CMakeLists were deleted on all models for some reason, even though
   the annotations profile look correct.
 - in quilt we correctly generated the forward declarations for
   registrar error and workflow error without including boost
   exception. Not sure why that is, nor why it is that we are including
   them for forward declarations.
 - Missing include of registrar serialisation in
   =all_ser.hpp=. Instability in =registrar_ser.cpp=, but content is
   correct otherwise.
 - =database.json= generated invalid JSON.
 - references in dia diagrams have the dia extension. This means that
   they do not resolve when converted to JSON.

 "Script":

 #+begin_src
rm *.json
A="dia knit quilt.cpp wale yarn.json annotations formatters quilt yarn database options stitch yarn.dia"
for a in $A; do /home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/bin/dogen.tailor -t $a.dia -o $a.json; done
for a in $A; do /home/marco/Development/DomainDrivenConsulting/dogen/build/output/gcc/Release/stage/bin/dogen.knitter -t ${a}.json --cpp-project-dir /home/marco/Development/DomainDrivenConsulting/dogen/projects --ignore-files-matching-regex .*/CMakeLists.txt --ignore-files-matching-regex .*/test/.* --ignore-files-matching-regex .*/tests/.* --verbose --delete-extra-files; done
 #+end_src

 In an ideal world, we should probably have a script that we run as
 part of =knit_and_stitch= that converts to tailor and then runs
 knitter on the models, so that we keep track of tailor breaks outside
 of JSON test models.

*** Log file names do not have frontend                               :story:

Add extension to log file name so that we can see both Dia and JSON
logs at the same time. At present, one overwrites the other because we
do not have the frontend (e.g. the extension) on the log file name.

*** Update static strings to string views                             :story:

Now we're on C++17 we can start making use of its new features. One
low hanging fruit is string view. We use static strings quite a lot
for logging etc. We can just replace these with string views.

Links:

- [[https://www.bfilipek.com/2018/10/strings17talk.html][Let's Talk About String Operations in C++17]]

*** Add basic "diff mode"                                             :story:

We need a very simple way of checking all generated files in memory
against what's in the file system and returning a flag if they are
different. We can then use these flags to determine if tests pass. In
the future we can extend this approach to include a proper diff of the
files, but for now we just need a reliable way to run system tests
again.

Actually the right solution for this is to see the processing as part
of a chain:

- out of the generator come a set of artefacts with operations (write,
  merge, ignore)
- these get joined with a transform that reads the state of the file
  system. It then adds more operations: delete, etc. If there are no
  diffs, it marks those files as skip.
- the final step is a processor which gets that model and executes the
  operations. This can then be replaced by a "reporter" that simply
  states what the operations would be.

Diff mode is using the report to see if there are any diffs.

Merged Stories:

*Validation-only or dry-run mode*

Both stitcher and knitter could do with a "dry-run" mode in which we'd
do everything except for actually outputting.

*For Knitter*

It would be nice if one could just check if a dia diagram is valid for
code generation, e.g. =--validate= or something along those lines.

*For Stitch*

We are interested in performing the parsing. This would be useful for
example for a flymake mode in emacs.

An additional feature of dry-run would be to run, generate the model
and then produce a unified diff, e.g. tell me what you'd change. For
this we'd have to link against a diff library. We need to
automatically exclude non-overwrite files (or have an option to
exclude/include them).

Links:

- [[https://github.com/google/diff-match-patch/tree/master/cpp][google Diff Match Patch library]]
- [[https://github.com/cubicdaiya/dtl][DTL: Diff Template Library]]
- [[https://stackoverflow.com/questions/1451694/is-there-a-way-to-diff-files-from-c][SO: Is there a way to diff files from C++?]]

*Dry-run option to just diff with existing generated code*

#+begin_quote
*Story*: As a dogen user, I want to know what has changed with the
next code generation so that I can evaluate if the changes are as
expected or not.
#+end_quote

It would be useful to have an option that would do everything except
writing the files to disk; instead, it would diff them with the
existing files and report if there are any differences. This would be
useful to make sure the source code matches the latest version of the
diagram.

We could use something like the [[https://code.google.com/p/dtl-cpp/wiki/Tutorial][DTL library]].

*** Consider adding compiler name to package                          :story:

At present we are not uploading clang packages into bintray. This is
because they have the same name as the GCC and MSVC packages. If we
add the compiler name to the package we can then upload them too. This
would be good because we can then test to make sure all packages are
working correctly.

*** Fix clang-cl broken test                                          :story:

We have one test failing on clang-cl, ref impl:

: Running 1 test case...
: unknown location(0): fatal error: in "boost_model_tests/validate_serialisation": class boost::archive::archive_exception: unregistered void cast class masd::cpp_ref_impl::boost_model::class_derived<-class masd::cpp_ref_impl::boost_model::class_base
: ..\..\..\..\projects\masd.cpp_ref_impl.test_model_sanitizer\tests\boost_model_tests.cpp(56): last checkpoint: validate_serialisation
:
: *** 1 failure is detected in the test module "test_model_sanitizer_tests"

It seems that the boost registration is failing on debug. This is very
strange as it works on MSVC and Linux, release and debug but fails on
clang-cl release.

** Deprecated
*** CANCELLED Update =yarn.dia= traits to external                    :story:
    CLOSED: [2018-11-22 Thu 10:38]

*Rationale*: superseded by the MASD rename.

We renamed the model but did not update the traits.
*** CANCELLED Update backend shape to match yarn                      :story:
    CLOSED: [2019-02-08 Fri 13:55]

*Rationale*: this story has been superseded by the latest refactor.

In an ideal world, the backends should be made up of two components:

- *meta-model*: a set of types that augment yarn with backend
  specific elements. This is what we call fabric at present.
- *transforms*: of these we have two kinds:
  - the model-to-model transforms that involve either yarn meta-model
    elements or backened specific meta-model elements. These live in
    fabric at present.
   - the model-to-text transforms that convert a meta-model element
     (yarn or backend specific) into an artefact. These we call
     formatters at present.

The ultimate destination for the backend is then to have a shape that
reflects this:

- rename formatters to transforms
- move artefact formatter into yarn; with this it means we can also
  move all of the top-level workflow formatting logic into
  yarn. However, before we can do this we must make all of the backend
  specific code in the formatter interface go away.
- note that at this point we no longer need to know what formatters
  belong to what backend other than perhaps to figure out if the
  backend is enabled. This means yarn can now have the registrars for
  formatters and organise them by backend. Which means the
  model-to-text chain will own all of these. However, we still have
  the managed directories to worry about; somehow, someone has to be
  able to compute the managed directories per kernel. This could be
  done at yarn level if the locator is clever enough.

Of course, before we can contemplate this change, we must first get
rid of formattables altogether.

We must also somehow model canonical formatters in yarn. Take this
into account when we do:

:        /*
:         * We must have one canonical formatter per type per facet.
:         * FIXME: this check is broken at the moment because this is
:         * only applicable to yarn types, not fabric types. It is also
:         * not applicable to forward declarations. We need some
:         * additional information from yarn to be able to figure out
:         * which types must have a canonical archetype.
:         */

Notes from MASD:

- Formatters are now seen as merely *text transforms* that convert
  from the generational model to the extractional model. We could
  house them under "text transforms" rather than transforms because we
  will also need regular model transforms.
- Formatters model is the extractional model. It provides primitives
  to create transforms to generate its types. It needs to be augmented
  with the model types, and divided using the traditional namespaces
  (metamodel, transforms, helpers).
- moving towards having multiple components per model means that its
  much easier to support facets in this way. The other great advantage
  of this approach is that now each facet can have its DLL main / main
  if a binary is to be made for it, on its own folder. Conversely, the
  top-level DLL main / main is the cross-facet component, so its
  slightly clearer who includes what. We should also start specifying
  explicitly what is included in each target.
- when tests become a facet rename it to testing.

Merged Stories:

*Rename fabric and formattables*

In the long run, we should use proper names for these namespaces:

- fabric is meta-model;
- formattables houses transformations.

Unfortunately this will cause problems with the yarn names.

*** CANCELLED Tidy-up fabric                                          :story:
    CLOSED: [2019-02-08 Fri 13:57]

*Rationale*: this story has been superseded by the latest refactor.

Now we have dynamic transforms, we don't really need all the classlets
we've created in fabric. We can get away with probably just the
dynamic transform, calling all the factories.
*** CANCELLED Keep track of sewing terms allocation                    :epic:
    CLOSED: [2019-02-09 Sat 06:52]

*Rationale*: we are no longer using sewing terms.

This story just keeps track of how we are using the different sewing
terms in Dogen. We are only tracking terms which are not yet
incorporated into the product. It also keeps track of ideas that have
not yet allocated a term.

| Term   | Meaning | Dogen usage                                               |
|--------+---------+-----------------------------------------------------------|
| weave  |         | Reserved for AOP support?                                 |
| [[https://en.wikipedia.org/wiki/Glossary_of_sewing_terms#D][dart]]   |         | Skeleton generator tool.                                  |
| [[https://en.wikipedia.org/wiki/Yoke_(clothing)][yoke]]   |         |                                                           |
| tailor |         | Format converter. e.g. Dia to JSON, etc.                  |
| jersey |         | Code generation service.                                  |
| hem    |         | HTTP Wrapper around jersey.                               |
| twine  |         | Tool to infer model from XML/JSON/CSV instance documents. |
|        |         | Tool to infer model from SQL database schemas.            |
| pleat  |         |                                                           |

*** CANCELLED Consider renaming LAM to a sewing term                  :story:
    CLOSED: [2019-02-09 Sat 06:53]

*Rationale*: we are no longer using sewing terms.

In keeping with the rest of Dogen we should also use a sewing term for
LAM. Wool is an interesting one.
*** CANCELLED Consider adding a writing policy to files               :story:
    CLOSED: [2019-02-12 Tue 18:01]

*Rationale*: this will be moved to meta-data.

At present we are using a single flag to describe several
possibilities with regards to file writing:

- write if its a new file;
- write if the contents have changed;
- write always. No use case yet.

It may make more sense to have an enum for this. Having said that, we
removed the "force write" feature so there is less of a need for this
at present.

*** CANCELLED Remove unused features                                   :epic:
    CLOSED: [2019-02-12 Tue 18:02]

*Rationale*: we are still using all of the features below and this
story does not help in capturing the notion of deprecated features. We
should just open stories for each feature as required.

This story captures any features that we no longer require and will
remove at some point. We have already removed most of the unused
features, but the story keeps track of any remnants.

At the very start of dogen we added a number of features that we
thought were useful such as suppressing model directory, facet
directories etc. We should look at all the features and make a list of
all features that we are not currently making use of and create
stories to remove them.

We may have to split this story into several but we should at least
trim down the obvious ones:

- delete extra files: we always do so why make it optional.
- disable facet folders: no use case.
- force write: we never force write and now the logic is a bit at odds
  with the overwriting logic: should we force write even if overwrite
  is set to false? This would break hand-crafted code.
- etc.

Basically any feature which we are not using at present and cannot
think of an obvious use case.
