#+title: Sprint Backlog 09
#+options: date:nil toc:nil author:nil num:nil
#+todo: STARTED | COMPLETED CANCELLED POSTPONED
#+tags: { story(s) epic(e) }

* Mission Statement

- Start the planning towards v2: get a good idea of what we think is
  in and what should be out. Make the product backlog reflect this
  triage.
- Address build issues; the build must work reliably and the tests
  must give us confidence that we did not break the code generator
  across the entire feature set.

* Stories

** Active

#+begin: clocktable :maxlevel 3 :scope subtree :indent nil :emphasize nil :scope file :narrow 75 :formula %
#+CAPTION: Clock summary at [2018-10-14 Sun 07:46]
| <75>                                                                        |         |       |       |       |
| Headline                                                                    | Time    |       |       |     % |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| *Total time*                                                                | *67:14* |       |       |   0.0 |
|-----------------------------------------------------------------------------+---------+-------+-------+-------|
| Stories                                                                     | 67:14   |       |       |   0.0 |
| Active                                                                      |         | 67:14 |       |   0.0 |
| Edit release notes for previous sprint                                      |         |       |  1:30 |   0.0 |
| Sprint and product backlog grooming                                         |         |       |  2:42 |   0.0 |
| Update readme to reflect org move                                           |         |       |  1:58 |   0.0 |
| Create project for C# test model                                            |         |       |  5:23 |   0.0 |
| Fix boost path serialisation errors                                         |         |       |  0:47 |   0.0 |
| Create project for C++ test model                                           |         |       |  6:55 |   0.0 |
| Update readme with information on reference models                          |         |       |  0:10 |   0.0 |
| Remove test models from dogen project                                       |         |       |  1:13 |   0.0 |
| Add commit info to version                                                  |         |       |  0:57 |   0.0 |
| Investigate adding package management support to dogen                      |         |       | 11:10 |   0.0 |
| Add vcpkg support to Linux builds                                           |         |       | 13:15 |   0.0 |
| Add vcpkg support to windows builds                                         |         |       |  9:16 |   0.0 |
| Resurrect CDash agents                                                      |         |       |  7:00 |   0.0 |
| Add vcpkg support to osx builds                                             |         |       |  1:13 |   0.0 |
| Recap of the current situation                                              |         |       |  1:25 |   0.0 |
| High-level model thoughts                                                   |         |       |  2:20 |   0.0 |
#+TBLFM: $5='(org-clock-time% @3$2 $2..$4);%.1f
#+end:

*** COMPLETED Edit release notes for previous sprint                  :story:
    CLOSED: [2018-10-02 Tue 17:51]
    :LOGBOOK:
    CLOCK: [2018-10-02 Tue 15:30]--[2018-10-02 Tue 17:00] =>  1:30
    :END:

Add github release notes for previous sprint.

Title: Dogen v1.0.08, "Caminhada"

#+begin_src markdown
![Caminhada](https://i2.wp.com/cookthebeans.com/wp-content/uploads/2017/03/img_5465.jpg) _Long walk towards a traditional village, Huambo, Angola. [(C) Ana Rocha 2017](https://cookthebeans.com/2017/03/09/benguela-huambo-bie-in-the-route-of-angolas-up-country)_.

# Overview

After a rather long hiatus of some nine months, Dogen development resumes once more. In truth, the break was only related to the open source aspect of the Dogen project; behind the scenes I have been hard at work on my PhD, which has morphed into an attempt to lay the theoretical foundations for all the software engineering that has been done with Dogen. Sadly, I cannot perform that work out in the open until the thesis or papers are published, so it is expected to remain closed for at least another year or two.

On the bright side, after performing an extensive literature review of the field of [Model Driven Engineering](https://en.wikipedia.org/wiki/Model-driven_engineering) - the technical name used in academia for the field Dogen is in - a lot of what we have been trying to do has finally become clear. The down side is that, as a result of all of this theoretical work, very little has changed with regards to the code during this period. As such, this sprint contains only some minor analysis work that was done in parallel, and I am closing it just avoid conflating it with the new work going forward.

The future for Dogen is bright, though. We are now starting the long road towards the very ambitious release that will be Dogen 2.0. The objective is to sync the code to match all of the work done on the theory side. This work as already started; you will not fail to notice that the repository has been moved to the _MASD project_ - Model Assisted Software Development.

User visible changes
================

There are no user visible changes this sprint.

Next Sprint
===========

The next few sprints will be extremely active, addressing a number of long standing issues such as moving test models outside of the main repo and concluding ongoing refactorings.

Binaries
======

Due to the transition of organisations, we did not generate any binaries for this release. As there are no code changes, please use the binaries for the previous release ([v1.0.07](https://github.com/MASD-Project/dogen/releases/tag/v1.0.07)) or build Dogen from source. Source downloads are available at the top.
#+end_src

- [[https://twitter.com/MarcoCraveiro/status/948594830267043840][Tweet]]
- [[https://www.linkedin.com/feed/update/urn:li:activity:6354361007493775361][LinkedIn]]
- [[https://gitter.im/DomainDrivenConsulting/dogen][Gitter]]

*** STARTED Sprint and product backlog grooming                       :story:
    :LOGBOOK:
    CLOCK: [2018-10-05 Fri 15:28]--[2018-10-05 Fri 15:40] =>  0:12
    CLOCK: [2018-10-12 Fri 14:56]--[2018-10-12 Fri 14:34] => -0:22
    CLOCK: [2018-10-05 Fri 10:14]--[2018-10-05 Fri 11:25] =>  1:11
    CLOCK: [2018-10-05 Fri 09:06]--[2018-10-05 Fri 10:13] =>  1:07
    CLOCK: [2018-10-04 Thu 17:44]--[2018-10-04 Thu 17:56] =>  0:12
    :END:

Updates to sprint and product backlog.

*** COMPLETED Update readme to reflect org move                       :story:
    CLOSED: [2018-10-03 Wed 10:39]
    :LOGBOOK:
    CLOCK: [2018-10-03 Wed 10:02]--[2018-10-03 Wed 10:38] =>  0:36
    CLOCK: [2018-10-03 Wed 09:54]--[2018-10-03 Wed 10:01] =>  0:07
    CLOCK: [2018-10-03 Wed 09:15]--[2018-10-03 Wed 09:53] =>  0:38
    CLOCK: [2018-10-02 Tue 17:52]--[2018-10-02 Tue 18:29] =>  0:37
    :END:

Now that dogen is under MASD, we have a number of links that are
pointing to the old Domain Driven Consulting org. Update those.

*** COMPLETED Analysis on reducing build times to avoid timeouts      :story:
    CLOSED: [2018-10-03 Wed 10:40]

Refactoring at the moment is painful because every time we change
CMakeFiles we end up rebuilding everything. At 2K plus ninja targets,
it is a long wait. In addition, we have been getting really close to
the maximum travis time, resulting in lots of manual fiddling to get
things to work. However, there is one very easy win: split test models
from production code. This is more than just a quick hack, really:

- we are compiling the test models with every build at present, but
  since they are not production code, we only really need to validate
  them whenever they change. That is - for a given OS, compiler, etc -
  once a test model compiles, links and its tests run, nothing else
  needs to be said until the test model changes.
- test models change very infrequently; only when we do a breaking
  change on Dogen and we rebase.
- test models by definition do not reference production code (or at
  least, /should/ not).

As a first step we should try to isolate the two builds (production,
test models) via variables so that we can create separate
travis/appveyor builds for them. In the future we should make the
separation even more explicit, by moving the folder away from the
production code.

*Previous Understanding*

At present we get random build time violations on travis due to builds
taking longer than 50 mins. We need to think of ways to reduce the
build time. Things to try:

- remove all of the hashing etc for the types we don't need to hash.
- get rid of the warnings for boost.

*** COMPLETED Create project for C# test model                        :story:
    CLOSED: [2018-10-03 Wed 16:18]
    :LOGBOOK:
    CLOCK: [2018-10-04 Thu 13:45]--[2018-10-04 Thu 13:56] =>  0:11
    CLOCK: [2018-10-04 Thu 08:47]--[2018-10-04 Thu 09:02] =>  0:15
    CLOCK: [2018-10-04 Thu 08:15]--[2018-10-04 Thu 08:46] =>  0:31
    CLOCK: [2018-10-03 Wed 15:46]--[2018-10-03 Wed 16:18] =>  0:32
    CLOCK: [2018-10-03 Wed 15:40]--[2018-10-03 Wed 15:45] =>  0:05
    CLOCK: [2018-10-03 Wed 12:45]--[2018-10-03 Wed 14:59] =>  2:14
    CLOCK: [2018-10-03 Wed 10:45]--[2018-10-03 Wed 12:18] =>  2:20
    CLOCK: [2018-10-03 Wed 10:42]--[2018-10-03 Wed 10:44] =>  0:02
    :END:

We need to create a separate repo for the C# test model. This also
means we need to generate the LAM model in two different locations.

*** COMPLETED Fix boost path serialisation errors                     :story:
    CLOSED: [2018-10-04 Thu 13:11]
    :LOGBOOK:
    CLOCK: [2018-10-04 Thu 12:47]--[2018-10-04 Thu 13:11] =>  0:24
    CLOCK: [2018-10-04 Thu 11:02]--[2018-10-04 Thu 11:25] =>  0:23
    :END:

When we use boost path outside of dogen, the code fails to compile:

: /home/marco/Development/DomainDrivenConsulting/hedgr/projects/hedgr.personae.comms.llcp_server/src/serialization/options_ser.cpp:27:10: fatal error: dogen.utility/serialization/path.hpp: No such file or directory
: #include "dogen.utility/serialization/path.hpp"

Dogen has hard-coded the serialisation to its own utilities. We should
be using a helper instead.

*** COMPLETED Create project for C++ test model                       :story:
    CLOSED: [2018-10-04 Thu 16:01]
    :LOGBOOK:
    CLOCK: [2018-10-04 Thu 16:20]--[2018-10-04 Thu 16:41] =>  0:21
    CLOCK: [2018-10-04 Thu 13:57]--[2018-10-04 Thu 16:01] =>  2:04
    CLOCK: [2018-10-04 Thu 13:13]--[2018-10-04 Thu 13:44] =>  0:31
    CLOCK: [2018-10-04 Thu 09:29]--[2018-10-04 Thu 11:01] =>  1:32
    CLOCK: [2018-10-04 Thu 09:03]--[2018-10-04 Thu 09:28] =>  0:25
    CLOCK: [2018-10-03 Wed 16:18]--[2018-10-03 Wed 18:20] =>  2:02
    :END:

Create a separate repo for the C++ test model.

Notes on testing:

- some tests do not make sense in a reference implementation:
  - class without a name, package without a name: these are just
    validation tests so we should do it as a unit test.
  - disable all kernels: doesn't generate anything. Not sure where it
    should go.
  - empty and two empty layers: not even valid any more as we must
    supply model modules. Can be done as a unit test once defaulting
    is in place.
- we have failures on hasing on both OSX and Windows. However, its
  very difficult to debug these due to the heavy use of templates in
  tests. We should probably wait until tests become facets and then
  ensure the boost log message contains a dump of the object state for
  each test.

Problems to fix:

- at present we have oracle support on ODB. Oracle libs are not
  distributed with debian. If we do not find oracle we do not compile
  northwind. This is not ideal. We should remove oracle support from
  northwind, and install odb support in the build machine (hopefully
  available as debs).
- all path and directories is a LAM model. Move the C# part into C#.
- models are under external module path =dogen::test_models=. Move
  them to =cpp_ref_impl=.
- path serialisation depends on dogen utility. Fix code generation so
  that it doesn't.
- some models have the postfix "model". Remove it.
- rename =cpp_model= to =cpp_11=.
- rename =std_model= to =stl=.
- we are generating solutions and VC projects but not testing
  these. We should probably have a separate build on AppVeyor that
  uses the solutions instead of CMake. However, as we do not have
  project level support yet, this will be hard to do (e.g. we generate
  one solution per component).
- not clear what the seam model does.

Notes:

- remove story about not building all the tests.

*** COMPLETED Add flat directory model to C#                          :story:
    CLOSED: [2018-10-04 Thu 16:01]

It seems this model is also a LAM model. Add it to C#.

*** COMPLETED Update readme with information on reference models      :story:
    CLOSED: [2018-10-05 Fri 11:36]
    :LOGBOOK:
    CLOCK: [2018-10-05 Fri 11:26]--[2018-10-05 Fri 11:36] =>  0:10
    :END:

We need to add some minor blurb about MASD and refer to the reference
implementation.

*** COMPLETED Remove test models from dogen project                   :story:
    CLOSED: [2018-10-05 Fri 15:27]
    :LOGBOOK:
    CLOCK: [2018-10-05 Fri 15:35]--[2018-10-05 Fri 15:41] =>  0:06
    CLOCK: [2018-10-05 Fri 15:21]--[2018-10-05 Fri 15:27] =>  0:06
    CLOCK: [2018-10-04 Thu 16:42]--[2018-10-04 Thu 17:43] =>  1:01
    :END:

Once we have created projects for both C# and C++, we need to delete
all references to test models:

- delete source code from projects;
- delete test data sets;
- remove environment variables (WITH_CSHARP, WITH_CPP etc);
- comment out generation tests for now (JSON and Dia).

*** COMPLETED Add commit info to version                              :story:
    CLOSED: [2018-10-12 Fri 15:06]
    :LOGBOOK:
    CLOCK: [2018-10-12 Fri 13:58]--[2018-10-12 Fri 14:55] =>  0:57
    :END:

In the past we had the git commit SHA key on the version. This was
useful, but caused the build to rebuild every time we committed
locally. Since we only build the final binaries on tags, there wasn't
much of a need for this so it was removed. However, we now build again
on each commit so there is a requirement for it.

To avoid the previous problems we should create some kind of macro
that only adds the commit if we are building from the build machine;
otherwise it just stamps something like "developer build". On the
build machine we should also add a timestamp and if possible the
travis/appveyor build number.

*** COMPLETED Investigate adding package management support to dogen  :story:
    CLOSED: [2018-10-12 Fri 15:34]
    :LOGBOOK:
    CLOCK: [2018-10-08 Mon 19:33]--[2018-10-08 Mon 19:48] =>  0:15
    CLOCK: [2018-10-08 Mon 19:12]--[2018-10-08 Mon 19:32] =>  0:20
    CLOCK: [2018-10-08 Mon 16:27]--[2018-10-08 Mon 18:25] =>  1:58
    CLOCK: [2018-10-08 Mon 15:55]--[2018-10-08 Mon 16:26] =>  0:31
    CLOCK: [2018-10-08 Mon 15:30]--[2018-10-08 Mon 15:54] =>  0:24
    CLOCK: [2018-10-08 Mon 14:53]--[2018-10-08 Mon 15:08] =>  1:01
    CLOCK: [2018-10-08 Mon 13:12]--[2018-10-08 Mon 14:52] =>  1:40
    CLOCK: [2018-10-08 Mon 09:10]--[2018-10-08 Mon 12:05] =>  5:42
    CLOCK: [2018-10-07 Sat 14:10]--[2018-10-07 Sat 17:02] =>  2:52
    :END:


At present we are building our deps manually and adding them to
dropbox. This has worked ok in the past, but it does have a few
problems:

- upgrades are a bit of a nightmare; we just have to take a bit of
  time of when we have to rebuild all deps, across all OSs and try to
  remember what we did last time.
- we end up not adding new deps just out of fear. For example, we are
  not building or testing ODB on the build machine due to this.
- we have two completely different setups, build machine and
  development machine. For development machines we can rely on debian
  testing because the boost packages are recent enough. On the build
  machine we use our prebuilt binaries.

In the past we have investigated using conan, but there were problems:
we could never get it to work for all libraries on windows - there
were subtle problems linking with boost that we couldn't get to the
bottom of - and we ended up with a very confusing setup were some
packages on windows are installed via conan but others come from our
deps. This makes it hard for us to maintain and hard for new users to
build and use dogen.

The best solution at present appears to be vcpkg. It seems to take the
ports approach - e.g. instead of supplying binaries, it compiles them
for you - but it also allows exporting the current state of the
packages:

./vcpkg export --zip boost-coroutine2

This means we can continue using our current dropbox setup, but rely
on a vcpkg export instead. It also builds debug and release, and
integrates seamlessly with CMake, requiring no changes at all to
CMakeFiles (unlike conan). In addition, we can also use vcpkg for our
private projects; we can create a copy of the project and add links to
our private repos. Also, rebuilding is now trivial, and we can easily
script it (e.g. update && export). This means we can pickup latest
boost as soon as it is released.

There are some limitations:

- only builds static libaries. OK for now.
- not all libraries are present. The coverage seems wide enough for
  now (600 and growing).
- not all libraries present build on all configurations. See [[https://github.com/Microsoft/vcpkg/issues/3436][this PR]].

The best way of doing this is to actually CI the deps themselves. This
would work as follows:

- create travis/appveyor builds that build vcpkg, install the deps and
  export them.
- copy the export into drop box. See [[https://github.com/andreafabrizi/Dropbox-Uploader][Dropbox-Uploader]]
- update dogen build path to pickup new dependencies, so its a
  controlled exercise.
- we should also have a "manual" setup of vcpkg for users, that builds
  the packages locally.

The great thing about this approach is that we can simply ocassionally
do a pull from remote vcpkg projec to get latest, ensure it all builds
correctly and then update dogen. The whole process is very simple and
does not require having access to OSX and Windows boxes locally, etc.

This would be fantastic but sadly it does not work out of the box. At
present the version of XCode available on travis OSX does not compile
vcpkg out of the box:

: CMake Error at CMakeLists.txt:10 (message):
:   Building the vcpkg tool requires support for the C++ Filesystem TS.
:   Apple clang versions 9 and below do not have support for it.
:   Please install gcc6 or newer from homebrew (brew install gcc6).
:   If you would like to try anyway, set VCPKG_ALLOW_APPLE_CLANG.

In addition, the linux GCC build also failed, even more misteriously:

: The command "${TRAVIS_BUILD_DIR}/bootstrap-vcpkg.sh" exited with 1.

We'll spin this off as a separate story into the backlog for the
future; even just building with vcpkg locally its an improvement in
dependency management.

Links:

- [[https://github.com/Microsoft/vcpkg/issues/4447][Link error LNK2005 when linking against Boost.Test on Windows]]

*** COMPLETED Add vcpkg support to Linux builds                       :story:
    CLOSED: [2018-10-12 Fri 15:32]
    :LOGBOOK:
    CLOCK: [2018-10-10 Wed 17:12]--[2018-10-10 Wed 17:30] =>  0:18
    CLOCK: [2018-10-10 Wed 15:29]--[2018-10-10 Wed 15:45] =>  0:16
    CLOCK: [2018-10-10 Wed 14:12]--[2018-10-10 Wed 15:28] =>  1:16
    CLOCK: [2018-10-10 Wed 09:06]--[2018-10-10 Wed 12:23] =>  3:17
    CLOCK: [2018-10-09 Tue 20:29]--[2018-10-09 Tue 21:52] =>  1:23
    CLOCK: [2018-10-09 Tue 19:55]--[2018-10-09 Tue 20:28] =>  0:33
    CLOCK: [2018-10-09 Tue 17:18]--[2018-10-09 Tue 18:25] =>  1:07
    CLOCK: [2018-10-09 Tue 16:40]--[2018-10-09 Tue 17:17] =>  0:37
    CLOCK: [2018-10-09 Tue 14:12]--[2018-10-09 Tue 16:05] =>  1:53
    CLOCK: [2018-10-09 Tue 13:49]--[2018-10-09 Tue 14:11] =>  0:22
    CLOCK: [2018-10-09 Tue 10:51]--[2018-10-09 Tue 13:04] =>  2:13
    :END:

Following on from our investigation, we need to add vcpkg to the linux
builds (clang and gcc). While we're there, update all the tools to
latest in preparation to switching to C++ 17. We also need to fix the
dropbox upload story as it was broken with the GitHub organisation
changes. While we're there, we should upload releases on all commits
rather than just on tags.

*** COMPLETED Add vcpkg support to windows builds                     :story:
    CLOSED: [2018-10-12 Fri 15:34]
    :LOGBOOK:
    CLOCK: [2018-10-12 Fri 13:30]--[2018-10-12 Fri 13:58] =>  0:28
    CLOCK: [2018-10-12 Fri 11:19]--[2018-10-12 Fri 12:21] =>  1:02
    CLOCK: [2018-10-12 Fri 09:30]--[2018-10-12 Fri 11:18] =>  1:48
    CLOCK: [2018-10-11 Thu 21:49]--[2018-10-11 Thu 22:20] =>  0:31
    CLOCK: [2018-10-11 Thu 20:12]--[2018-10-11 Thu 20:27] =>  0:15
    CLOCK: [2018-10-11 Thu 14:05]--[2018-10-11 Thu 16:05] =>  2:00
    CLOCK: [2018-10-11 Thu 09:12]--[2018-10-11 Thu 12:24] =>  6:53
    :END:

Following on from our investigation, we need to add vcpkg to the
appveyor windows builds (msvc). While we're there, update visual
studio and all the tools to latest in preparation to switching to
C++ 17. Also try to add support for =clang-cl= if its easy.

*** STARTED Resurrect CDash agents                                    :story:
    :LOGBOOK:
    CLOCK: [2018-10-14 Sun 07:33]--[2018-10-14 Sun 07:46] =>  0:13
    CLOCK: [2018-10-14 Sun 07:08]--[2018-10-14 Sun 07:32] =>  0:24
    CLOCK: [2018-10-13 Sat 23:12]--[2018-10-13 Sat 23:50] =>  0:38
    CLOCK: [2018-10-13 Sat 22:47]--[2018-10-13 Sat 23:11] =>  0:24
    CLOCK: [2018-10-13 Sat 22:30]--[2018-10-13 Sat 22:46] =>  0:16
    CLOCK: [2018-10-13 Sat 22:20]--[2018-10-13 Sat 22:29] =>  0:09
    CLOCK: [2018-10-13 Sat 22:05]--[2018-10-13 Sat 22:19] =>  0:14
    CLOCK: [2018-10-13 Sat 21:54]--[2018-10-13 Sat 22:04] =>  0:10
    CLOCK: [2018-10-13 Sat 21:40]--[2018-10-13 Sat 21:53] =>  0:13
    CLOCK: [2018-10-13 Sat 20:41]--[2018-10-13 Sat 21:09] =>  0:28
    CLOCK: [2018-10-13 Sat 20:30]--[2018-10-13 Sat 20:40] =>  0:10
    CLOCK: [2018-10-13 Sat 19:28]--[2018-10-13 Sat 19:43] =>  0:15
    CLOCK: [2018-10-13 Sat 16:54]--[2018-10-13 Sat 17:40] =>  0:46
    CLOCK: [2018-10-13 Sat 14:44]--[2018-10-13 Sat 16:53] =>  2:09
    CLOCK: [2018-10-12 Fri 21:46]--[2018-10-12 Fri 22:17] =>  0:31
    :END:

CDash has bitrotted and is no longer working.

- we need to get the build green on the Windows agent again.
- we need to get the linux agent up and running again.

Actually, the right thing to do is to connect CDash to travis like
this project:

- https://github.com/ned14/boost.outcome

We can still have our own agents for the nightlies etc but at least
this way all travis builds are pushing into CDash.

Command line:

: CMAKE_TOOLCHAIN_FILE=~/Development/DomainDrivenConsulting/masd/vcpkg/master/scripts/buildsystems/vcpkg.cmake ctest --extra-verbose --script ".ctest.cmake,configuration_type=Release,generator=Ninja,compiler=clang6,number_of_jobs=3"

Given we cannot get the project/sub-project setup to work, we probably
should just create multiple to-level projects:

- MASD Project - Dogen
- MASD Project - cpp_ref_impl

This would probably give us 20 daily builds as well, whereas the
sub-project setup seems to have a maximum limit of 10 altogether.

Tickets:

- https://github.com/Kitware/CDash/issues/732
- https://github.com/Kitware/CDash/issues/733

Links:

- https://github.com/ned14/outcome/blob/develop/.ci.cmake
- https://my.cdash.org/index.php?project=Boost.Outcome&date=2018-10-12
- https://public.kitware.com/pipermail/cdash/2010-June/000819.html
- https://github.com/ned14/quickcpplib/blob/master/cmakelib/QuickCppLibUtils.cmake
- [[https://gitlab.kitware.com/zackgalbreath/cmake/commit/2cf643d2cba4a819108e0e284cd58cf356378df3][Example of sub-projects]]
- [[https://github.com/Kitware/CDash/issues/732][GitHub issue with CDash problems]]
- [[http://webcache.googleusercontent.com/search?q=cache:vEhBG9OAeSAJ:https://blog.kitware.com/cdash-integration-with-github/&hl=en&gl=uk&strip=1&vwsrc=0][CDash integration with GitHub]]

*** STARTED Add vcpkg support to osx builds                           :story:
    :LOGBOOK:
    CLOCK: [2018-10-13 Sat 18:35]--[2018-10-13 Sat 19:28] =>  0:53
    CLOCK: [2018-10-13 Sat 17:41]--[2018-10-13 Sat 18:01] =>  0:20
    :END:

Following on from our investigation, we need to add vcpkg to the
travis osx builds (clang). While we're there, update all the tools to
latest in preparation to switching to C++ 17.

*** STARTED Recap of the current situation                            :story:
    :LOGBOOK:
    CLOCK: [2018-10-05 Fri 15:41]--[2018-10-05 Fri 17:06] =>  1:25
    :END:

We have started a number of simultaneous refactors and now its very
hard to understand where we are and where we are going. We need to go
though the code and ascertain the state of the onion.

Notes:

- the external model refactoring seems to be complete.
- the modeling model refactoring seems to have been tangled with the
  formatters refactor. We have moved some but not all properties into
  the modeling model but then we realised that some of them should
  really be in the generation model. However, we then hit the usual
  problem: how do we decorate element with the generation properties?
  See the discussion in story "Create the =generation= model" for
  details on why this is non-trivial. At that point we were left with
  a series of not particularly ideal options:
  - go forward and create a pair of element and generatable properties
    and somehow fix all transforms. In a way this is what we had done
    with the formatters, except that was after all of the transforms
    had been applied.
  - create the idea of "opaque properties" in the modeling model and
    then unpack the opaque properties in the generation transforms.
  - add the properties directly to the modeling model (to the element,
    at least) but only populate them in the generation transforms.
- the problem we are trying to solve seems to fall somewhere in
  between the decorator pattern and the mixin pattern but its not
  quite either.
- this problem started because we wanted to make a clear separation
  between modeling space and generation space; modeling space is not
  aware of the archetype expansion. This makes sense to an extent: we
  do not want to create dependencies between modeling space and
  formatters (source of the cycles between components). However, we
  also do not want to have to define all of the meta-model elements
  again in order to attach the generatable properties.

*** STARTED High-level model thoughts                                 :story:
    :LOGBOOK:
    CLOCK: [2018-10-11 Thu 16:06]--[2018-10-11 Thu 18:26] =>  2:20
    :END:

Jot down ideas on the separation between the API and the
implementation in dogen products.

*** Rename debian package                                             :story:

At present our package is called =dogen-applcations=. Since there will
only be one dogen application/package, this is a confusion name. We
should rename it. Names:

- masd-dogen

*** Finish adding support for clang-cl builds                         :story:

We have added preliminary support for building with clang-cl on
windows, but the build is not green. Most of the errors seem to be on
boost.

Links:

- [[https://ci.appveyor.com/project/mcraveiro/dogen/builds/19463961/job/6bnv6ppljlklu2ag][Release build]]
- [[https://ci.appveyor.com/project/mcraveiro/dogen/builds/19463961/job/45yhn8sdhexvsdmi][Debug build]]

*** Tidy-up dogen windows package                                     :story:

There are a few inconsistencies with the package:

- dogen components have a strange structure:
  "Dogen/runtime/dogen".
- we should probably have a top-level umbrella for MASD, under which
  dogen installs.
- package name is windows amd64. We should use the vcpkg triplets for
  simplicity (e.g. x64-windows).

*** Mapping of third-party dependencies                               :story:

System models should follow the physical structure of
dependencies. That is, we should not have a "boost" system model, but
instead a boost-test etc. Each of these can then have mappings
(e.g. vcpkg name, build2 name, etc). Users must declare these
references just like they do with user models. Dogen can then create
code for:

- cmake targets, properly linking against libraries;
- vcpkg install, at product level, by de-duplicating component
  dependencies;
- possibly distro dependencies.

We should only have a mandatory dependency, which is the STL. In
addition, we need different models for each version (e.g. c++ 03,
etc). This makes it easier to include the right types.

Note that each model must have an associated version. The version
should be part of the file name. However, maybe we need to distinguish
between TS version (11, 17, etc) from library version.

*** Upgrade to c++ 17                                                 :story:

There are quite a few dependencies for this to happen:

- on windows we need to somehow include =/std:c++latest=
- we need to move to latest boost as it seems Boost 1.62 breaks on c++
  17. We should wait until Beast is included in Boost before we do
  this.
- we need to install latest CMake, which is not available on nuget; so
  we need to fetch the zip/msi from https://cmake.org/files/v3.10/ and
  unpack it. Only latest supports VS 2017. Then set the CMake
  generator:

:    $generator="Visual Studio 15 2017 Win64";

- set the appveyor image:

: image:
:  - Visual Studio 2017

- set the CMake version:

:     set(CMAKE_CXX_STANDARD 14)

*** Rename input models directory to models                           :story:

We need to move the dogen project to the new directory layout whereby
all models are kept in the =models= directory.

*** Add basic "diff mode"                                             :story:

We need a very simple way of checking all generated files in memory
against what's in the file system and returning a flag if they are
different. We can then use these flags to determine if tests pass. In
the future we can extend this approach to include a proper diff of the
files, but for now we just need a reliable way to run system tests
again.

Actually the right solution for this is to see the processing as part
of a chain:

- out of the generator come a set of artefacts with operations (write,
  merge, ignore)
- these get joined with a transform that reads the state of the file
  system. It then adds more operations: delete, etc. If there are no
  diffs, it marks those files as skip.
- the final step is a processor which gets that model and executes the
  operations. This can then be replaced by a "reporter" that simply
  states what the operations would be.

Diff mode is using the report to see if there are any diffs.

*** Add reporting support to dogen model testing                      :story:

Dogen should have a mode which generates a report for a run rather
than code generate. The report could look like so:

:              /project_a
:                  /summary for this commit
:                  /diffs
:                  /errors
:                  /benchmark data
:                  /probing data
:                  /log

If the report was largely in HTML we could link it to the dogen docs
and save it into git. This would make troubleshooting much easier. If
the report contains the probing data it would be easier to figure out
what went wrong. We should also keep track of the model that was
generated (e.g. its location and git commit) so we can download it and
reproduce it locally.

*** Rework the tests using diff mode                                  :story:

Once we have diff mode, we need to find some kind of workflow for
tests:

- each product is composed of a git URL and a list of models.
- we git clone all repos as part of the build process.
- directories and model locations are hard-coded in each test.
- test runs against the model and hard-coded location, produces the
  diff. Test asserts of the diff being non-zero.

*** Fix the northwind model                                           :story:

There are numerous problems with this model:

- at present we have oracle support on ODB. Oracle libs are not
  distributed with debian. If we do not find oracle we do not compile
  northwind. This is not ideal. We should remove oracle support from
  northwind, and install odb support in the build machine (hopefully
  available as debs).
- the tests are commented out and require a clean up.
- the tests require a database to be up.

Notes:

- it is possible to setup [[https://docs.travis-ci.com/user/database-setup/#postgresql][postgres on travis]]

*** Simplify split configuration configuration                        :story:

At present we have two separate command line parameters to configure
the main output directory and the directory for header files. The
second parameter is used for split configurations. The problem is that
we now need to treat split configuration projects specially because of
this. It makes more sense to force the header directory to be relative
to the output path and make it a meta-data parameter.

*** Update all stereotypes to masd                                    :story:

We need to start distinguishing MASD from dogen. The profile for UML
is part of MASD rather than dogen, so we should update all stereotypes
to match. We need to make a decision regarding the "dia extensions" -
its not clear if its MASD or dogen.

*** Make "ignore regexes" a model property                            :story:

At present we have a command line option:
=--ignore-files-matching-regex=. It is used to ignore files in a
project. However, the problem is, because it is a command line option,
it must be supplied with each invocation of Dogen. This means that if
we want to run dogen from outside the build system, we need to know
what options were set in the build scripts or else we will have
different results. This is a problem for testing. We should make it a
meta-data option, which is supplied with each model and even more
interesting, can be used with profiling. This means we can create
profiles for specific purposes (ODB, lisp, etc) and then reuse them in
different projects.

*** Incorrect generation when changing external modules               :story:

When fixing the C# projects, we updated the external modules, from
=dogen::test_models= to =CSharpRefImpl=. Regenerating the model
resulted in updated project files but the rest of the code did not
change. It worked by using =-f=. It should have worked without forcing
the write.

*** Code coverage does not work for C#                                :story:

It seems that using NUnit and OpenCov does not work. The main reason
appears to be the use of shadow copying, which is no longer optional
on NUnit 3.

Links:

- https://github.com/Ullink/gradle-opencover-plugin/issues/1
- https://github.com/codecov/example-csharp/blob/master/appveyor.yml
- https://www.appveyor.com/blog/2017/03/17/codecov/

*** Improve comments on reference implementation                      :story:

At present it is very difficult to understand what each model and/or
each type does in the reference implementations. We need to add some
comments to make it more obvious.

*** Code generate C# models using msbuild                             :story:

At present we did a quick hack to code generate in C#: a simple bash
script that runs dogen. However, this is not how we expect the end
user to consume it; there should be a msbuild target that:

- detects the code generator;
- contains the configuration (e.g. options, location of models);'
- runs the code generator - possibly every time models change;
- has a tailor target to generate JSON.

*** Add project documentation                                         :story:

We should be able to create a simple set of docs following on from the
[[https://ned14.github.io/outcome/][outcome project]]. They seem to be using Hugo.

Links:

- https://github.com/foonathan/standardese
- https://github.com/ned14/outcome/tree/develop/doc/src

*** Create the =generation= model                                     :story:

Create a new model called =generation= and move all code-generation
related class to it.

We need to create classes for element properties and make model have a
collection that is a pair of element and element properties. We need a
good name for this pair:

- extended element
- augmented element
- decorated element: though not using the decorator pattern; also, we
  already have decoration properties so this is confusing.

Alternatively we could just call it =element= and make it contain a
modeling element.

Approach:

- create a new generation model, copying across all of the meta-model
  and transform classes from yarn. Get the model to transform from
  endomodel to generation model.
- augment formattables with the new element properties. Supply this
  data via the context or assistant.

Problems:

- all of the transforms assume access to the modeling element means
  access to the generation properties. However, with the introduction
  of the generation element we now have a disconnect. For example, we
  sometimes sort and bucket the elements, and then modify them; this
  no longer works with generation elements because these are not
  pointers. It would be easier to make the generation properties a
  part of the element. This is an ongoing discussion we've had since
  the days of formattables. However, in formattables we did write all
  of the transforms to take into account the formattable contained
  both the element and the formattable properties, whereas now we need
  to update all transforms to fit this approach. This is a lot more
  work. The quick hack is to slot in the properties directly into the
  element as some kind of "opaque properties". We could create a base
  class =opaque_properties= and then have a container of these in
  element. However, to make it properly extensible, the only way is to
  make it a unordered set of pointers.
- actually the right solution for this is to use multiple
  inheritance. For each modeling element we need to create a
  corresponding generation version of it, which is the combination of
  the modeling element and a generation element base class. Them the
  generation model is made up of pointers to generation elements and
  it dispatches into generation elements descendants in the
  formatter. The key point is to preserve the distinction between
  modeling (single element) vs generation (projection across facet
  space).

*** Create a =ci= folder in build                                     :story:

We should use the same approach as nupic for organising the scripts: a
top-level =ci= folder with folders per CI system. We should also
follow their naming convention for the build scripts which seem to
follow the CI events.

Links:

- https://github.com/numenta/nupic.core/tree/master/ci

** Deprecated
*** CANCELLED Split dogen testing from core                           :story:
    CLOSED: [2018-10-05 Fri 15:33]

*Rationale*: this story was cleaned up and split into several stories.

At present we have tests in modeling that perform "code generation";
that is, regenerate all dogen test models from JSON and Dia. These are
boost unit tests. Due to this, we have welded the test models with the
core models, which means that we cannot easily separate repos without
a lot of hacks. However, if we were to generalise the problem: there
is no reason why test models should be coupled with the core or
treated specially; they are just an instance of a project with dogen
models which can be used to validate dogen. A better approach is to
move all this work to "system testing", done using the dogen binary
rather than within unit tests. This would work as follows:

- add a mode in dogen called "validation mode" or diagnostics, etc. In
  this mode, dogen does not write files to the file system but instead
  produces a number of "reports":
  - a list of all validation errors, if any, in GCC format, pointing
    to the original models.
  - a set of diff files with all the differences, if any.
  - a benchmark report.
  - a top-level report with the project name, its git repo and the git
    commit.
- projects that wish to help dogen must have a well-defined target to
  generate the reports for all models under test.
- dogen project contains a script with a list of such projects and
  their git repos. Every time we build dogen core we install the
  package into the travis VM and run the reports.
- a environment variable containing the path into which to write the
  reports must be set before running dogen.
- a git repo is created with all the reports, and a structure as
  follows:
  /repo
      /branch
          /dogen_commit
              /summary for this commit
              /project_a
                  /summary for this commit
                  /diffs
                  /errors
                  /benchmark data
              /project_b
 ...
- to avoid clashes, make the branches named after the build,
  e.g. travis osx etc.
- git clones are shallow (1 commit)
- once all reports are generated into the git report repo, the build
  commits the report. The comment is the dogen commit.
- a travis build is triggered on the back of the commit. It checks the
  latest commit. If the report is a pass the build is green, if its a
  fail the build is red.
- in an ideal world the system tests build is separate from the dogen
  core build, and triggered from a bintray upload. However, as we do
  not know how to do this yet, we can just run the system tests at the
  end of the dogen build.
- we should split the reporting work from the build separation. We
  could have a simple build that just fails if there are any diffs to
  start off with and worry about reporting later.

With this approach we can have any number of projects contributing to
validate dogen (including dogen itself). The only slight downside is
that the models must always be up-to-date (e.g. if the user has
changed the model but not regenerated, system tests will
fail). Perhaps we could have different categories of test models:
mandatory and optional. Mandatory must pass, optional do not
contribute to the build failing. However, they still show up in the
report.

Links:

- https://github.com/cubicdaiya/dtl


*** CANCELLED Create a build script just for C#                       :story:
    CLOSED: [2018-10-04 Thu 17:50]

*Rationale*: no longer needed after the split of reference models.

At the moment we are doing C++ and C# on the same build script, making
it really complex. It would be much easier to have a separate C# build
script. We should also have a separate install script for C# so we
don't have to waste time installing packages if we're not going to use
them.

*** CANCELLED Create a new exoelement chain                           :story:
    CLOSED: [2018-10-04 Thu 17:54]

*Rationale*: given the amount of churn the refactor stories have had,
this story is no longer relevant.

We need to create a new exoelement chain that uses the new exoelements
to bootstrap a endomodel.

*** CANCELLED Start documenting the theoretical aspects of Dogen      :story:
    CLOSED: [2018-10-05 Fri 10:28]

*Rationale*: this will be taken care of by the thesis.

Up to now we have more or less coded Dogen as we went along; we
haven't really spent a lot of time worrying about the theory behind
the work we were carrying out. However, as we reached v1.0, the theory
took center stage. We cannot proceed to the next phase of the product
without a firm grasp of the theory. This story is a starting point so
we can decide on how to break up the work.

*** CANCELLED Sections to add to manual                               :story:
    CLOSED: [2018-10-05 Fri 10:29]

*Rationale*: this will be taken care of by the thesis.

Random list of things that we need to have in manual:

- Drivers/frontends: The importance of drivers to allow existing
  frameworks to interoperate; eCore, MSVC, Dia, JSON.  Structural
  variability at modeling level. Dia frontend: use of colours,
  validation (checking of stereotypes), "on the impact of layout
  quality to understanding UML diagrams", this constrains the size of
  a model.
- Stitch. Variability regions vs aspects (Oberweis paper "modeling
  variability in template-based code generators"). Why we need both
  feature modeling and variability regions / aspects: because features
  are a high-level concept that is implemented using variability
  regions. We need to map layers to facets and to our generation
  model. Dependencies between features and variability regions.
- External integration and its importance, cartridges. integration
  with Clang, ODB, XML tool.
- Agile and MDD: tight integration. Lightweight MDD with agile

*** CANCELLED Use the in-memory interface of LibXml                   :story:
    CLOSED: [2018-10-05 Fri 10:30]

*Rationale*: we should just drop libxml altogether and use XSD tool.

At present, our C++ wrappers on top of LibXml are using the file based
interface. We should do in-memory processing of the XML file. Once
this is in place, we can change the exogenous transformers to use
strings rather than paths to files.

*** CANCELLED Consider simplifying frontend testing                   :story:
    CLOSED: [2018-10-05 Fri 11:01]

*Rationale*: this will be resolved with the new diff based tests.

At present we are outputting code for every supported frontend, and
then checking they are binary identical. This is fine given that we
only have two frontends. Once we had a visual studio frontend, it may
make more sense to stop generating code for all frontends and simply
diff the middle-end to ensure we generate an identical yarn model. We
can continue to test end to end one of the frontends (dia).

We had command line options available in the past that generated only
a merged model. We need to look into the backlog for these.

This is a problem specially in light of adding new backends because
now we are code-generating the cross product of frontends and
backends.

*** CANCELLED Update dynamic section in manual                        :story:
    CLOSED: [2018-10-05 Fri 11:08]

*Rationale*: this will be taken care of by the thesis.

We need to talk about the new fields, field templates, etc.

*** CANCELLED Some test models do not build on run all specs          :story:
    CLOSED: [2018-10-05 Fri 11:09]

*Rationale*: should no longer be a problem after the repo splitting.

For some reason we are not building some of the test models when doing
a run all specs, in particular:

- exception
- comments

this may be because we have no specs for them. We need to find a way
to build them somehow.

Merged stories:

*Add test model sanitizer to test models target*

At present if we build test models we don't seem to build the
sanitizer.

*** CANCELLED C++ workflow should perform a consistency check         :story:
    CLOSED: [2018-10-05 Fri 11:11]

*Rationale*: this will no longer be required when we implement proper
feature model support.

We should ensure that all facets and formatters available in the
registrar have corresponding field definitions and vice-versa. This
was originally to be done by some kind of "feature graph" class, but
since we need to use this data for other purposes, the main workflow
could take on this responsibility - or we could create some kind of
"validator" class to which the workflow delegates.

*** CANCELLED Implement module expander test                          :story:
    CLOSED: [2018-10-05 Fri 11:14]

*Rationale*: code has changed quite a bit since then.

We copied across the code for the module expander test from yarn json
but didn't actually finished implementing it.

*** CANCELLED Consider using the same API as boost property tree in selector :story:
    CLOSED: [2018-10-05 Fri 11:14]

*Rationale*: no longer required once we have proper feature support.

At present we have the type of the value in the method names in the
selector, e.g. =get_text_content=. It would be better to have a =get=
that takes in a template parameter, e.g. =get<text>=. However, in
order to do this we need to have some kind of mapping between the
schema value (=text=) and the raw value (=std::string=). This requires
some template magic.

Once this is done we can also make the API a bit more like the
property tree API such as for example returning =boost::optional= for
the cases where the field may not exist.

We have started introducing =try_select...=. This was preferred to
=get_optional= because we are not getting an optional but instead
trying to get.

*** CANCELLED Add dynamic consistency validation                      :story:
    CLOSED: [2018-10-05 Fri 11:15]

*Rationale*: no longer required once we have proper feature support.

We need to check that the default values supplied for a field are
consistent with the field's type. This could be done with a
=validate()= method in workflow.

Actually since we can only create fields from JSON, we should just add
a check there.

*** CANCELLED Update manual with detailed model descriptions           :epic:
    CLOSED: [2018-10-05 Fri 11:18]

*Rationale*: this will be taken care of by the thesis.

#+begin_quote
*Story*: As a dogen developer, I want to read about the architecture
of the application so that I don't have to spend a lot of time trying
to understand the source code.
#+end_quote

We should add CRCs for the main classes, with an explanation of what
each class does; we should also explain the separation of the
transformation logic between the core model (e.g. =dia=) and the
transformation model (e.g. =dia_to_sml=). We should describe what the
workflow does in each model.

We should only implement this story when all of the major refactoring
has been done.

*** CANCELLED Add tests for general settings factory                  :story:
    CLOSED: [2018-10-05 Fri 11:21]

*Rationale*: once these become part of the meta-model, most of these
won't make any sense.

Some simple tests come to mind:

- empty data files directory results in empty factory;
- valid data files directory results in non-empty factory;
- invalid data files directory results in exception;
- more than one data files directory results in expected load;
- creating annotation for test model types works as expected.
- missing fields result in expected exceptions.

*** CANCELLED Add tests for =general_settings_factory=                :story:
    CLOSED: [2018-10-05 Fri 11:21]

*Rationale*: once these become part of the meta-model, most of these
won't make any sense.

Tests:

- missing licence
- missing modeline
- empty marker
- different marker for two objects
- consider moving generate preamble into annotation
