% -*- mode: latex; tab-width: 4; indent-tabs-mode: nil -*-
%
% Copyright (C) 2012-2015 Marco Craveiro <marco.craveiro@gmail.com>
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License as published by
% the Free Software Foundation; either version 3 of the License, or
% (at your option) any later version.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
% GNU General Public License for more details.
%
% You should have received a copy of the GNU General Public License
% along with this program; if not, write to the Free Software
% Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
% MA 02110-1301, USA.
%
\documentclass{book}

%
% packages
%
\usepackage{qlbook}
\usepackage{lettrine}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{etoolbox}
\usepackage{titlesec}

\newtheorem{concept}{Definition}

\makeatletter

% First, modify the \@endpart macro.
\def\@endpart{}

% Next, copy the \chapter macro to \nonewpagechapter, and ...
% ... suppress page-breaking instructions in the modified macro
\let\nonewpagechapter\chapter
\patchcmd\nonewpagechapter{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}

% Third, suppress vertical whitespace before "Part xx" material
\patchcmd{\part}{\null\vfil}{}{}{}

\makeatother

\titleformat{\part}[hang]
{\lstlolbreak\loabreak\Huge\bfseries\filleft}{\thepart}{1em}{}

\hypersetup{
  pdfauthor={Marco Craveiro},
  pdftitle={Dogen: The Domain Generator},
  pdfcreator={Emacs 24.5.1}}

\begin{document}

\clearpage

\newcommand\nbvspace[1][3]{\vspace*{\stretch{#1}}}
\newcommand\nbstretchyspace{\spaceskip0.5em plus 0.25em minus 0.25em}
\newcommand{\nbtitlestretch}{\spaceskip0.6em}
\pagestyle{empty}
\begin{center}
\bfseries
\nbvspace[1]
\Huge
{\nbtitlestretch\huge
  DOGEN\\THE DOMAIN GENERATOR}

\nbvspace[1]
\normalsize

\nbvspace[1]
\small BY\\
\Large MARCO CRAVEIRO\\[0.5em]

\nbvspace[2]

\begin{center}
  \includegraphics[width=4.5in]{images/application_dogen}
\end{center}
\nbvspace[3]
\normalsize

Hatfield\\
\large
PUBLISHED IN THE WILD
\nbvspace[1]
\end{center}

%
% Colophon
%
\newpage
\pdfbookmark[0]{Colophon}{Colophon}
\section*{}
\pagestyle{empty}
\vfill
\begingroup
\footnotesize
\parindent 0pt
\parskip \baselineskip

\textcopyright{} 2012--\the\year\ Marco Craveiro. All rights reserved.

Revision \textbf{DRAFT}. Generated for Dogen v${DOGEN_VERSION}, git
commit ${CURRENT_GIT_COMMIT}.

This book was typeset with \LaTeXe. It uses a slightly modified
version of the \textit{QL Book} style developed by Luigi Ballabio.
Download the \textit{QL Book} style at
\url{http://www.implementingquantlib.com/p/the-book.html}.

The cover was typeset using a modified version of the design by
Yiannis Lazarides, available at
\url{http://tex.stackexchange.com/questions/17579/how-can-i-design-a-book-cover}.

The cover picture depicts the project icon. \textcopyright{} 2012
Patrick Gannon. It is an abstract representation of of a cog in
Babbage's Difference Engine.

\par Permission is granted to copy, distribute and/or modify this
document under the terms of the GNU Free Documentation License,
Version 1.2 or any later version published by the Free Software
Foundation; with no Invariant Sections, no Front-Cover Texts, and no
Back-Cover Texts. A copy of the license is included in the appendix
entitled ``GNU Free Documentation License.''

\endgroup
\clearpage

\newpage

\setcounter{tocdepth}{2}
\tableofcontents
\listoffigures
\listoftables

\chapter*{Preface}

\epigraph{In fact, my main conclusion after spending ten years of my
  life working on the \TeX project is that software is hard. It's
  harder than anything else I've ever had to do.}{Donald Knuth}

\lettrine{T}{hat many open source} and free software projects are a
labour of love, we are all well too aware. The tragedy is that most of
the code consumers do not know \emph{just how much love} is required
to create and maintain a successful project in your spare time. The
only way to find out is to do one yourself. When I started Dogen, I
had absolutely no idea of the long and tortuous journey that awaited
me. This is in itself rather ironic, as I have spent the best part of
two decades in the commercial software industry~--- including many a
time on the front lines of new product development~--- and I've used
free software on a daily basis for decades. Looking back, the only
explanation I can conceive of for this blind spot is that programmers
are perennial optimists by nature or have \emph{very} short term
memory. Or both.

As it was, I attempted to ``create a simple code generator and then
get on with real work''; I thought ``it would take me a couple of
months or so, since I've written them before''. Years later, I still
find myself working towards fulfilling this very simple idea to my
satisfaction. And, lest you start blaming my organisational skills, it
wasn't as if there has been a lack of ``professionalism and
focus''. I've been to enough management school lessons to learn all
the buzzwords, including the need to define the \emph{vision} before
embarking on the road to \emph{execution}.

Dogen's vision eventually stabilised to the following:

\begin{itemize}
\item to create a suite of code generation tools targeting model and
  product generation, according to a well-specified product line;
\item for these tools to have the potential to be useful to developers
  in multiple programming languages, and to be designed to be
  integrated with IDEs and text editors;
\item to make the suite extensible, such that users can come up with
  modifications suited to their particular domain.
\end{itemize}

What you have on your hands then is a document describing how Dogen
realises this vision. I hope you enjoy reading this book as much as I
have enjoyed this wild ride.

\begin{flushright}\emph{Marco Craveiro}, \the\year\end{flushright}

\chapter*{Introduction}

\epigraph{First write the code generator; don't worry about the code,
  it will write itself.}{\emph{author}}

\lettrine{T}{his book is} the official manual for
\emph{Dogen}. Dogen~--- the domain generator~--- is a suite of code
generation tools designed specifically to target domain models. Dogen
was created to make the modeling process simpler: the user creates a
domain model using a supported UML editor and the tools provided by
Dogen use its output to generate a source code representation. The
generated code contains most of the services required from a typical
domain object such as serialisation, hashing, debug printing and so
on.

The book is split into three parts. Part I summarises the key domain
knowledge around code generation and Domain Driven Development. It
provides a basic theoretical foundation that serves as the background
to the remainder of the book and, hopefully, supplies enough context
to understand the design choices made. The second part describes the
internals of the product, allowing one to make sense of the code base
and adapt it as required. The third part is a user manual for the
different tools in the suite.

For those that are also interested in coding folklore, the appendix
provides a number of ``lessons learned'' stories. They were originally
published as blog posts on my column Nerd Food and give the reader a
good feel for the development process and the steepness of the
journey.

\section*{On the Web}

The following web resources are available:

\begin{itemize}
\item if you are reading a printed copy of this manual, you can always
  access the latest version online (in raw \LaTeXe form): \url{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.tex}
\item you can find the latest source code for Dogen at the official
  repository in GitHub:
  \url{https://github.com/DomainDrivenConsulting/dogen}
\item there is also a mirror in BitBucket:
  \url{https://bitbucket.org/marco_craveiro/dogen/overview}
\item continuous builds for Dogen are available via Travis:
  \url{https://travis-ci.org/DomainDrivenConsulting/dogen/builds}
\item you can chat to us on IRC via Gitter:
  \url{https://gitter.im/DomainDrivenConsulting/dogen}
\item you can find details on the ongoing work in the Agile folder
  (look for the latest sprint):
  \url{https://github.com/DomainDrivenConsulting/dogen/tree/master/doc/agile}
\end{itemize}

\part{Theory}

\epigraph{Malembe, Malembe.}{Angola proverb}

Unfortunately, these topics are not the most approachable. This is
both due to their unfamiliarity~--- most developers do not spend a lot
of time contemplating these matters on their day to day jobs~--- as
well as the innate complexity of these ideas. Alas, just like
Tanenbaum's \emph{Operating Systems Design and Implementation}
provided Linus with a solid foundation on which to build Linux upon,
so do these theories inform the decisions around Dogen's development;
you cannot understand the latter without the former.

The part is organised as follows:

\begin{itemize}
  \item chapter \ref{mdsd} provides an explanation of the core
    concepts of the Model Driven Software Development methodology.
  \item chapter
\end{itemize}

underpinnings behind Dogen, its internal architecture and assorted
aspects around its development.

\chapter{Conceptual Model}

\lettrine{A}{t the core} of Dogen lies the \texttt{conceptual model}:
the set of fundamental domain concepts and the ubiquitous language
that defines them. It's important to understand that the
\texttt{conceptual model} is not the union of the documentation of
Dogen's key classes; instead, it lives at a higher level. Its
objective is to provide a narrative that ties all of Dogen into a
single, coherent and unified whole, creating the conceptual framework
needed to implement its vision. Not all of the elements of the
\texttt{conceptual model} map directly to code, and those that do
don't always map with a low impedance mismatch.

Entities that live in the \texttt{conceptual model} are called
\texttt{conceptual elements}. We shall now define all of the
\texttt{conceptual elements}, adding some commentary where applicable
to try to clarify the ideas behind them.

\begin{concept}
  The \texttt{physical model} is defined as the set of files deemed of
  interest by the end user.
\end{concept}

\begin{concept}
  The \texttt{physical model} has a physical location called the
  \texttt{physical model directory}~--- also known as the
  \texttt{physical directory}.
\end{concept}

\begin{concept}
  Each file in the \texttt{physical model} is uniquely identifiable by
  its path, relative to the \texttt{physical directory}.
\end{concept}

The \texttt{physical model} is called ``physical'' because it has a
physical representation as a set of files on a file system
somewhere. A ``file'' here is understood as the operative system
concept, as are ``directories'' and related notions. It is important
to notice that there is no direct relationship between the
\texttt{physical model} and the end user's view of a ``project'' in a
``programming language'', or any other such constructs; the
\texttt{physical model} generalises these notions. Mappings can of
course be made between these two worlds~--- and we will do so via
examples to clarify intent~--- but there is no requirement for such
mapping to exist at the conceptual level.

Note also that each file under the \texttt{physical directory} is
considered a distinct element in the set of the \texttt{physical
  model}, even if it has the same content as other files in the
\texttt{physical directory}.

\begin{concept}
  An \texttt{artefact} is defined as a logical representation of a
  file. A \texttt{logical model} is defined as a set of
  \texttt{artefacts}, where each \texttt{artefact} maps to a file in a
  \texttt{physical model}.
\end{concept}

The \texttt{logical model} is a generalisation of the notion of the
\texttt{physical model}. By ``logical'' we mean to say that an
\texttt{artefact} may exist in memory as an object, in a database as a
record and so forth, whereas a \emph{file} here is understood as
living just on a file system. It is always trivially possible to
obtain an \texttt{artefact} from a file and vice-versa. Both files and
\texttt{artefacts} have an associated \texttt{content}, defined as a
set of bytes; as with files, two \texttt{artefacts} are distinct
entities with identity even if they happen to have the same content.

The main objective of this definition is to move from a physical world
to a logical world, on which we will focus from now on.

\begin{concept}
  The \texttt{generatable artefact set} is the subset of artefacts in
  the \texttt{logical model} which can be created by automated means.
\end{concept}

\begin{concept}
  The \texttt{handcrafted artefact set} is the subset of artefacts in
  the \texttt{logical model} which cannot be created by automated
  means.
\end{concept}

\begin{concept}
  The union of the \texttt{handcrafted artefact set} and the
  \texttt{generatable artefact set} is equal to the \texttt{logical
    model}.
\end{concept}

\begin{concept}
  A \texttt{generatable artefact} is an \texttt{artefact} which is a
  member of the \texttt{generatable artefact set}.
\end{concept}

The definitions so far are probably best clarified by means of a
device: let's imagine that you have a hard-drive full of text files,
organised in multiple directories. Let's say you choose a directory at
random, take all the files located on it and below it and convert them
all to \texttt{artefacts}~--- whatever that conversion entails. Now
say you perform a classification which simply splits the
\texttt{artefacts} into two buckets: those which you can produce using
a ``trivial function''~--- a notion which we shall not bother to
further qualify~--- and those which you cannot. For the purpose of
this example, let's imagine some files are merely permutations of
letters like ``AABBCC'' whilst others contain complex content like
Shakespeare's Hamlet. We can now map these to the definitions above:
the \texttt{physical directory} is the directory you have chosen as
the starting point for the categorisation exercise; the
\texttt{artefacts} with patterns like ``AABBCC'' make up the
\texttt{generatable artefact set} whilst the remaining files are part
of the \texttt{handcrafted artefact set}.

\begin{concept}
An \texttt{artefact} is composed of a set of instances of
\texttt{structural features}.
\end{concept}

Continuing with our previous example, lets further suppose that we
choose a different directory and the files we find can be grouped
according to their structure; some of these files define C++ class
headers, whilst others contain their implementations. The
\texttt{structural features} you may be interested in would be
something like ``C++ header file'', ``class definition'',
``constructor'' and so forth. In some cases, you find that
\texttt{structural features} contain other \texttt{structural
  features}, whilst others do not.

It is important to keep in mind that \texttt{structural features} are
not directly related to programming language constructs. It just so
happens that it is possible to map some \texttt{structural features}
to programming language constructs~--- but it's not always possible on
all cases. Note also that at the \texttt{conceptual model} level we
are not particularly interested in defining an exhaustive list of
\texttt{structural features}; it will later became clearer why that is
the case.

\begin{concept}
An \texttt{artefact} may have a meta-type associated with it, called
its \texttt{archetype}. An \texttt{archetype} is an entity with
identity. An \texttt{artefact} is an instance of one and only one
\texttt{archetype}.
\end{concept}

We first moved ``side-ways'' from physical space to logical space; we
now have moved ``upwards'' inside logical space. Let's take a C++
header file as an example. A given header file~--- say one declaring
``class X''~--- is an \texttt{artefact} in our \texttt{logical
  model}. One can conceive of many such header files in the
\texttt{logical model}, all of them are instances of the same
\texttt{archetype} representing declarations of C++
classes. Similarly, the class implementation would be another
\texttt{archetype}.

Just as with \texttt{artefacts}, \texttt{archetypes} are associated
with a set of \texttt{structural features}. Not all \texttt{structural
  features} that belong to an \texttt{archetype} need be expressed on
all of its \texttt{artefacts}; however, all \texttt{structural
  features} of an \texttt{artefact} must be part of its
\texttt{archetype}. \texttt{Structural elements} which are common
across artefacts are called \texttt{aspects}.

Finally, whilst \texttt{artefacts} are specific to a \texttt{logical
  model}, \texttt{archetypes} span a set of \texttt{logical model}
because, by definition, their objective is to extract structural
commonalities between them.

\begin{concept}
A \texttt{formatting function} or just \texttt{formatter} is defined
as a function that takes in a set of inputs and produces a set of
\texttt{artefacts}.
\end{concept}

\begin{concept}
A \texttt{formatting function} is associated with one and only one
\texttt{archetype}. However, an \texttt{archetype} can map to zero,
one or many \texttt{formatting functions}.
\end{concept}

Whereas the \texttt{archetype} represents the general concept of
``structural commonality'' between a set of \texttt{artefacts}, a
\texttt{formatter} can be thought of as a generating function
which~--- given a minimal set of inputs~--- can reproduce an
\texttt{artefact}.

\begin{concept}
The \texttt{archetype space} is the set of all possible
\texttt{archetypes}.
\end{concept}

\begin{concept}
  The \texttt{archetype space} is a taxonomic hierarchy made up of the
  following grouping levels: \texttt{family}, \texttt{kernel},
  \texttt{facet} and the \texttt{archetype} simple name.
\end{concept}

\begin{concept}
The \texttt{archetype location} is a location within the
\texttt{archetype space}, which maps one-to-one to the
\texttt{archetype}'s identity. It is made up of all the hierarchical
levels.
\end{concept}

\texttt{Archetype space} is just the general notion of a ``space'' in
which to place all of the possible \texttt{archetypes} one might
conceive. Within this space, each \texttt{archetype} has an identity
and that identity can be expressed as an \texttt{archetype location}.

\texttt{Artefacts}~--- and, by implication, \texttt{archetypes} and
\texttt{formatting functions}~--- can be grouped into
\texttt{facets}. A \texttt{facet} is a set of elements with a common,
well-defined purpose such as for example ``providing a definition of a
domain type in a programming language P''. \texttt{Facets} can be
grouped into \texttt{kernels}: these are sets of \texttt{facets} that
provide related functionality such as for example ``providing domain
models for a programming language P''. \texttt{Kernels} themselves can
be grouped into \texttt{families}: ``providing domain models''.





 Thus, an
\texttt{output model} can map to one or more ``projects'' from a user
perspective, across several programming languages; as far as Dogen is
concerned, these can all be part of the same \texttt{output model}.




  The \texttt{archetype model} is defined as a set of elements called
  \texttt{archetypes}.






\begin{quote}
  \textbf{\texttt{Output Model} and Programming Languages}
\end{quote}





An \texttt{archetype} has a meta-type: its \texttt{modeling
  element}. A \texttt{modeling element} is an entity with
identity. The objective of the \texttt{element} is to factor out the
information needed to recreate its set of associated
\texttt{archetypes}. Example: if the user intends to generate a domain
model with a class called ``hello'', there will be a one-to-one
mapping to a \texttt{modeling element} representing
``hello''. However, its important to note that the \texttt{modeling
  element} \emph{is not a programming language construct}; it is
merely mappable to them. This means that the same \texttt{modeling
  element} can be mapped to constructs in many programming languages,
depending on their structure. Other modeling elements may specialise
in different kinds of content such as build system information and so
on.

A \texttt{model} is then a container of \texttt{modeling elements}.  A
\texttt{product} is composed of a set of \texttt{models}. It is up to
users to add any semantic meaning to these two concepts; from the
\texttt{conceptual model's} perspective these are just containers. For
each \texttt{model} there are two important representations: the
\texttt{input model} and the \texttt{output model}~--- the latter as
defined above.

The \texttt{input model} contains the minimal information required to
produce the \texttt{output model}. An \texttt{input model} is
considered well-formed if it follows the rules of one of the
well-defined \texttt{input model specifications}.

There are three \texttt{spaces} in the \texttt{conceptual model},
arranged in a hierarchy:

\begin{itemize}
  \item the \texttt{modeling space}, where \texttt{modeling elements}
    live.
  \item the \texttt{archetype space}, where \texttt{archetypes} and
    \texttt{formatters} live.
  \item the \texttt{artefact space}, where \texttt{artefacts}.
\end{itemize}

\chapter{Model Driven Software Development}
\label{mdsd}

\epigraph{There is no better or truer description of the physical
  phenomenon of light than to say it \emph{is} precisely the
  electromagnetic field defined by Maxwell's equations. The
  electromagnetic field is not \emph{like} Maxwell's equations; it
  \emph{is} Maxwell's equations. While a model builder knows that his
  model airplane is \emph{like} a true airplane, and a climate modeler
  is aware that his equations only simulate the atmosphere, a
  physicist knows that light and Maxwell's equations are one and the
  same.}{Emanuel Derman}

\lettrine{L}{ike many} a Free and Open Source Software project, Dogen
too started with only a cursory knowledge of the field it ended up
covering\footnote{In fact, one could even say that Dogen's development
  \emph{itself} has been a tool in gaining knowledge in this
  field.}. Several years of reading and researching revealed an
incredibly wealthy area in terms of theory, as well as a number of
successful engineering projects based on those theories. Thus it
became important to have some kind of fundamental understanding in
order to frame ideas from the right conceptual lens, and also to
figure out where Dogen stood with regards to these other tools and
projects. But soon, a second problem emerged: too much theory, too
many papers and too many tools. The practitioner~--- as opposed to the
academic~--- can ill afford to do a thorough literature review before
starting to engineer a solution. Some kind of balance must be struck
between the two. Fortunately, it was at this point that the book
\emph{Model Driven Software Development}\cite{2004model} was found,
and it is the topic of this chapter.

This book is very important to anyone interested in code generation
for two reasons. First, it distils the major developments in the
field, providing an overview of the different trade-offs available. In
effect, it is a ready-made literature review for the
practitioner. Secondly~---- and more importantly for our purposes~---
it provides us with a well-defined ubiquitous language that allows us
to discuss the domain. Clearly, it would be a futile exercise to try
to summarise a \emph{tour de force} such as \cite{2004model} book in a
few pages. We shall instead use it to tease out a minimum set of
concepts required in order to describe Dogen.

The first and foremost concept is that of a \emph{model}.

\section{Models}

Programming is the art of refining abstractions. It is the
programmer's job to create a set of constructs that represent concepts
found in the problem domain; and to get those concepts to cooperate
successfully in producing work that is defined as useful by something
or someone~--- that is, to solve a \emph{problem} in the problem
domain. Together, this collection of concepts \emph{models} the
problem domain because it is a representation of a subset of the
problem domain inside of the computer. One may even further qualify it
as a ``special'' kind of model~--- a \emph{domain model}. \emph{Domain
  modeling} is then the activity of identifying a set of \emph{domain
  types} that describe the domain in question via their properties,
relationships and behaviours.


FIXME: change this into a bibliography for the chapter/
%% **** Links of books and papers to read

%% - [[http://www.voelter.de/data/books/mdsd-en.pdf][Model-Driven Software Development]]
%% - [[http://researcher.ibm.com/researcher/files/zurich-jku/mdse-08.pdf][Model-Driven Software Engineering]]
%% - [[http://people.cs.umass.edu/~brun/pubs/pubs/Edwards11ase.pdf][Isomorphism in Model Tools and Editors]]
%% - [[http://met.guc.edu.eg/Repository/Faculty/Publications/371/2009.SCP.pdf][A type-centric framework for specifying heterogeneous, large-scale,
%%   component-oriented, architectures]]
%% - [[http://gsd.uwaterloo.ca/sites/default/files/2014-Bak-Clafer-Unifying-Class-Feature-Modeling(SOSYM).pdf][Clafer: Unifying Class and Feature Modeling]]
%% - [[http://research.microsoft.com/en-us/um/people/pcosta/slides/generativeprogramming.pdf][Generative Programming]]
%% - [[http://www.issi.uned.es/doctorado/generative/Bibliografia/TesisCzarnecki.pdf][Principles and Techniques of Software Engineering Based on Automated
%%   Configuration and Fragment-Based Component Models]]
%% - [[http://essay.utwente.nl/57286/1/scriptie_Overbeek.pdf][Meta Object Facility (MOF): investigation of the state of the art]]
%% - [[http://www2.informatik.hu-berlin.de/sam/lehre/MDA-UML/UML-Infra-03-09-15.pdf][UML 2.0 Infrastructure Specification]]


- *Model-based development*: the traditional kind of software
  development, where developers create UML diagrams to represent the
  code that they are working or going to work on, and perhaps use some
  kind of simple tool to automate the generation of skeleton code; but
  fundamentally, the models are there just as documentation. The idea
  is to provide a visual representation of what is or will be in code.
- *MDSD*: Model Driven Software Development. Models do not constitute
  documentation, but are considered equal to code. Conversion of
  models to code is automated. Models are blueprints like in CAD.
  MDSD aims to find domain-specific abstractions and make them
  accessible through formal modeling. Models can also be understood by
  domain experts. "Driven" means that models are not peripheral but
  central to the development process and as artefacts are at the same
  level as source code. MDSD attempts to FIXME
  - *MDE*:
  %% [[https://en.wikipedia.org/wiki/Model-driven_engineering][Model-driven engineering]]
  is a software development
  methodology which focuses on creating and exploiting domain models,
  which are conceptual models of all the topics related to a specific
  problem. Seems very similar to MDD and MDSD.
  - ADL:
  %% [[https://en.wikipedia.org/wiki/Architecture_description_language][Architecture description language]].
  A computer language to
  create a description of a software architecture. In the case of a
  so-called technical architecture, the architecture must be
  communicated to software developers; a functional architecture is
  communicated to various stakeholders and users. Some ADLs that have
  been developed are: Acme (developed by CMU), AADL (standardized by
  the SAE), C2 (developed by UCI), Darwin (developed by Imperial
  College London), and Wright (developed by CMU).
- *Reverse Engineering*: The generation of UML diagrams or other
  models from source code. The model has the same level of abstraction
  as the code.
- *Forward Engineering*: The generation of source code from high-level
  models such as UML diagrams.
- *Roundtripping*: The ability to do both Forward and Reverse
  Engineering. The concept of being able to make any kind of change to
  a model as well as to the code generated from that model. The
  changes always propagate bidirectionally and both artifacts are
  always consistent.
- *MDD*: Model-Driven Development. A less precise but common name for
  MDSD. For all intents and purposes can be thought of as a synonym of
  MDSD.
- MDSD is normally incompatible with roundtripping. The model is
  definitely more abstract than the code generated from it. Thus it is
  generally impossible to keep the model consistent automatically
  after a manual change of the generated code. For this reason, manual
  changes to generated code should be avoided.
- *MDA*: Model Driven Architecture. Initiative by OMG to standardise
  concepts around MDSD. Can be thought of as one flavour of
  MDSD. Designed to fit UML since OMG is also responsible for it. Its
  primary goal is interoperability between tools and the long-term
  standardisation of models for popular application
  domains. Ontologically, MDA is a specialization of MDSD.
- Three "kinds" of code in an application:
  - Individual Code: and finally an application-specific part that
    cannot be generalized.
  - Generic Code: a generic part that is identical for all future
    applications
  - Schematic Repetitive Code: a schematic part that is not identical
    for all applications, but possesses the same systematics (for
    example, based on the same design patterns). Also called
    Infrastructure code: The existence of a software infrastructure
    also implies the existence of corresponding infra- structure code
    in the software systems using it. This is source code, which
    mostly serves to establish the technical coupling between
    infrastructure and applications to facilitate the development of
    domain-specific code on top of it. 60% and 70% of modern
    e-business applications typically consists of infrastructure code.
    %% - *[[https://en.wikipedia.org/wiki/Profile_(UML)][
        UML Profile: A profile in the Unified Modeling Language (UML)
  provides a generic extension mechanism for customizing UML models
  for particular domains and platforms. Extension mechanisms allow
  refining standard semantics in strictly additive manner, preventing
  them from contradicting standard semantics. Profiles are defined
  using stereotypes, tag definitions, and constraints. A Profile is a
  collection of such extensions that collectively customize UML for a
  particular domain (e.g., aerospace, healthcare, financial) or
  platform (J2EE, .NET). UML models are not per se MDA models. The
  most important difference between common UML models (for example
  analysis models) and MDA models is that the meaning (semantics) of
  MDA models is defined formally. This is guaranteed through the use
  of a corresponding modeling language which that is typically
  realised by a UML profile and its associated transformation rules.
- PIM and PSM: PIM: Platform-Independent Model. Can be done via a UML
  Profile. PSM: Platform-Specific Model. Can be done via a UML
  Profile. It is important to note that a PIM and a PSM are relative
  concepts – relative to the platform
- transformation: map models to the respective next level, be it
  further models or source code:
  - Model-to-model transformation
  - Model-to-code transformation
- mapping: the mapping of one mode to another. Transformation is done
  by means of mapping.
- *Model*: an abstract representation of a system’s structure,
  function or behaviour.
- *Platform*: Anything that can be targeted such as CORBA, C++, etc.
- *Generative Software Architecture*: all implementation details of
  the architecture’s definition – that is, all architectural schemata
  – are incorporated in software form. This requires a domain model of
  the application as its input, and as output it generates the
  complete infrastructure code of the application – the very code that
  otherwise would need to be generated via a tedious copy/paste/modify
  process.
- *Protected regions*: also known as protected areas. Syntactically,
  these are comments in the target language, but are interpreted by
  the MDSD generator. Each protected region within the generated code
  possesses a globally unique identifier disguised as a comment, and
  is thus uniquely linked to a model element.
- *Domain*: bounded field of interest or knowledge. To internalise and
  process this knowledge, it is useful to create an ontology of a
  domain’s concepts.
- *Subdomains*: describe single parts or aspects of an entire system
  for which a specialised modeling language is appropriate.
- *Partition*: A comprehensive system can be broken down into
  partitions or content increments. In an insurance domain, for
  example, partitions could be defined for single sections or product
  types, such as a "life", "vehicle", "liability" and so on.
- *Abstract Syntax*: specifies what the language’s structure looks
  like. An abstraction is introduced from such details as the spelling
  of keywords, etc. The concrete syntax is the realisation of an
  abstract syntax. Various concrete syntax forms can have a common
  abstract syntax. Put anther way, the meta-model of a domain can be
  expressed in different notations. How can the abstract syntax or the
  meta-model of a domain be specified? Via a meta-model.
- *MOF*: the meta object facility. specified by OMG.
- *Static semantics*: property of a language that determines its
  criteria for well-formedness.
- *DSL*: Domain-specific language. Makes the key aspects of a domain –
  but not all of its contents – formally expressable and
  modelable.Possesses a meta-model, including its static semantics,
  and a corresponding concrete syntax. The DSL should adopt concepts
  from the problem space, so that a domain expert will recognize its
  "domain language".
- *Formal Models*: needs a DSL, and is thus obviously connected with
  the respective domain.
- *Platform*: has the task of supporting the realization of the
  domain, that is, the transformation of the formal model into
  something concrete.
- *transformations*: A model-to-model transformation creates another
  model. However, this model is typically based on a different
  metamodel than the source model. A model-to-platform transformation,
  in contrast, ‘knows’ the platform and generates artefacts (generated
  artifacts) that are based on the platform.
- *Platform Idioms*: Idioms that exist within the platform
  transformation alone and need not be specified in the source model.
- *Product*: MDSD pursues the goal of creating a software product in
  part or in whole through one or more transformations. The product
  can be an entire application or merely a component to be used as a
  building block elsewhere. Such a product aggregates the platform,
  generated, and sometimes even non-generated artefacts.
- *Domain architecture*: The metamodel of a domain, a platform, and
  the corresponding transformations, including the implemented idioms,
  are the tools that are needed to make the transition from the model
  to the product, whether completely or partially automated.
- *Software System Families*: The set of all products that can be
  created with a certain domain architecture.
- *Product line*: set of complementary single products. From a user’s
  perspective, the products in a product line can constitute
  alternatives – that is, they be applicable in different but related
  contexts – or can complement each other content-wise and thus define
  a suite.
- *Domain-Driven Design*: not directly related to MDSD. The only
  connection is that DDD talks about deep domain understanding and the
  importance of models. MDSD talks about automation. They are
  complementary technologies.
- *Software architecture*: describes to a certain level of detail the
  structure (layering, modularization etc.) and the systematics
  (patterns, conventions etc.) of a software system.
- *Component*: is a self-contained piece of software with
  clearly-defined interfaces and explicitlydeclared context
  dependencies.
- *Generate Good-Looking Code – Whenever Possible*: It is unrealistic
  to assume that developers will never see the generated code. Even if
  developers don’t have to change generated code, for example by
  inserting manually-written sections, they will be confronted with it
  if they debug the generated applications with conventional tools, or
  if they have to check the generator configuration.
- *Cartridges*: a cartridge is a ‘piece of generator’ for a certain
  architectural aspect. ODB etc. Third-party off-the-shelf
  cartridges. The problem then often becomes how to combine these
  different cartridges, especially if they have been developed
  independently and thus use different metamodels – different
  stereotypes, tagged values, and so on. You certainly don’t want to
  model things several times merely to be able to use various
  incompatible cartridges. NOTE: this is the approach we use for ODB.
- *Explicit Integration of Generated Code and Manual Parts*: this is
  what dia2code does, but as a one off.
- *3-tier implementation*: split generated code from manual code via
  inheritance. We need to explain why we didn't take this approach.
- *Code-generators*: meta-programs that process specifications (or
  models) as input parameters, and which generate source code as
  output. Meta-programs can be run at different times in relation to
  the generated program: a) Completely independently of the base
  program – that is, before it. b) During compilation of the base
  program. c) While the base program runs.
- Separation/Mixing of Program and Meta-program: a common (or at least
  integrated) language exists for programming and metaprogramming, and
  the source code components are not separated, but mixed. This can
  lead to invocation relationships between the two levels, in
  principle in both directions. C++ template metaprogramming can fall
  into this category, as well as Lisp and CLOS. If program and
  metaprogram are separated, the system’s creation takes place in two
  distinct phases. The metaprogram is run and creates the base program
  (or parts of it) as output, then ter- minates. The program does not
  know that the metaprogram exists 1 . The separation is maintained
  throughout the (meta-)programming process, including the build
  process.
- Separation of code classes. This involves the adaptation of the
  target architecture in such a way that manually-created code must be
  written into classes specifically created for this purpose.
- *Model markings*: In some cases it is necessary to configure the
  intermediate products manually to control their further
  transformation stages. The OMG calls such a configuration model
  markings. Model markings cannot be annotated directly in the PIM,
  because this would involve the risk of losing platform independence.

- we should define a UML profile in Dia that contains all of the
  required concepts for Dogen. Perhaps we don't even need
  meta-data/KVPs. In particular it seems that "tagged values" are
  already KVPs. There is also "extensions". These are used in
  conjunction with stereotypes. So for example we could "extend" the
  UML notion of an attribute with say a new UML meta-class called
  Key. We would then mark attributes as =<<Key>>=. This would mean
  they are not regular UML attributes, but instead they are an
  instance of our extension.
- we seem to have layers of "abstraction" around generation. The first
  layer is simply: if I define a class called X, create me a class X
  in language y. This is Dia2Code. The second is the application of
  some minimal infrastructural behaviour: create boost serialisation
  for X, hashing for X, etc. The third is another level up in
  abstraction: annotating types with "architectural concepts". For
  example we could have a =RemoteService= stereotype which by default
  in C++ results in the generation of all boilerplate code required
  for ASIO. All that is required is to associate commands and queries
  with the service. This could be achieved by marking a type with some
  other stereotype - or even better, to define an attribute such as
  =<<Command>>= or something more meaningful and apply that to a UML
  attribute. From this we have all the information required to
  generate the networking code.
- for ODB it would be nice if we could mark types with generic
  stereotypes which could be mapped to ODB specific pragmas:
  =<<PrimaryKey>>= etc. Actually, just =Key=; it is somebody else's
  job to map it to a foreign or primary key. We could even have some
  stereotypes which are PSM: =BoostSerialisable=, etc. Such that by
  default =Serialisable= maps to =BoostSerialisable= in
  C++. =RelationalEntity=?
- does UML have a concept of stereotype sets or groups? If it did we
  could create some such as =DomainEntity=: =Serialisable=,
  =Printable= etc. =ComparableDomainEntity= and so on; we could create
  an entire hierarchy of stereotypes. These would then be translated
  to having facets on and off. These are also "additive": =Visitable=,
  =DomainEntity= etc could be applied to a single entity. Actually
  maybe it should just be called =Entity= as domain does not add much
  value. Or =ValueObject=. We need to create a hierarchy of these.
- this means that Concepts are very much like all other kinds of
  stereotypes. It just means that sometimes a stereotype resolves to a
  hard-coded meaning (=Serialisable=, etc) whereas sometimes it is
  user defined (e.g. the user created a concept).
- all backends should be grouped under one name (say quilt) but there
  is no need to have multiple kinds of backends (at least for
  now). All the use cases we have can be handled by one kind of
  backend, with a way to toggle SML transformation. So if a user is
  making use of LAM (a PIM), we should have the option to either map
  LAM types to native types or to use LAM "natively". This would mean
  we need a C++ implementation of LAM, etc. Other LAM like models
  could be defined.
- users should be able to add the same kind of mappings to UML
  stereotypes to facets of their own making. That is, I should be able
  to create a set of stitch templates, register them against a
  stereotype and then load up my DLL via a plugin and have knit
  generate my code. This would mean that you could make DSL
  extensions. The classic case is when you spot repetitive
  infrastructural code in your system which is not common to other
  systems.
- targeting of platforms: a given stereotype can map to very different
  implementations depending on the platform the user chooses to
  target. In addition, the user may choose to target multiple
  platforms. The name "platform" is not a very good one. For example,
  lets say the user marks a type as serialisable and there are N
  different types of serialisation. It would be really nice if we
  could define the set of target serialisations (e.g. boost, POF,
  etc).
- this is actually quite simple. There are meta-concepts: Visitable,
  Entity, and so on. One of these meta-concepts is
  Concept. Meta-concepts are refinable: thus Entity can refine
  Serialisable and so on. Meta-concepts can be modeled by one or
  more implementations: thus Hashable is modeled by Boost Hash,
  std::hash and so on. There is a mapping between meta-concepts
  (modeling world) and facets/formatters (implementation). A formatter
  is (or can be) a model of a meta-concept. Actually, we don't need to
  call it a meta-concept; its just a meta-class. This is because the
  translation of a meta-class from meta-model to platform concrete
  artefact could involve the generation of multiple classes. That is,
  a single meta-class could be transformed into multiple concrete
  classes. Conceptually its still the same meta-class.
- we are creating our own specialisation of MDSD with its own
  values. We need to define dogen's MDSD infrastructure.
- how do we intend to allow a restricted form of roundtripping:
  - abstraction: can only be done when the meta-model is not that far
    removed from code. Certain aspects cannot be roundtripped. For
    example, if you need to change a visitor manually, you need to
    disable generation. Because it is a concept at a higher level of
    abstraction, it cannot be roundtripped.
  - tagging the model: we are tagging the model via dynamic to provide
    additional information that is not at the right level of
    abstraction.
  - separation of code classes: only certain classes are
    roundtrippable (e.g. types). The rest are ignored.
  - tagging the code: language attributes and other markers will be
    used to keep track of what was manually generated versus what was
    automatically generated.
- *Viewpoint*: interesting word that can be used in the context of
  facets. One way of using this term is to imagine DDD as a way of
  defining the Domain Model and then having multiple representations,
  each being a viewpoint: presentation layer, business layer,
  persistence.

\chapter{Domain Driven Development}
\label{ddd}

\epigraph{That the Ideas are themselves manifestations (of the
  Idea-Idea) and that the Idea-Idea is a-kind-of
  Manifestation-Idea~---which is a-kind-of itself, so that the system
  is completely self-describing~--- would have been appreciated by
  Plato as an extremely practical joke [Plato].}{Alan Key}

\chapter{Fundamental Building Blocks}

\epigraph{Le mieux est l'ennemi du bien.}{Voltaire}

\lettrine{T}{his} section focuses on the more abstract aspects of
Dogen.

\section{The Question}

Dogen is largely the result of exploring a simple question: what
portion of the objects required to model a problem domain is
generatable by a program, such that the generated code is as good as,
or better than, code crafted by humans? This question stems from many
years of looking at object models and their limitations, in various
incarnations.

One of the main problems one often finds in production code is a lack
of a large number of simple but very useful ``facilities'', which all
objects on all domain models should have. For instance, a simple way
of dumping current state to a stream, in a format that can be
understood by external tools. Developers tend to add facilities like
these on a haphazard sort of manner, because it is laborious and not
particularly exciting functionality to work on. By the time the domain
model has matured, it is often too late to find time for it.

A further observation is that a large part of the objects required to
model a problem domain have fairly straightforward behaviour; in many
cases they are but glorified structs with a few trivial abilities
bolted on, such as serialisation and hashing. A lot of programmer time
is taken on generating getters, setters, serialisation code and so
on. It is easy to get sloppy with this kind of code because its so
repetitive.

Trying to answer this question led us along a road less travelled by
regular developers: the world of \emph{meta}.

\section{On Models and Modeling}

Before we go too much further, it is important to clarify what is
meant by a model. After all,

Whilst all of this may seem pretty obvious, it is a terrain rife for
misconceptions. First, in a strict sense, all models are ``domain
models'' because they exist as a model of a domain somewhere. However,
in practice, one tends to make an important distinction between the
domain we're interested in at a given point in time, and all other
``helper models'' that are there to just give us a hand. It is in this
context that we say that ``domain models'' are ``special''. Second, it
is important to notice that \emph{the} domain model is a conceptual
entity. It manifests itself in a myriad of representations~--- UML
diagrams, source code, database schemas and so on~--- but none of
these representations \emph{is} the domain model. For instance, if you
were to ``write down the domain model'' (via specifications or
otherwise), you would do nothing but create yet another such
representation, this time expressed in the medium of natural
language. The logical consequence is twofold: there is no such thing
as a ``complete'' representation of the domain model~--- it is only
``complete'' insofar as the purposes for which it has been created are
satisfied; and any representation is highly sensitive to the
properties of the medium it is expressed in.

To ram the point home: there is no such thing as the \emph{better} or
\emph{best} representation of a domain model; it only makes sense to
make comparative statements of this kind when there is an objective
task against which two or more representations can be evaluated. Any
non-trivial software system requires several representations, each
specialising on a particular area, and transformations between
representations are required.

We should now be able to drop the ``domain'' prefix and refer to just
``models'' or ``modeling'' and presume that all of the above is implied.

You may think that we're sliding down the conceptual slippery-slope
for no good reason. As we shall see, these fundamental ideas are
important in shaping how you use Dogen. For now you can start to think
of it as the tooling infrastructure that tries to automate as much as
possible the transformation of one representation of a model into
another.

\section{On Code Generation and Meta-Models}

Dogen didn't come to exist in a vacuum, but rather on a continuum, and
the continuum had it's genesis very early on. In fact, the concept of
programs that generate programs is probably as old as computer science
itself: it certainly was a common feature in the days of machine code
and assembler code programming. These ideas were incorporated in early
languages such as LISP, where there was a blurring of the lines
between hand crafted source code and machine generated source
code. Sadly, these progressive thoughts faded into the background as
the C family of languages took front stage.

It's not as if code generation disappeared~--- it just went into
hiding. In fact, today there are many widely used tools in the Open
Source ecosystem that generate code:

\begin{itemize}
\item \href{https://developers.google.com/protocol-buffers/}{Google Protocol Buffers}
\item \href{http://www.codesynthesis.com/products/odb/}{ODB}: C++ Object-Relational Mapping (ORM)
\item \href{http://www.codesynthesis.com/products/xsde/}{eXSD}: XSD/e: XML for Light-Weight C++ Applications
\item \href{http://msdn.microsoft.com/en-us/library/windows/desktop/aa367300(v\%3Dvs.85).aspx}{MIDL}: COM IDL compiler
\item and many more.
\end{itemize}

Each of these tools are designed to do a specific task and to do it
well, hiding as much as possible of the code generation details from
the end user. We call these \emph{special purpose} code generators~---
although, as we shall see, in a sense all code generators are special
purpose. The code generated by these tools contains both the data
structures they require as well as hard-coded behaviour associated
with them: how to read and write them from raw storage (in the case of
Protocol Buffers), how to read and write them from the database (ODB),
and so on. One is not expected to tamper with the generated code.

All code generators have an internal set of data structures that
represent the entities to generate~--- explicitly or implicitly. These
data structures are known as the \emph{meta-model}. Meta-models are a
class of domain models that focus on describing domain models
themselves. They allow code to introspect and to think about code; to
reflect. In this form, code generation is simply the transformation of
a model, described in one such representation (the meta-model) into
another representation (the source code), following the rules laid out
by the grammar of a programming language. The richer the meta-model,
the more expressive the generated code can be~--- and vice-versa. It
is in this sense that certain classes of code generators are called
special purpose, because they have meta-models that are very focused,
designed only for the task at hand. Don't think of this as a
disadvantage though: there is a price to pay in complexity for every
ounce of flexibility, so its best to have simple code that does one
thing and does it well.

Nevertheless, meta-models can be useful in a more general form when
designing software applications: they can allow one to reason about
the structure of the code. One of the most common meta-models in
existence is
\href{http://en.wikipedia.org/wiki/Unified_Modeling_Language}{UML}. UML
is used widely in the industry and there are many tools that can be
used to generate source code from UML diagrams. It is simultaneously
ubiquitous~--- it is available everywhere~--- and complete~--- that
is, as a meta-model, it defines a extensive list of concepts for
pretty much any aspect of programming. Thus it is common for tools to
take a UML representation and use it to generate source code; as
examples of Open Source tools that can generate source code from a UML
diagram see:

\begin{itemize}
\item \href{http://dia2code.sourceforge.net/}{dia2code}
\item \href{http://umbrello.kde.org/}{Umbrello} (see \href{http://docs.kde.org/development/en/kdesdk/umbrello/code-import-generation.html}{this} for code generation)
\end{itemize}

In a sense one, one may think of these tools as \emph{general purpose}
code generators because they output code that is not tied up to any
specific purpose, other than to model the problem domain. Unlike the
special purpose tools, the generated code is very much skeleton code,
code that adds little in terms of behaviour. This is all as it should
be: the more specific your intent is, the more the code generator can
do for you and, conversely, the less specific your intent is, the less
helpful the code generator can be.

The astute reader would have already devised a simple solution to the
behaviour conundrum: nothing stops us from modeling the signatures of
methods in the meta-model~--- after all UML provides us with all the
required machinery~--- and then hand-craft an implementation for these
methods. Indeed there are code generators which permit such workflows;
they are known as \emph{merging code generators}. The merging aspect
comes from the fact that the code generator must be able to
distinguish between the hand-crafted code and the machine generated
code in order to handle meta-model updates.

So these are three key themes for Dogen: special purpose code
generation, general purpose code generation and merging code
generation. But before we can proceed, we need to add one more actor
to the scene.

\section{On Domain Driven Design}

One of the main problems facing software engineers working on large
systems is the need to clearly separate business rules from
scaffolding code. In many ways, this need originates from the long
forgotten days when the word \emph{Application} was coined: the use of
computer science \emph{applied} to a specific problem to provide an
automated solution to the set of people with the problem~--- the
\emph{users}. During the process of development, users will provide
all sorts of insights into what it is they want solved, and these are
ultimately captured in code (see
\href{http://www.developerdotstar.com/mag/articles/PDF/DevDotStar_Reeves_CodeAsDesign.pdf}{Jack
  Reeves' essays} on this topic). Code will also be made up of reading
and writing records to a database, socket communication, reading and
writing to file and so on; the challenge then is to avoid obscuring
the former while dealing with the latter.

Many people have thought deeply about this dichotomy. Arguably, the
most significant advance was made by Eric Evans with his seminal book
\href{http://www.amazon.co.uk/Domain-driven-Design-Tackling-Complexity-Software/dp/0321125215}{Domain-Driven
  Design}: Tackling Complexity in the Heart of
Software\cite{evans2004domain}. Domain Driven Design (DDD) is a
software engineering methodology that places great emphasis on
understanding the problem domain and, coupled with Agile, it provides
a great platform for iterative improvements both to the understanding
and to its expression in code. DDD focuses on defining a clear and
concise domain model~--- a set of classes and relationships that model
the insights provided by the users and domain experts in general. It
also explains the difference between the conceptual domain model and
myriad of representations: UML diagrams, specification documents, oral
conversations and, most importantly, source code.

\section{Adding It All Together}

The key idea behind Dogen is that all of the aspects we described up
til now are deeply interrelated. That is to say that we store deep
knowledge about the domain in meta-models, which tend to be
represented graphically~--- say in UML class diagrams; and we do so
because these representations provide a quick and yet expressive way
to communicate domain knowledge. But those very same documents are~---
or can be made~--- sufficiently complete to be used as a basis for the
code generation of skeleton code by some general purpose code
generation tool. Furthermore, there are a large number of facilities
that are required of most domain models, and these can be thought of
as special purpose extensions to such a general purpose tool; and,
finally, that which cannot be code generated can be manually added and
merged in. And thus all the strands are weaved into a single tool.

Lets return to the ``facilities'' required by all domain models. What
do we mean exactly? Well, ODB and the like already hinted at some of
the things one may wish to do with C++ objects~--- persist them in a
database~--- but there are other even more fundamental requirements:

\begin{itemize}
\item the ability to support getters and setters, hashing,
  comparisons, assignment, move construction and many other
  fundamental behaviours;
\item the ability to dump the current state of the object to a C++
  stream in a format that is parsable by external tools (like say
  JSON);
\item the ability to generate
  \href{http://stackoverflow.com/questions/5140475/how-to-write-native-c-debugger-visualizers-in-gdb-totalview-for-complicated-t}{debugger
    visualisers};
\item the ability to serialise and deserialise objects using a
  multitude of technologies such as
  \href{http://download.oracle.com/otn_hosted_doc/coherence/353CPP/index.html}{POF},
  \href{http://www.boost.org/doc/libs/1_55_0/libs/serialization/doc/index.html}{Boost
    Serialisation}, \href{https://github.com/hjiang/jsonxx}{JSON},
  \href{http://libxmlplusplus.sourceforge.net/}{XML} and many others;
\item the ability to generate objects populated with random data for
  testing;
\item \ldots{}
\end{itemize}

And on and on. Other languages would have a similar list~--- if
perhaps not so extensive, as the use of reflection already allows them
to satisfy some of these use cases generically, at the cost of
performance. The more we looked, the more boilerplate code we
found~--- code that could easily be generated for the vast majority of
the cases. There are, of course, quite a few corner cases which are
just too hard to automate, but they can easily be manually coded.

The picture that emerges from this
\href{http://en.wikipedia.org/wiki/Thought_experiment}{gedankenexperiment}
is some kind of ``cyborg'' coding. A type of programming where any and
all aspects that can be reduced to a set of rules~--- applicable to
instances of the meta-model~--- are implemented as extensions of the
code generator; and this process of extension continues over time, as
the meta-model becomes more and more expressive.

Dogen is an attempt to create such a tool. As we are C++ developers we
started off by trying to implement the vision as a C++ tool; but the
notions are general enough that they would apply to any programming
language.

\part{Implementation}

\chapter{The Architecture}

\lettrine{D}{ogen} is made up of a large number of domain
models. These fall into two broad categories: \emph{test models} and
\emph{main models}. Test models are models we created specifically to
test some aspect of code generation~--- such as say inheritance~---
and whose code is not used by the main binary. The main models are
what really makes up the application.

Lets look at each of these in more detail.

\section{Main Models}

Dogen is made up of a number of main models, hooked together in
different ways. But before we look into the main models, we need to
understand the users of these models: the suite of tools.

\subsection{Dogen as a suite of tools}

Dogen is really just a suite of different tools that are related to
code generation. Since we could not think of natural names to use,
sewing was used as a theme. The naming scheme uses a couple of simple
conventions:

\begin{itemize}
\item top-level libraries that implement a tool use the infinitive
  form of the verb (minus the ``to'', where applicable).
\item where applicable, we may have an executable binary that wraps
  the top-level library. In this case, we use the noun closest to the
  verb~--- even if in some cases we have to ``nounify'' the verb.
\end{itemize}

At present, the following tools exist or have been planned:

\begin{center}
\begin{tabular}{llll}
Tool Name & Top-level library & Status & Description\\
\hline
knitter & knit & started & Code-generation of models.\\
stitcher & stitch & started & Basic text-templating support (similar to T4).\\
darner & darn & vision & Generates JSON from Dia diagrams\\
 & needle & vision & Library with supporting code, used by generated models.\\
patch & patcher & vision & Updates a dia Diagram given a C++ code base.\\
 & quilt & vision & Native domain model representation\\
 & pleat & vision & Cross-language domain model representation\\
\end{tabular}
\end{center}

Note that all binaries are prefixed with \texttt{dogen} to avoid
clashes, e.g. \texttt{dogen\_knitter}. The next sections provide
details on each of these tools.

It is important to notice that a main model may be used by more than
one tool; however, to simplify things we describe the main models
below from a tool's perspective.

\paragraph{Knit}

The main objective of dogen was always to do code generation. As such,
\texttt{knit} can be thought as its most important part, since that is
it's objective. \texttt{knit} uses a number of main models, and hooks
them together in a fashion similar to that of the internals of a
compiler. Thus, these main models belong to one of three groups: the
\emph{front-end}, the \emph{middle-end} and the \emph{backend}. The
front-end group of models allows for different sources of domain
information to be plugged into \texttt{knit}. The middle-end model~---
as there is only one~--- is where all the language neutral
transformations take place; It can be thought of as a bridge between
domain modeling and code generation. Finally, the backend group of
models are responsible for expressing the middle-end as code.

\paragraph{The Frontends}

When we started developing Dogen, we chose Dia as our main input
format. Dia is a simple yet very powerful tool for drawing structured
diagrams that focuses almost exclusively on diagram editing, and
leaves all other use cases to external tools. To their credit, a
number of tools have sprung up around Dia and that is in no small part
due to the simplicity and stability of their XML file format. We aimed
for Dogen to be another chain in that tooling ecosystem.

At the same time, Dogen has been developed from the start with the
intention to support multiple input formats. We knew that different
people would have different modeling needs and for some Dia or even
UML would not be the correct choice. So we imagined a pipeline that
was made up with a pair of front-end models: one to model closely the
input model and a \emph{transformation} model responsible for
converting the input model into the middle-end. Each front-end would
have one such pair, starting with Dia. In Dia's case we have the
following models:

\begin{itemize}
\item \texttt{dia}
\item \texttt{dia\_to\_sml}
\end{itemize}

The \texttt{dia} model has a representation of the Dia XML types, and
tries to do so as faithfully as possible. It was created to avoid
having a direct dependency with Dia's code base. Since Dia XML changes
very infrequently and since we use such a small part of Dia's
functionality, this turned out to be a good decision.

Dogen also supports JSON as an input. However, since it was done
specifically for Dogen, we added this directly to the middle-end. See
the next section for details.

\paragraph{The middleend}

We store the domain model internally as SML~--- the Simplified
Modeling Language. SML is a \emph{meta-model} largely based on Domain
Driven Design. SML is designed to capture all the details of the
domain model that are required for code generation, but in a form that
is programming-language-agnostic. It is an intermediate model in
between the front-ends (specific to a tool, for example) and the
backends (specific to a programming language, for example).

It is important not to confuse SML with other, more generic
meta-models such as UML or the language-specific Reflection
meta-models. SML is not designed for modeling in the generic sense
like UML is; it is instead a special purpose meta-model for code
generation, so it may appear to be very terse and not particularly
obvious.

Models in SML can be in one of two forms: \emph{partial} or
\emph{merged}. A partial model is a model that has been read directly
from the input, but which is incomplete; it may not provide
definitions for all types, for example. A \emph{merged} model is one
that was created by merging a number of partial models together, by
resolving all definitions and by doing a number of required
transformations so that the model is ready to be consumed. The job of
the middle-end is to provide infrastructure to do all of these.

\paragraph{The backends}

The role of the backend is to express the meta-model as code. We can
think of this as the transformation of one representation of the
domain model~--- the SML meta-model in memory~--- to another
representation~--- a set of files in the file system. These files are
expected to obey the rules of a well-known \emph{grammar}. Typical
grammars are those of programming languages such as C++, C\# or
SQL. In practice, a single backend has more than one grammar, as we
must also generate the supporting infrastructure like CMake files and
so on, but conceptually the idea still holds.

SML has a ``functionally agnostic'' view of domain types. That is to
say that within SML there is no behaviour, just a pure representation
of the data structures that we have deemed to be representative of the
fundamental concepts of the problem domain. As part of the
transformation process, the backend performs an expansion of these
data structures, providing useful behaviours or ``facilities''. These
facilities are aggregated in logical groups called \emph{facets}. They
are specific to the backend in question, although there are
commonalities between backends~--- for example, all backends need to
provide a definition of the domain types.

A perhaps more intuitive way to look at facets is as follows: a
backend generates a variety of files, of different types. It is thus
useful to group these files in a logical manner, so we can talk about
them in aggregate. \emph{Facets} provide the first level of grouping.

Facets are housed in one or more folders in the file system, named
after the facet. The facet folders are in turn composed of zero or
more files and folders, with folders representing modules in the
programming language in question~--- if it supports such a concept
(namespaces in C++ and C\#, packages in Java and so on). Within a
facet, files are grouped into ``kinds'': in Dogen parlance these are
known as \emph{features} because each kind provides a new chunk of
functionality to the system. A feature is effectively a type of
file. Within a feature we have atomic chunks which we call
\emph{aspects}. Aspects can be related to other aspects in a chain of
dependencies. An aspect may exist in multiple features, with different
expressions. For example, ``constructors'' is one such an aspect.

\emph{Traits} are the final building block. These are points of
configuration, knobs or dials that control behaviour in the backend. A
trait may have an effect at the backend level, or it may affect only a
feature or even an aspect within a feature.

A concrete example should make these concepts clearer. Lets look at
the \emph{types} facet in the C++ backend, the most fundamental of all
C++ facets.

\begin{center}
\begin{tabular}{ll}
Term & Description\\
\hline
facet name & types\\
facet purpose & contains the definition of the domain types.\\
facet folders & ``include/\ldots{}/types'' for the headers and ``src/types'' for the implementation.\\
includer feature & include file that includes all files or groups of files for that facet\\
main header feature & the class declaration\\
main implementation feature & the class implementation\\
aspect & constructors. defines all of the available constructors for the class.\\
traits & example: complete constructor enabled. if false, disables the complete constuctor.\\
\end{tabular}
\end{center}

One can imagine a logical graph that unites backends, facets, features
and aspects, such that when a trait switches something on or off, all
other dependent elements are switched on or off accordingly. Note also
that a facet needs not be specific to a backend: this is the case with
\texttt{types}, which is common to all backends.

Before we go into the backends specifically, one word on the
\texttt{formatters} model. This is a utility model that contains all
formatting code generic to all backends, so that we can reuse it. It
is then used by concrete formatter models such as
\texttt{cpp\_formatters}, and so on.

\paragraph{The C++ backend}

The objective of the C++ backend \texttt{cpp} is to generate a C++
representation of the domain model. At present only C++-11 is
supported.

It is implemented by three models:

\begin{itemize}
\item \texttt{cpp}: all the types required for generating C++ code.
\item \texttt{sml\_to\_cpp}: transforms an SML model into its
  corresponding \texttt{cpp} representation.
\item \texttt{cpp\_formatters}: creates C++ source code from the
  instances of \texttt{cpp} types.
\end{itemize}

These models are hooked up as follows: the \texttt{sml} model is
transformed into the \texttt{cpp} model by \texttt{sml\_to\_cpp}; we
then use the workflow of the \texttt{cpp\_formatters} model to convert
these types into C++ source code.

The C++ backend defines the following facets:

\begin{center}
\begin{tabular}{ll}
Facet Name & Description\\
\hline
types & definition of domain types\\
io & responsible for dumping the contents of the instance of the domain type as a JSON object.\\
hashing & provides std::hash support.\\
serialisation & provides boost::serialization support.\\
test\_data provides a set of ``generators'' that create test instances of the domain types.\\
odb & provides ODB support; the odb compiler is executed against the model to generate ORM mappings for it.\\
\end{tabular}
\end{center}

\paragraph{Design and evolution of the C++ backend}

Whilst the purpose of the C++ backend is to generate standard's
compliant C++ code, it is important to note that the Dogen's
\texttt{cpp} model is \textbf{not} a model of the C++ type
system. This approach was indeed tried, with a model that had types
taken directly from \texttt{ISO/IEC 14882:2011(E)}~--- an approach
rather close to an AST representation of the language. The idea was
that one could express all the intricacies of the code to generate via
the constructs of this model and then, using a single formatter, write
it according to the grammar of the language. The Clang infrastructure
was a suitable candidate for implementing this kind of formatter.

In practice, we found that expressing code in such low-level fashion
was non-trivial, particularly when one considered all required
behaviours such as serialisation, hashing and so on. Instead, we
settled on building a model at a much higher level than the AST,
where~--- borrowing terminology from Microsoft's T4~--- formatters are
seen as ``text templates'' and the \texttt{cpp} model is designed to
provide these text templates with \emph{exactly} the data they
need. With this approach, one can abstract away all of the
domain-related logic: entity composition, relationships between the
entities and so on. These high-level C++ types are informally known as
``view models'' and the formatters can be thought of as ``views''; the
approach is extremely similar to MVVM or the Presentation Model as
described by Martin Fowler.

The job of a given formatter is to take a specific number of types in
the C++ model such as \texttt{class\_info} and generate corresponding
C++ source code~--- for example the domain model header file. Ideally
one would like a \texttt{cpp} model that knows nothing about
formatters, and formatters that take one type and output it to a
stream. However, as it turns out, our hopes for such a clean model
were severely dashed. The problem is that in order to build a complete
view model, one needs to know what view it belongs to.

Take for example the case of an entity \texttt{a}, a simple value
object. From a domain type definition perspective~--- the
\texttt{types} facet~--- we have one set of include files: say all the
properties used by this class that are not simple types. From a
serialisation perspective we would have another set of include files:
say the domain type header and the serialisation headers for each type
of each property. And since C++ has an header and an implementation
file, these too have different sets of include files. This causes
problems because the include file list cannot be computed unless we
know for \emph{whom} it is being built for, and the \emph{whom} is,
effectively, the formatter (not quite, but almost). Things are made
considerably worse by the fact that some formatters depend on knowing
what files were generated by other formatters (includers,
serialisation registration code, etc). So it was that we ended up
having to represent these notions in the \texttt{cpp} model.

The approach taken was to create a \texttt{content\_descriptor}, which
is a crude way of mapping coordinates in the formatting space. It
tells:

\begin{itemize}
\item to which facet the file belongs: e.g. types, serialisation, etc.
\item within that facet, to which feature the file belongs to:
  e.g. the main domain header, forward declarations, etc.
\item if the type is a class, what \emph{kind} of class it is; we have
  formatters specifically for plain value object, exceptions,
  visitors, etc.
\item whether the file is a header file or an implementation file.
\end{itemize}

This approach is very unfortunate, because it meant adding a new
formatter was not as simple as adding a new type in
\texttt{cpp\_formatters}. It required you to have a deep understanding
of core things such how the inclusion lists are computed, how the file
names are generated, what enumerations to update, and so on. Ideally
we wanted a simple registration process, possibly in the
implementation file of the formatter, that totally encapsulated the
formatter from the rest of the code. Intuitively it sounds like this
is the right approach:

\begin{itemize}
\item the formatter knows about what facets and aspects it applies to;
\item the formatter knows what types it will process;
\item the formatter could generate a file name, adding some kind of
  post-fix/prefix related to the aspect.
\end{itemize}

However, the downside of all of this is that we now need really
complex logic in the formatter to be able to build the inclusion
lists; and this logic actually requires the formatter to know about
other formatters~--- for instance, the serialisation example above
required access to the domain type definition~--- so they would not be
encapsulated from each other.

After much, much thinking, it was decided that the best way to handle
this was to augment SML with the data required by the formatters. The
\emph{meta-data subsystem} was born.

\chapter{The Dynamic Subsystem}

The \texttt{dynamic} subsystem has two main responsibilities:

\begin{itemize}
\item to augment the front-end, providing a way of expressing
  middle-end constructs that are not naturally available in front-end
  language;
\item to provide a way of storing backend-specific information in the
  middle-end, without coupling the middle-end too much to the
  backends.
\end{itemize}

The \texttt{dynamic} subsystem is called ``dynamic'' because it uses
weakly-typed annotations on top of a strongly typed object model to
avoid coupling. The \texttt{dynamic} subsystem is made up of two
models:

\begin{itemize}
\item \texttt{dynamic::schema}: provides the type system that
  describes the meta-data, including a way of validating it.
\item \texttt{dynamic::expansion}: provides a way to hook
  transformations that augment the meta-data, possibly in several
  steps, before its consumed.
\end{itemize}

We will now describe this machinery detail and delve into the data
structures that implement them. But first we will explore the need for
\texttt{dynamic} in the first place.

\section{Evolution of Dynamic}

Dynamic evolved over time. It start off to solve problems we had in
the front-end and then it was also used to solve problems in the
backend. Lets look at this in turn.

\subsection{Dynamic in the Front-End}

As explained in the
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#the-front-end}{front-end
  section}, the SML meta-model is bootstrapped from Dia diagrams. We
soon found the need to transport information into SML that was
inexpressible in Dia XML. In some cases these were just limitations of
Dia's modeling of UML and could be solved by improving the
application. But it wasn't always the case; sometimes the data
required by Dogen just made no sense at all in a UML-like world. To
solve this problem in a general manner, we created a set of
\emph{instructions} that are interpreted by Dogen much like a
\texttt{\#pragma} is interpreted by a compiler. These instructions are
passed in by adding lines to UML Comments that start with the
well-known prefix \texttt{\#DOGEN}:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
#DOGEN [KEY]=[VALUE]
\end{pseudocode}

There is only one valid form for the instructions, which is the
key-value-pair form shown above. The key-value-pair is called a
\emph{meta-data definition}. Note that \texttt{[KEY]} and
\texttt{[VALUE]} are left to the user to define within Dia but, of
course, only those keys that Dogen is aware of will have an effect,
and the domain of \texttt{[VALUE]} is defined by the owner of the key
in question. Thus it is up to the user to make sure he formulates the
instruction correctly, according to the specification provided later
on in this manual.

Of course, Dia is not the only front-end supported (or supportable) by
Dogen. For instance, one can supply all of the required inputs via a
JSON document using a schema defined by Dogen. These other front-ends
may or may not require Dogen instructions; if they do not, they must
provide some other way to supply the meta-data definitions.

\subsection{Meta-data in the Middle-End}

As explained in the
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#design-and-evolution-of-the-c-backend}{Design
  and evolution of the C++ backend}, it is quite difficult to split
duties between the middle-end and the backends. The gist of the
problem is that the transformation process that converts SML into a
backend-specific representation~--- say the CPP model~--- also
requires information that is only available to the backends and more
specifically it's formatters. Sadly, certain aspects of the
transformation process are simultaneously formatter-specific
\emph{and} require access to the richness of detail provided by SML,
thus breaking the nice clean model of a pipeline with isolated
elements.

At the same time, it is vital that we loosely-couple the formatters to
the rest of the architecture. It would be easy to move some of the
formatter logic into transformation process~--- we originally
implemented it that way~--- but this approach carries with it its own
problems such as a need to understand the guts of SML just to add a
trivial formatter. It also means that code that belongs logically
together needs to be scattered in different locations around the code
base, making maintenance harder. Finally, we had envisioned a future
where one could add new formatters at will without changing anything
else~--- possibly even by supplying a backend as a DLL at run-time,
via a plug-in system. For this to work, there can't be any backend
logic manually hard-coded in the transformation process.

The solution to this conundrum was to leverage the instructions above
to augment the middle-end with formatter specific information. This
was done by keeping the required information in SML but storing it in
a format that is transparent to the middle-end. We first had a weakly
typed implementation that used \texttt{boost::property\_tree} but we
soon found the need for a more strongly typed approach, that included
validation. This is now \texttt{dynamic::schema}.

\texttt{dynamic::schema} provides a generic mechanism to decorate
objects with very simply structured data. We then allow each subsystem
to take responsibility of its own keys and values: they define them,
populate them with defaults, perform validation and ultimately consume
them. This is done by creating JSON files with field definitions.

This approach also has the side-benefit that we can expose all the
configuration knobs directly to users via the front-end instructions;
in the past this had to be done by creating new command line options,
but again that would be far too static in a world of plug-ins and
run-time decisions.

\subsection{Tags}

A simple flat structure of key-value-pairs was sufficient for the
needs of the front-end, but unfortunately it was not good enough to
express all the complex data structures required by the formatters. So
we interpreted the meta-data definitions to mean paths in a logical
tree, with a corresponding value which can be a scalar or an array.

So it was that, as part of this formalisation process, we named the
keys as \emph{tags} because they require a well-defined syntax in
order to express a valid tree.

A tag is composed by one or more \emph{node names} and zero or more
\emph{separators}. A separator is the dot character \texttt{.}. Node
names are made of a sequence characters that can be digits
\texttt{0-9}, letters \texttt{a-z A-Z} or the underscore character
\texttt{\_}. The \emph{leaf node name} of a tag is defined to be the
node name after the last separator, if any separators are present; or
the only node name, if the tag has no separators.

Examples of valid tags:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
123_abc.12fc.344
\end{pseudocode}

A non-leaf node name is called a \emph{parent node}. A parent node
\emph{contains} one or more leaf nodes. It may also contain other
non-leaf nodes. A node is called a \emph{child node} if it has a
parent node. When no groups are present, the leaf element is said to
belong to the \emph{global node}. Examples:

\begin{center}
\begin{tabular}{ll}
Tag & Description\\
\hline
parent.child.leaf & parent contains child and leaf.\\
parent.leaf & parent contains leaf.\\
leaf & node \texttt{[global.]} is implied.\\
\end{tabular}
\end{center}

Whilst SML does not enforce how tags are used, it is expected that
backends use the following form:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
BACKEND.FACET.ASPECT.TRAIT
\end{pseudocode}

For details on the meaning of these terms, see the
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#the-backends}{backends}
section. Example tags: \texttt{cpp.types.main.enabled},
\texttt{cpp.serialisation.main.enabled}.

\subsection{Complete List of Tags}

The following table lists all of the available tags, the model that
makes them available and describes what the tag controls. Providers of
third party backends are expected to have a similar section in their
documentation.

\begin{center}
\begin{tabular}{lll}
Model & Tag & Description\\
\hline
Dia & dia.comment & Comment provided by user when dia does not allow for it.\\
Dia & dia.identity\_attribute Attribute that provides this entity its identity.\\
\end{tabular}
\end{center}

\paragraph{Test Models}

Each feature we add to dogen is tested via a test model: larger
features have their own test models, whereas smaller but related
features are grouped together in a single test model. This is done to
avoid the proliferation of such models, since the maintenance cost of
each model is not zero. Also its important to bear in mind that the
test models have been created as dogen has evolved, so some of them
don't make a lot of sense at this late stage of development. A story
exists in the product backlog to collapse a lot of these earlier test
models into a smaller, more cohesive set of models.

The diagrams for the test models are stored under
\texttt{test\_data/dia\_sml/input}. There is a story in the backlog to
refactor these and move them into the \texttt{diagrams}
directory. Test models are then generated by dogen into the
\texttt{test\_models} directory (inside of \texttt{projects} folder).

At present we have the following test models (in loose order of when
they got added to dogen):

\begin{itemize}
\item \texttt{stand\_alone\_class}: most basic test, a single class
  with a single attribute.
\item \texttt{class\_without\_package}: most basic test, a single
  class with a single attribute. Appears to be a duplicate of
  \texttt{stand\_alone\_class}.
\item \texttt{all\_primitives}: tests the C++ language primitives such
  as \texttt{int}, \texttt{bool}, etc.
\item \texttt{trivial\_association}: tests different kinds of
  association relationships.
\item \texttt{trivial\_inheritance}: tests different kinds of
  inheritance relationships.
\item \texttt{classes\_without\_package}: tests generation of several
  classes, with one property each
\item \texttt{class\_without\_attributes}: tests support for
  namespaces; generation of a single empty class in a package.
\item \texttt{class\_in\_a\_package}: tests support for namespaces;
  single class in a package, with attributes.
\item \texttt{classes\_in\_a\_package}: tests support for namespaces
  with multiple classes, each of which with one attribute.
\item \texttt{classes\_inout\_package}: tests support for namespaces,
  ensuring we can cope with classes inside and outside of packages.
\item \texttt{comments}: tests support for different kinds of
  comments, ensuring they get translated correctly by dogen as code
  comments, added to the correct namespaces, and so on.
\item \texttt{stereotypes}: tests most of the supported
  stereotypes. Some, such as \texttt{exception} and
  \texttt{enumeration} are tested on their own models.
\item \texttt{compressed}: tests that we process compressed dia
  diagrams correctly.
\item \texttt{two\_layers\_with\_objects}: tests multiple layers in
  dia.
\item \texttt{disable\_cmakelists}: tests that we can generate a
  project without creating \texttt{CMakeLists} files.
\item \texttt{disable\_facet\_folders}: does not create individual
  folders for each facet.
\item \texttt{disable\_full\_ctor}: does not add a full constructor to
  classes.
\item \texttt{enable\_facet\_domain}: only the domain facet is
  enabled.
\item \texttt{enable\_facet\_hash}: only the domain and hash facets
  are enabled.
\item \texttt{enable\_facet\_io}: only the domain and io facets are
  enabled.
\item \texttt{enable\_facet\_serialization}: only the domain and
  serialisation facets are enabled.
\item \texttt{enumeration}: tests the generation of enumerations.
\item \texttt{exception}: tests the generation of exception classes.
\item \texttt{split\_project}: tests splitting the include and source
  directories.
\item \texttt{boost\_model}: tests all of the supported boost types.
\item \texttt{std\_model}: tests all of the supported standard library
  types.
\item \texttt{database}: tests support for ODB, a object-relational
  mapping tool.
\item \texttt{eos\_serialization}: test support for EOS serialisation,
  a cross-platform serialisation add-in to boost serialisation.
\item \texttt{test\_model\_sanitizer}: external test model. This is
  required because we cannot add specs directly to test models, or
  else the binary diff tests would fail~-- since dogen cannot generate
  these specs.
\end{itemize}

\subsection{Development Matters}

In this section we cover various aspects of Dogen development.

\paragraph{Feedback Loops}

Almost all code in Dogen is implemented as Dogen models; that is, we
use Dogen to generate the vast majority of Dogen itself. We do so for
several reasons:

\begin{itemize}
\item \textbf{dog-fooding}: using your own tool frequently is a great
  way of making sure the tool does what it is meant to do and does so
  in a workable, pragmatic manner. You have at least one user to test
  it.
\item \textbf{keeping our feet on the ground}: if we have some crazy
  ideas and break Dogen, we can no longer develop Dogen. Thus Dogen
  must always be able to code-generate itself at all points in the
  development cycle, which forces one to think \emph{extremely}
  incrementally.
\item \textbf{code faster and test our theoretical underpinnings}: if
  our ideas around code generation are correct, Dogen should
  significantly speed-up development of Dogen.
\end{itemize}

In summary, the Dogen approach is to try to create a positive feedback
loop in Dogen development.

\paragraph{Versions and Build Numbers}

Dogen uses a single version number for all of its components. As with
most version numbers, Dogen's versions are made up of three components
separated by dots (\texttt{.}). For example:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
0.49.2369
\end{pseudocode}

The first component is the major version number, and it is incremented
whenever we deem that there have been enough changes to warrant
it. For now, the major version is zero but once dogen reaches it's
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/definition_of_done.org}{definition
  of done}, it will be incremented to one. The middle component is the
sprint number. It gets updated whenever a new sprint is started. The
last component is the commit ``number''; that is, the number of commits
done in the master branch, as given by:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
git rev-list master | wc -l
\end{pseudocode}

To know how recent this version is, go to the
\href{https://github.com/DomainDrivenConsulting/dogen}{project page}
in GitHub and look up the number of commits there.

In addition to the version, Dogen also makes use of a build
number. Every time a build is done, a new UUID is generated. This
makes it easier to identify a build with defects for example. It is up
to the person performing the build to keep track of the properties of
that build (compiler, operative system, etc). In order to generate
build numbers you must have the tool
\href{http://www.linuxcommand.org/man_pages/uuidgen1.html}{uuidgen} in
the path. If the tool is not available, the default build number is
assigned: \texttt{no\_build\_number\_assigned}.

\part{Use}

In this part we describe how to build and install Dogen, and how to
use it effectively~--- from very simple use cases all the way to the
more complex setups. We also explain how Dogen can be integrated with
a build system, how to manage the multitude of diagrams that soon get
created and many other such practical aspects.

\chapter{Obtaining Dogen}

There are two ways of obtaining Dogen: you can either install one of
the available binary packages or compile it yourself from source.

\section{Installing Dogen Using the Binary Packages}

Dogen uses Continuous Integration (CI) and Trunk Development. We use
CDash for CI. In practice, this means that it should always be safe
(and preferable) to install the most recent packages available.

You can monitor the build status
\href{http://my.cdash.org/index.php?project\%3DDogen}{here}. When the
build is green, latest is always greatest; when the build is not
green, it is our top priority to make it green again.

We have build agents for the following Operative Systems:

\begin{itemize}
\item Linux: 32-bit and 64-bit with Clang and GCC.
\item Mac OS X: 64-bit with GCC.
\item Windows: 32-bit using MinGW (GCC for Windows).
\end{itemize}

The generated packages are named after the build agents, and contain
the Operative System name and bitness (e.g. 64-bit or 32-bit) in their
names.

\begin{quote}
\emph{IMPORTANT}: Installable packages generated off of CI used to be
available at github
\href{https://github.com/DomainDrivenConsulting/dogen/downloads}{here},
but since they decommissioned the downloads section, we found no place
to upload them to. So, at present, there is no way of downloading the
packages generated by the build agents. We are trying to find a new
location to upload the packages to.
\end{quote}

\subsection{Building Dogen from Source}

We officially support Linux, Mac OS X and Win32 since we have build
agents for these platforms. However, any platform that meets the
dependencies below should be able to build Dogen.

\paragraph{Dependencies}

In order to compile Dogen you need:

\begin{itemize}
\item a fairly recent version of \href{http://gcc.gnu.org/}{GCC} (>
  \href{http://gcc.gnu.org/gcc-4.7/}{4.7}) or
  \href{http://clang.llvm.org/index.html}{Clang} (>
  \href{http://llvm.org/releases/3.0/docs/ClangReleaseNotes.html}{3.0})
  or any compiler with good C++-11 support;
\item \href{http://www.cmake.org/}{CMake}
  \href{http://www.kitware.com/news/home/browse/CMake?2013_05_22&CMake\%2B2.8.11\%2BNow\%2BAvailable}{2.8}
  or later;
\item Boost
  \href{http://www.boost.org/users/history/version_1_55_0.html}{1.55};
\item for portable serialisation, you need
  \href{http://epa.codeplex.com/}{EOS} support (optional);
\item for relational database support you need
  \href{http://www.codesynthesis.com/products/odb/}{ODB} support
  (optional);
\end{itemize}

\paragraph{Building Instructions}

Once all dependencies have been installed, follow the following steps:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
git clone git://github.com/kitanda/dogen.git
mkdir output
cd output
cmake ../dogen -G "Unix Makefiles"
make -j5 # number of cores available
\end{pseudocode}

The dogen \texttt{knitter} binary will be in
\texttt{output/stage/bin/dogen\_knitter}. For more complex builds, see
More Complex Build Setups below.

If you are on a non-Unix platform you need to use the appropriate
CMake generator (the \texttt{-G} parameter above). At present the
Ninja generator is known not to work. No other generator has been used
by the Dogen team.

Once the build has completed successfully, you should run the unit
tests to make sure your system is fully supported.

\paragraph{Running Unit Tests}

In order to ensure your platform is properly supported by Dogen, you
should run the test suite and ensure that all tests pass.

\paragraph{Setting up PostgreSQL}

If you have configured ODB support, you need a PostgreSQL database in
order to run the unit tests. To do so:

\begin{itemize}
\item install and configure a version of
  \href{http://www.postgresql.org/}{PostgreSQL} of your choice.
\item
  \href{http://www.cyberciti.biz/tips/postgres-allow-remote-access-tcp-connection.html}{configure}
  access to local and remote users.
\item create a database called \texttt{musseque} and a user called
  \texttt{build} with a password of your choice.
\item create a \texttt{.pgpass} file as described
  \href{http://wiki.postgresql.org/wiki/Pgpass}{here} (more details in
  the Postgres manual, section
  \href{http://www.postgresql.org/docs/current/static/libpq-pgpass.html}{The
    Password File}). Example, replacing \emph{PASSWORD}:
\end{itemize}

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
echo localhost:5342:musseque:build:PASSWORD > ~/.pgpass
chmod 0600 ~/.pgpass
\end{pseudocode}

Test access to the database before proceeding:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
psql -U build -d musseque -w
\end{pseudocode}

It should ask you for no password. Then in psql run:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
create schema kitanda;
\end{pseudocode}

In file \texttt{projects/test\_models/database/spec/main.cpp},
uncomment the line:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
// odb::schema_catalog::create_schema(*db);
\end{pseudocode}

Run the database tests:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
make run_database_spec
\end{pseudocode}

Comment the line again.

\paragraph{Running all tests}

To run all tests you can simply do:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
make run_all_specs
\end{pseudocode}

If there are no failures, you are good to go. If there are failures,
you should report them to help improve Dogen.

\paragraph{More Complex Build Setups}

The
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#building-instructions}{Building
  Instructions} described the most vanilla use case for building
Dogen. However, over time we have spotted a few others. These are
described in this section.

\paragraph{Personal Libraries Directory}

It may be that you have compiled and installed some libraries that
Dogen depends on. Assuming your files have been installed in
\texttt{/usr/local/personal/lib}, To make them visible to Dogen add
the following to the CMake line:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
CMAKE_INCLUDE_PATH=/usr/local/personal/include CMAKE_LIBRARY_PATH=/usr/local/personal/lib cmake ../dogen -G "Unix Makefiles"
\end{pseudocode}

Ensure that the expected libraries are detected. For example, in this
case we were looking for ODB and EOS in the CMake output:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
-- Found ODB Include folder (ODB_INCLUDE_DIR = /usr/local/personal/include)
-- Found ODB Core Libraries (ODB_LIBRARIES = /usr/local/personal/lib/libodb.so;/usr/local/personal/lib/libodb-boost.so)
-- Found ODB PostgreSQL Library (ODB_PGSQL_LIBRARY = /usr/local/personal/lib/libodb-pgsql.so)
-- Found odb...
-- Found EOS Include folder (EOS_INCLUDE_DIR = /usr/local/personal/include)
-- Found eos...
<snip>
\end{pseudocode}

\paragraph{Multi-Compiler Setup}

If you are developing Dogen~--- rather than just trying to use it~---
we recommend the creation of sub-directories in the \texttt{output}
directory, each with their own purpose. A common use case is the
multi-compiler setup, where one builds on both Clang and GCC. This can
be achieved as follows (assuming GCC at the expected version is the
default compiler):

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
cd output
mkdir clang_3.5 gcc_4.9
cd clang_3.5
CC=clang-3.5 CXX=clang++-3.5 cmake ../../dogen/ -G "Unix Makefiles"
make -j5 # number of cores available
<build finishes>
cd ../gcc_4.9
cmake ../../dogen/ -G "Unix Makefiles"
make -j5 # number of cores available
\end{pseudocode}

In this setup, directories \texttt{output/clang\_3.5} and
\texttt{output/gcc\_4.9} have a complete build of Dogen. We tend to
use Clang as our development compiler and GCC as out official platform
compiler; before doing a push one should always check that GCC builds.

\paragraph{Debugging}

It is also useful to have debug builds side-by-side release builds. To
do so we follow a similar approach to the multi-compiler setup:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
cd output
mkdir clang_3.5 clang_3.5_debug
cd clang_3.5
CC=clang-3.5 CXX=clang++-3.5 cmake ../../dogen/ -G "Unix Makefiles"
make -j5 # number of cores available
<build finishes>
cd ../clang_3.5_debug
CC=clang-3.5 CXX=clang++-3.5 cmake ../../dogen/ -DWITH_DEBUG=on -G "Unix Makefiles": cmake ../../dogen/ -G "Unix Makefiles"
make -j5 # number of cores available
\end{pseudocode}

Note the \texttt{-DWITH\_DEBUG=on} on the second invocation of
CMake. Also, make sure CMake did recognise the option. When debug is
off, you should see:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
-- Building WITHOUT DEBUG symbols...
\end{pseudocode}

When debug is on, you should see:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
-- Building WITH DEBUG symbols...
\end{pseudocode}

Debugging with GDB is pretty straightforward. Assuming you are
currently in \texttt{clang\_3.5\_debug} and you want to debug the
\texttt{knit} unit tests, do:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
cd stage/bin
gdb dogen_knit_spec
\end{pseudocode}

\paragraph{Submitting Bug Reports}

If you have a failure building Dogen or running its unit tests, please
submit a bug report that includes:

\begin{itemize}
\item the error messages;
\item the compiler version;
\item the Operative System.
\end{itemize}

If you find a bug whilst using Dogen, please send the log file as
well; it is located under the directory where you executed Dogen and
named \texttt{dogen.log}.

Bugs can be submitted using
\href{https://github.com/kitanda/dogen/issues}{github Issues}.

\paragraph{Submitting Patches}

Dogen is an open source project with very few entry barriers. All we
ask is for you to use your real name when submitting a patch and to
use the pull request functionality in GitHub to do so.

\chapter{Running Dogen}

Once you got access to Dogen, either by installing it or building it,
the next logical step is to try to use it. This section provides an
overview of common use cases. Note that Dogen is a command line tool,
and as such there is a presumption that the user has at least a
rudimentary knowledge of the shell of his or her operative system.

This section is dedicated to understanding the command line tool,
rather than the code it generates or the diagrams it receives as an
input; latter sections will deal with these topics exclusively.

\subsection{Validating the Setup}

The first thing one should do is to make sure Dogen is operational. To
do so, run:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --version
\end{pseudocode}

If you are running it from the build directory \texttt{stage/bin} and
on UNIX, you may need to refer to the current directory:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ ./dogen_knitter --version
\end{pseudocode}

Alternatively, you may find yourself in a sub-directory of the build
directory; in that case you should use a relative path to the binary:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ ../dogen_knitter --version
\end{pseudocode}

If you are in any of these cases, from now on you will have to add the
required relative path to all of the following examples~---
e.g. \texttt{./}, \texttt{../}, etc. Note that you \textbf{should not}
try to copy the binary around, as it must be setup properly in order
to work; this is done by the build system for both binary packages and
builds. You should always use relative paths if the binary is not on
the path.

If all is well, you should see something along the lines of:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
Dogen Knitter v0.49.2370
Build: 794bc01d-7fac-41dc-91eb-11a9f8be70a8
Copyright (C) 2012-2014 Marco Craveiro.
License: GPLv3 - GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.
\end{pseudocode}

Ideally you want the most recent dogen version. See the
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#versions-and-build-numbers}{section}
on versions for details on how to interpret the version number. Now
that we have confirmed Dogen is operational, lets have a look at all
the available options. Run:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --help
\end{pseudocode}

A text similar to the below will come up:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
Dogen - the domain generator.
Generates domain objects from a Dia diagram.


General options:
 -h [ --help ]         Display this help and exit.
 --version             Output version information and exit.
...
\end{pseudocode}

We will cover all of these options in more detail later, but for now
it suffices to say that command line options belong to option groups,
which attempt to aggregate related functionality. For instance,
\emph{General options} are those that are not directly related to
operational aspects, but provide information about the application. We
have already seen both \texttt{help} and \texttt{version}, the most
important of this group.

At this point we now know our Dogen setup is operational so lets make
use of it.

\subsection{Generating Hello World}

Before we can generate any code, we need a model. It also helps if we
keep all files isolated so we know what Dogen has been up to. We will
meet both of these conditions by placing ourselves in a new directory
and copying across the ``Hello World'' model from the Dogen git
repository:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
mkdir dogen_examples
cd dogen_examples
mkdir source
cd source
wget https://raw.githubusercontent.com/DomainDrivenConsulting/dogen/master/diagrams/hello_world.dia
\end{pseudocode}

The source directory is created so we can separate our source code
from the build files, as you'll see in a moment. All that is left is
to code generate:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --target hello_world.dia --cpp-enable-facet domain
\end{pseudocode}

We use the \texttt{-{}-target} command line option to tell Dogen about
the ``Hello World'' model. The target file must be a UML Dia diagram,
crafted according to the rules stipulated by Dogen. Notice that the
diagram's file name does not contain any spaces, camel case, and so
on. This is important because it will be used as the name of the
project and as the top-level namespace, so it must be valid as a C++
identifier and it should follow the conventions of the other C++
identifiers.

The second point of note is the \texttt{-{}-cpp-enable-facet} command
line option. It ensures that only the select \emph{facets} are on. A
facet is just Dogen-speak for a logically distinct portion of code
generation, which can be switched on or off independently of other
such portions. In this particular case we asked for the core facet
\texttt{domain} to be on~--- it makes no sense to code generate
otherwise, really. All other facets are thus switched off, so there
are no requirements for third-party libraries. This is done because it
is possible you do not have a development environment set up with all
of the third-party libraries and tools that are supported by Dogen by
default, such as EOS, ODB and so on.

After generation, your directory should look like so:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
ls -l
total 24
drwxr-xr-x 4 marco marco 4096 Mar 13 07:56 hello_world
-rw-r--r-- 1 marco marco 7315 Mar 12 18:51 hello_world.dia
drwxr-xr-x 2 marco marco 4096 Mar 12 18:51 log
\end{pseudocode}

The \texttt{log} directory is where the log file is stored; it is
named \texttt{dogen.log}. By default Dogen is not particularly
expressive, so there won't be much in the log file to look at. If you
wish to increase the verbosity of the logging, you can do so using
\texttt{-{}-verbose}:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --target hello_world.dia --cpp-enable-facet domain --verbose
\end{pseudocode}

See the Advanced Command Line Options section for more details on
\texttt{-{}-verbose}.

The other directory of interest is \texttt{hello\_world}. This is
where the generated C++ code is stored. To understand the meaning and
the rationale of the directory structure you should read sections
\texttt{File and Directory Standards} and also \texttt{Physical
  Layout}.

For now we'll just have a quick peek at one of the generated files,
the class \texttt{hello\_world} itself:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ grep -e class -B5 -A5  hello_world/include/hello_world/types/hello_world.hpp
namespace hello_world {
/**
 * @brief Welcome to Dogen!
 *
 * This is one of the simplest models you can generate, a single class with one
 * property. You can see the use of comments at the class level and property
 * level.
 */
class hello_world final {
public:
   hello_world() = default;
   hello_world(const hello_world&) = default;
   hello_world(hello_world&&) = default;
   ~hello_world() = default;
\end{pseudocode}

As you can see, a C++ 11 class was generated. At this point it is
recommended you look at the \texttt{hello\_world.dia} using Dia, and
the generated sources using your preferred text editor.

\subsection{Supporting Infrastructure}

In order to compile the generated code, we need two additional bits of
infrastructure: a CMake file and a main.

Dogen models are designed to be integrated with an existing CMake
build, so we have to generate a minimal
\texttt{CMakeLists.txt}. Something as simple as this would do:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
cmake_minimum_required(VERSION 2.8 FATAL_ERROR) # 2.6 should work too
project(hello_world)
set(CMAKE_CXX_FLAGS "-std=c++11") # Dogen requires C++ 11 or greater
include_directories(${CMAKE_SOURCE_DIR}/hello_world/include)
add_subdirectory(${CMAKE_SOURCE_DIR}/hello_world)
add_executable(main main.cpp)
target_link_libraries(main hello_world)
\end{pseudocode}

Take the above code and slap it on a \texttt{CMakeLists.txt} in your
\texttt{source} directory; granted, you could get much fancier, but
this suffices for the purposes of our minimalist example. The contents
of the file shouldn't be that surprising, unless you are unfamiliar
with CMake. If that is the case, I'm rather afraid that an
introduction to CMake is outside of the scope of this manual. On the
plus side, there are plenty of good articles on the subject.

We also need to create a basic \texttt{main.cpp} to make use of the
genrate code. It is equally straightforward~--- a few lines over the
traditional C++ ``Hello World'':

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
#include <iostream>
#include "hello_world/types/one_property.hpp"

int main() {
    hello_world::one_property op("hello world!");
    std::cout << op.property() << std::endl;
    return 0;
}
\end{pseudocode}

We are making use of the full constructor that the \texttt{domain}
makes available; because the property is of type \texttt{std::string}
we can stream it directly into the console.

At this point in time, your directory should look roughly like this:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ ls -l
total 24
-rw-r--r-- 1 marco marco  284 Mar 13 18:14 CMakeLists.txt
drwxr-xr-x 4 marco marco 4096 Mar 13 18:34 hello_world
-rw-r--r-- 1 marco marco 7316 Mar 13 18:19 hello_world.dia
drwxr-xr-x 2 marco marco 4096 Mar 13 18:20 log
-rw-r--r-- 1 marco marco  230 Mar 13 18:55 main.cpp
\end{pseudocode}

It is time to compile.

\subsection{Compiling and Running}

The compilation steps are fairly simple. We need to create a folder to
house the build paraphernalia, to avoid getting it all mixed with the
source code. There we shall build and run our main. The following
achieves that (assuming you are currently in \texttt{source}):

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ cd ..
$ mkdir output
$ cd output/
$ cmake ../source
<lots of cmake output>
$ make
<lots of make output>
$ ./main
hello world!
\end{pseudocode}

And with that, we have built and instantiated our simple Dogen model.

\subsection{Version Controlling the Models}

We strongly recommend you store all of the code generated by Dogen in
the version control system (VCS) of your choice. This may sound
counter-intuitive at first. After all, you wouldn't want to store
Protocol Buffers code in version control, or the output of an IDL
compiler. However, the same logic doesn't \emph{quite} apply to
Dogen. As you will see later, we strive to allow intermixing of
manually crafted code with generated code, and we also want the
generated code to look as if it was generated by humans; granted, some
rather boring, robot-like humans, but still. Finally, we want you to
actively distrust Dogen~--- every time you code generate, you should
inspect the output and make sure it looks exactly the way you want it
to look. The best way to do that is to validate diffs. At any rate, if
none of these arguments convince you, please suspend disbelief for a
second and humour us in thinking that the rightful place of the code
generated by Dogen is in version control.

Git is our preferred VCS~--- it is, in fact, a distributed VCS, so
DVCS would be the right term, but it's distributed nature is not
relevant for the current argument. Anyway, we shall use git to
demonstrate how VCS in general can be used to \emph{see} what Dogen is
up to. If \texttt{\$\{VCS\_OF\_CHOICE\}} is not git, feel free to do
the equivalent commands in \texttt{\$\{VCS\_OF\_CHOICE\}} instead.

To start off with, we need to initialise a repository in our source
folder:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git init .
<git output>
$ echo log > .gitignore
$ git add -A
$ git commit -m "initial import"
[master (root-commit) a6b706a] initial import
 13 files changed, 581 insertions(+)
 create mode 100644 .gitignore
 create mode 100644 CMakeLists.txt
 create mode 100644 hello_world.dia
 create mode 100644 hello_world/CMakeLists.txt
 create mode 100644 hello_world/include/hello_world/types/all.hpp
 create mode 100644 hello_world/include/hello_world/types/one_property.hpp
 create mode 100644 hello_world/include/hello_world/types/one_property_fwd.hpp
 create mode 100644 hello_world/src/CMakeLists.txt
 create mode 100644 hello_world/src/types/one_property.cpp
 create mode 100644 main.cpp
\end{pseudocode}

Now that we have committed our changes, we can use \texttt{git diff}
and \texttt{git status} to see the results of all Dogen commands. For
example, lets say we decide to add more comments to the class using
Dia. After saving, git tells us the following:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git diff
diff --git a/hello_world.dia b/hello_world.dia
index dfbb93b..46074d7 100644
--- a/hello_world.dia
+++ b/hello_world.dia
@@ -182,7 +182,9 @@ level.#</dia:string>
             <dia:string>##</dia:string>
           </dia:attribute>
           <dia:attribute name="comment">
-            <dia:string>#This is a sample property.#</dia:string>
+            <dia:string>#This is a sample property.
+
+This is an additional comment.#</dia:string>
           </dia:attribute>
           <dia:attribute name="visibility">
             <dia:enum val="0"/>
\end{pseudocode}

Because we chose to save the diagram in text format, its very easy to
see what the changes are. We can now code generate, very much the same
way as we did before:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --target hello_world.dia --cpp-enable-facet domain
\end{pseudocode}

Other than the diagram file itself, one would expect to see exactly
one modified file; and for that file to be
\texttt{one\_property.hpp}. And this is what \texttt{git status} tells
us:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git status
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

 modified:   hello_world.dia
 modified:   hello_world/include/hello_world/types/one_property.hpp

no changes added to commit (use "git add" and/or "git commit -a")
\end{pseudocode}

But are these the expected changes? Again, \texttt{git diff} comes to
the rescue:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git diff hello_world/include/hello_world/types/one_property.hpp
diff --git a/hello_world/include/hello_world/types/one_property.hpp b/hello_world/include/hello_world/types/one_property.hpp
index 1759275..b0759ec 100644
--- a/hello_world/include/hello_world/types/one_property.hpp
+++ b/hello_world/include/hello_world/types/one_property.hpp
@@ -50,6 +50,8 @@ public:
 public:
     /**
      * @brief This is a sample property.
+     *
+     * This is an additional comment.
      */
     /**@{*/
     const std::string& property() const;
\end{pseudocode}

As you can see, Dogen did exactly the modifications we expected it to
do and no more than those, and git provided us with a quick and
deterministic way of validating that.

Just for good measure, we'll commit these changes:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git add -A
$ git commit -m "add comment to property"
\end{pseudocode}

Now we're ready to start working on the next set of changes. Two key
points emerge from here:

\begin{itemize}
\item VCS are really useful to keep up with what Dogen is doing. But
  in order for it to work, you should save your diagrams in Dia as
  plain text rather than in compressed form.
\item you should commit early and commit often, probably even more so
  than what you are used to. A very large diff is hard to parse,
  particularly when we start mixing generated code with non-generated
  code. We tend to do a large number of local commits and then do a
  single large push to \texttt{origin} to trigger builds in the
  Continuous Integration.
\end{itemize}

\subsection{Integrating Dogen with the Build}

You will soon tire of running the same Dogen commands every time you
want to change your model. The easiest thing is to integrate it with
the build system, so that you have a target for code generation. This
can easily be accomplished with CMake. In your top-level
\texttt{CMakeLists.txt}, add the following at the end:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
add_custom_target(codegen_hello_world
    COMMENT "Generating Hello World model" VERBATIM
    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}
    COMMAND ../../dogen_knitter
    --target ${CMAKE_SOURCE_DIR}/hello_world.dia
    --cpp-enable-facet domain)
\end{pseudocode}

We called the target \texttt{codegen\_hello\_world} but it can be
named whatever you choose. To avoid any confusion, we should check
these changes in:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
git add -A
git commit -m "add target for code generation"
\end{pseudocode}

Now, in your output directory you can simply do:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ cmake ../source # just in case, shouldn't be necessary
<cmake output>
$ make codegen_hello_world
Scanning dependencies of target codegen_hello_world
[100%] Generating Hello World model
[100%] Built target codegen_hello_world
\end{pseudocode}

When you go back to your source directory, git status should show you
the following:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
On branch master
nothing to commit, working directory clean
\end{pseudocode}

As expected, no changes were done. But how do we know the code
generator actually executed at all? This is where the log file comes
in handy:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ date
Fri 14 Mar 08:32:33 GMT 2014
$ tail -n 1 log/dogen.log
2014-03-14 08:32:29.763230 [INFO] [knit.workflow] Workflow finished.
\end{pseudocode}

As you can see, the timestamp of the last thing Dogen wrote to the log
is very close to now, so we know it executed the code generation.  As
there was nothing to change, nothing was changed.

The more advanced CMake~--- and make users in general~--- may, at this
juncture, be tempted to add a dependency between the diagram and code
generation. In such a setup, if the Dia diagram has been modified, a
code generation would take place when you build. From experience, we
do not recommend this approach. This sounds like a great idea in
theory, but in practice it actually doesn't work that well. When you
start using Dogen in anger, you will find yourself many a time with
``work-in-progress'' changes; you will be speculating with the design
for quite a bit until it makes sense. At the same time, you or other
team members may also be doing unrelated code changes. This will put
in a bind: either you don't check-in the diagram changes, or your
create a branch for them (which is not always a bad idea, to be fair)
or you check them in and break everyone else's code.

The other reason why this is a bad idea is that if someone checked in
a diagram but forgot to run the code generator, the build machine
could break in mysterious ways. The code that is building is not the
code that was checked in, and this can result in a lot of wasted time
investigating strange issues.

In conclusion, its better to code generate and check in manually, as
and when you are ready to do so, and to make sure the build machine is
as dumb as possible.

\subsection{Deleting Extra Files}

As you start adding and removing classes from your diagram, you may
find that Dogen starts leaving a lot of artefacts behind. You may even
conclude that the best way is to manually delete the code generation
directory before code generation to ensure you're in a good state. In
fact, there is a better way of handling this situation.

Let's imagine a fairly simple but common use case: you just added a
brand new class to your model~--- \texttt{two\_properties} say~--- and
you code generated it. It all looks fine from git:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git status
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

  modified:   hello_world.dia
  modified:   hello_world/include/hello_world/types/all.hpp

Untracked files:
  (use "git add <file>..." to include in what will be committed)

  hello_world/include/hello_world/types/two_properties.hpp
  hello_world/include/hello_world/types/two_properties_fwd.hpp
  hello_world/src/types/two_properties.cpp
no changes added to commit (use "git add" and/or "git commit -a")
\end{pseudocode}

Alas, after much soul searching you decide that
\texttt{two\_properties} was a mistake: it doesn't reflect the domain
you intend to model at all. So you remove it from the diagram. What is
Dogen to do? Well, lets look at the git output after we removed the
new class:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git status
On branch master
Untracked files:
  (use "git add <file>..." to include in what will be committed)

  hello_world/include/hello_world/types/two_properties.hpp
  hello_world/include/hello_world/types/two_properties_fwd.hpp
  hello_world/src/types/two_properties.cpp

nothing added to commit but untracked files present (use "git add" to track)
\end{pseudocode}

Dogen got rid of all the changes to the \emph{existing} files, but
left the new files lying around! This is because Dogen does not
consider these files to be its responsibility any longer; after all,
there is no matching class that ``owns'' them in the diagram, so they
are totally ignored. This may not be the ideal behaviour~--- after all
you wanted to get rid of the class altogether. To do so you need to
instruct Dogen to delete all files that it thinks are
``unnecessary''. This can be done via the
\texttt{-{}-delete-extra-files} option. We can add it to the top-level
\texttt{CMakeLists.txt} like so:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git diff CMakeLists.txt
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 0f6e9c3..2db53b8 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -12,4 +12,5 @@ add_custom_target(codegen_hello_world
     COMMAND ../../dogen_knitter
     --target ${CMAKE_SOURCE_DIR}/hello_world.dia
-    --cpp-enable-facet domain)
+    --cpp-enable-facet domain
+    --delete-extra-files)
\end{pseudocode}

As usual we'll commit this change:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git add -A
$ git commit -m "add delete extra files"
<git output>
\end{pseudocode}

When we code generate again, the result is quite different:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ cd ../output
$ make codegen_hello_world
-- Configuring done
-- Generating done
-- Build files have been written to: YOUR_PATH/dogen_examples/output
[100%] Generating Hello World model
[100%] Built target codegen_hello_world
$ cd ../source/
$ git status
On branch master
nothing to commit, working directory clean
\end{pseudocode}

Dogen has now deleted all the files we're no longer interested in.

\subsection{Ignoring Extra Files}

It is not always appropriate to delete \emph{all} files that Dogen
knows nothing of. Imagine a second use case: you decide to manually
create a file with a stand alone function
\texttt{my\_function.cpp}. This file needs to be part of the model,
but it cannot be code generated by Dogen. If you attempt to use
\texttt{delete-extra-files}, this file would be removed by Dogen as
the following example shows. First we'll create the file and commit
it:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
echo "void my_function() { }" > hello_world/my_function.cpp
$ git add -A
$ git commit -m "add my function"
[master 9fbc00a] add my function
1 file changed, 1 insertion(+)
create mode 100644 hello_world/my_function.cpp
\end{pseudocode}

Then we'll code generate and check git:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ make codegen_hello_world
[100%] Generating Hello World model
[100%] Built target codegen_hello_world
$ cd ../source/
$ git status
On branch master
Changes not staged for commit:
 (use "git add/rm <file>..." to update what will be committed)
 (use "git checkout -- <file>..." to discard changes in working directory)

deleted:    hello_world/my_function.cpp

no changes added to commit (use "git add" and/or "git commit -a")
\end{pseudocode}

Again you can see the usefulness of committing early and often:
instead of losing all our work, all we need to do is to checkout the
file to restore it:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
git checkout hello_world/my_function.cpp
\end{pseudocode}

Our file got deleted as it is an ``extra'' file as far as Dogen is
concerned. The simplest way to avoid this is to use the command
\texttt{-{}-ignore-files-matching-regex}. We can add it to the CMake
file like so:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ git diff CMakeLists.txt
diff --git a/CMakeLists.txt b/CMakeLists.txt
index ae3c12c..518f99c 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -12,4 +12,5 @@ add_custom_target(codegen_hello_world
     COMMAND ../../dogen_knitter
     --target ${CMAKE_SOURCE_DIR}/hello_world.dia
     --cpp-enable-facet domain
-    --delete-extra-files)
+    --delete-extra-files
+    --ignore-files-matching-regex .*/my_function.*)
\end{pseudocode}

If we repeat the code generation steps again, the result is a bit more
sensible:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ cmake ../source/ # should't really be necessary
-- Configuring done
-- Generating done
-- Build files have been written to: YOUR_PATH/dogen_examples/output
$ make codegen_hello_world
[100%] Generating Hello World model
[100%] Built target codegen_hello_world
$ cd ../source/
$ git status
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

modified:   CMakeLists.txt

no changes added to commit (use "git add" and/or "git commit -a")
\end{pseudocode}

The file was not deleted this time round.

This is not the only way to ignore files as we shall see, but its a
quick way of doing so, and is particularly suitable for files which do
not have a clear representation in the model. For example, this is a
good solution for adding unit tests to a model:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
--ignore-files-matching-regex .*/test/.*
\end{pseudocode}

This would ignore all files in a directory called test. The regular
expressions can be as complex as desired, as they internally use C++
11's regular expression library.

\paragraph{Referring to Other Models}
\label{sec-3-2-9}

Soon in your modeling life you will outgrow a single model~--- e.g. a
single Dia diagram. This could happen for many reasons: perhaps a
model is becoming too crowded and there are so many classes it has
lost its cohesiveness; or there is an obvious logical split between
two sets of classes, and just does not make sense to keep them in the
same model.

As soon as there several models, its highly likely that relationships
between models will emerge: model A will make use of model B and C,
and so on. Dogen supports this use case via the command line option
\texttt{-{}-reference}. You can have as many instances of this option
as there are dependencies for the target model you are building. For
example, lets say create a second model which uses the class we
defined in the ``Hello World'' model; to generate this model one would
invoke Dogen as follows:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --target hello_references.dia --reference hello_world.dia \
  --cpp-enable-facet domain
\end{pseudocode}

``Hello References'' can now make use of all the types available in
``Hello World'', provided they are qualified with the source model:
\texttt{hello\_world::one\_property} in this particular case.

\paragraph{The External Module Path}

It is possaible to define modules inside a models to aggregate related
sub-functionality. As we shall see later on, these are defined as UML
packages in Dia and get translated into namespaces at the C++ code
generation level. However, sometimes there are top-level modules for
which a UML representation would be counterproductive. A common use
case is when the models all belong to some umbrella project, which may
have one or more top-level namespaces common to all models. For
instance, in Dogen, all models are inside the \texttt{dogen}
namespace; it really adds no value to create a UML package in every
model under these circumstances.

This is where the \texttt{-{}-external-module-path} command line
option comes in handy. This is a way to inject information directly
into SML which is not obtained via the UML diagram. You can provide as
many modules as required, separated by \texttt{::}. For example:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --target hello_references.dia --external-module-path a::b::c \
  --cpp-enable-facet domain
\end{pseudocode}

All types in ``Hello World'' would now be encased inside of namespaces
\texttt{a}, \texttt{b} and \texttt{c}. Note that the external module
path does not affect references in diagrams: we should still refer to
the types \emph{without} it. However, the \texttt{-{}-reference}
parameter must then be augmented with it so that Dogen places the
types in the correct modules:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
$ dogen_knitter --target hello_references.dia \
--reference hello_world.dia,a::b::c \
  --cpp-enable-facet domain
\end{pseudocode}

Notice the comma followed by the external package path in the ``Hello
World'' reference. It ensures that the code generated for ``Hello
References'' makes use of the fully-qualified names when referring to
``Hello World'' types, even though in the diagram they are partially
qualified~--- e.g. \texttt{hello\_world::one\_property}. Without this
the generated code would not compile.

\paragraph{Disabling the Model Module}

On very rare cases, it may be required that the types of the model are
not placed inside of a namespace with the model name. We do not
particularly like this use case, and are likely to make it obsolete
unless we find good reasons not to do so, but it is available at
present.

The \texttt{-{}-disable-model-module} command line option is used to
trigger this functionality.

\subsection{Intelligent Rebuilds}

In languages such as C++, rebuilds are expensive: they involve a lot
of file system activity and thus should be avoided as much as
possible. One way in which Dogen tries to avoid them is by touching
only those files which have actually changed. That is, by default
Dogen only performs a write if it finds that there are differences
between the file in the file system and the one in memory that it is
about to write. Otherwise, the build system would detect a timestamp
changes on \emph{every} file for \emph{every} code generation, making
it a very expensive process.

This has proved to be such a successful feature that we have it turned
on by default. However, on occasions it may make sense to switch it
off~--- perhaps whilst debugging some tricky code generation
issue. This can be achieved by adding \texttt{-{}-force-write} to your
Dogen invocation. With this command line option, Dogen will skip the
binary diff check and always write the files to the file system.

\subsection{Outputting to the Console}

By default, Dogen outputs the results of the code generation to file;
that is, option \texttt{-{}-output-to-file} is on. However, it is also
possible to send the output to the console. This could be useful for
debugging purposes or to preview what the code generator would do
given a model. This can be achieved by adding
\texttt{-{}-output-to-stdout} to your Dogen invocation.

\paragraph{Troubleshooting}

Dogen provides you with a number of command line options to
troubleshoot it when things go wrong. These options are really meant
to be used by advanced Dogen users, but its good to know they exist
because you may need to provide information generated by them in order
to help troubleshoot problems with your model.

As we mentioned previously, Dogen generates a log file called
\texttt{dogen\_TARGET\_MODEL.log} under the \texttt{log} directory,
where \texttt{TARGET\_MODEL} is the file name of the target without
extension. This folder is always generated in the current working
directory. You can control the verbosity of the log file with the
\texttt{-{}-verbose} option, but be warned that, in verbose mode, log
files grow dramatically in size. This mode is meant mainly for Dogen
developers~--- you are required send verbose logs when you attach the
log file to any bug report you submit~--- but it is also an
instructive way to learn about the application.

Other useful options that can be used in conjunction with
\texttt{-{}-verbose} are \texttt{-{}-stop-after-merging} and
\texttt{-{}-stop-after-formatting}. If there is a problem with a
particular part of the Dogen pipeline, it may not make sense to run a
complete code generation whilst investigating the issue. Doing so
would increase investigation time with no tangible benefits. This is
where the ``early-stop'' options come in: they force Dogen to halt at
different stages of the processing pipeline. In the first case, Dogen
combines all input models into what is called the \emph{merged} model,
validates the merged model and then stops. On the second case, Dogen
does everything except actually outputting files.

Finally, it is also possible to dump some of the intermediate state
into file: this can be achieved using \texttt{-{}-save-dia-model} and
\texttt{-{}-save-sml-model}. The first saves a processed
representation of the dia models loaded, and the second saves their
equivalent in SML. These files can be dumped into a directory of
choice via \texttt{-{}-debug-dir}. By default they come out in the
current directory.

\subsection{The C++ Backend Options}

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
--cpp-disable-backend                 Do not generate C++ code.
--cpp-disable-complete-constructor    Do not generate a constructor taking as
                                      arguments all member variables
--cpp-disable-cmakelists              Do not generate 'CMakeLists.txt' for
                                      C++.
-y [ --cpp-split-project ]            Split the model project into a source
                                      and include directory, with
                                      individually configurable locations.
-x [ --cpp-project-dir ] arg          Output directory for all project files.
                                      Defaults to '.'Cannot be used with
                                      --cpp-split-project
-s [ --cpp-source-dir ] arg           Output directory for C++ source files.
                                      Defaults to '.'Can only be used with
                                      --cpp-split-project.If supplied,
                                      include directory must be supplied too.
-i [ --cpp-include-dir ] arg          Output directory for C++ include files.
                                      Defaults to '.'Can only be used with
                                      --cpp-split-project.If supplied, source
                                      directory must be supplied too.
--cpp-enable-facet arg                If set, only domain and enabled facets
                                      are generated. By default all facets
                                      are generated. Valid values: [io | hash

--cpp-header-extension arg (=.hpp)    Extension for C++ header files,
                                      including leading '.'.
--cpp-source-extension arg (=.cpp)    Extension for C++ source files,
                                      including leading '.'.
--cpp-disable-facet-includers         Do not create a global header file that
                                      includes all header files in that
                                      facet.
--cpp-disable-facet-folders           Do not create sub-folders for facets.
--cpp-disable-unique-file-names       Do not make file names unique. Defaults
                                      to true. Must be true if not generating
                                      facet folders.
--cpp-domain-facet-folder arg (=types)
                                      Name for the domain facet folder.
--cpp-hash-facet-folder arg (=hash)   Name for the hash facet folder.
--cpp-io-facet-folder arg (=io)       Name for the io facet folder.
--cpp-serialization-facet-folder arg (=serialization)
                                      Name for the serialization facet
                                      folder.
--cpp-test-data-facet-folder arg (=test\data)
                                      Name for the test data facet folder.
--cpp-odb-facet-folder arg (=odb)     Name for the ODB facet folder.
--cpp-disable-xml-serialization       Do not add NVP macros to boost
                                      serialization code. This is used to
                                      support boost XML archives.
--cpp-disable-eos-serialization       Do not add EOS serialisation support to
                                      boost serialization code.
--cpp-use-integrated-io               Add inserters directly to domain facet
                                      rather than using IO facet.
--cpp-disable-versioning              Do not generate entity versioning code
                                      for domain types.

[marco@erdos bin]\$
\end{pseudocode}

\chapter{Authoring Diagrams in Dia}

As described previously, Dia is a an open source tool for creating
diagrams. Unlike other diagramming tools, Dia does not enforce too
much domain-specific logic; this turns out to be one of its major
strengths. In addition, its file format has been quite stable over
time, making it a perfect candidate for the kind of ``domain
overloading'' required by Dogen. Due to this~--- and its simple user
interface~--- we decided to make it Dogen's first front end tool.

This section describes the practical aspects of creating diagrams in
Dia for code generation in Dogen. It is not, however, an introduction
to Dia. If you are not familiar with the tool, we recommend reading
the \href{http://dia-installer.de/doc/en/}{Dia User Manual}.

\section{Hello World}

We shall start by creating a very simple diagram, identical to the one
available in the dia sources
\href{https://raw.githubusercontent.com/DomainDrivenConsulting/dogen/master/diagrams/hello_world.dia}{here}. If
you'd like to use the one in the sources, just save the file locally
(as described
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#generating-hello-world}{previously}). If
you'd like to create it from scratch, follow these steps.

In the main Dia window, using the UML shapes, create a class called
\texttt{one\_property}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{images/dia_hello_world_diagram.png}
    \caption{}
\end{figure}

Feel free to add any comment you'd like to the class; for instance, we
added ``Welcome to Dogen!'', followed by a bit of a blurb about Hello
World.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/dia_hello_world_class.png}
    \caption{}
\end{figure}

Also, make sure you add a property called \texttt{property} to the
class, and give it the type \texttt{std::string}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/dia_hello_world_attributes.png}
    \caption{}
\end{figure}

This diagram is now ready for generation, as described in section
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#generating-hello-world}{Generating
  Hello World}.

\section{UML Types and Their Roles}

Dogen does not support every single UML construct; in fact, it only
supports a surprisingly small subset of the language, but this subset
is rich enough to express all of Dogen's internal complexity. The
supported UML types, according to their Dia names, are described in
the following sections.

\paragraph{UML Class}

The UML Class defines a class, its properties and methods, its
documentation, and all of the associated meta-data that allows Dogen
to treat it specially. For example, depending on the stereotype you
set, a UML Class can be code-generated as a simple value object, an
enumeration or an exception. The following table lists all of the
available stereotypes:

\begin{center}
\begin{tabular}{ll}
Stereotype & Description\\
\hline
service & No code is generated for the class.\\
enumeration & The class is generated as an enumeration.\\
exception & The class is generated as an exception class\\
concept & The class is a ``meta-class''\\
visitable & An associated visitor is created for the class.\\
entity & The class is expected to have an identity.\\
aggregate root & The class is the root of an aggregate.\\
 & \\
\end{tabular}
\end{center}

Some of the above meanings are taken from Domain Driven Design:
service, entity, aggregate root. The remaining ones are convenience
facilities that were added to Dogen. Note that \texttt{visitable} only
makes sense in the presence of inheritance, as explained
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#uml-generalization}{below},
and must be applied to the base class of an inheritance tree.

\paragraph{UML Large Package}

A large package is a container of packages and classes. Dogen uses the
package to define the appropriate scoping in the backend
languages. For example, in C++ it results in placing the contained
types in namespaces.

Note that in Dia it is not possible to add documentation to a
package. This is solved by extending the UML Note, as explained
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#uml-note}{below}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/class_in_package.png}
    \caption{}
\end{figure}

\paragraph{UML Generalization}

The UML Generalization is used to define inheritance relationships. A
class can have many descendents; however, we do not support multiple
inheritance at present.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/generalization.png}
    \caption{}
\end{figure}

\paragraph{UML Association}

The UML Association is used for informational purposes only; Dogen
ignores all UML Associations (either aggregation or composition)
because it does not have enough information to model them. Instead, it
relies on the properties defined in the class to determine
relationships between classes. However, we still find it very useful
to convey graphical meaning to other users of the diagram.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/association_aggregation.png}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/association_composition.png}
    \caption{}
\end{figure}

\paragraph{UML Note}

As with standard UML diagrams, the UML Note is used to provide user
comments that clarify intent, add deeper explanations to design
choices and so on. In Dogen, we have overloaded it to supplement
Dia. The main use case is a lack of comments at the model and package
level, which means one could not produce documentation for these
types. We make use of the UML Note to provide this information, by
marking it with meta-data.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/model_level_comment.png}
    \caption{}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{images/package_level_comment.png}
    \caption{}
\end{figure}

\paragraph{UML Message}

The UML Message has, somewhat incorrectly, been overloaded by Dogen as
a way to link comments with classes and other UML types. For instance,
it is useful when you have many classes but you need to provide a UML
Note specific to a class; in that case you can define a UML Message
between the UML Note and the UML Class.

\paragraph{UML realization}

The UML realization is ignored by Dogen. It is useful when you want to
implement interfaces but you do not want Dogen to generate the usual
inheritance infrastructure~--- as it would do if you use a UML
Generalization.

\paragraph{Other Dia object types}

Note that if you attempt to use any other Dia type~--- UML or
otherwise~--- Dogen will fail to process the document and throw an
error. For example, if you try to use the Standard Line you will see
the following in the log file:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
2014-09-10 08:09:43.480906 [ERROR] [dia_to_sml.processor] Invalid value for object type: Standard - Line
\end{pseudocode}

This strict behaviour was done to avoid subtle bugs. It may be relaxed
in the future as use cases demand it.

\section{Supported Types}

You couldn't fail to notice that we made use of the type
\texttt{std::string}, provided by the C++ Standard Library. This is a
type that is not

\section{Todo}

FIXME: we need to add a section on the supported object types and the
handling of unsupported object types.

\begin{itemize}
\item Properties
\item Comments
\item Packages
\item Association
\item Inheritance: same model, across models.
\item Enumerations
\item System Models: JSON format, Standard C++ System Model, Boost System Model
\end{itemize}

\chapter{Frequently Asked Questions}

\textbf{Q}: When I tried running Dogen I get the following error
message:

\begin{pseudocode}[backgroundcolor=\color{lightgray}]
Error: File not found: Could not find data directory.
Base directory: /A/FULL/PATH. Locations searched: ./data ../data ../share/data
See the log file for details: 'log/dogen_knitter_MODEL_NAME.log'
Failed to generate model: 'MODEL_NAME'.
\end{pseudocode}

\textbf{A}: Your installation of Dogen is faulty. This could happen
for example if you copy the Dogen binary from a build directory into
another location without copying all the associated
infrastructure. The correct way is to run the binary using a relative
path to the build directory: \texttt{../../dogen\_knitter}.

This error should not occur if you are using a binary package as the
binary should be in the system path.

\textbf{Q}: I get the error ``Invalid value for object type'' when
generating; what does that mean?

You have tried to use a Dia object type that is not supported by
Dogen. Locate the object type and remove it from your diagram. For a
list of supported object types see FIXME.

\appendix

\chapter{Related Work}

This section is a bit of a general research bucket. It contains a set
of links to the C++ code generators we have found on our wanderings on
the internet, as well as other interesting projects in this space~---
including those in other programming languages. It also contains books
and papers on the subject we have read, or intend to read.

\begin{itemize}
  \item \href{http://www.csg.ci.i.u-tokyo.ac.jp/~chiba/opencxx/html/index.html} Open C++
  \item
  \href{http://www.amazon.co.uk/Domain-Driven-Design-Tackling-Complexity-ebook/dp/B00794TAUG/ref\%3Dsr_1_2?ie\%3DUTF8&qid\%3D1368380797&sr\%3D8-2&keywords\%3Dmodel\%2Bdriven\%2Bdesign}{Domain-Driven
    Design: Tackling Complexity in the Heart of Software}: The Eric
  Evans book from which we tried to steal most concepts in Dogen. A
  must read for any developer.
\item
  \href{http://www.amazon.co.uk/EMF-Eclipse-Modeling-Framework-ebook/dp/B0013TPYVW/ref\%3Dsr_1_2?s\%3Dbooks&ie\%3DUTF8&qid\%3D1368380262&sr\%3D1-2&keywords\%3DEclipse\%2BModeling\%2BFramework\%2B\%255BPaperback\%255D}{EMF:
    Eclipse Modeling Framework}: The original EMF book. Useful read
  for anyone interested in code generation.
\item
  \href{http://www.scribd.com/doc/78264699/Model-Driven-Architecture-for-Reverse-Engineering-Technologies-Strategic-Directions-and-System-Evolution-Premier-Reference-Source}{Model
    Driven Architecture for Reverse Engineering Technologies}: Preview
  of a potentially interesting MDA book.
\item
  \href{http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf}{A
    Common Metamodel for Code Generation}: This paper will be of
  interest if we decide to support multiple languages.
\item
  \href{http://www.vollmann.com/pubs/meta/meta/meta.html}{Metaclasses
    and Reflection in C++}: Some (early) ideas on implementing a MOP
  (Meta Object Protocol) in C++.
\item
  \href{https://code.google.com/a/eclipselabs.org/p/cppgenmodel/}{cppgenmodel
    - A model driven C++ code generator}: This seems more like a run
  time / reflection based generator.
\item \href{https://code.google.com/p/emf4cpp/}{EMF4CPP - Eclipse
  Modeling Framework}: C++ port of the EMF/eCore eclipse framework. As
  with Java it includes run time support. There is also
  \href{http://apps.nabbel.es/dsdm2010/download_files/dsdm2010_senac.pdf}{a
    paper} on it.
\item
  \href{http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf}{A
    Common Metamodel for Code Generation}: Describes a meta-model
  designed to model Java and C++.
\item
  \href{http://marofra.com/oldhomepage/MetaCPlusPlusDoc/metacplusplus-1.html}{The
    Meta-C++ User Manual}: Another early C++ meta-modeling
  tool. Contains interesting ideas around C++ meta-models.
\item The Columbus C++ Schema: Useful tool for re-engineering large
  C++ code bases. Contains a meta-model for C++. A number of papers
  have been written about it:
\begin{itemize}
\item
  \href{http://www.inf.u-szeged.hu/~beszedes/research/tech27_ferenc_r.pdf}{Columbus
    – Reverse Engineering Tool and Schema for C++}
\item
  \href{http://journal.ub.tu-berlin.de/eceasst/article/download/10/19}{Third
    Workshop on Software Evolution through Transformations}: Embracing
  the Change
\item
  \href{http://www.inf.u-szeged.hu/~ferenc/research/ferencr_schema.ppt.pdf}{Towards
    a Standard Schema for C/C++}
\item
  \href{http://www.inf.u-szeged.hu/~ferenc/research/ferencr_columbus_schema_cpp.pdf}{Data
    Exchange with the Columbus Schema for C++}
\end{itemize}
\item \href{http://www.cpgf.org/}{CPGF}: An open source C++ library
  for reflection, script binding, serialisation and callbacks.
\item \href{http://www.artima.com/articles/dci_vision.html}{DCI}: The
  DCI Architecture: A New Vision of Object-Oriented Programming. Some
  fundamental insights on the nature of OO.
\item \href{http://www.ischo.com/xrtti/index.html}{xrtti}: Extending
  C++ with a richer reflection.
\item
  \href{http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3883.html}{Code
    checkers and generators}: adding AngularJS-like capabilities to
  C++.
\item
  \href{http://stackoverflow.com/questions/355650/c-html-template-framework-templatizing-library-html-generator-library}{Text
    Template libraries for C++}: T4 like implementations for C++.
\item The XML binding tools:
  \href{http://msdn.microsoft.com/en-us/library/x6c1kb0s(v\%3Dvs.110).aspx}{Xsd
    tool}, \href{http://www.codesynthesis.com/products/xsd/}{xsd for
    c++}, \href{https://jaxb.java.net/2.2.4/docs/xjc.html}{xjc}, etc
  provide ideas on code generation and can be used to generate plain
  domain objects.
\end{itemize}

\chapter{Nerd Food: Dogen: Lessons in Incremental Coding}

A lot of interesting lessons have been learned during the development
of \href{https://github.com/DomainDrivenConsulting/dogen}{Dogen} and I'm rather afraid many more are still in store. As it is
typical with agile, I'm constantly reviewing processes in search of
improvements. One such idea was that putting pen to paper could help
improving the retrospective process itself. The result is this rather
long blog post, which hopefully is of use to developers in similar
circumstances. Unlike the typical bullet-point based retrospective,
this post it is a rambling narrative as it aims to provide context to
the reader. Subsequent retrospectives will be a lot smaller and more
to the point.

Talking about context: I haven't spoken very much about Dogen in this
blog, so a small introduction is in order. Dogen is an attempt to
create a domain model generator. The \href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/manual/manual.org#fundamental-building-blocks}{manual} goes into quite a bit more
detail, but for the purposes of this exercise, it suffices to think of
it as a C++ code generator. Dogen has been developed continuously
since 2012 - with a few dry spells - and reached its \href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_50.org}{fiftieth sprint}
recently. Having said that, our road to a finished product is still a
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/definition_of_done.org}{long one}.

The remainder of this article looks at what what has worked and what
has not worked so well thus far into Dogen's development history.

\section*{Understanding Time}
\label{sec-1}

Dogen was conceived when we were trying to do our \href{http://kitanda.co.uk/html/index.html}{first start up}. Once
that ended - around the back end of 2012 - I kept working on the tool
in my spare time, and this was a setup that has continued ever
since. There are no other contributors; development just keeps
chugging along, slowly but steadily, with no pressures other than to
enjoy the sights.

Working on my own and in my spare time meant that I had two
conflicting requirements: very little development resources and very
ambitious ideas that required lots of work. With family commitments
and a full time job, I quickly found out that there weren't a lot of
spare cycles left. In fact, after some analysis, I realised I was in a
conundrum. Whilst there is was a lot of "dead-time" in the average
week, it was mostly "low-quality grade time": lots of discontinued
segments of varying and unpredictable lengths. Summed together in a
naive way it seemed like a lot, but - as every programmer knows - six
blocks of ten minutes do not one solid hour make.

Nevertheless, one has to play the game with the cards that were dealt.
I soon realised that the correct question to ask was: "what kind of
development style makes one productive under these conditions?". The
answer turned out to be opportunistic coding. This is rooted in having
a better understanding of the different "qualities" of time and how
best to exploit them. For example, when you have say five to fifteen
minutes available, it makes sense to do small updates to the manual or
fix trivial problems - a typo in the documentation, renaming variables
in a function, mopping up the backlog and other activities of that
ilk. A solid block of forty minutes to an hour affords you more: for
instance, implementing part or the whole of stories for which the
analysis has been completed, or doing some analysis for existing
stories. On those rare cases where half-a-day or longer is available,
one must make the most of it and take on a complex piece of work that
requires sustained concentration. This sessions proved to be most
valuable when the output is a set of well defined stories that are
ready for implementation.

One needs very good processes in order to be able to manage the usage
of time in this fashion. Luckily, agile provides it.

\section*{Slow Motion Agile}
\label{sec-2}

Looking back on \textasciitilde{}2.4k commits, one of the major wins in terms of
development process was to think incrementally. Of course, agile
already gives you a mental framework for that, and we had a
functioning scrum process during our start up days: daily stand-ups,
bi-weekly sprints, pre-sprint planning, post-sprint reviews, demos and
all of that good stuff. It worked really well, and keep us honest and
clean. We used a very simple org-mode file to keep track of all the
open stories, and at one point we even built a simple burn-down chart
generator to allow us to measure velocity.

Granted, when you are working alone in your spare time, a chunk of
agile may not make sense; for instance, providing status updates to
yourself may not be the most productive use of scarce
time. Surprisingly, I found quite a bit of process to be vital. I've
kept the bi-weekly sprint cycle, the sprint logs, the product backlog
and the time-tracking we had originally setup and found them
\textbf{extremely} useful - quite possibly the thing that has kept me going
for such an extended period of time, to be brutally honest. When you
are working on an open source project it is very easy to get lost in
its open-ended-ness and find yourself giving up, particularly if you
are not getting (or expecting) any user feedback. Even Linus himself
has said many times he would have given up the kernel if it wasn't for
other people bringing him problems to keep him interested.

Lacking Linus' ability to attract crowds of interested developers, I
went for the next best thing: I made them up. Well, at least in
metaphorical way, I guess, as this is what user stories are when you
have no external users to drive them. As I am using the product in
anger, I find it very easy to put myself in the head of a user and
come up with requirements that push development forward. These stories
really help, because they transform the cloud of possibilities into
concrete, simple, measurable deliverables that one can choose to
deliver or not. Once you have a set of stories, you have no excuse to
be lazy because you can visualise in your head just how much effort it
would require you to implement a story - and hey, since nerds are
terrible at estimating, it's never that much effort at all. As
everyone knows, it's not quite that easy in the end; but once you've
started, you get the feeling you have to at least finish the task at
hand, and so on, one story at a time, one sprint at a time, until a
body of work starts building up. It's slow, excruciatingly slow, but
it's steady like water working in geological time; when you look back
5 sprints, you cannot help but be amazed on how much can be achieved
in such a incremental way - and how much is still left.

And then you get hooked into measurements. I now love measuring
everything, from how long it takes me to complete a story, to where
time goes in an sprint, to how many commits I do a day, to, well,
everything that can easily be measured without adding any
overhead. There is no incentive for you to game the system - hell, you
could create a script that commits 20 times a day, if the commit count
is all you care about. But it's not, so why bother. Due to this,
statistics start to actually tell you valuable information about the
world and to impel you forward. For instance, GitHub streaks mean that
I always try to at least make one commit per day. Because of this,
even on days when I'm tired, I always force my self to do \emph{something}
and sometimes that quick commit morphs into an hour or two of work
that wouldn't have happened otherwise.

As I mentioned before, it was revealing to find out that there are
different types of time. In order to to take advantage of this
heterogeneity, one must make scrupulous use of the product
backlog. This has proven invaluable, as you can attest by its \href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org}{current}
size. Whether we are part way through a story or just idly
daydreaming, each and every idea must be added to the product backlog,
with sufficient detail to allow one to reconstruct one's train of
thought at that point in time. Once in the backlog, items can be
continuously refined until eventually we find a suitable sprint to
tackle them or they get deprecated altogether. But without an healthy
backlog it is not possible to make the most these illusive time
slots. Conversely, it is important to try to make each story as small
and as focused as possible, and to minimise spikes unless they really
are on the critical path of the story. This is mainly for
psychological reasons: one needs to mark stories as complete, to feel
like work has been done. Never-ending stories are just bad for morale.

In general, this extreme incrementalism has served us well. Not all is
positive though. The worst problem has been a great difficulty in
tackling complex problems - those that require several hours just to
load them into your head. These are unavoidable in any sufficiently
large code base. Having lots of discontinued segments of unpredictable
duration have reduced efficiency considerably. In particular, I notice
I have spent a lot more time lost in conceptual circles, and I've
taken a lot longer to explore alternatives when compared to working
full time.

\section*{DVCS to the Core}
\label{sec-3}

We had already started to use git during the start-up days, and it had
proved to be a major win at the time. After all, one never quite knows
where one will be coding from, and whether internet access is
available or not, so it's important to have a self-contained
environment. In the end we found out it brought many, many more
advantages such as great collaborative flows, good managed web
interfaces/hosting providers (\href{http://www.github.com}{GitHub} and, to some extent, \href{http://www.bitbucket.com}{BitBucket}),
amazing raw speed even on low-powered machines, and a number of other
wins - all covered by lots and lots of posts around the web, so I
won't bore you with that.

On the surface it may seem that DVCS is most useful on a
multi-developer team. This is not the case. The more discontinued your
time is, the more you start appreciating its distributed nature. This
is because each "kind" of time has a more suitable device - perhaps a
netbook for the train, a desktop at someone's house or even a phone
while waiting somewhere. With DVCS you can easily to switch devices
and continue exactly where you left off. With GitHub you can even
author using the web interface, so a mobile phone suddenly becomes
useful for reading and writing.

Another decision that turned out to be a major win is still not the
done thing. Ever the trailblazers, we decided to put everything
related to the project in version control. And by "everything" I do
mean \textbf{everything}: documentation, bug reports, agile process, blog
posts, the whole lot. It did seem a bit silly not to use GitHub's Wiki
and Issues at the time, but, on hindsight, having everything in one
versioned controlled place proved to be a major win:

\begin{itemize}
\item searching is never further than a couple of greps away, and it's not
sensitive to connectivity;
\item all you need is a tiny sliver of connectivity to push or pull, and
work can be batched to wait for that moment;
\item updates by other people come in as commits and can be easily
reviewed as part of the normal push/pull process - not that we got
any of late, to be fair;
\item changes can easily be diffed;
\item history can be checked using the familiar version control interface,
which is available wherever you go.
\end{itemize}

When you have little time, these advantages are life-savers.

The last but very important lesson learned was to commit early and
commit often. It's rather obvious in hindsight, really. After all, if
you have very small blocks of time to do work, you want to make sure
you don't break anything; last thing you need is to spend a week
debugging a tricky problem, with no idea of where you're going or how
far you still have to travel. So it's important to make your commits
\emph{very small} and \emph{very focused} such that a bisection would almost
immediately reveal a problem - or at least provide you with an obvious
rollback strategy. This has proved itself to be invaluable far too
many times to count. The gist of this approach it is to split changes
in an almost OCD sort of way, to the point that anyone can look at the
commit comment and the commit diff and make a judgement as to whether
the change was correct or not. To be fair, it's not quite always that
straightforward, but that has been the overall aim.

\section*{Struggling to stay Continuously Integrated}
\label{sec-4}

After the commit comes the build, and the proof is in the pudding, as
they say. When it comes to code, that largely means CI; granted, it
may not be a very reliable proof, but nevertheless it is the best
proof we've got. One of the major wins from the start up days was to
setup CI, and to give it as wide a coverage as we could muster. We
setup multiple build agents across compilers and platforms, added
dynamic analysis, code coverage, packaging and basic sanity tests on
those packages.

All of these have proven to be major steps in keeping the show on the
road, and once setup, they were \emph{normally} fairly trivial to
maintain. We did have a couple of minor issues with \href{http://www.cdash.org/}{CDash} whilst we
were running our own server. Eventually we moved over to the \href{http://my.cdash.org/index.php?project\%3DDogen}{hosted
CDash server} but it has limitations on the number of builds, which
meant I had to switch some build agents off. In addition to this, the
main other stumbling block is finding the time to do large
infrastructural updates to the build agents such as setting up new
versions of \href{http://www.boost.org/users/history/version_1_56_0.html}{Boost}, new compilers and so on. These are horrendously
time consuming across platforms because you never know what issues you
are going to hit, and each platform has their own way of doing things.

The biggest lesson we learned here is that CI is vital but software
product with no time at all should not waste time managing their own
CI. There are just not enough hours in the day. I have been looking
into \href{https://travis-ci.org/}{travis} to make this process easier in the future. Also, whilst
being cross-platform is a very worthy objective, one has to weigh the
costs with the benefits. If you have a tiny user base, it may make
sense to stick to one platform and continue to do portable coding
without "proof"; once users start asking for multiple platforms, it is
then worth considering doing the work required to support them.

The packaging story was also a very good one to start off with - after
all, most users will probably rely on those - but it turned out to be
\textbf{much} harder than first thought. We spent quite a bit of time
integrating with the GitHub API, uploading packages into their
downloads section, downloading them from there, testing, and then
renaming them for user consumption. Whilst it lasted, this setup was
very useful. Unfortunately it didn't last very long as GitHub decided
to decommission their downloads section. Since most of the upload and
download code was GitHub specific, we could not readily move over to a
different location. The lesson here was that this sort of
functionality is extremely useful, and it is worth dedicating time to
it, but one should always have a plan B and even a plan C. To make a
long story short, the end result is that we don't have any downloads
available at all - not even a stale ones - nor do we have any sanity
checks on packages we produce; they basically go to \texttt{/dev/null}.

In summary, all of our pains led us to conclude that one should
externalise early, externalise often and externalise everything. If
there is a free (or cheap) provider in the cloud that can take on some
or all of your infrastructure work away, you should always consider
using them first rather than host your own infrastructure. And
remember: your time is worth some money, and it is better spent
coding. Of course, it is important to ensure that the provider is
reliable, has been around for a while and is used by a critical
mass. There is nothing worse than spending a lot of effort migrating
to a platform, only to find out that it is about to dramatically
change its APIs, prices, terms and conditions - or even worse, to be
shutdown altogether.

\section*{Loosely Coupled}
\label{sec-5}

Another very useful lesson I learned was to keep the \emph{off-distro}
dependencies to a minimum. This is rather related to the previous
points on CI and cross-platform-ness, really. During the start up days
we started off by requiring a C++ compiler with good C++ 11 support,
and a Boost library with a few off-tree libraries - mainly
\href{http://www.boost.org/doc/libs/1_56_0/libs/log/doc/html/index.html}{Boost.Log}. This meant we had to have our own little "chroot" with all
of these, and we had to build them by hand, sprinkled with plenty of
helper scripts. In those dark days, almost nothing was supplied by the
distro and life was painful. It was just about workable when we had
time on our hands, but this is really not the sort of thing you want
to spend time maintaining if you are working on a project in your
spare time.

To be fair, I had always intended to move to distro-supplied packages
as soon as they caught up, and when that happened the transition was
smooth enough. As things stand, we have a very small off-distro
footprint - mainly \href{http://www.codesynthesis.com/products/odb/}{ODB} and \href{http://epa.codeplex.com/}{EOS}. The additional advantage of not having
off-distro dependencies is that you can start to consider yourself for
inclusion on a distro. Even in these days of Docker, being shipped by
a distro is still a good milestone for any open source project, so it's
important to aim for it. Once more, it's the old psychological factors.

All and all, it seems to me we took the right decisions as both C++ 11
and Boost.Log have proven quite useful; but in the future I certainly
will think very carefully about adding dependencies to off-distro
libraries.

\section*{Conclusions}
\label{sec-6}

In general, the first fifty iterations of Dogen have been very
positive. It has been a rather interesting journey, and dealing with
pure uncertainty is not always easy - after all, one always wants to
reach a destination. At the same time, much has been learned in the
process, and a setup has been created that is sustainable given the
available resources. In the near future I intend to improve the
visibility of the project as I believe that, for all it's faults, it
is still useful in its current form.
% Emacs 24.5.1 (Org mode 8.2.10)

\chapter{Nerd Food: Dogen: The Package Management Saga}

Over the last few years I've had a little project on the side called
\emph{\href{https://github.com/DomainDrivenConsulting/dogen}{Dogen}}. Dogen is a code generator designed to target domain models,
with the lofty ambition of automating the modeling process as much as
possible: users create domain models using a supported UML tool and
respecting a set of predefined restrictions; Dogen uses the tool’s
diagram files to generate the source code representation. The
generated code contains most of the services required from a typical
domain object such as serialisation, hashing, streaming and so on.
Dogen is written in C++ 14 and generates C++ 14 too, but other
languages will eventually be supported as well.

Now, for a four-year-old project, I guess it's fair to say that Dogen
hasn't exactly set the Open Source world on fire. Nevertheless, it has
proven to be a personal fountain of lessons and experiences on
software development; one such lesson was package management and
that's what I shall reminisce about in this article.

\section*{The Conundrum}
\label{sec-1}

Like any other part-time C++ developer whose professional mainstay is
C\# and Java, I have keenly felt the need for a package manager when in
C++-land. The problem is less visible when you are working with mature
libraries and dealing with just Linux, due to the huge size of the
package repositories and the great tooling built around them. However,
things get messier when you start to go cross-platform, and messier
still when you are coding on the bleeding edge of C++: either the
package you need is not available in the distro's repos or even \href{https://launchpad.net/ubuntu/\%2Bppas}{PPA's};
or, when it is, its rarely at the version you require.

Alas, for all our sins, that's exactly where we were when Dogen got
started.

\section*{A Spoonful of Dogen History}
\label{sec-2}

Dogen sprung to life just a tad after C++-0x became \href{https://en.wikipedia.org/wiki/C\%252B\%252B11}{C++-11}, so we
experienced first hand the highs of a quasi-new-language followed by
the lows of feeling the brunt of the bleeding edge pain. For starters,
\emph{nothing} we ever wanted was available out of the box, on any of the
platforms we were interested in. Even Debian Testing was a fair bit
behind the curve - probably stalled due to a compiler transition or
other. In those days, Real Programmers were Real Programmers and mice
were mice: we had to \href{http://mcraveiro.blogspot.co.uk/2012/06/nerd-food-c-11-with-gcc.html}{build and install the C++ compilers ourselves}
and, even then, C++-11 support was new, a bit flaky and limited. We
then had to use those compilers to compile all of the dependencies in
C++-11 mode.

\subsection*{The PFH Days}
\label{sec-2-1}

After doing this manually once or twice, it soon stopped being
fun. And so we solved this problem by creating the PFH - the Private
Filesystem Hierarchy - a gloriously over-ambitious name to describe a
set of wrapper scripts that helped with the process of downloading
tarballs, unpacking, building and finally installing them into
well-defined locations. It worked well enough in the confines of its
remit, but we were often outside those, having to apply out-of-tree
patches, adding new dependencies and so on. We also didn't use Travis
then; not even sure it existed, but if it did, the rigmarole of the
bleeding edge experience would certainly put a stop to any ideas of
using it. So we used a local install of CDash with a number of build
agents on OSX, Windows (MinGW) and Linux (32-bit and 64-bit). Things
worked beautifully when nothing changed and the setup was stable; but
every time a new version of a library - or god forbid, of a compiler -
was released, one had that sense of dread: do I \textbf{really} need to
upgrade? And yet we often did, because we needed the features.

Since one of the main objectives of Dogen was to learn about C++-11,
one has to say that the pain was worth it. But all of the moving parts
described above were not ideal and they were certainly not the thing
you want to be wasting your precious time on when it is very
scarce. They were certainly not scalable.

\subsection*{The Good Days and the Bad Days}
\label{sec-2-2}

Things improved slightly for a year or two when distros started
shipping C++-11 compliant compilers and recent Boost versions. This
led to an attack of pragmatism, during which we ditched all platforms
except for Linux, got rid of almost all our private infrastructure and
moved over to \href{https://travis-ci.org/DomainDrivenConsulting/dogen}{Travis}. For a while things looked really good. However,
due to Travis' \href{https://wiki.ubuntu.com/LTS}{Ubuntu LTS} policy, we were stuck with a rapidly ageing
Boost version. At first PPAs were a good solution, but over time these
became stale too.

Soon we were stuck, unable to afford to revert back to the bad old
days of the PFH but also unable to freeze all dependencies in time, as
it would provide a worse development experience. So it was that the
only route left was to break the build on Travis and hope that a
solution would manifest itself. The red build painfully lingered on,
commit after commit, whilst alternatives such as Drone.io and GitLab
were unsuccessfully tried.

Finally, there was nothing else for it. We simply needed a package
manager to manage the development dependencies.

\subsection*{Nuget Hopes Dashed}
\label{sec-2-3}

Having used \href{https://www.nuget.org/}{Nuget} in anger for both C\# and C++ projects - and given
Microsoft's recent change of heart with regards to open source - I was
secretly hoping that Nuget would get some traction in the wider C++
world. Nuget works \href{http://mcraveiro.blogspot.co.uk/2014/05/nerd-food-using-mono-in-anger-part-ii_3422.html}{well enough in Mono}, and C++ support for Windows
was added fairly \href{http://blogs.msdn.com/b/vcblog/archive/2013/04/26/nuget-for-c.aspx}{early on}. It was somewhat limited and a bit quirky at
the start but it kept on getting better, to the point of being
actually usable; we now use Nuget to manage our C++ dependencies at
work - a Windows shop on the main - and it has improved our quality of
life dramatically.

Unfortunately, the troubles begun on closer inspection. The truth is
that Microsoft's current Nuget focus is C\# and Visual Studio, not
Linux and C++. Also, it seems that outside Microsoft and Xamarin there
just isn't enough traction for this tool at present.

However, there have been a couple of recent announcements from
Microsoft that give me hope things may change in the future:

\begin{itemize}
\item \href{http://blogs.msdn.com/b/vcblog/archive/2015/12/04/introducing-clang-with-microsoft-codegen-in-vs-2015-update-1.aspx}{Clang with Microsoft CodeGen in VS 2015 Update 1}
\item \href{http://blogs.msdn.com/b/vcblog/archive/2015/12/15/support-for-android-cmake-projects-in-visual-studio.aspx}{Support for Android CMake projects in Visual Studio}
\end{itemize}

Surely the logical consequence is to be able to manage packages in a
consistent way across platforms? We can but hope.

\subsection*{Biicode Comes to the Rescue?}
\label{sec-2-4}

Nuget did not pan out but what did happen was even more unlikely: some
crazy-cool Spaniards had decided to create a stand alone package
manager. Being from the same peninsula, I felt compelled to use their
wares, and was joyful as they went from strength to strength -
including the success of their \href{https://www.biicode.com/biicode-open-source-challenge}{open source campaign}. And I loved the
fact that it integrated really well with \href{https://cmake.org}{CMake}, and that \href{https://www.jetbrains.com/clion/}{CLion}
provided Biicode integration very early on.

However, my biggest problem with Biicode was that it was just too
complicated. I don't mean to say the creators of the product didn't
have very good reasons for their technical choices - lord knows
creating a product is hard enough, so I have nothing but praise to
anyone who tries. However, for me personally, I never had the time to
understand why Biicode needed its own version of CMake, nor did I want
to modify my CMake files too much in order to fit properly with
Biicode and so on. Basically, I needed a solution that worked well and
required minimal changes at my end. Having been brought up with Maven
and Nuget, I just could not understand why there wasn't a simple
\texttt{packages.xml} file that specified the dependencies and then some
non-intrusive CMake support to expose those into the CMake files. As
you can see from some of \href{http://forum.biicode.com/t/building-out-of-tree-using-biicode/460}{my posts}, it just seemed it required
"getting" Biicode in order to make use of it, which for me was not an
option.

Another thing that annoyed me was the difficulty on knowing what the
"real" version of a library was. I wrote, at the time:

\begin{quote}
One slightly confusing thing about the process of adding dependencies
is that there may be more than one page for a given dependency and it
is not clear which one is the "best" one. For RapidJson there are
three options, presumably from three different Biicode users:

\begin{itemize}
\item \href{https://www.biicode.com/fenix/rapidjson}{fenix}: authored on 2015-Apr-28, v1.0.1.
\item \href{https://www.biicode.com/hithwen/rapidjson}{hithwen}: authored 2014-Jul-30
\item \href{https://www.biicode.com/denis/rapidjson}{denis}: authored 2014-Oct-09
\end{itemize}

The "fenix" option appeared to be the most up-to-date so I went with
that one. However, this illustrates a deeper issue: how do you know
you can trust a package? In the ideal setup, the project owners would
add Biicode support and that would then be the one true
version. However, like any other project, Biicode faces the initial
adoption conundrum: people are not going to be willing to spend time
adding support for Biicode if there aren't a lot of users of Biicode
out there already, but without a large library of dependencies there
is nothing to draw users in. In this light, one can understand that it
makes sense for Biicode to allow anyone to add new packages as a way
to bootstrap their user base; but sooner or later they will face the
same issues as all distributions face.

A few features would be helpful in the mean time:

\begin{itemize}
\item popularity/number of downloads
\item user ratings
\end{itemize}

These metrics would help in deciding which package to depend on.
\end{quote}

For all these reasons, I never found the time to get Biicode setup and
these stories lingered in Dogen's backlog. And the build continued to
be red.

Sadly Biicode the company \href{http://blog.biicode.com/biicode-just-the-company-post-mortem/}{didn't make it either}. I feel very sad for
the guys behind it, because their heart was on the right place.

Which brings us right up to date.

\section*{Enter Conan}
\label{sec-3}

When I was a kid, we were all big fans of Conan. No, not \href{https://en.wikipedia.org/wiki/Conan_the_Barbarian}{the
barbarian}, the Japanese Manga \href{https://en.wikipedia.org/wiki/Future_Boy_Conan}{Future Boy Conan}. For me the name Conan
will always bring back great memories of this show, which we watched
in the original Japanese with Portuguese subtitles. So I was secretly
pleased when I found \href{https://www.conan.io/}{conan.io}, a new package management system for
C++. The guy behind it seems to be one of the original Biicode
developers, so a lot of lessons from Biicode were learned.

To cut a short story short, the great news is I managed to add Conan
support to Dogen in roughly \href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_77.org#add-support-for-conanio}{3 hours} and with very minimal knowledge
about Conan. This to me was a litmus test of sorts, because I have
very little interest in package management - creating my own product
has proven to be challenging enough, so the last thing I need is to
divert my energy further. The other interesting thing is that roughly
half of that time was taken by trying to get Travis to behave, so its
not quite fair to impute it to Conan.

\subsection*{Setting Up Dogen for Conan}
\label{sec-3-1}

So, what changes did I do to get it all working? It was a very simple
3-step process. First I installed Conan using a Debian package from
\href{https://www.conan.io/downloads}{their site}.

I then created a \texttt{conanfile.txt} on my top-level directory:

\begin{verbatim}
[requires]
Boost/1.60.0@lasote/stable

[generators]
cmake
\end{verbatim}

Finally I modified my top-level \texttt{CMakeLists.txt}:

\begin{verbatim}
# conan support
if(EXISTS "${CMAKE_BINARY_DIR}/conanbuildinfo.cmake")
    message(STATUS "Setting up Conan support.")
    include("${CMAKE_BINARY_DIR}/conanbuildinfo.cmake")
    CONAN_BASIC_SETUP()
else()
    message(STATUS "Conan build file not found, skipping include")
endif()
\end{verbatim}

This means that it is entirely possible to build Dogen without Conan,
but if it is present, it will be used. With these two changes, all
that was left to do was to build:

\begin{verbatim}
$ cd dogen/build/output
$ mkdir gcc-5-conan
$ conan install ../../..
$ make -j5 run_all_specs
\end{verbatim}

\emph{Et voila}, I had a brand spanking new build of Dogen using
Conan. Well, actually, \emph{not quite}. I've omitted a couple of problems
that are a bit of a distraction on the Conan success story. Let's look
at them now.

\subsection*{Problems and Their Solutions}
\label{sec-3-2}

The first problem was that Boost 1.59 does not appear to have an
overridden \texttt{FindBoost}, which means that I was not able to link. I
moved to Boost 1.60 - which I wanted to do anyway - and it worked out
of the box.

The second problem was that Conan seems to get confused with \href{https://ninja-build.org/manual.html}{Ninja}, my
build system of choice. For whatever reason, when I use the Ninja
generator, it fails like so:

\begin{verbatim}
$ cmake ../../../ -G Ninja
$ ninja -j5
$ ninja: error: '~/.conan/data/Boost/1.60.0/lasote/stable/package/ebdc9c0c0164b54c29125127c75297f6607946c5/lib/libboost_system.so', needed by 'stage/bin/dogen_utility_spec', missing and no known rule to make it
\end{verbatim}

This is very strange because boost system is clearly available in the
Conan download folder. Going back to \texttt{make} solved this problem. I've
opened an issue in Conan (\href{https://github.com/conan-io/conan/issues/56}{\#56}) and its currently under investigation.

The third problem is more boost related than anything else. Boost
Graph has not been as well maintained as it should, really. Thus users
now find themselves carrying patches, and all because no one seems to
be able to apply them upstream. Dogen is in this situation as we've
hit the issue described here: \href{http://stackoverflow.com/questions/25395805/compile-error-with-boost-graph-1-56-0-and-g-4-6-4}{Compile error with boost.graph 1.56.0
and g++ 4.6.4.} Sadly this is still present on Boost 1.60; the patch
exists in Trac but remains unapplied (\href{https://svn.boost.org/trac/boost/ticket/10382}{\#10382}). This is a tad worrying
as we make a lot of use of Boost Graph and intend to increase the
usage in the future.

At any rate, as you can see, none of the problems were showstoppers,
nor can they all be attributed to Conan.

\subsection*{Getting Travis to Behave}
\label{sec-3-3}

Once I got Dogen building locally, I then went on a mission to
convince Travis to use it. It was painful, but mainly because of the
lag between commits and hitting an error. The core of the changes to
my YML file were as follows:

\begin{verbatim}
install:
<snip>
  # conan
  - wget https://s3-eu-west-1.amazonaws.com/conanio-production/downloads/conan-ubuntu-64_0_5_0.deb -O conan.deb
  - sudo dpkg -i conan.deb
  - rm conan.deb
<snip>
script:
  - export GIT_REPO="`pwd`"
  - cd ${GIT_REPO}/build
  - mkdir output
  - cd output
  - conan install ${GIT_REPO}
  - hash=`ls ~/.conan/data/Boost/1.60.0/lasote/stable/package/`
  - cd ~/.conan/data/Boost/1.60.0/lasote/stable/package/${hash}/include/
  - sudo patch -p0 < ${GIT_REPO}/patches/boost_1_59_graph.patch
  - cmake ${GIT_REPO} -DWITH_MINIMAL_PACKAGING=on
  - make -j2 run_all_specs
<snip>
\end{verbatim}

I probably should have a bash script by know, given the size of the
YML, but hey - if it works. The changes above deal with installation
of the package, applying the boost patch and using Make instead of
Ninja. Quite trivial in the end, even though it required a lot of
iterations to get there.

\section*{Conclusions}
\label{sec-4}

Having a red build is a very distressful event for a developer, so you
can imagine how painful it has been to have red builds for \emph{several
months}. So it is with unmitigated pleasure that I got to see \href{https://travis-ci.org/DomainDrivenConsulting/dogen/builds/98304957}{build
\#628} in a shiny emerald green. As far as that goes, it has been an
unmitigated success.

In a broader sense though, what can we say about Conan? There are many
positives to take home, even at this early stage of Dogen usage:

\begin{itemize}
\item it is a lot less intrusive than Biicode and easier to setup. Biicode
was very well documented, but it was easy to stray from the beaten
track and that then required reading a lot of different wiki
pages. It seems easier to stay on the beaten track with Conan.
\item as with Biicode, it seems to provide solutions to Debug/Release and
multi-platforms and compilers. We shall be testing it on Windows
soon and reporting back.
\item hopefully, since it started Open Source from the beginning, it will
form a community of developers around the source with the know-how
required to maintain it. It would also be great to see if a business
forms around it, since someone will have to pay the cloud bill. It
certainly is gaining popularity, as \href{http://cppcast.com/2016/05/diego-rodriguez-losada/}{the recent CppCast} attests.
\end{itemize}

In terms of negatives:

\begin{itemize}
\item I still believe the most scalable approach would have been to extend
Nuget for the C++ Linux use case, since Microsoft is willing to take
patches and since they foot the bill for the public repo. However,
I can understand why one would prefer to have total control over the
solution rather than depend on the whims of some middle-manager in
order to commit.
\item it seems publishing packages requires getting down into
Python. Haven't tried it yet, but I'm hoping it will be made as easy
as importing packages with a simple text file. The more complexity
around these flows the tool adds, the less likely they are to be
used.
\item there still are no "official builds" from projects. As explained
above, this is a chicken and egg problem, because people are only
willing to dedicate time to it once there are enough users
complaining. Having said that, since Conan is easy to setup, one
hopes to see some adoption in the near future.
\item even when using a GitHub profile, one still has to define a Conan
specific password. This was not required with Biicode. Minor pain,
but still, if they want to increase traction, this is probably an
unnecessary stumbling block. It was sufficient to make me think
twice about setting up a login, for one.
\end{itemize}

In truth, these are all very minor negative points, but still worth
making them. All and all, I am quite pleased with Conan thus far.

\chapter{Nerd Food: On Product Backlog}

Many developers in large companies tend to be exposed to a strange
variation of agile which I like to call "Enterprise Grade Agile", but
I've also heard it called "Fragile" and, most aptly, "Cargo-Cult
Agile". However you decide to name the phenomena, the gist of it is
that these setups contain nearly all of the ceremony of agile -
including stand-ups, sprint planning, retrospectives and so on - but
none of its spirit. Tweets such as this are great at capturing the
essence of the problem:

Once you start having that nagging feeling of doing things "because
you are told to", and once your stand-ups become more of a status
report to the "project manager" and/or "delivery manager" - the
existence of which, in itself, is rather worrying - your Cargo Cult
Agile alarm bells should start ringing. As I see it, agile is a
toolbox with a number of tools, and they only start to add value once
you've adapted them to your personal circumstances. The fitness
function that determines if a tool should be used is how much value it
adds to all (or at least most) of its users. If it does not, the tool
must be further adapted or removed altogether. And, crucially, you
learn about agile tools by using them and by reflecting on the lessons
learned. There is no other way.

This post is one such exercise and the tool I'd like to reflect on is
the \emph{Product Backlog}. Now, before you read through the whole rant,
its probably worth saying that this post takes a slightly narrow and
somewhat "advanced" view of agile, with a target audience of those
already using it. If you require a more introductory approach, you are
probably better off looking at other online resources such as \href{http://zerodollarbill.blogspot.co.uk/2012/06/how-to-learn-scrum-in-10-minutes-and.html}{How to
learn Scrum in 10 minutes and clean your house in the process}. Having
said that, I'll try to define terms best I can to make sure we are all
on the same page.

\section*{Working Definition}
\label{sec-1}

Once your company has grokked the basics of agile and starts to move
away from those lengthy specification documents - those that no one
reads properly until implementation and those that never specified
anything the customer wanted, but everything we thought the customer
wanted and then some - you will start to use the product backlog in
anger. And that's when you will realise that it is not quite as simple
as memorising text books.

So what do the "text books" say? Let's take a fairly typical
definition - this one from \href{https://en.wikipedia.org/wiki/Scrum_(software_development)}{Scrum}:

\begin{quote}
The agile product backlog in Scrum is a prioritized features list,
containing short descriptions of all functionality desired in the
product. When applying Scrum, it's not necessary to start a project
with a lengthy, upfront effort to document all
requirements. Typically, a Scrum team and its product owner begin by
writing down everything they can think of for agile backlog
prioritization. This agile product backlog is almost always more than
enough for a first sprint. The Scrum product backlog is then allowed
to grow and change as more is learned about the product and its
customers.\footnote{Source: \href{https://www.mountaingoatsoftware.com/agile/scrum/product-backlog}{Scrum Product Backlog}, Mountain Goat Software.}
\end{quote}

This is a good working definition, which will suffice for the purposes
of this post. It is deceptively simple. However, as always, one must
remember Yogi Berra: "In theory, there is no difference between theory
and practice. But in practice, there is."

\section*{Potmenkin Product Backlogs}
\label{sec-2}

Many teams finish reading one such definition, find it amazingly
inspiring, install the "agile plug-in" on their bug-tracking software
of choice and then furiously start typing in those tickets. But if you
look closely, you'd be hard-pressed to find any difference between the
bug tickets of old versus the "stories" in the new and improved
"product backlog" that apparently you are now using.

This is a classic management disconnect, whereby a renaming exercise
is applied and suddenly, \href{https://en.wikipedia.org/wiki/Potemkin_village}{Potemkin village-style}, we are now in with
the kool kids and our company suddenly becomes a modern and desirable
place to work. But much like Potemkin villages were not designed for
real people to live in, so "Potmenkin Product Backlogs" are not
designed to help you manage the lifecycle of a real product; they are
there to give you the \emph{appearance} of doing said management, for the
purposes of reporting to the higher eschelons and so that you can tell
stakeholders that "their story has been added to the product backlog
for prioritisation".

Alas, very soon you will find that the bulk of the "user stories" are
nothing but glorified one-liners that no one seems to recall what
exactly they're supposed to mean, and those few elaborately detailed
tickets end up rotting because they keep being deprioritised and now
describe a world long gone. Soon enough you will find that your sprint
planning meetings will cover less and less of the product backlog -
after all, who is able to prioritise this mess?  Some stories don't
even make any sense! The final act is when all stories worked on are
stories raised directly on the sprint backlog, and the product backlog
is nothing but the dumping ground for the stories that didn't make it
on a given sprint. At this stage, the product backlog is in such a
terrible mess that no one looks at it, other than for the occasional
historic search for valuable details on how a bug was
fixed. Eventually the product backlog is zeroed - maybe a dozen or so
of the most recent stories make it through the cull - and the entire
process begins anew. Alas, enlightenment is never achieved, so you are
condemned to repeat this cycle for all eternity.

As expected, the Potmenkin Product Backlog adds very little value - in
fact it can be argued that it detracts value - but it must be kept
because "agile requires a product backlog".

\section*{Bug-Trackers: Lessons From History}
\label{sec-3}

In order to understand the difficulties with a product backlog, we
turn next to their logical predecessors: bug-tracking systems such as
\href{https://www.bugzilla.org/}{Bugzilla} or \href{https://www.atlassian.com/software/jira}{Jira}. This post starts with a quote from the kernel's
Benevolent Dictator that illustrates the problem with these. Linus has
long taken the approach that there is no need for a bug-tracker in
kernel development, although he does not object if someone wants to
use one for a subsystem. You may think this is a very primitive
approach but in some ways it is also a \emph{very} modern approach, very
much in line with agile; if you have a bug-tracking system which is
taking time away from developers without providing any value, you
should \emph{remove} the bug-tracking system. In kernel development, there
simply is no space for ceremony - or, for that matter, for anything
which slows things down\footnote{A topic which I covered some time ago here: \href{http://mcraveiro.blogspot.co.uk/2008/06/nerd-food-on-evolutionary-methodology.html}{On
Evolutionary Methodology}. It is also interesting to see how the kernel
processes are organised for speed: \href{http://lwn.net/Articles/670209/}{How 4.4's patches got to the
mainline}.}.

All of which begs the question: what makes bug-tracking systems so
useless? From experience, there are a few factors:

\begin{itemize}
\item they are a "fire and forget" capture system. Most users only care
about entering new data, rather than worrying about the lifecycle of
a ticket. Very few places have some kind of "ticket quality control"
which ensures that the content of the ticket is vaguely sensible,
and those who do suffer from another problem:
\item they require dedicated teams. By this I don't just mean running the
bug-tracking software - which you will most likely have to do in a
proprietary shop; I also mean the entire notion of Q\&A and Testing
as separate from development, with reams of people dedicated to
setting "environments" up (and keeping them up!), organising
database restores and other such activities that are incompatible
with current best practices of software development.
\item they are temples of ceremony: a glance at the myriad of fields you
need to fill in - and the rules and permutations required to get
them exactly right - should be sufficient to put off even the most
ardent believer in process. Most developers end up memorising some
safe incantation that allows them to get on with life, without
understanding the majority of the data they are entering.
\item as the underlying product ages, you will be faced with \href{http://tinyletter.com/programming-beyond-practices/letters/the-sad-graph-of-software-death}{the sad graph
of software death}. The main problem is that resources get taken away
from systems as they get older, a phenomena that manifests itself as
a growth in the delta between the number of open tickets against the
number of closed tickets. This is actually a \emph{really} useful metric
but one that is often ignored.\footnote{Another topic which I also covered here some time
ago: \href{http://mcraveiro.blogspot.co.uk/2007/05/nerd-food-on-maintenance.html}{On Maintenance}.}.
\end{itemize}

And what of the newest iterations on this venerable concept such as
\href{https://guides.github.com/features/issues/}{GitHub Issues}? Well, clearly they solve a number of the problems
above - such as lowering the complexity and cost barriers - and
certainly they do serve a very useful purpose: they allow the
efficient management of user interactions. Every time I create an
issue - such as this \href{https://github.com/flycheck/flycheck/issues/852}{one} - it never ceases to amaze me how easily the
information flows within GitHub projects; one can initiate comms with
the author(s) or other users with \emph{zero setup} - something that
previously required mailinglist membership, opening an account on a
bug-tracker and so forth. We now take all of this for granted, of
course, but it is important to bear in mind that many open source
projects would probably not even have \emph{any} form of user interaction
support, were it not for GitHub. After all, most of them are a
one-person shop with very little disposable time, and it makes no
sense to spend part of that time maintaining infrastructure for the
odd person or two who may drop by to chat.

However, for all of its glory, it is also important to bear in mind
that GitHub Issues is \textbf{not} a product backlog solution. What I mean by
this is that the product backlog must be owned by the team that owns
the product and, as we shall see, it must be carefully groomed if it
is to be continually useful. This is at loggerheads with allowing free
flow of information from users. Your Issues will eventually be filled
up with user requests and questions which you may not want to address,
or general discussions which may or may not have a story behind
it. They are simply different tools for different jobs, albeit with an
overlap in functionality.

So, history tells us what does not work. But is the product backlog
even worth all this hassle?

\section*{Voyaging Through Strange Seas of Thought}
\label{sec-4}

One of the great things about agile is how much it reflects on itself;
a strange loop of sorts. Presentations such as Kevlin Henney's \href{http://www.infoq.com/presentations/architecture-uncertainty}{The
Architecture of Uncertainty} are part of this continual process of
discovery and understanding, and provide great insights about the
fundamental nature of the development process. The product backlog
plays - or should play - a crucial role exactly because of this
uncertain nature of software development. We can explain this by way
of a device.

Imagine that you start off by admitting that you know very little
about what it is that you are intending to do and that the problem
domain you are about to explore is vast and complex. In this scenario,
the product backlog is the sum total of the knowledge gained whilst
exploring this space that has yet not been transformed into source
code. Think of it like the explorer's maps in the fifteen-hundreds. In
those days, "users" knew that much of it was incorrect and a great
part was sketchy and ill-defined, but it was all you had. Given that
the odds of success were stacked against you, you'd hold that map
pretty tightly while the storms were raging about you. Those that made
it back would provide corrections and amendments and, over time, the
maps eventually converged with the real geography.

The product backlog does something similar, but of course, the space
you are exploring does not have a fixed geometry or topography and
your knowledge of the problem domain can actively \emph{change} the domain
itself too - an unavoidable consequence of dealing with pure thought
stuff. But the general principle applies. Thus, in the same way \href{http://www.joelonsoftware.com/articles/fog0000000069.html}{a code
base is precious} because it embodies the sum total knowledge of a
domain - heck, in many ways it \emph{is} the sum total knowledge of a
domain! - so the product backlog is precious because it captures all
the known knowledge of these yet-to-be-explored areas. In this light,
you can understand statements such as this:

So, if the backlog is this important, how should one manage it?

\section*{Works For Me, Guv!}
\label{sec-5}

Up to this point - whilst we were delving into the problem space - we
have been dealing with a fairly general argument, likely applicable to
many. Now, as we enter the solution space, I'm afraid I will have to
move from the general to the particular and talk only about the
specific circumstances of my one-man-project \href{https://github.com/DomainDrivenConsulting/dogen}{Dogen}. You can find
Dogen's product backlog \href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org}{here}.

This may sound like a bit of a cop out, you may say, and not without
reason: how on earth are you supposed to extrapolate conclusions from
a one-person open source project to a team of N working on a
commercial product? However, it is also important to take into account
what I said at the start: agile is what you make of it. I personally
think of it as a) the smallest amount of processes required to make
your development process work smoothly and b) and the continual
improvement of those processes. Thus, there are no one-size-fits-all
solutions; all one can do is to look at others for ideas. So, lets
look at my findings\footnote{I am self-plagiarising a little bit here and
rehashing some of the arguments I've used before in \href{http://mcraveiro.blogspot.co.uk/2014/09/nerd-food-dogen-lessons-in-incremental.html}{Lessons in
Incremental Coding}, mainly from section DVCS to the Core.}.

The first and most important thing I did to help me manage my product
backlog was to use a simple text file in \href{http://orgmode.org/}{Org Mode} notation. Clearly,
this is not a setup that is workable for a development team much
larger than a set of one, or one that doesn't use Emacs (or \href{https://github.com/hsitz/VimOrganizer}{Vim}). But
for my particular circumstances it has worked \emph{wonders}:

\begin{itemize}
\item the product backlog is close to the code, so wherever you go, you
take it with you. This means you can always search the product
backlog and - most importantly - add to it \emph{wherever} you are and
\emph{whenever} an idea happens to come by. I use this flexibility
frequently.
\item the Org Mode interface makes it really easy to move stories up and
down (order is taken to mean priority here) and to create "buckets"
of stories according to whatever categorisation you decide to use,
up to any level of nesting. At some point you end up converging to a
reasonable level of nesting, of course. It is surprising how one can
manage \textbf{very} large amounts of stories thanks to this flexible tree
structure.
\item it's trivial to move stories in and out of a sprint, keeping track
of all changes to a story - they are just text that can be copy and
pasted and committed.
\item Org Mode provides a very capable \href{http://orgmode.org/manual/Tags.html}{tagging system}. I first started by
overusing these, but when tagging got too fine grained it became
unmaintainable. Now we use too few - just \texttt{epic} and \texttt{story} - so
this will have to change again in the near future. For example, it
should be trivial to add tags for different components in the system
or to mark stories as bugs or features, etc. \href{http://orgmode.org/manual/Tag-searches.html#Tag-searches}{Searching} then allows
you to see a subset of the stories that match those labels.
\end{itemize}

A second decision which has proven to be a very good one has been to
groom the product backlog \emph{very often}. And by this I don't just mean
a cursory look, but a deep inspection of \emph{all} stories, fixing them
where required. Again, the choice of format has proved very helpful:

\begin{itemize}
\item it is easy to mark all stories as "non-reviewed" or some other
suitable tag in Org Mode, and then unmark them as one finishes the
groom - thereby ensuring all stories get some attention. As the
product backlog becomes larger, a full groom could take multiple
sprints, but this is not an issue once you understand its value and
the cost of having it rot.
\item because the product backlog is with the code, any downtime can be
used for grooming; those idle weekends or that long wait at the
airport are perfect candidates to get a few stories looked at. Time
spent waiting for the build is also a good candidate.
\item you get an HTML representation of the Org Mode file for free in
GitHub, meaning you can read your backlog from your phone. And with
the new editing functionality, you can also edit stories too.
\end{itemize}

Thirdly, I decided to take a "multi-pass" approach at managing the
story lifecycle. These are some of the key aspects of this lifecycle
management:

\begin{itemize}
\item stories can only be captured if they are aligned with the
\href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/vision.org}{vision}. This filter saves me from adding all sorts of ideas which
are just too "out of the left field" to be of practical use, but
keeps \href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/product_backlog.org#visionary-work-and-random-ideas}{those that may sound crazy} are but aligned with the vision.
\item stories can only be captured if there is no "prior art". I always
perform a number of searches in the backlog to look for anything
which covers similar ground. If found, I append to that.
\item new stories tend to start with very little content - just the
minimum required to allow resetting state back to the idea I was
trying to capture. Due to this, very little gets lost. At this
point, we have a "proto-story".
\item as time progresses, I end up having more ideas on this space, and I
update the story with those ideas - mainly bullet points with one
liners and links.
\item at some point the story begins to mature; there is enough on it that
we can convert the "proto-story" to a full blown story. After a
number of grooms, the story becomes fully formed and is then a
candidate to be moved to a sprint backlog for implementation. It may
stay in this state \emph{ad-infinitum}, with periodic updates just to
make sure it does not rot.
\item A candidate story can still get refined: trimmed in scope,
re-targeted, or even cancelled because it no longer fits with the
current architecture or even the vision. Cancelled stories are
important because we may came back to them - its just very unlikely
that we do.
\item every sprint has a "sprint mission"\footnote{See the \href{https://github.com/DomainDrivenConsulting/dogen/blob/master/doc/agile/sprint_backlog_78.org}{current sprint backlog} for an example.}. When we start to
move stories into the sprint backlog, we look for those which
resonate with the sprint mission. Not all of them are fully formed,
and the work on the sprint can entail the analysis required to
create a full blown story. But many will be implementable directly
off of the product backlog.
\item some times I end up finding related threads in multiple stories and
decide to merge them. Merging of related stories is done by simply
copying and pasting them into a single story; over time, with the
multiple passes done in the grooms, we end up again with a single
consistent story.
\end{itemize}

What all of this means is that a story can evolve over time in the
product backlog, only to become the exact thing you need at a given
sprint; at that point you benefit from the knowledge and insight
gained over that long period of time. Some stories in Dogen's backlog
have been there for years, and when I finally get to them, I find them
extremely useful. Remember: they are a map to the unknown space you
are exploring.

With all of this machinery in place, we've ended up with a very useful
product backlog for Dogen - one that certainly adds a lot of
value. Don't take me wrong, the cost of maintenance is high and I'd
rather be coding instead of maintaining the product backlog,
especially given the limited resources. But I keep it because I can
see on a daily basis how much it improves the overall quality of the
development process. It is a price I find worth paying, given what I
get in return.

\section*{Final Thoughts}
\label{sec-6}

This post was an attempt to summarise some of the thoughts I've been
having on the space of product backlogs. One of its main objectives
was to try to convey the importance of this tool, and to provide ideas
on how you can improve the management of your own product backlog by
discussing the approach I have taken with Dogen.

If you have any suggestions or want to share your own tips on how to
manage your product backlog please reach me on the comments section -
there is always space for improvement.

\chapter{Assorted Text}

The main objective of creating a domain generator is to avoid having
to maintain manually a significant amount of trivial code; this not
only speeds up the development process but it also improves code
quality as programmers do not tend to perform repetitive tasks
terribly well.

We developed our own domain generator because we could not find one
that fitted our requirements~--- open source or otherwise. You can see
the results of our research
\href{https://github.com/kitanda/dogen/blob/master/doc/manual/manual.org#appendix-a---related-work}{in
  the manual's appendix}.

Note that Dogen is specifically tailored for our needs. We are,
however, wiling to accept any patches for functionality not directly
required by us.

\backmatter
\bibliographystyle{plain}
\bibliography{manual}

\end{document}
