#+title: Domain Driven Development with Dogen
#+options: author:nil

Copyright (c) 2012, 2013, 2014 Marco Craveiro

Permission is granted to copy, distribute and/or modify this document under the
terms of the GNU Free Documentation License, version 1.3; with no Invariant
Sections, no Front-Cover Texts and no Back-Cover Texts.

Revision *DRAFT*, May 2014

This revision of the manual describes Dogen *master* and is available
in the following formats: HTML and PDF.

#+toc: headlines 2
#+toc: listings
#+toc: tables

* Preface

** About This Document

This document is the official manual for Dogen. Dogen - the domain
generator - is a code generation tool designed specifically to target
domain models. Dogen was created to make the modeling process simpler:
the user creates a domain model using a UML tool and Dogen uses it to
generate its source code representation. The generated code contains
most of the services required from a typical C++ domain object such as
serialisation, hashing, streaming and so on.

If you are reading a printed copy of this manual, you can always
access the latest version online:

- https://github.com/kitanda/dogen/blob/master/doc/manual/manual.org

** More Information

You can find the latest source code for Dogen at the official
repository in GitHub:

- https://github.com/kitanda/dogen

There is also a mirror in BitBucket:

- https://bitbucket.org/marco_craveiro/dogen/overview

You can find details on the ongoing work in the Agile folder (look for
the latest sprint):

- https://github.com/kitanda/dogen/tree/master/doc/agile

Continuous builds are available via CDash:

- http://hedgr.co.uk/cdash/index.php?project=dogen

* PART I - THEORY

In this part we describe the philosophical underpinnings behind Dogen,
its internal architecture and the development model.

** Introduction

Dogen is largely the result of exploring a simple question: what
portion of the objects required to model a problem domain are
generatable by a program - such that the generated code is as good
as or even better than code crafted by humans?

The question stems from many years of looking at object models and
their limitations. For instance, one of the main problems one often
finds in production C++ code is a lack of a large number of useful
"facilities" that all objects on all domain models should have; for
instance, a simple way of dumping current state to a stream, like
/toString/ in Java. Developers tend to add facilities like these on a
haphazard sort of way, because it is laborious and not particularly
exciting functionality to work on; by the time the domain model has
matured, its too late to find time for these sort of fundamental
activities.

We further observed that a large portion of the objects required to
model a problem domain have fairly straightforward behaviours; in many
cases they are but glorified structs with a few trivial behaviours
bolted on such as serialisation and hashing. A lot of programmer time
is taken on generating getters, setters, serialisation code and so
on. This is also code that is easy to get sloppy with, because its so
repetitive.

Dogen aims to code generate all such code - and /only/ such code;
anything which is deemed non-trivial is expected to be done by
humans. For everything else, we aim to add support in Dogen.

*** Models and modeling

Programming is the art of refining abstractions. In general, the
programmer's job is to create a set of constructs that represent
entities in some problem domain or other; and to get those entities to
cooperate successfully in producing work that is defined as useful by
something or someone. Together, these entities form /a model/ because
they are representation inside of the computer of a subset of the
problem domain.

/Domain modeling/ is then the activity of finding a set of domain
types that describe the domain in question via their properties and
their relationships.

Many programmers don't dwell too much on the fact that they are
modeling - its just an implicit activity done as part of
coding. However, as we shall see, the modern way of looking at
programming puts a strong emphasis in understanding and introspecting
about this activity.

*** Code generation and meta-models

The concept of programs that generate programs is probably as old as
computer science itself: it certainly was a common feature in the days
of machine code and assembler code programming. These ideas were
incorporated in early languages such as LISP, where there was a
blurring of the lines between hand crafted source code and machine
generated source code. Sadly, these progressive thoughts faded into
the foreground as the C family of languages took front stage.

It's not as if code generation disappeared - it just went into
hiding. In fact, there are many widely used tools in the Open Source
ecosystem that generate code:

- [[https://developers.google.com/protocol-buffers/][Google Protocol Buffers]]
- [[http://www.codesynthesis.com/products/odb/][ODB]]: C++ Object-Relational Mapping (ORM)
- [[http://www.codesynthesis.com/products/xsde/][eXSD]]: XSD/e: XML for Light-Weight C++ Applications
- [[http://msdn.microsoft.com/en-us/library/windows/desktop/aa367300(v%3Dvs.85).aspx][MIDL]]: COM IDL compiler
- and many more.

Each of these tools are designed to do a specific task and to do it
well, hiding as much as possible of the code generation details from
the end user. We call these are special-purpose code generators -
although, as we shall see, in a sense all code generators are special
purpose. The code generated by these tools contains both the data
structures they require as well as hard-coded behaviour associated
with them: how to read and write them from raw storage (in the case of
Protocol Buffers), how to read and write them from the database (ODB),
and so on.

All code generators have an internal set of data structures that
represent the entities to generate - explicitly or implicitly. These
data structures are known as the /meta-model/. Meta-models are a class
of models that focus on describing models themselves. They allow code
to introspect and to think about code; to reflect. In this form, code
generation is simply the transformation of a model, described in one
such representation, into another representation - source code -
following the rules laid out by the grammar of a programming
language. The richer the meta-model, the more expressive the generated
code can be - and vice-versa. It is in this light that we call certain
classes of code generators /special purpose/, because they have
meta-models that are very focused, designed only for the task at
hand. Don't think of this as a disadvantage though: there is a price
to pay in complexity for every ounce of flexibility, so its best to
have simple code that does one thing and does it well.

Nevertheless, meta-models can be useful in a more general form when
designing software applications: they can allow one to reason about
the structure of the code. One of the most common meta-models in
existence is [[http://en.wikipedia.org/wiki/Unified_Modeling_Language][UML]]. UML is used widely in the industry and there are
many tools that can be used to generate source code from UML
diagrams. It is simultaneously ubiquitous - that is, available
everywhere - and complete - that is, as a meta-model, it defines a
extensive list of concepts for pretty much any aspect of
programming. Thus it is common for tools to take a UML representation
and use it to generate source code; as examples of Open Source tools
that can generate source code from a UML diagram see:

- [[http://dia2code.sourceforge.net/][dia2code]]
- [[http://umbrello.kde.org/][Umbrello]] (see [[http://docs.kde.org/development/en/kdesdk/umbrello/code-import-generation.html][this]] for code generation)

In a sense one, may think of these as /general purpose/ code
generators because they output code that is not tied up to any
specific purpose, other than to model the problem domain. Unlike the
special purpose tools, the generated code is very much skeleton code,
code that adds little in terms of behaviour. This is all as it should
be: the more specific your intent is, the more the code generator can
do for you and, conversely, the less specific your intent is, the less
helpful the code generator can be.

The astute reader would have already devised a simple solution to the
behaviour conundrum: nothing stops us from modeling the signatures of
methods in the meta-model - after all UML provides us with all the
required machinery - and then hand-craft an implementation for these
methods. Indeed there are code generators which permit such workflows;
they are known as /merging code generators/. The merging aspect comes
from the fact that the code generator must be able to distinguish
between the hand-crafted code and the machine generated code in order
to handle meta-model updates.

So these are three key themes for Dogen: special purpose code
generation, general purpose code generation and merging code
generation. But before we can proceed, we need to add one more actor
to the scene.

*** Domain Driven Design

One of the main problems facing software engineers working on large
systems is the need to clearly separate business rules from
scaffolding code. In many ways, this need originates from the long
forgotten days when the word /Application/ was coined: the use of
computer science /applied/ to a specific problem to provide an
automated solution to the set of people with the problem - the
/users/. During the process of development, users will provide all
sorts of insights into what it is they want solved, and these are
ultimately captured in code. Code will also be made up of reading and
writing records to a database, socket communication, reading and
writing to file and so on; the challenge then is to avoid obscuring
the former while dealing with the latter.

Many people have thought deeply about this dichotomy. Arguably, the
most significant advance was made by Eric Evans with his seminal book
[[http://www.amazon.co.uk/Domain-driven-Design-Tackling-Complexity-Software/dp/0321125215][Domain-Driven Design]]: Tackling Complexity in the Heart of
Software. Domain Driven Design (DDD) is a software engineering
methodology that places great emphasis on understanding the problem
domain and, coupled with Agile, it provides a great platform for
iterative improvements both to the understanding and to its expression
in code. DDD places great emphasis in defining a clear and concise
domain model - a set of classes and relationships that model the
insights provided by the users and domain experts in general. It also
explains the difference between the conceptual domain model and myriad
of representations: UML diagrams, specification documents, oral
conversations and, most importantly, source code.

*** Adding It All Together

The key idea behind Dogen is that all of the aspects we described up
til now are deeply interrelated. That is to say that we store deep
knowledge about the domain in meta-models, which tend to be
represented graphically - say in UML class diagrams; and we do so
because these representations provide a quick and yet expressive way
to communicate domain knowledge. But those very same documents are -
or can be made - sufficiently complete to be used as a basis for the
code generation of skeleton code by some general purpose code
generation tool. Furthermore, there are a large number of services
that are required of most domain models, and these can be thought of
as special purpose extensions to such a general purpose tool; and,
finally, that which cannot be code generated can be manually added and
merged in.

Lets return to the "basic services" required by all domain
models. What do we mean exactly? Well, ODB and the like already hinted
at some of the things one may wish to do with C++ objects - persist
them in a database - but there are other even more fundamental
requirements:

- the ability to support getters and setters, hashing, comparisons,
  assignment, move construction and many other fundamental behaviours;
- the ability to dump the current state of the object to a C++ stream
  in a format that is parsable by external tools (like say JSON);
- the ability to generate [[http://stackoverflow.com/questions/5140475/how-to-write-native-c-debugger-visualizers-in-gdb-totalview-for-complicated-t][debugger visualisers]];
- the ability to serialise and deserialise objects using a multitude
  of technologies such as [[http://download.oracle.com/otn_hosted_doc/coherence/353CPP/index.html][POF]], [[http://www.boost.org/doc/libs/1_55_0/libs/serialization/doc/index.html][Boost Serialisation]], [[https://github.com/hjiang/jsonxx][JSON]], [[http://libxmlplusplus.sourceforge.net/][XML]] and many
  others;
- the ability to generate objects populated with random data for
  testing;
- ...

And on and on. The more we looked, the more boilerplate code we
found - code that could easily be generated for the vast majority of
the cases. Of course, there are quite a few corner cases which are
just too hard to automate but they can easily be manually coded.

The picture that emerges from this [[http://en.wikipedia.org/wiki/Thought_experiment][gedankenexperiment]] is some kind of
"cyborg" coding - a type of programming where any and all aspects that
can be reduced to a set of rules inferable from the structure of the
domain model, are implemented as extensions of the code
generator. Dogen is an attempt to create such a tool. As we are C++
developers we started off by trying to implement the vision as a C++
tool; but the notions are general enough that they would apply to any
programming language.

** The Dogen Architecture

Almost all code in Dogen is implemented as Dogen domain models; that
is, we use Dogen to generate the vast majority of Dogen itself, and we
do so for several reasons:

- dog-fooding: using your own tool frequently is a great way of making
  sure the tool does what it is meant to do and does so in a workable,
  pragmatic manner.
- keeping our feet on the ground: if we have some crazy ideas and
  break Dogen, we can no longer develop Dogen. Thus Dogen must always
  be able to code-generate itself at all points in the development
  cycle, which forces one to think "extremely" incrementally.
- code faster and test our theoretical underpinnings: if our ideas
  around code generation are correct, Dogen should significantly
  speed-up development.

Dogen is made up of a large number of domain models. These fall into
two broad categories: /test models/ and /core models/. Test models are
models we created specifically to test some aspect of code
generation - such as say inheritance - and whose code is not used by
the main binary. The core models are what really makes up the
application and that is what is of interest for this chapter.

The core models are hooked together in a fashion similar to that of
the internals of a compiler, and so core models belong to one of three
groups: the front-end, the middle-end and the back-end. The front-end
group of models allows for different sources of domain information to
be plugged into Dogen. The middle-end model - as there is only one -
is where all the language neutral transformations take place; It can
be thought of as a bridge between domain modeling and code
generation. Finally, the back-end group of models are responsible for
expressing SML as code according to the grammar of a programming
language like C++.

Lets look at each of these in more detail.

*** The Front-end

When we started developing Dogen, we chose Dia as our main input
format. Dia is a simple yet very powerful tool for drawing structured
diagrams that focuses almost exclusively on diagram editing, and
leaves other use cases to external tools. To their credit, a number of
tools have sprung up around Dia, in no small part due to the
simplicity and stability of their XML file format. Some of these tools
generate code from Dia XML, others convert code into diagrams; we
aimed for Dogen to be another chain in that tooling ecosystem.

At the same time, Dogen has been developed from the start with the
intention to support multiple input formats. We knew that different
people would have different needs and for some Dia would not be
sufficient. So we imagined a pipeline that was made up with a pair of
models: one to model closely the input model and a /transformation/
model responsible for converting the input model into the middle-end

Dogen. Furthermore, all of Dogen's own models have been created and
are being maintained using this application, so it is very core to the
code generation experience.

There are two models responsible for implementing this use case:

- =dia=
- =dia_to_sml=

The =dia= model has a representation of the Dia XML types, and tries
to do so as faithfully as possible. It was created to avoid having a
direct dependency with Dia's code base. Since Dia XML changes very
infrequently and since we use such a small part of Dia's
functionality, this turned out to be a good decision.

**** Meta-data and tags

In certain cases we had the need to pass certain information to SML
for which there was no available equivalent in Dia. In some cases
these were just shortcomings of the application and could be solved by
patching it; in some other cases, it just made no sense at all to
convey this kind of information in Dia. To solve this problem in a
general manner, we created a set of special "instructions" that are
interpreted by Dogen. These instructions are passed in to Dogen via
UML Comments, with a special form:

: #DOGEN KEY=VALUE

All lines starting with the well-known prefix =#DOGEN= are considered
special instructions. They must follow the key-value-pair form defined
above.

Initially this was done to fix a couple of minor problems with Dia,
but this infrastructure has taken a life of its own, and its now used
through Dogen. Each sub-system takes responsibility of its own keys -
it defines them and validates to ensure the values for a key are
valid. In the remainder of this manual you will find sections with a
name similar to this one, where we will define the tags available for
that component and their semantics.

These keys within Dogen are known as /tags/ and they are part of the
meta-data processing sub-system.

Dia defines the following tags:

- =dia.comment=: Comment provided by user when dia does not allow for
  it.
- =dia.identity_attribute=: Attribute that provides this entity its
  identity.
- =is_final=: If true, the type cannot be inherited from.

*** The middle-end

We store the domain model internally as SML - a /meta-model/ largely
based on Domain Driven Design. A meta-model is simply a model whose
sole purpose is to describe other models. SML is designed to capture
all the details of the domain model that are required for code
generation. SML is not designed for anything else, so it is very terse
and not a particularly obvious model.

*** The back-ends

We need to express the meta-model as code. That is, we need to
generate a /representation/ of the different "parts" the domain type,
according to the rules of some well-known /grammar/: that is, it must
obey to a set of rules defined somewhere. Typical grammars are
programming languages such as C++ or SQL, but they can also be more
esoteric such as a Dia diagram; it uses the Dia XML grammar.

A /representation/ in this context is understood to be a physical
expression of a domain type - e.g. as zeroes and ones stored in a file
somewhere.

Representations have two related concepts: facets and aspects. These
are best explained by way of an example. The most fundamental facet is
the @e types facet. This is the class definition itself. A facet is
made up of @e aspects - for example in C++ there is a header file and
an implementation file. However, an aspect need not map directly to a
file - its perfectly possible to have more than one aspect in a file.

* PART II - PRACTICE

In this part we describe how to use Dogen, from very simple use cases
building up to more complex ones. We also explain how Dogen can be
integrated with a build system and other practical aspects of its
usage.

** Hello World Example

* PART III - SPECIFICATIONS

This part is made up of a set of specifications of different aspects
of dogen, such as project structure, coding standards, and so on. The
objective is to create norms as we go along so that new developers
understand the reasons behind historical decisions.

In general, we follow the [[http://www.boost.org/development/requirements.html][Boost Library Requirements and Guidelines]]
document.

** RFC 2119

The definitions in RFC 2119 apply to the following sub-sections:

#+begin_quote
The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
"SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
document are to be interpreted as described in RFC 2119.
#+end_quote

** Project structure

*** File and directory names

1. All files and folders in must have lower-case names. /Rationale/:
   avoid as much as possible case-sensitive issues in platforms where
   casing is not that well designed, such as Windows. /Exceptions:/
   Files or folders that follow well-known naming conventions take
   precedence, such as the GNU files (e.g. =README=, =INSTALL=, etc.)
   and cmake files (=CMakeLists.txt=).

2. The dot character =.= shall only be used to separate the file name
   from its extension (e.g. =file.txt=). /Rationale/: For some reason,
   dots in the middle of the name seems to confuse cmake and force
   rebuilding targets that are up-to-date.

3. Multi-word file and directory names must make use of the underscore
   character =_= to separate words (e.g.: =folder_name=).

4. In cases where there is a need to distinguish between several
   multi-word components in a file name, the dash character =-= must
   be used (e.g. =some_word-some_other_word.extension=).

5. Names must contain only lowercase ASCII letters (=a-z=), numbers
   (=0-9=), underscores (=_=), hyphens (=-=), and periods
   (=.=). Spaces must not be used.

6. The first and last character of a file name must not be a period
   (=.=). /Rationale/: POSIX has special rules for names beginning
   with a period. Windows prohibits names ending in a period (source:
   boost). /Exceptions:/ Control files such as =.gitignore=.

7. The first character of names must not be a hyphen =-=. /Rationale/:
   It would be too confusing or ambiguous in certain contexts (source:
   [[http://www.boost.org/development/requirements.html][boost]]).

8. The maximum length of directory and file names is 31
   characters. /Rationale/: Long file names cause obscure issues.

9. The total path length must not exceed 207 characters. /Rationale/:
   Dictated by ISO 9660:1999 (source: boost). In addition, many
   file systems don't really like paths longer than 255 characters, so
   its best to keep well below this limit.

10. Files have additional naming conventions that are dependent on the
    file type. See language specific policies for details.

*** Root folder

1. The top-level folder is known as the /root folder/; this is the
   directory you create when you clone the project from git - i.e. it
   contains the =.git= folder. The folder should be named dogen.

2. The =build= folder contains extensions to cmake (both third party
   and our own), code templates and configuration file templates
   (logging, etc.). /Rationale/: In general, =build= should contain
   either files used directly by the makefiles or files that get
   copied over to the build's output directory for further processing.

3. The =projects= folder contains core source code for the project, in
   any of the supported programming languages. The code directory is
   named after each individual "project". /Rationale/: This directory
   could have been called =src= for source code, but we reserved this
   for the directory of the implementation files in C++.

4. The =doc= folder contains manually crafted documentation such as
   manuals, illustrative diagrams (not for code generation purposes),
   project plans, screen-shots, etc. All documents should be saved as
   plain text.

5. The =tools= folder houses scripts and other utilities.

*** Output folder

1. The folder under which the build is performed is called the
   /output/ folder. This is the location where all artefacts generated
   by the build are placed, such as binaries, shared objects,
   automatically generated documentation, instantiated templates,
   etc.

2. We have a policy of not permitting in-source builds, so the output
   folder must not be the same folder as the root folder. This is
   enforced by the CMake files.

3. The output folder should be located at the same level as the root
   folder. This is a convention; the directory can be located anywhere
   else outside the root folder. The CMake files must still work in
   this setup.

4. The /output/ folder should be name =output=. /Rationale/: it can be
   named anything you like and there must not be any dependencies on
   the name of the output folder. However, to increase
   interoperability with other developers its preferable to give it a
   standard name.

5. Users can partition the output folder, creating directories for
   each supported platform, compiler, debug and release, etc. This is
   known as a /multi-target/ setup. /Rationale/ Keeping all the
   targets within a single folder means its easy to start from scratch
   by deleting the top-level folder.

6. When using multi-target, the output folder's sub-folders should be
   named after the [[http://wiki.debian.org/Multiarch/Tuples][GNU triplet]] - including the compiler
   version. /Rationale/ These directories are setup manually because
   target folders contain a full-blown cmake environment, independent
   from the others. Its not possible to generate this multi-targeted
   setup directly from the makefiles. Example multi-target folder
   structure:

#+begin_example
output/linux-amd64-gcc-4_6_1-debug
output/win32-x86-clang-3_0-release
#+end_example

7. Within a target folder - whether on single or multi-target setups -
   the key folder is =stage=; it contains all the binaries and
   documentation ready for packaging.

*** Third party content

1. The project's git repository is expected to only contain code owned
   by dogen; all the external dependencies must be installed by the
   user as a build prerequisite (see doc/BUILD for details).

2. In exceptional cases where the third party dependency is both small
   and not readily available in packaged form, it is acceptable to add
   it to the repository. This is the case with CMake extensions and
   with the boost portable serialisation library. Once these projects
   are packaged they shall be removed from the repository.

* APPENDIX

** Appendix A - Related Work

This section is a bit of a general research bucket. It contains a set
of links to the C++ code generators we have found on our wanderings on
the internet, as well as other interesting projects in this space -
including those in other programming languages. It also contains books
and papers on the subject we have read, or intend to read.

- [[http://www.amazon.co.uk/Domain-Driven-Design-Tackling-Complexity-ebook/dp/B00794TAUG/ref%3Dsr_1_2?ie%3DUTF8&qid%3D1368380797&sr%3D8-2&keywords%3Dmodel%2Bdriven%2Bdesign][Domain-Driven Design: Tackling Complexity in the Heart of Software]]:
  The Eric Evans book from which we tried to steal most concepts in
  Dogen. A must read for any developer.
- [[http://www.amazon.co.uk/EMF-Eclipse-Modeling-Framework-ebook/dp/B0013TPYVW/ref%3Dsr_1_2?s%3Dbooks&ie%3DUTF8&qid%3D1368380262&sr%3D1-2&keywords%3DEclipse%2BModeling%2BFramework%2B%255BPaperback%255D][EMF: Eclipse Modeling Framework]]: The original EMF book. Useful read
  for anyone interested in code generation.
- [[http://www.scribd.com/doc/78264699/Model-Driven-Architecture-for-Reverse-Engineering-Technologies-Strategic-Directions-and-System-Evolution-Premier-Reference-Source][Model Driven Architecture for Reverse Engineering Technologies]]:
  Preview of a potentially interesting MDA book.
- [[http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf][A Common Metamodel for Code Generation]]: This paper will be of
  interest if we decide to support multiple languages.
- [[http://www.vollmann.com/pubs/meta/meta/meta.html][Metaclasses and Reflection in C++]]: Some (early) ideas on
  implementing a MOP (Meta Object Protocol) in C++.
- [[https://code.google.com/a/eclipselabs.org/p/cppgenmodel/][cppgenmodel - A model driven C++ code generator]]: This seems more
  like a run time / reflection based generator.
- [[https://code.google.com/p/emf4cpp/][EMF4CPP - Eclipse Modeling Framework]]: C++ port of the EMF/eCore
  eclipse framework. As with Java it includes run time support. There
  is also [[http://apps.nabbel.es/dsdm2010/download_files/dsdm2010_senac.pdf][a paper]] on it.
- [[http://www2.informatik.hu-berlin.de/~piefel/Documents/06CITSA-CMMCG.pdf][A Common Metamodel for Code Generation]]: Describes a meta-model
  designed to model Java and C++.
- [[http://marofra.com/oldhomepage/MetaCPlusPlusDoc/metacplusplus-1.html][The Meta-C++ User Manual]]: Another early C++ meta-modeling
  tool. Contains interesting ideas around C++ meta-models.
- The Columbus C++ Schema: Useful tool for re-engineering large C++
  code bases. Contains a meta-model for C++. A number of papers have
  been written about it:
  - [[http://www.inf.u-szeged.hu/~beszedes/research/tech27_ferenc_r.pdf][Columbus â€“ Reverse Engineering Tool and Schema for C++]]
  - [[http://journal.ub.tu-berlin.de/eceasst/article/download/10/19][Third Workshop on Software Evolution through Transformations]]:
    Embracing the Change
  - [[http://www.inf.u-szeged.hu/~ferenc/research/ferencr_schema.ppt.pdf][Towards a Standard Schema for C/C++]]
  - [[http://www.inf.u-szeged.hu/~ferenc/research/ferencr_columbus_schema_cpp.pdf][Data Exchange with the Columbus Schema for C++]]
- [[http://www.cpgf.org/][CPGF]]: An open source C++ library for reflection, script binding,
  serialisation and callbacks.
- [[http://www.artima.com/articles/dci_vision.html][DCI]]: The DCI Architecture: A New Vision of Object-Oriented
  Programming. Some fundamental insights on the nature of OO.
- [[http://www.ischo.com/xrtti/index.html][xrtti]]: Extending C++ with a richer reflection.
- [[http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3883.html][Code checkers and generators]]: adding AngularJS-like capabilities to
  C++.
- [[http://stackoverflow.com/questions/355650/c-html-template-framework-templatizing-library-html-generator-library][Text Template libraries for C++]]: T4 like implementations for C++.
